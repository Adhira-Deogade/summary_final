{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import struct\n",
    "import csv\n",
    "from collections import namedtuple\n",
    "import json, re, shutil, sys\n",
    "import collections, itertools\n",
    "import numpy as np\n",
    "\n",
    "import data\n",
    "from data import Vocab\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_TOKEN = '[UNK]'\n",
    "\n",
    "def get_file_names(path,root):\n",
    "    name = os.path.split(path)[1]\n",
    "    example_id = name.split('_')[0]\n",
    "    article_file = os.path.join(root, f\"reference/{example_id}_article.txt\")\n",
    "    ref_file = os.path.join(root, f\"reference/{example_id}_reference.txt\")\n",
    "    return example_id,article_file,ref_file\n",
    "    \n",
    "    \n",
    "def read_contents(file):\n",
    "    with open(file, 'r') as f:\n",
    "        summary = f.read()\n",
    "    return summary\n",
    "\n",
    "def get_article_oov(article, vocab):\n",
    "    oov_list = []\n",
    "    unk_token = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    words = article.split(' ')\n",
    "    for w in words:\n",
    "        if vocab.word2id(w)==unk_token:\n",
    "            oov_list.append(w)\n",
    "    return oov_list\n",
    "\n",
    "def get_novel_words(abstract, vocab, article_oovs):\n",
    "    unk_token = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    words = abstract.split(' ')\n",
    "    new_words = []\n",
    "    copy_words = []\n",
    "    all_words = []\n",
    "    for w in words:\n",
    "        if vocab.word2id(w) == unk_token: \n",
    "            if w in article_oovs:\n",
    "                copy_words.append(w)\n",
    "            else:\n",
    "                new_words.append(w)\n",
    "    return new_words,copy_words\n",
    "\n",
    "\n",
    "def summary_repetition(abstract,count=1):\n",
    "    words = abstract.split(' ')\n",
    "    bi_grams = list(nltk.ngrams(words, 2))\n",
    "    bi_grams_freq = nltk.FreqDist(bi_grams)     \n",
    "    tri_grams = list(nltk.ngrams(words, 3))\n",
    "    tri_grams_freq = nltk.FreqDist(tri_grams)     \n",
    "    four_grams = list(nltk.ngrams(words, 4))\n",
    "    four_grams_freq = nltk.FreqDist(four_grams)   \n",
    "    return list(filter(lambda x: x[1]>=count,bi_grams_freq.items())),list(filter(lambda x: x[1]>=count,tri_grams_freq.items())),list(filter(lambda x: x[1]>=count,four_grams_freq.items()))\n",
    "\n",
    "\n",
    "def get_freq_dist(text,n):\n",
    "    words = text.split(' ')\n",
    "    n_grams = list(nltk.ngrams(words, n))\n",
    "    n_grams_freq = nltk.FreqDist(n_grams)\n",
    "    return n_grams_freq\n",
    "    \n",
    "\n",
    "def log_details(message,log):\n",
    "    if log:\n",
    "        print(message)\n",
    "\n",
    "def get_analysis_data(input_df,model,vocab,log=False):\n",
    "    \n",
    "    data = []\n",
    "    path = f'/home/ubuntu/W266/final_0/W266_Final/{model}/saved/decode'\n",
    "    summary_dir = os.path.join(path, \"generated/*\")\n",
    "    filelist = glob.glob(summary_dir) \n",
    "    column_list = ['model','Article OOV','Generated','Copied','2-Gram(repeat)','3-Gram(repeat)','4-Gram(repeat)']\n",
    "\n",
    "    \n",
    "    for f in filelist:\n",
    "        log_details('=============================================================',log)\n",
    "        example_id,article_f,reference_f = get_file_names(f,path)\n",
    "        log_details(f'EXAMPLE: {example_id}',log)\n",
    "        summary = read_contents(f)\n",
    "        article = read_contents(article_f)\n",
    "        reference = read_contents(reference_f)\n",
    "\n",
    "        article_oovs = get_article_oov(article,vocab)\n",
    "        generated_words,copied_words = get_novel_words(summary, vocab, article_oovs)\n",
    "        bi,tri,four = summary_repetition(summary,2)\n",
    "\n",
    "        d = [model,\n",
    "               len(article_oovs),\n",
    "               len(generated_words),\n",
    "               len(copied_words),\n",
    "               len(bi),\n",
    "               len(tri),\n",
    "               len(four)]\n",
    "\n",
    "        data.append(d)\n",
    "\n",
    "        log_details(f'[OOV       : {len(article_oovs)}] : {article_oovs}',log)\n",
    "        log_details(f'[GENERATED : {len(generated_words)}] : {generated_words}',log)\n",
    "        log_details(f'[COPIED    : {len(copied_words)}] : {copied_words}',log)\n",
    "        log_details(f'[REPEAT]    :  Bi - {len(bi)} ;; Tri - {len(tri)} ;; Four - {len(four)}',log)\n",
    "        log_details('=============================================================',log)\n",
    "    \n",
    "    df = pd.DataFrame(data,columns=column_list)  \n",
    "    \n",
    "    if input_df is not None:\n",
    "        input_df = input_df.append(df,ignore_index=True)\n",
    "    else:\n",
    "        input_df = df\n",
    "        \n",
    "    return input_df        \n",
    "\n",
    "\n",
    "def get_top_repetitions(model):\n",
    "    path = f'/home/ubuntu/W266/final_0/W266_Final/{model}/saved/decode'\n",
    "    summary_dir = os.path.join(path, \"generated/*\")\n",
    "    filelist = glob.glob(summary_dir) \n",
    "    summary_all = ' '    \n",
    "    reference_all = ' '\n",
    "    for f in filelist:\n",
    "        example_id,article_f,reference_f = get_file_names(f,path)\n",
    "        summary = read_contents(f)\n",
    "        summary_all = summary_all + summary\n",
    "        \n",
    "        reference = read_contents(reference_f)\n",
    "        reference_all = reference_all + reference        \n",
    "    \n",
    "    summary_bi = get_freq_dist(summary_all,2)\n",
    "    summary_tri = get_freq_dist(summary_all,3)\n",
    "    summary_four = get_freq_dist(summary_all,4)\n",
    "\n",
    "    ref_bi = get_freq_dist(reference_all,2)\n",
    "    ref_tri = get_freq_dist(reference_all,3)\n",
    "    ref_four = get_freq_dist(reference_all,4)\n",
    "        \n",
    "    return summary_bi,summary_tri,summary_four,ref_bi,ref_tri,ref_four        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-51bd8584d850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/ubuntu/W266/final_0/W266_Final/data/final_processed/vocab'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Vocab' is not defined"
     ]
    }
   ],
   "source": [
    "vocab = Vocab('/home/ubuntu/W266/final_0/W266_Final/data/final_processed/vocab',50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'model_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = get_analysis_data(None,model,vocab,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline.mean().plot(kind='bar')#discuss-1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfd_2,sfd_3,sfd_4,rfd_2,rfd_3,rfd_4 = get_top_repetitions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointer Generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'model_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pointgen = get_analysis_data(None,'model_4',vocab,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pointgen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pointgen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pointgen.mean().plot(kind='bar') #discuss-1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfd_2,sfd_3,sfd_4,rfd_2,rfd_3,rfd_4 = get_top_repetitions(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfd_2.plot(5,cumulative=False) #discuss-2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfd_2.plot(5,cumulative=False)#discuss-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfd_3.plot(5,cumulative=False) #discuss-3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfd_3.plot(5,cumulative=False)#discuss-3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path, random\n",
    "import subprocess\n",
    "import hashlib\n",
    "import struct\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "\n",
    "nlp = StanfordCoreNLP(r'/home/ubuntu/stanford-corenlp-full-2018-02-27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(stories_dir,train=0.5,test=0.3):\n",
    "    val = 1-(train+test)\n",
    "    \n",
    "    all_files = os.listdir(stories_dir)\n",
    "    total = len(all_files)\n",
    "    shuffled = all_files[:]\n",
    "    random.shuffle(shuffled)\n",
    "    \n",
    "    train_stories = int(round(train*total))\n",
    "    test_stories = int(round(test*total))\n",
    "    val_stories = int(round(val*total))\n",
    "\n",
    "    train_data = shuffled[:train_stories]\n",
    "    test_data = shuffled[train_stories:train_stories+test_stories]\n",
    "    val_data = shuffled[train_stories+test_stories:train_stories+test_stories+val_stories]\n",
    "    return train_data,test_data,val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_stories(stories_dir, tokenized_stories_dir):\n",
    "    \"\"\"Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer\"\"\"\n",
    "    print(f\"Preparing to tokenize {stories_dir} to {tokenized_stories_dir}...\") \n",
    "    stories = os.listdir(stories_dir)\n",
    "    # make IO list file\n",
    "    print(\"Making list of files to tokenize...\")\n",
    "    with open(\"mapping.txt\", \"w\") as f:\n",
    "        for s in stories:\n",
    "            f.write(\"%s \\t %s\\n\" % (os.path.join(stories_dir, s), os.path.join(tokenized_stories_dir, s)))\n",
    "    \n",
    "    command = ['java', 'edu.stanford.nlp.process.PTBTokenizer', '-ioFileList', '-preserveLines', 'mapping.txt']\n",
    "    print(f\"Tokenizing {len(stories)} files in {stories_dir} and saving in {tokenized_stories_dir}...\")\n",
    "    subprocess.call(command)\n",
    "    \n",
    "    print(\"Stanford CoreNLP Tokenizer has finished.\")\n",
    "    \n",
    "    os.remove(\"mapping.txt\")\n",
    "\n",
    "    # Check that the tokenized stories directory contains the same number of files as the original directory\n",
    "    num_orig = len(os.listdir(stories_dir))\n",
    "    num_tokenized = len(os.listdir(tokenized_stories_dir))\n",
    "    print (f\"Successfully finished tokenizing {stories_dir} to {tokenized_stories_dir}.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_single_close_quote = u'\\u2019' # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "VOCAB_SIZE = 200000\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "\n",
    "def read_text_file(text_file):\n",
    "    lines = []\n",
    "    with open(text_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip())\n",
    "    return lines\n",
    "\n",
    "def fix_missing_period(line):\n",
    "    \"\"\"Adds a period to a line that is missing a period\"\"\"\n",
    "    if \"@highlight\" in line: return line\n",
    "    if line==\"\": return line\n",
    "    if line[-1] in END_TOKENS: return line\n",
    "    return line + \" .\"\n",
    "\n",
    "\n",
    "def get_art_abs(story_file):\n",
    "    lines = read_text_file(story_file)\n",
    "\n",
    "    # Lowercase everything\n",
    "    lines = [line.lower() for line in lines]\n",
    "\n",
    "    # Put periods on the ends of lines that are missing them (this is a problem in the dataset because many image captions don't end in periods; consequently they end up in the body of the article as run-on sentences)\n",
    "    lines = [fix_missing_period(line) for line in lines]\n",
    "\n",
    "    # Separate out article and abstract sentences\n",
    "    article_lines = []\n",
    "    highlights = []\n",
    "    next_is_highlight = False\n",
    "   \n",
    "    for idx,line in enumerate(lines):\n",
    "        if line == \"\":\n",
    "            continue # empty line\n",
    "        elif line.startswith(\"@highlight\"):\n",
    "            next_is_highlight = True\n",
    "        elif next_is_highlight:\n",
    "            highlights.append(line)\n",
    "        else:\n",
    "            article_lines.append(line)\n",
    "\n",
    "        # Make article into a single string\n",
    "        article = ' '.join(article_lines)\n",
    "\n",
    "        # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n",
    "        abstract = ' '.join([\"%s %s %s\" % (SENTENCE_START, sent, SENTENCE_END) for sent in highlights])\n",
    "\n",
    "    return article, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_bin(file_names,story_dir,out_file,finished_files_dir, makevocab=False):\n",
    "    \"\"\"Reads the tokenized .story files corresponding to the names listed in the url_file and writes them to an out_file.\"\"\"\n",
    "    \n",
    "    story_fnames = file_names\n",
    "    num_stories = len(story_fnames)\n",
    "\n",
    "    if makevocab:\n",
    "        vocab_counter = collections.Counter()\n",
    "\n",
    "    with open(out_file, 'wb') as writer:\n",
    "        for idx,s in enumerate(story_fnames):\n",
    "            if idx % 1000 == 0:\n",
    "                print (f\"Writing story {idx} of {num_stories}; {float(idx)*100.0/float(num_stories)} percent done\")\n",
    "\n",
    "            story_file = os.path.join(story_dir, s)\n",
    "            \n",
    "            # Get the strings to write to .bin file\n",
    "            article, abstract = get_art_abs(story_file)\n",
    "\n",
    "            # Write to tf.Example\n",
    "            if bytes(article, 'utf-8') == 0:\n",
    "                print('error!')\n",
    "                \n",
    "            tf_example = example_pb2.Example()\n",
    "            tf_example.features.feature['article'].bytes_list.value.extend([bytes(article, 'utf-8')])\n",
    "            tf_example.features.feature['abstract'].bytes_list.value.extend([bytes(abstract, 'utf-8')])\n",
    "            tf_example_str = tf_example.SerializeToString()\n",
    "            str_len = len(tf_example_str)\n",
    "            writer.write(struct.pack('q', str_len))\n",
    "            writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "\n",
    "            # Write the vocab to file, if applicable\n",
    "            if makevocab:\n",
    "                art_tokens = article.split(' ')\n",
    "                abs_tokens = abstract.split(' ')\n",
    "                abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]] # remove these tags from vocab\n",
    "                tokens = art_tokens + abs_tokens\n",
    "                tokens = [t.strip() for t in tokens] # strip\n",
    "                tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "                vocab_counter.update(tokens)\n",
    "\n",
    "    print(f\"Finished writing file {out_file}\\n\") \n",
    "    # write vocab to file\n",
    "    if makevocab:\n",
    "        print (\"Writing vocab file...\")\n",
    "        with open(os.path.join(finished_files_dir, \"vocab\"), 'w') as writer:\n",
    "            for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "                writer.write(word + ' ' + str(count) + '\\n')\n",
    "        print (\"Finished writing vocab file\" )   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_dir = \"/home/ubuntu/W266/final_0/W266_Final/data/test\"\n",
    "tokenized_stories_dir = \"/home/ubuntu/W266/final_0/W266_Final/data/test_tokenized\"\n",
    "processed_dir = \"/home/ubuntu/W266/final_0/W266_Final/data/test_processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test,val = split_data(stories_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7f3317b1e717c0efb427552c486ea0dad471cc32.story',\n",
       " 'ffff2dc1cc4888253a4733f808959f0b4eab26a6.story',\n",
       " 'fffd170a9d15b1f9751e969e6f5b0ce5b9f7d027.story']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7f33bc95458ae729aab0eed1c0b6a18223176a83.story',\n",
       " '7f340bb7216273f9b59d26516e3460a94c2c8125.story',\n",
       " 'ffff522cebe5ad9dcfb6dfc476b8f423f3f8dd34.story',\n",
       " 'ffff11a2f44d731cd80c86819a89b7e227581415.story',\n",
       " '7f35b0a3a1a60e35e560b0d9ae17c84d848d1637.story']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7f35cb654a1627dfa9b3458a5e0356256e694941.story',\n",
       " 'fffe0c4eb70bde9733b858adfd5b4eeeae631f28.story']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to tokenize /home/ubuntu/W266/final_0/W266_Final/data/test to /home/ubuntu/W266/final_0/W266_Final/data/test_tokenized...\n",
      "Making list of files to tokenize...\n",
      "Tokenizing 10 files in /home/ubuntu/W266/final_0/W266_Final/data/test and saving in /home/ubuntu/W266/final_0/W266_Final/data/test_tokenized...\n",
      "Stanford CoreNLP Tokenizer has finished.\n",
      "Successfully finished tokenizing /home/ubuntu/W266/final_0/W266_Final/data/test to /home/ubuntu/W266/final_0/W266_Final/data/test_tokenized.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenize_stories(stories_dir, tokenized_stories_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_out_file = os.path.join(processed_dir, \"train.bin\")\n",
    "test_out_file = os.path.join(processed_dir, \"test.bin\")\n",
    "validation_out_file = os.path.join(processed_dir, \"validation.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making bin file for stories listed in ['7f33bc95458ae729aab0eed1c0b6a18223176a83.story', '7f340bb7216273f9b59d26516e3460a94c2c8125.story', 'ffff522cebe5ad9dcfb6dfc476b8f423f3f8dd34.story', 'ffff11a2f44d731cd80c86819a89b7e227581415.story', '7f35b0a3a1a60e35e560b0d9ae17c84d848d1637.story']...\n",
      "Writing story 0 of 5; 0.0 percent done\n",
      "Finished writing file /home/ubuntu/W266/final_0/W266_Final/data/test_processed/train.bin\n",
      "\n",
      "Writing vocab file...\n",
      "Finished writing vocab file\n"
     ]
    }
   ],
   "source": [
    "write_to_bin(train,tokenized_stories_dir,train_out_file,processed_dir, makevocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making bin file for stories listed in ['7f3317b1e717c0efb427552c486ea0dad471cc32.story', 'ffff2dc1cc4888253a4733f808959f0b4eab26a6.story', 'fffd170a9d15b1f9751e969e6f5b0ce5b9f7d027.story']...\n",
      "Writing story 0 of 3; 0.0 percent done\n",
      "Finished writing file /home/ubuntu/W266/final_0/W266_Final/data/test_processed/test.bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "write_to_bin(test,tokenized_stories_dir,test_out_file,processed_dir, makevocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making bin file for stories listed in ['7f35cb654a1627dfa9b3458a5e0356256e694941.story', 'fffe0c4eb70bde9733b858adfd5b4eeeae631f28.story']...\n",
      "Writing story 0 of 2; 0.0 percent done\n",
      "Finished writing file /home/ubuntu/W266/final_0/W266_Final/data/test_processed/validation.bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "write_to_bin(val,tokenized_stories_dir,validation_out_file,processed_dir, makevocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_count(DIR):\n",
    "    return len(os.listdir(DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_stories_dir = \"/home/ubuntu/W266/final_0/W266_Final/data/cnn/stories\"\n",
    "dm_stories_dir = \"/home/ubuntu/W266/final_0/W266_Final/data/dailymail/stories\"\n",
    "\n",
    "tokenized_stories_dir = \"/home/ubuntu/W266/final_0/W266_Final/data/final_tokenized\"\n",
    "processed_dir = \"/home/ubuntu/W266/final_0/W266_Final/data/final_processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92579"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_count(cnn_stories_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219506"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_count(dm_stories_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to tokenize /home/ubuntu/W266/final_0/W266_Final/data/cnn/stories to /home/ubuntu/W266/final_0/W266_Final/data/final_tokenized...\n",
      "Making list of files to tokenize...\n",
      "Tokenizing 92579 files in /home/ubuntu/W266/final_0/W266_Final/data/cnn/stories and saving in /home/ubuntu/W266/final_0/W266_Final/data/final_tokenized...\n",
      "Stanford CoreNLP Tokenizer has finished.\n",
      "Successfully finished tokenizing /home/ubuntu/W266/final_0/W266_Final/data/cnn/stories to /home/ubuntu/W266/final_0/W266_Final/data/final_tokenized.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenize_stories(cnn_stories_dir, tokenized_stories_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92579"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_count(tokenized_stories_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to tokenize /home/ubuntu/W266/final_0/W266_Final/data/dailymail/stories to /home/ubuntu/W266/final_0/W266_Final/data/final_tokenized...\n",
      "Making list of files to tokenize...\n",
      "Tokenizing 219506 files in /home/ubuntu/W266/final_0/W266_Final/data/dailymail/stories and saving in /home/ubuntu/W266/final_0/W266_Final/data/final_tokenized...\n",
      "Stanford CoreNLP Tokenizer has finished.\n",
      "Successfully finished tokenizing /home/ubuntu/W266/final_0/W266_Final/data/dailymail/stories to /home/ubuntu/W266/final_0/W266_Final/data/final_tokenized.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenize_stories(dm_stories_dir, tokenized_stories_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312085"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_count(tokenized_stories_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test,val = split_data(tokenized_stories_dir,train=0.6,test=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187251 93626 31208\n"
     ]
    }
   ],
   "source": [
    "print(len(train),len(test),len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing story 0 of 187251; 0.0 percent done\n",
      "Writing story 1000 of 187251; 0.5340425418288821 percent done\n",
      "Writing story 2000 of 187251; 1.0680850836577642 percent done\n",
      "Writing story 3000 of 187251; 1.6021276254866463 percent done\n",
      "Writing story 4000 of 187251; 2.1361701673155284 percent done\n",
      "Writing story 5000 of 187251; 2.6702127091444106 percent done\n",
      "Writing story 6000 of 187251; 3.2042552509732927 percent done\n",
      "Writing story 7000 of 187251; 3.738297792802175 percent done\n",
      "Writing story 8000 of 187251; 4.272340334631057 percent done\n",
      "Writing story 9000 of 187251; 4.806382876459939 percent done\n",
      "Writing story 10000 of 187251; 5.340425418288821 percent done\n",
      "Writing story 11000 of 187251; 5.874467960117703 percent done\n",
      "Writing story 12000 of 187251; 6.408510501946585 percent done\n",
      "Writing story 13000 of 187251; 6.942553043775467 percent done\n",
      "Writing story 14000 of 187251; 7.47659558560435 percent done\n",
      "Writing story 15000 of 187251; 8.010638127433232 percent done\n",
      "Writing story 16000 of 187251; 8.544680669262114 percent done\n",
      "Writing story 17000 of 187251; 9.078723211090995 percent done\n",
      "Writing story 18000 of 187251; 9.612765752919877 percent done\n",
      "Writing story 19000 of 187251; 10.14680829474876 percent done\n",
      "Writing story 20000 of 187251; 10.680850836577642 percent done\n",
      "Writing story 21000 of 187251; 11.214893378406524 percent done\n",
      "Writing story 22000 of 187251; 11.748935920235406 percent done\n",
      "Writing story 23000 of 187251; 12.282978462064287 percent done\n",
      "Writing story 24000 of 187251; 12.81702100389317 percent done\n",
      "Writing story 25000 of 187251; 13.351063545722052 percent done\n",
      "Writing story 26000 of 187251; 13.885106087550934 percent done\n",
      "Writing story 27000 of 187251; 14.419148629379816 percent done\n",
      "Writing story 28000 of 187251; 14.9531911712087 percent done\n",
      "Writing story 29000 of 187251; 15.48723371303758 percent done\n",
      "Writing story 30000 of 187251; 16.021276254866464 percent done\n",
      "Writing story 31000 of 187251; 16.555318796695346 percent done\n",
      "Writing story 32000 of 187251; 17.089361338524228 percent done\n",
      "Writing story 33000 of 187251; 17.62340388035311 percent done\n",
      "Writing story 34000 of 187251; 18.15744642218199 percent done\n",
      "Writing story 35000 of 187251; 18.691488964010873 percent done\n",
      "Writing story 36000 of 187251; 19.225531505839754 percent done\n",
      "Writing story 37000 of 187251; 19.759574047668636 percent done\n",
      "Writing story 38000 of 187251; 20.29361658949752 percent done\n",
      "Writing story 39000 of 187251; 20.827659131326403 percent done\n",
      "Writing story 40000 of 187251; 21.361701673155284 percent done\n",
      "Writing story 41000 of 187251; 21.895744214984166 percent done\n",
      "Writing story 42000 of 187251; 22.429786756813048 percent done\n",
      "Writing story 43000 of 187251; 22.96382929864193 percent done\n",
      "Writing story 44000 of 187251; 23.49787184047081 percent done\n",
      "Writing story 45000 of 187251; 24.031914382299693 percent done\n",
      "Writing story 46000 of 187251; 24.565956924128574 percent done\n",
      "Writing story 47000 of 187251; 25.09999946595746 percent done\n",
      "Writing story 48000 of 187251; 25.63404200778634 percent done\n",
      "Writing story 49000 of 187251; 26.168084549615223 percent done\n",
      "Writing story 50000 of 187251; 26.702127091444105 percent done\n",
      "Writing story 51000 of 187251; 27.236169633272986 percent done\n",
      "Writing story 52000 of 187251; 27.770212175101868 percent done\n",
      "Writing story 53000 of 187251; 28.30425471693075 percent done\n",
      "Writing story 54000 of 187251; 28.83829725875963 percent done\n",
      "Writing story 55000 of 187251; 29.372339800588517 percent done\n",
      "Writing story 56000 of 187251; 29.9063823424174 percent done\n",
      "Writing story 57000 of 187251; 30.44042488424628 percent done\n",
      "Writing story 58000 of 187251; 30.97446742607516 percent done\n",
      "Writing story 59000 of 187251; 31.508509967904043 percent done\n",
      "Writing story 60000 of 187251; 32.04255250973293 percent done\n",
      "Writing story 61000 of 187251; 32.57659505156181 percent done\n",
      "Writing story 62000 of 187251; 33.11063759339069 percent done\n",
      "Writing story 63000 of 187251; 33.64468013521957 percent done\n",
      "Writing story 64000 of 187251; 34.178722677048455 percent done\n",
      "Writing story 65000 of 187251; 34.71276521887734 percent done\n",
      "Writing story 66000 of 187251; 35.24680776070622 percent done\n",
      "Writing story 67000 of 187251; 35.7808503025351 percent done\n",
      "Writing story 68000 of 187251; 36.31489284436398 percent done\n",
      "Writing story 69000 of 187251; 36.84893538619286 percent done\n",
      "Writing story 70000 of 187251; 37.382977928021745 percent done\n",
      "Writing story 71000 of 187251; 37.91702046985063 percent done\n",
      "Writing story 72000 of 187251; 38.45106301167951 percent done\n",
      "Writing story 73000 of 187251; 38.98510555350839 percent done\n",
      "Writing story 74000 of 187251; 39.51914809533727 percent done\n",
      "Writing story 75000 of 187251; 40.05319063716615 percent done\n",
      "Writing story 76000 of 187251; 40.58723317899504 percent done\n",
      "Writing story 77000 of 187251; 41.121275720823924 percent done\n",
      "Writing story 78000 of 187251; 41.655318262652806 percent done\n",
      "Writing story 79000 of 187251; 42.18936080448169 percent done\n",
      "Writing story 80000 of 187251; 42.72340334631057 percent done\n",
      "Writing story 81000 of 187251; 43.25744588813945 percent done\n",
      "Writing story 82000 of 187251; 43.79148842996833 percent done\n",
      "Writing story 83000 of 187251; 44.325530971797214 percent done\n",
      "Writing story 84000 of 187251; 44.859573513626096 percent done\n",
      "Writing story 85000 of 187251; 45.39361605545498 percent done\n",
      "Writing story 86000 of 187251; 45.92765859728386 percent done\n",
      "Writing story 87000 of 187251; 46.46170113911274 percent done\n",
      "Writing story 88000 of 187251; 46.99574368094162 percent done\n",
      "Writing story 89000 of 187251; 47.529786222770504 percent done\n",
      "Writing story 90000 of 187251; 48.063828764599386 percent done\n",
      "Writing story 91000 of 187251; 48.59787130642827 percent done\n",
      "Writing story 92000 of 187251; 49.13191384825715 percent done\n",
      "Writing story 93000 of 187251; 49.66595639008604 percent done\n",
      "Writing story 94000 of 187251; 50.19999893191492 percent done\n",
      "Writing story 95000 of 187251; 50.7340414737438 percent done\n",
      "Writing story 96000 of 187251; 51.26808401557268 percent done\n",
      "Writing story 97000 of 187251; 51.802126557401564 percent done\n",
      "Writing story 98000 of 187251; 52.336169099230446 percent done\n",
      "Writing story 99000 of 187251; 52.87021164105933 percent done\n",
      "Writing story 100000 of 187251; 53.40425418288821 percent done\n",
      "Writing story 101000 of 187251; 53.93829672471709 percent done\n",
      "Writing story 102000 of 187251; 54.47233926654597 percent done\n",
      "Writing story 103000 of 187251; 55.006381808374854 percent done\n",
      "Writing story 104000 of 187251; 55.540424350203736 percent done\n",
      "Writing story 105000 of 187251; 56.07446689203262 percent done\n",
      "Writing story 106000 of 187251; 56.6085094338615 percent done\n",
      "Writing story 107000 of 187251; 57.14255197569038 percent done\n",
      "Writing story 108000 of 187251; 57.67659451751926 percent done\n",
      "Writing story 109000 of 187251; 58.210637059348144 percent done\n",
      "Writing story 110000 of 187251; 58.74467960117703 percent done\n",
      "Writing story 111000 of 187251; 59.278722143005915 percent done\n",
      "Writing story 112000 of 187251; 59.8127646848348 percent done\n",
      "Writing story 113000 of 187251; 60.34680722666368 percent done\n",
      "Writing story 114000 of 187251; 60.88084976849256 percent done\n",
      "Writing story 115000 of 187251; 61.41489231032144 percent done\n",
      "Writing story 116000 of 187251; 61.94893485215032 percent done\n",
      "Writing story 117000 of 187251; 62.482977393979205 percent done\n",
      "Writing story 118000 of 187251; 63.01701993580809 percent done\n",
      "Writing story 119000 of 187251; 63.55106247763697 percent done\n",
      "Writing story 120000 of 187251; 64.08510501946586 percent done\n",
      "Writing story 121000 of 187251; 64.61914756129474 percent done\n",
      "Writing story 122000 of 187251; 65.15319010312362 percent done\n",
      "Writing story 123000 of 187251; 65.6872326449525 percent done\n",
      "Writing story 124000 of 187251; 66.22127518678138 percent done\n",
      "Writing story 125000 of 187251; 66.75531772861027 percent done\n",
      "Writing story 126000 of 187251; 67.28936027043915 percent done\n",
      "Writing story 127000 of 187251; 67.82340281226803 percent done\n",
      "Writing story 128000 of 187251; 68.35744535409691 percent done\n",
      "Writing story 129000 of 187251; 68.89148789592579 percent done\n",
      "Writing story 130000 of 187251; 69.42553043775467 percent done\n",
      "Writing story 131000 of 187251; 69.95957297958356 percent done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing story 132000 of 187251; 70.49361552141244 percent done\n",
      "Writing story 133000 of 187251; 71.02765806324132 percent done\n",
      "Writing story 134000 of 187251; 71.5617006050702 percent done\n",
      "Writing story 135000 of 187251; 72.09574314689908 percent done\n",
      "Writing story 136000 of 187251; 72.62978568872796 percent done\n",
      "Writing story 137000 of 187251; 73.16382823055685 percent done\n",
      "Writing story 138000 of 187251; 73.69787077238573 percent done\n",
      "Writing story 139000 of 187251; 74.23191331421461 percent done\n",
      "Writing story 140000 of 187251; 74.76595585604349 percent done\n",
      "Writing story 141000 of 187251; 75.29999839787237 percent done\n",
      "Writing story 142000 of 187251; 75.83404093970125 percent done\n",
      "Writing story 143000 of 187251; 76.36808348153014 percent done\n",
      "Writing story 144000 of 187251; 76.90212602335902 percent done\n",
      "Writing story 145000 of 187251; 77.4361685651879 percent done\n",
      "Writing story 146000 of 187251; 77.97021110701678 percent done\n",
      "Writing story 147000 of 187251; 78.50425364884566 percent done\n",
      "Writing story 148000 of 187251; 79.03829619067454 percent done\n",
      "Writing story 149000 of 187251; 79.57233873250343 percent done\n",
      "Writing story 150000 of 187251; 80.1063812743323 percent done\n",
      "Writing story 151000 of 187251; 80.64042381616119 percent done\n",
      "Writing story 152000 of 187251; 81.17446635799008 percent done\n",
      "Writing story 153000 of 187251; 81.70850889981897 percent done\n",
      "Writing story 154000 of 187251; 82.24255144164785 percent done\n",
      "Writing story 155000 of 187251; 82.77659398347673 percent done\n",
      "Writing story 156000 of 187251; 83.31063652530561 percent done\n",
      "Writing story 157000 of 187251; 83.84467906713449 percent done\n",
      "Writing story 158000 of 187251; 84.37872160896337 percent done\n",
      "Writing story 159000 of 187251; 84.91276415079226 percent done\n",
      "Writing story 160000 of 187251; 85.44680669262114 percent done\n",
      "Writing story 161000 of 187251; 85.98084923445002 percent done\n",
      "Writing story 162000 of 187251; 86.5148917762789 percent done\n",
      "Writing story 163000 of 187251; 87.04893431810778 percent done\n",
      "Writing story 164000 of 187251; 87.58297685993666 percent done\n",
      "Writing story 165000 of 187251; 88.11701940176555 percent done\n",
      "Writing story 166000 of 187251; 88.65106194359443 percent done\n",
      "Writing story 167000 of 187251; 89.18510448542331 percent done\n",
      "Writing story 168000 of 187251; 89.71914702725219 percent done\n",
      "Writing story 169000 of 187251; 90.25318956908107 percent done\n",
      "Writing story 170000 of 187251; 90.78723211090995 percent done\n",
      "Writing story 171000 of 187251; 91.32127465273884 percent done\n",
      "Writing story 172000 of 187251; 91.85531719456772 percent done\n",
      "Writing story 173000 of 187251; 92.3893597363966 percent done\n",
      "Writing story 174000 of 187251; 92.92340227822548 percent done\n",
      "Writing story 175000 of 187251; 93.45744482005436 percent done\n",
      "Writing story 176000 of 187251; 93.99148736188324 percent done\n",
      "Writing story 177000 of 187251; 94.52552990371213 percent done\n",
      "Writing story 178000 of 187251; 95.05957244554101 percent done\n",
      "Writing story 179000 of 187251; 95.59361498736989 percent done\n",
      "Writing story 180000 of 187251; 96.12765752919877 percent done\n",
      "Writing story 181000 of 187251; 96.66170007102765 percent done\n",
      "Writing story 182000 of 187251; 97.19574261285653 percent done\n",
      "Writing story 183000 of 187251; 97.72978515468542 percent done\n",
      "Writing story 184000 of 187251; 98.2638276965143 percent done\n",
      "Writing story 185000 of 187251; 98.79787023834318 percent done\n",
      "Writing story 186000 of 187251; 99.33191278017208 percent done\n",
      "Writing story 187000 of 187251; 99.86595532200096 percent done\n",
      "Finished writing file /home/ubuntu/W266/final_0/W266_Final/data/final_processed/train.bin\n",
      "\n",
      "Writing vocab file...\n",
      "Finished writing vocab file\n"
     ]
    }
   ],
   "source": [
    "train_out_file = os.path.join(processed_dir, \"train.bin\")\n",
    "write_to_bin(train,tokenized_stories_dir,train_out_file,processed_dir, makevocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing story 0 of 93626; 0.0 percent done\n",
      "Writing story 1000 of 93626; 1.0680793796594963 percent done\n",
      "Writing story 2000 of 93626; 2.1361587593189926 percent done\n",
      "Writing story 3000 of 93626; 3.204238138978489 percent done\n",
      "Writing story 4000 of 93626; 4.272317518637985 percent done\n",
      "Writing story 5000 of 93626; 5.340396898297482 percent done\n",
      "Writing story 6000 of 93626; 6.408476277956978 percent done\n",
      "Writing story 7000 of 93626; 7.4765556576164744 percent done\n",
      "Writing story 8000 of 93626; 8.54463503727597 percent done\n",
      "Writing story 9000 of 93626; 9.612714416935466 percent done\n",
      "Writing story 10000 of 93626; 10.680793796594964 percent done\n",
      "Writing story 11000 of 93626; 11.74887317625446 percent done\n",
      "Writing story 12000 of 93626; 12.816952555913955 percent done\n",
      "Writing story 13000 of 93626; 13.885031935573451 percent done\n",
      "Writing story 14000 of 93626; 14.953111315232949 percent done\n",
      "Writing story 15000 of 93626; 16.021190694892443 percent done\n",
      "Writing story 16000 of 93626; 17.08927007455194 percent done\n",
      "Writing story 17000 of 93626; 18.157349454211438 percent done\n",
      "Writing story 18000 of 93626; 19.225428833870932 percent done\n",
      "Writing story 19000 of 93626; 20.29350821353043 percent done\n",
      "Writing story 20000 of 93626; 21.361587593189928 percent done\n",
      "Writing story 21000 of 93626; 22.42966697284942 percent done\n",
      "Writing story 22000 of 93626; 23.49774635250892 percent done\n",
      "Writing story 23000 of 93626; 24.565825732168413 percent done\n",
      "Writing story 24000 of 93626; 25.63390511182791 percent done\n",
      "Writing story 25000 of 93626; 26.70198449148741 percent done\n",
      "Writing story 26000 of 93626; 27.770063871146903 percent done\n",
      "Writing story 27000 of 93626; 28.8381432508064 percent done\n",
      "Writing story 28000 of 93626; 29.906222630465898 percent done\n",
      "Writing story 29000 of 93626; 30.974302010125392 percent done\n",
      "Writing story 30000 of 93626; 32.042381389784886 percent done\n",
      "Writing story 31000 of 93626; 33.11046076944439 percent done\n",
      "Writing story 32000 of 93626; 34.17854014910388 percent done\n",
      "Writing story 33000 of 93626; 35.246619528763375 percent done\n",
      "Writing story 34000 of 93626; 36.314698908422876 percent done\n",
      "Writing story 35000 of 93626; 37.38277828808237 percent done\n",
      "Writing story 36000 of 93626; 38.450857667741865 percent done\n",
      "Writing story 37000 of 93626; 39.518937047401366 percent done\n",
      "Writing story 38000 of 93626; 40.58701642706086 percent done\n",
      "Writing story 39000 of 93626; 41.655095806720354 percent done\n",
      "Writing story 40000 of 93626; 42.723175186379855 percent done\n",
      "Writing story 41000 of 93626; 43.79125456603935 percent done\n",
      "Writing story 42000 of 93626; 44.85933394569884 percent done\n",
      "Writing story 43000 of 93626; 45.92741332535834 percent done\n",
      "Writing story 44000 of 93626; 46.99549270501784 percent done\n",
      "Writing story 45000 of 93626; 48.06357208467733 percent done\n",
      "Writing story 46000 of 93626; 49.13165146433683 percent done\n",
      "Writing story 47000 of 93626; 50.19973084399633 percent done\n",
      "Writing story 48000 of 93626; 51.26781022365582 percent done\n",
      "Writing story 49000 of 93626; 52.335889603315316 percent done\n",
      "Writing story 50000 of 93626; 53.40396898297482 percent done\n",
      "Writing story 51000 of 93626; 54.47204836263431 percent done\n",
      "Writing story 52000 of 93626; 55.540127742293805 percent done\n",
      "Writing story 53000 of 93626; 56.608207121953306 percent done\n",
      "Writing story 54000 of 93626; 57.6762865016128 percent done\n",
      "Writing story 55000 of 93626; 58.744365881272294 percent done\n",
      "Writing story 56000 of 93626; 59.812445260931796 percent done\n",
      "Writing story 57000 of 93626; 60.88052464059129 percent done\n",
      "Writing story 58000 of 93626; 61.948604020250784 percent done\n",
      "Writing story 59000 of 93626; 63.01668339991028 percent done\n",
      "Writing story 60000 of 93626; 64.08476277956977 percent done\n",
      "Writing story 61000 of 93626; 65.15284215922928 percent done\n",
      "Writing story 62000 of 93626; 66.22092153888877 percent done\n",
      "Writing story 63000 of 93626; 67.28900091854827 percent done\n",
      "Writing story 64000 of 93626; 68.35708029820776 percent done\n",
      "Writing story 65000 of 93626; 69.42515967786726 percent done\n",
      "Writing story 66000 of 93626; 70.49323905752675 percent done\n",
      "Writing story 67000 of 93626; 71.56131843718626 percent done\n",
      "Writing story 68000 of 93626; 72.62939781684575 percent done\n",
      "Writing story 69000 of 93626; 73.69747719650525 percent done\n",
      "Writing story 70000 of 93626; 74.76555657616474 percent done\n",
      "Writing story 71000 of 93626; 75.83363595582423 percent done\n",
      "Writing story 72000 of 93626; 76.90171533548373 percent done\n",
      "Writing story 73000 of 93626; 77.96979471514322 percent done\n",
      "Writing story 74000 of 93626; 79.03787409480273 percent done\n",
      "Writing story 75000 of 93626; 80.10595347446223 percent done\n",
      "Writing story 76000 of 93626; 81.17403285412172 percent done\n",
      "Writing story 77000 of 93626; 82.24211223378121 percent done\n",
      "Writing story 78000 of 93626; 83.31019161344071 percent done\n",
      "Writing story 79000 of 93626; 84.3782709931002 percent done\n",
      "Writing story 80000 of 93626; 85.44635037275971 percent done\n",
      "Writing story 81000 of 93626; 86.5144297524192 percent done\n",
      "Writing story 82000 of 93626; 87.5825091320787 percent done\n",
      "Writing story 83000 of 93626; 88.65058851173819 percent done\n",
      "Writing story 84000 of 93626; 89.71866789139769 percent done\n",
      "Writing story 85000 of 93626; 90.78674727105718 percent done\n",
      "Writing story 86000 of 93626; 91.85482665071667 percent done\n",
      "Writing story 87000 of 93626; 92.92290603037618 percent done\n",
      "Writing story 88000 of 93626; 93.99098541003568 percent done\n",
      "Writing story 89000 of 93626; 95.05906478969517 percent done\n",
      "Writing story 90000 of 93626; 96.12714416935466 percent done\n",
      "Writing story 91000 of 93626; 97.19522354901416 percent done\n",
      "Writing story 92000 of 93626; 98.26330292867365 percent done\n",
      "Writing story 93000 of 93626; 99.33138230833316 percent done\n",
      "Finished writing file /home/ubuntu/W266/final_0/W266_Final/data/final_processed/test.bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_out_file = os.path.join(processed_dir, \"test.bin\")\n",
    "write_to_bin(test,tokenized_stories_dir,test_out_file,processed_dir, makevocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing story 0 of 31208; 0.0 percent done\n",
      "Writing story 1000 of 31208; 3.204306588054345 percent done\n",
      "Writing story 2000 of 31208; 6.40861317610869 percent done\n",
      "Writing story 3000 of 31208; 9.612919764163035 percent done\n",
      "Writing story 4000 of 31208; 12.81722635221738 percent done\n",
      "Writing story 5000 of 31208; 16.021532940271726 percent done\n",
      "Writing story 6000 of 31208; 19.22583952832607 percent done\n",
      "Writing story 7000 of 31208; 22.430146116380417 percent done\n",
      "Writing story 8000 of 31208; 25.63445270443476 percent done\n",
      "Writing story 9000 of 31208; 28.838759292489105 percent done\n",
      "Writing story 10000 of 31208; 32.04306588054345 percent done\n",
      "Writing story 11000 of 31208; 35.247372468597796 percent done\n",
      "Writing story 12000 of 31208; 38.45167905665214 percent done\n",
      "Writing story 13000 of 31208; 41.65598564470648 percent done\n",
      "Writing story 14000 of 31208; 44.860292232760834 percent done\n",
      "Writing story 15000 of 31208; 48.06459882081518 percent done\n",
      "Writing story 16000 of 31208; 51.26890540886952 percent done\n",
      "Writing story 17000 of 31208; 54.473211996923865 percent done\n",
      "Writing story 18000 of 31208; 57.67751858497821 percent done\n",
      "Writing story 19000 of 31208; 60.88182517303255 percent done\n",
      "Writing story 20000 of 31208; 64.0861317610869 percent done\n",
      "Writing story 21000 of 31208; 67.29043834914124 percent done\n",
      "Writing story 22000 of 31208; 70.49474493719559 percent done\n",
      "Writing story 23000 of 31208; 73.69905152524994 percent done\n",
      "Writing story 24000 of 31208; 76.90335811330428 percent done\n",
      "Writing story 25000 of 31208; 80.10766470135863 percent done\n",
      "Writing story 26000 of 31208; 83.31197128941297 percent done\n",
      "Writing story 27000 of 31208; 86.51627787746732 percent done\n",
      "Writing story 28000 of 31208; 89.72058446552167 percent done\n",
      "Writing story 29000 of 31208; 92.924891053576 percent done\n",
      "Writing story 30000 of 31208; 96.12919764163036 percent done\n",
      "Writing story 31000 of 31208; 99.33350422968469 percent done\n",
      "Finished writing file /home/ubuntu/W266/final_0/W266_Final/data/final_processed/validation.bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_out_file = os.path.join(processed_dir, \"validation.bin\")\n",
    "write_to_bin(val,tokenized_stories_dir,validation_out_file,processed_dir, makevocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_file(set_name):\n",
    "    in_file = f\"{processed_dir}/{set_name}.bin\"\n",
    "    print(in_file)\n",
    "    reader = open(in_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk)) # new chunk\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(CHUNK_SIZE):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "        chunk += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_all():\n",
    "    # Make a dir to hold the chunks\n",
    "    if not os.path.isdir(chunks_dir):\n",
    "        os.mkdir(chunks_dir)\n",
    "    # Chunk the data\n",
    "    for set_name in ['train', 'validation', 'test']:\n",
    "        print(f\"Splitting {set_name} data into chunks...\")\n",
    "        chunk_file(set_name)\n",
    "    print(f\"Saved chunked data in {chunks_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_dir = \"/home/ubuntu/W266/final_0/W266_Final/data/final_chunked\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train data into chunks...\n",
      "/home/ubuntu/W266/final_0/W266_Final/data/final_processed/train.bin\n",
      "Splitting validation data into chunks...\n",
      "/home/ubuntu/W266/final_0/W266_Final/data/final_processed/validation.bin\n",
      "Splitting test data into chunks...\n",
      "/home/ubuntu/W266/final_0/W266_Final/data/final_processed/test.bin\n",
      "Saved chunked data in /home/ubuntu/W266/final_0/W266_Final/data/final_chunked\n"
     ]
    }
   ],
   "source": [
    "chunk_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

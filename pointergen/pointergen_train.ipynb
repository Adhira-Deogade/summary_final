{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "from tensorflow.python import debug as tf_debug\n",
    "from tensorflow.core.example import example_pb2\n",
    "from importlib import reload\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import struct\n",
    "import csv\n",
    "import batch; reload(batch)\n",
    "import data; reload(data)\n",
    "import model; reload(model)\n",
    "\n",
    "from batch import Example,Batch\n",
    "from data import Vocab\n",
    "from model import SummarizationModel\n",
    "from decode import BeamSearchDecoder\n",
    "from batcher import Batcher\n",
    "import util\n",
    "import training_util as tutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_params():\n",
    "    hps_dict = {\n",
    "        'mode' : 'train',\n",
    "        'single_pass' : False,\n",
    "        'log_root' : '/home/ubuntu/W266/final_0/W266_Final/model_4/saved/train',\n",
    "        'exp_name' : '',\n",
    "        'hidden_dim' : 256,\n",
    "        'emb_dim' : 128,\n",
    "        'batch_size' : 100,\n",
    "        'max_enc_steps' : 400,\n",
    "        'max_dec_steps' : 100,\n",
    "        'beam_size' : 4,\n",
    "        'min_dec_steps' : 35,\n",
    "        'vocab_size' : 50000,\n",
    "        'lr' : 0.15,\n",
    "        'adagrad_init_acc' : 0.1,\n",
    "        'rand_unif_init_mag' : 0.02,\n",
    "        'trunc_norm_init_std' : 1e-4,\n",
    "        'max_grad_norm' : 2.0,\n",
    "        'pointer_gen' : True,\n",
    "        'coverage' : True,\n",
    "        'cov_loss_wt' : 1.0,\n",
    "        'vocab_path' : '/home/ubuntu/W266/final_0/W266_Final/data/final_processed/vocab',\n",
    "        'data_path' : '/home/ubuntu/W266/final_0/W266_Final/data/final_chunked/train_*'\n",
    "    }\n",
    "    \n",
    "    hps = namedtuple(\"HParams\", hps_dict.keys())(**hps_dict)\n",
    "    return hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = setup_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(hps,epochs,restore=False):\n",
    "    \n",
    "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        \n",
    "        session.run(initializer)\n",
    "        saver = tf.train.Saver(max_to_keep=3) # keep 3 checkpoints at a time\n",
    "        sv = tf.train.Supervisor(logdir=train_dir,\n",
    "                                 is_chief=True,\n",
    "                                 saver=saver,\n",
    "                                 summary_op=None,\n",
    "                                 save_summaries_secs=30, \n",
    "                                 save_model_secs=30, \n",
    "                                 global_step=lm.global_step)    \n",
    "\n",
    "        summary_writer = sv.summary_writer\n",
    "        \n",
    "        if restore:\n",
    "            ckpt_path = util.load_ckpt(saver, session,hps,hps.log_root)\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_step = None\n",
    "        curr_best = None\n",
    "        best_loss = None\n",
    "        avg_loss = 0\n",
    "        for i in range(epochs):\n",
    "            print(f\"[EPOCH {i+1}] Starting training..\")\n",
    "            random.shuffle(batches)\n",
    "            avg_loss,curr_best,train_step = tutil.run_epoch(lm,\n",
    "                                                            session,\n",
    "                                                            batches,\n",
    "                                                            summary_writer,\n",
    "                                                            train_dir,\n",
    "                                                            train_step,\n",
    "                                                            saver,\n",
    "                                                            hps,\n",
    "                                                            best_loss,\n",
    "                                                            avg_loss)\n",
    "            \n",
    "            if(best_loss is None or curr_best < best_loss):\n",
    "                best_loss = curr_best\n",
    "            print(f\"[EPOCH {i+1}] Complete. Avg Loss: {avg_loss}; Best Loss: {best_loss}\")\n",
    "        sv.stop()\n",
    "        time_total = tutil.pretty_timedelta(since=start_time)\n",
    "        print(f\"[END] Training complete: Best Loss: {best_loss}; Total time: {time_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 252\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 110\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 2 1/2 76\n",
      "\n",
      "\n",
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: 16:03\n",
      "INFO:tensorflow:Fetching data..\n",
      "INFO:tensorflow:Creating batches..\n",
      "INFO:tensorflow:[TOTAL Batches]  : 1872\n",
      "INFO:tensorflow:[TOTAL Examples] : 187193\n",
      "INFO:tensorflow:Creating batches..COMPLETE\n",
      "INFO:tensorflow:Building core graph...\n",
      "INFO:tensorflow:Adding attention_decoder timestep 0 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 1 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 2 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 3 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 4 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 5 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 6 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 7 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 8 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 9 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 10 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 11 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 12 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 13 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 14 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 15 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 16 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 17 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 18 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 19 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 20 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 21 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 22 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 23 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 24 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 25 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 26 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 27 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 28 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 29 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 30 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 31 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 32 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 33 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 34 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 35 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 36 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 37 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 38 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 39 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 40 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 41 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 42 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 43 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 44 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 45 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 46 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 47 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 48 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 49 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 50 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 51 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 52 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 53 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 54 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 55 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 56 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 57 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 58 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 59 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 60 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 61 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 62 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 63 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 64 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 65 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 66 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 67 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 68 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 69 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 70 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 71 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 72 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 73 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 74 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 75 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 76 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 77 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 78 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 79 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 80 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 81 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 82 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 83 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 84 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 85 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 86 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 87 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 88 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 89 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 90 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 91 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 92 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 93 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 94 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 95 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 96 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 97 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 98 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 99 of 100\n",
      "INFO:tensorflow:Building projection graph...\n",
      "INFO:tensorflow:Building projection graph...COMPLETE\n",
      "INFO:tensorflow:Building Loss graph...\n",
      "INFO:tensorflow:Building Loss graph...COMPLETE\n",
      "INFO:tensorflow:Building core graph...COMPLETE\n",
      "INFO:tensorflow:Building train graph...\n",
      "INFO:tensorflow:Building train graph...COMPLETE\n",
      "INFO:tensorflow:Building summary graph...\n",
      "INFO:tensorflow:Building summary graph...COMPLETE\n",
      "WARNING:tensorflow:From <ipython-input-5-a4c1f7b896ea>:18: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "[EPOCH 1] Starting training..\n",
      "    [batch 1]: seen 100 examples at 3.1 eps, loss = 7.475\n",
      "    [batch 4]: seen 400 examples at 8.5 eps, loss = 7.472\n",
      "    [batch 7]: seen 700 examples at 11.3 eps, loss = 7.469\n",
      "    [batch 10]: seen 1000 examples at 13.1 eps, loss = 7.464\n",
      "    [batch 13]: seen 1300 examples at 14.2 eps, loss = 7.459\n",
      "    [batch 15]: seen 1500 examples at 14.8 eps, loss = 7.455\n",
      "    [batch 18]: seen 1800 examples at 15.9 eps, loss = 7.451\n",
      "    [batch 21]: seen 2100 examples at 16.5 eps, loss = 7.447\n",
      "    [batch 24]: seen 2400 examples at 16.9 eps, loss = 7.443\n",
      "    [batch 27]: seen 2700 examples at 17.2 eps, loss = 7.440\n",
      "    [batch 30]: seen 3000 examples at 17.5 eps, loss = 7.436\n",
      "    [batch 33]: seen 3300 examples at 17.6 eps, loss = 7.432\n",
      "    [batch 36]: seen 3600 examples at 17.8 eps, loss = 7.428\n",
      "    [batch 39]: seen 3900 examples at 18.0 eps, loss = 7.424\n",
      "    [batch 42]: seen 4200 examples at 18.2 eps, loss = 7.422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 45]: seen 4500 examples at 18.6 eps, loss = 7.420\n",
      "    [batch 48]: seen 4800 examples at 18.9 eps, loss = 7.417\n",
      "    [batch 51]: seen 5100 examples at 19.0 eps, loss = 7.413\n",
      "    [batch 53]: seen 5300 examples at 19.0 eps, loss = 7.411\n",
      "    [batch 56]: seen 5600 examples at 19.3 eps, loss = 7.410\n",
      "    [batch 60]: seen 6000 examples at 19.8 eps, loss = 7.409\n",
      "    [batch 63]: seen 6300 examples at 19.9 eps, loss = 7.407\n",
      "    [batch 66]: seen 6600 examples at 19.9 eps, loss = 7.404\n",
      "    [batch 69]: seen 6900 examples at 20.1 eps, loss = 7.404\n",
      "    [batch 74]: seen 7400 examples at 20.7 eps, loss = 7.401\n",
      "    [batch 77]: seen 7700 examples at 20.7 eps, loss = 7.398\n",
      "    [batch 80]: seen 8000 examples at 20.7 eps, loss = 7.394\n",
      "    [batch 83]: seen 8300 examples at 20.8 eps, loss = 7.392\n",
      "    [batch 86]: seen 8600 examples at 21.0 eps, loss = 7.391\n",
      "    [batch 89]: seen 8900 examples at 21.0 eps, loss = 7.389\n",
      "    [batch 92]: seen 9200 examples at 21.1 eps, loss = 7.387\n",
      "    [batch 95]: seen 9500 examples at 21.1 eps, loss = 7.383\n",
      "    [batch 97]: seen 9700 examples at 21.1 eps, loss = 7.381\n",
      "    [batch 100]: seen 10000 examples at 21.0 eps, loss = 7.380\n",
      "    [batch 104]: seen 10400 examples at 21.3 eps, loss = 7.379\n",
      "    [batch 107]: seen 10700 examples at 21.3 eps, loss = 7.376\n",
      "    [batch 110]: seen 11000 examples at 21.4 eps, loss = 7.373\n",
      "    [batch 114]: seen 11400 examples at 21.7 eps, loss = 7.372\n",
      "    [batch 117]: seen 11700 examples at 21.6 eps, loss = 7.370\n",
      "    [batch 119]: seen 11900 examples at 21.6 eps, loss = 7.368\n",
      "    [batch 122]: seen 12200 examples at 21.6 eps, loss = 7.365\n",
      "    [batch 125]: seen 12500 examples at 21.5 eps, loss = 7.363\n",
      "    [batch 128]: seen 12800 examples at 21.5 eps, loss = 7.360\n",
      "    [batch 131]: seen 13100 examples at 21.5 eps, loss = 7.354\n",
      "    [batch 134]: seen 13400 examples at 21.5 eps, loss = 7.350\n",
      "    [batch 136]: seen 13600 examples at 21.4 eps, loss = 7.348\n",
      "    [batch 139]: seen 13900 examples at 21.5 eps, loss = 7.345\n",
      "    [batch 142]: seen 14200 examples at 21.5 eps, loss = 7.340\n",
      "    [batch 145]: seen 14500 examples at 21.5 eps, loss = 7.335\n",
      "    [batch 148]: seen 14800 examples at 21.5 eps, loss = 7.328\n",
      "    [batch 151]: seen 15100 examples at 21.5 eps, loss = 7.323\n",
      "    [batch 154]: seen 15400 examples at 21.4 eps, loss = 7.316\n",
      "    [batch 156]: seen 15600 examples at 21.4 eps, loss = 7.312\n",
      "    [batch 159]: seen 15900 examples at 21.4 eps, loss = 7.307\n",
      "    [batch 162]: seen 16200 examples at 21.4 eps, loss = 7.302\n",
      "    [batch 165]: seen 16500 examples at 21.4 eps, loss = 7.301\n",
      "    [batch 168]: seen 16800 examples at 21.4 eps, loss = 7.297\n",
      "    [batch 171]: seen 17100 examples at 21.3 eps, loss = 7.292\n",
      "    [batch 173]: seen 17300 examples at 21.3 eps, loss = 7.291\n",
      "    [batch 176]: seen 17600 examples at 21.3 eps, loss = 7.287\n",
      "    [batch 179]: seen 17900 examples at 21.3 eps, loss = 7.280\n",
      "    [batch 182]: seen 18200 examples at 21.3 eps, loss = 7.276\n",
      "    [batch 185]: seen 18500 examples at 21.3 eps, loss = 7.271\n",
      "    [batch 188]: seen 18800 examples at 21.3 eps, loss = 7.267\n",
      "    [batch 190]: seen 19000 examples at 21.2 eps, loss = 7.265\n",
      "    [batch 193]: seen 19300 examples at 21.2 eps, loss = 7.262\n",
      "    [batch 196]: seen 19600 examples at 21.2 eps, loss = 7.257\n",
      "    [batch 199]: seen 19900 examples at 21.2 eps, loss = 7.251\n",
      "    [batch 202]: seen 20200 examples at 21.2 eps, loss = 7.247\n",
      "    [batch 205]: seen 20500 examples at 21.2 eps, loss = 7.243\n",
      "    [batch 207]: seen 20700 examples at 21.2 eps, loss = 7.241\n",
      "    [batch 210]: seen 21000 examples at 21.2 eps, loss = 7.236\n",
      "    [batch 213]: seen 21300 examples at 21.2 eps, loss = 7.233\n",
      "    [batch 216]: seen 21600 examples at 21.2 eps, loss = 7.229\n",
      "    [batch 219]: seen 21900 examples at 21.2 eps, loss = 7.225\n",
      "    [batch 222]: seen 22200 examples at 21.2 eps, loss = 7.221\n",
      "    [batch 224]: seen 22400 examples at 21.2 eps, loss = 7.218\n",
      "    [batch 227]: seen 22700 examples at 21.2 eps, loss = 7.214\n",
      "    [batch 230]: seen 23000 examples at 21.2 eps, loss = 7.212\n",
      "    [batch 233]: seen 23300 examples at 21.3 eps, loss = 7.210\n",
      "    [batch 236]: seen 23600 examples at 21.3 eps, loss = 7.207\n",
      "    [batch 240]: seen 24000 examples at 21.4 eps, loss = 7.205\n",
      "    [batch 243]: seen 24300 examples at 21.4 eps, loss = 7.200\n",
      "    [batch 245]: seen 24500 examples at 21.4 eps, loss = 7.199\n",
      "    [batch 248]: seen 24800 examples at 21.3 eps, loss = 7.194\n",
      "    [batch 251]: seen 25100 examples at 21.3 eps, loss = 7.189\n",
      "    [batch 254]: seen 25400 examples at 21.3 eps, loss = 7.186\n",
      "    [batch 257]: seen 25700 examples at 21.4 eps, loss = 7.186\n",
      "    [batch 260]: seen 26000 examples at 21.4 eps, loss = 7.180\n",
      "    [batch 263]: seen 26300 examples at 21.4 eps, loss = 7.179\n",
      "    [batch 265]: seen 26500 examples at 21.4 eps, loss = 7.176\n",
      "    [batch 268]: seen 26800 examples at 21.4 eps, loss = 7.172\n",
      "    [batch 271]: seen 27100 examples at 21.4 eps, loss = 7.168\n",
      "    [batch 274]: seen 27400 examples at 21.4 eps, loss = 7.164\n",
      "    [batch 278]: seen 27800 examples at 21.5 eps, loss = 7.163\n",
      "    [batch 281]: seen 28100 examples at 21.5 eps, loss = 7.159\n",
      "    [batch 283]: seen 28300 examples at 21.5 eps, loss = 7.156\n",
      "    [batch 286]: seen 28600 examples at 21.5 eps, loss = 7.153\n",
      "    [batch 289]: seen 28900 examples at 21.5 eps, loss = 7.146\n",
      "    [batch 292]: seen 29200 examples at 21.5 eps, loss = 7.139\n",
      "    [batch 295]: seen 29500 examples at 21.5 eps, loss = 7.138\n",
      "    [batch 298]: seen 29800 examples at 21.5 eps, loss = 7.133\n",
      "    [batch 301]: seen 30100 examples at 21.5 eps, loss = 7.130\n",
      "    [batch 304]: seen 30400 examples at 21.5 eps, loss = 7.127\n",
      "    [batch 307]: seen 30700 examples at 21.5 eps, loss = 7.122\n",
      "    [batch 310]: seen 31000 examples at 21.5 eps, loss = 7.118\n",
      "    [batch 313]: seen 31300 examples at 21.5 eps, loss = 7.113\n",
      "    [batch 316]: seen 31600 examples at 21.5 eps, loss = 7.107\n",
      "    [batch 319]: seen 31900 examples at 21.5 eps, loss = 7.105\n",
      "    [batch 321]: seen 32100 examples at 21.5 eps, loss = 7.102\n",
      "    [batch 324]: seen 32400 examples at 21.5 eps, loss = 7.098\n",
      "    [batch 327]: seen 32700 examples at 21.5 eps, loss = 7.094\n",
      "    [batch 331]: seen 33100 examples at 21.6 eps, loss = 7.093\n",
      "    [batch 334]: seen 33400 examples at 21.6 eps, loss = 7.088\n",
      "    [batch 337]: seen 33700 examples at 21.6 eps, loss = 7.085\n",
      "    [batch 340]: seen 34000 examples at 21.6 eps, loss = 7.081\n",
      "    [batch 343]: seen 34300 examples at 21.7 eps, loss = 7.076\n",
      "    [batch 346]: seen 34600 examples at 21.7 eps, loss = 7.072\n",
      "    [batch 349]: seen 34900 examples at 21.6 eps, loss = 7.068\n",
      "    [batch 352]: seen 35200 examples at 21.6 eps, loss = 7.065\n",
      "    [batch 356]: seen 35600 examples at 21.7 eps, loss = 7.063\n",
      "    [batch 359]: seen 35900 examples at 21.7 eps, loss = 7.059\n",
      "    [batch 361]: seen 36100 examples at 21.7 eps, loss = 7.057\n",
      "    [batch 364]: seen 36400 examples at 21.7 eps, loss = 7.053\n",
      "    [batch 367]: seen 36700 examples at 21.7 eps, loss = 7.049\n",
      "    [batch 370]: seen 37000 examples at 21.7 eps, loss = 7.046\n",
      "    [batch 373]: seen 37300 examples at 21.7 eps, loss = 7.045\n",
      "    [batch 376]: seen 37600 examples at 21.7 eps, loss = 7.041\n",
      "    [batch 379]: seen 37900 examples at 21.7 eps, loss = 7.037\n",
      "    [batch 381]: seen 38100 examples at 21.7 eps, loss = 7.034\n",
      "    [batch 384]: seen 38400 examples at 21.7 eps, loss = 7.029\n",
      "    [batch 387]: seen 38700 examples at 21.7 eps, loss = 7.024\n",
      "    [batch 392]: seen 39200 examples at 21.9 eps, loss = 7.023\n",
      "    [batch 395]: seen 39500 examples at 21.9 eps, loss = 7.019\n",
      "    [batch 398]: seen 39800 examples at 21.8 eps, loss = 7.014\n",
      "    [batch 401]: seen 40100 examples at 21.8 eps, loss = 7.008\n",
      "    [batch 403]: seen 40300 examples at 21.8 eps, loss = 7.006\n",
      "    [batch 406]: seen 40600 examples at 21.8 eps, loss = 7.001\n",
      "    [batch 409]: seen 40900 examples at 21.8 eps, loss = 6.999\n",
      "    [batch 412]: seen 41200 examples at 21.8 eps, loss = 6.994\n",
      "    [batch 415]: seen 41500 examples at 21.8 eps, loss = 6.991\n",
      "    [batch 418]: seen 41800 examples at 21.8 eps, loss = 6.987\n",
      "    [batch 420]: seen 42000 examples at 21.8 eps, loss = 6.984\n",
      "    [batch 423]: seen 42300 examples at 21.8 eps, loss = 6.981\n",
      "    [batch 426]: seen 42600 examples at 21.8 eps, loss = 6.978\n",
      "    [batch 429]: seen 42900 examples at 21.8 eps, loss = 6.975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 432]: seen 43200 examples at 21.8 eps, loss = 6.969\n",
      "    [batch 435]: seen 43500 examples at 21.8 eps, loss = 6.966\n",
      "    [batch 437]: seen 43700 examples at 21.8 eps, loss = 6.962\n",
      "    [batch 440]: seen 44000 examples at 21.8 eps, loss = 6.959\n",
      "    [batch 443]: seen 44300 examples at 21.8 eps, loss = 6.957\n",
      "    [batch 446]: seen 44600 examples at 21.7 eps, loss = 6.953\n",
      "    [batch 449]: seen 44900 examples at 21.7 eps, loss = 6.949\n",
      "    [batch 452]: seen 45200 examples at 21.7 eps, loss = 6.946\n",
      "    [batch 454]: seen 45400 examples at 21.7 eps, loss = 6.944\n",
      "    [batch 457]: seen 45700 examples at 21.7 eps, loss = 6.942\n",
      "    [batch 460]: seen 46000 examples at 21.7 eps, loss = 6.939\n",
      "    [batch 463]: seen 46300 examples at 21.7 eps, loss = 6.933\n",
      "    [batch 466]: seen 46600 examples at 21.7 eps, loss = 6.929\n",
      "    [batch 469]: seen 46900 examples at 21.7 eps, loss = 6.927\n",
      "    [batch 471]: seen 47100 examples at 21.7 eps, loss = 6.924\n",
      "    [batch 474]: seen 47400 examples at 21.7 eps, loss = 6.922\n",
      "    [batch 477]: seen 47700 examples at 21.7 eps, loss = 6.921\n",
      "    [batch 480]: seen 48000 examples at 21.7 eps, loss = 6.920\n",
      "    [batch 483]: seen 48300 examples at 21.7 eps, loss = 6.916\n",
      "    [batch 486]: seen 48600 examples at 21.7 eps, loss = 6.912\n",
      "    [batch 488]: seen 48800 examples at 21.7 eps, loss = 6.910\n",
      "    [batch 491]: seen 49100 examples at 21.6 eps, loss = 6.907\n",
      "    [batch 494]: seen 49400 examples at 21.6 eps, loss = 6.903\n",
      "    [batch 497]: seen 49700 examples at 21.6 eps, loss = 6.899\n",
      "    [batch 500]: seen 50000 examples at 21.6 eps, loss = 6.897\n",
      "    [batch 503]: seen 50300 examples at 21.6 eps, loss = 6.894\n",
      "    [batch 508]: seen 50800 examples at 21.7 eps, loss = 6.893\n",
      "    [batch 510]: seen 51000 examples at 21.7 eps, loss = 6.890\n",
      "    [batch 513]: seen 51300 examples at 21.7 eps, loss = 6.887\n",
      "    [batch 516]: seen 51600 examples at 21.7 eps, loss = 6.883\n",
      "    [batch 519]: seen 51900 examples at 21.7 eps, loss = 6.882\n",
      "    [batch 522]: seen 52200 examples at 21.7 eps, loss = 6.880\n",
      "    [batch 525]: seen 52500 examples at 21.7 eps, loss = 6.877\n",
      "    [batch 528]: seen 52800 examples at 21.7 eps, loss = 6.876\n",
      "    [batch 531]: seen 53100 examples at 21.7 eps, loss = 6.873\n",
      "    [batch 535]: seen 53500 examples at 21.8 eps, loss = 6.871\n",
      "    [batch 538]: seen 53800 examples at 21.8 eps, loss = 6.869\n",
      "    [batch 541]: seen 54100 examples at 21.8 eps, loss = 6.868\n",
      "    [batch 544]: seen 54400 examples at 21.8 eps, loss = 6.866\n",
      "    [batch 547]: seen 54700 examples at 21.8 eps, loss = 6.865\n",
      "    [batch 550]: seen 55000 examples at 21.9 eps, loss = 6.862\n",
      "    [batch 553]: seen 55300 examples at 21.9 eps, loss = 6.861\n",
      "    [batch 556]: seen 55600 examples at 21.9 eps, loss = 6.857\n",
      "    [batch 559]: seen 55900 examples at 21.9 eps, loss = 6.857\n",
      "    [batch 562]: seen 56200 examples at 21.9 eps, loss = 6.854\n",
      "    [batch 565]: seen 56500 examples at 21.9 eps, loss = 6.852\n",
      "    [batch 568]: seen 56800 examples at 21.9 eps, loss = 6.849\n",
      "    [batch 570]: seen 57000 examples at 21.9 eps, loss = 6.847\n",
      "    [batch 573]: seen 57300 examples at 21.9 eps, loss = 6.846\n",
      "    [batch 576]: seen 57600 examples at 21.9 eps, loss = 6.844\n",
      "    [batch 579]: seen 57900 examples at 21.9 eps, loss = 6.842\n",
      "    [batch 582]: seen 58200 examples at 21.9 eps, loss = 6.840\n",
      "    [batch 585]: seen 58500 examples at 21.9 eps, loss = 6.837\n",
      "    [batch 588]: seen 58800 examples at 21.9 eps, loss = 6.834\n",
      "    [batch 590]: seen 59000 examples at 21.9 eps, loss = 6.832\n",
      "    [batch 593]: seen 59300 examples at 21.9 eps, loss = 6.831\n",
      "    [batch 596]: seen 59600 examples at 21.9 eps, loss = 6.828\n",
      "    [batch 601]: seen 60100 examples at 22.0 eps, loss = 6.828\n",
      "    [batch 604]: seen 60400 examples at 22.0 eps, loss = 6.824\n",
      "    [batch 608]: seen 60800 examples at 22.0 eps, loss = 6.821\n",
      "    [batch 611]: seen 61100 examples at 22.0 eps, loss = 6.820\n",
      "    [batch 614]: seen 61400 examples at 22.1 eps, loss = 6.819\n",
      "    [batch 617]: seen 61700 examples at 22.1 eps, loss = 6.818\n",
      "    [batch 620]: seen 62000 examples at 22.1 eps, loss = 6.817\n",
      "    [batch 623]: seen 62300 examples at 22.1 eps, loss = 6.814\n",
      "    [batch 626]: seen 62600 examples at 22.1 eps, loss = 6.811\n",
      "    [batch 633]: seen 63300 examples at 22.2 eps, loss = 6.811\n",
      "    [batch 638]: seen 63800 examples at 22.3 eps, loss = 6.810\n",
      "    [batch 641]: seen 64100 examples at 22.3 eps, loss = 6.810\n",
      "    [batch 644]: seen 64400 examples at 22.3 eps, loss = 6.807\n",
      "    [batch 647]: seen 64700 examples at 22.3 eps, loss = 6.806\n",
      "    [batch 652]: seen 65200 examples at 22.4 eps, loss = 6.805\n",
      "    [batch 655]: seen 65500 examples at 22.4 eps, loss = 6.805\n",
      "    [batch 658]: seen 65800 examples at 22.4 eps, loss = 6.803\n",
      "    [batch 663]: seen 66300 examples at 22.5 eps, loss = 6.803\n",
      "    [batch 666]: seen 66600 examples at 22.5 eps, loss = 6.801\n",
      "    [batch 669]: seen 66900 examples at 22.5 eps, loss = 6.801\n",
      "    [batch 674]: seen 67400 examples at 22.6 eps, loss = 6.800\n",
      "    [batch 677]: seen 67700 examples at 22.6 eps, loss = 6.798\n",
      "    [batch 680]: seen 68000 examples at 22.6 eps, loss = 6.798\n",
      "    [batch 685]: seen 68500 examples at 22.7 eps, loss = 6.797\n",
      "    [batch 688]: seen 68800 examples at 22.7 eps, loss = 6.794\n",
      "    [batch 691]: seen 69100 examples at 22.7 eps, loss = 6.791\n",
      "    [batch 696]: seen 69600 examples at 22.7 eps, loss = 6.791\n",
      "    [batch 701]: seen 70100 examples at 22.8 eps, loss = 6.789\n",
      "    [batch 706]: seen 70600 examples at 22.9 eps, loss = 6.789\n",
      "    [batch 709]: seen 70900 examples at 22.9 eps, loss = 6.788\n",
      "    [batch 713]: seen 71300 examples at 23.0 eps, loss = 6.787\n",
      "    [batch 716]: seen 71600 examples at 23.0 eps, loss = 6.785\n",
      "    [batch 720]: seen 72000 examples at 23.0 eps, loss = 6.784\n",
      "    [batch 723]: seen 72300 examples at 23.0 eps, loss = 6.782\n",
      "    [batch 727]: seen 72700 examples at 23.0 eps, loss = 6.782\n",
      "    [batch 730]: seen 73000 examples at 23.0 eps, loss = 6.782\n",
      "    [batch 733]: seen 73300 examples at 23.0 eps, loss = 6.780\n",
      "    [batch 735]: seen 73500 examples at 23.0 eps, loss = 6.778\n",
      "    [batch 738]: seen 73800 examples at 23.0 eps, loss = 6.776\n",
      "    [batch 741]: seen 74100 examples at 23.0 eps, loss = 6.775\n",
      "    [batch 746]: seen 74600 examples at 23.1 eps, loss = 6.773\n",
      "    [batch 751]: seen 75100 examples at 23.1 eps, loss = 6.774\n",
      "    [batch 756]: seen 75600 examples at 23.2 eps, loss = 6.772\n",
      "    [batch 760]: seen 76000 examples at 23.2 eps, loss = 6.772\n",
      "    [batch 763]: seen 76300 examples at 23.2 eps, loss = 6.769\n",
      "    [batch 766]: seen 76600 examples at 23.2 eps, loss = 6.768\n",
      "    [batch 769]: seen 76900 examples at 23.3 eps, loss = 6.767\n",
      "    [batch 772]: seen 77200 examples at 23.3 eps, loss = 6.766\n",
      "    [batch 777]: seen 77700 examples at 23.3 eps, loss = 6.767\n",
      "    [batch 784]: seen 78400 examples at 23.5 eps, loss = 6.767\n",
      "    [batch 789]: seen 78900 examples at 23.6 eps, loss = 6.766\n",
      "    [batch 793]: seen 79300 examples at 23.6 eps, loss = 6.764\n",
      "    [batch 798]: seen 79800 examples at 23.7 eps, loss = 6.764\n",
      "    [batch 801]: seen 80100 examples at 23.7 eps, loss = 6.764\n",
      "    [batch 805]: seen 80500 examples at 23.7 eps, loss = 6.762\n",
      "    [batch 808]: seen 80800 examples at 23.7 eps, loss = 6.759\n",
      "    [batch 813]: seen 81300 examples at 23.8 eps, loss = 6.759\n",
      "    [batch 817]: seen 81700 examples at 23.8 eps, loss = 6.757\n",
      "    [batch 820]: seen 82000 examples at 23.8 eps, loss = 6.756\n",
      "    [batch 825]: seen 82500 examples at 23.8 eps, loss = 6.755\n",
      "    [batch 828]: seen 82800 examples at 23.8 eps, loss = 6.754\n",
      "    [batch 833]: seen 83300 examples at 23.9 eps, loss = 6.753\n",
      "    [batch 836]: seen 83600 examples at 23.9 eps, loss = 6.753\n",
      "    [batch 840]: seen 84000 examples at 24.0 eps, loss = 6.752\n",
      "    [batch 845]: seen 84500 examples at 24.0 eps, loss = 6.751\n",
      "    [batch 847]: seen 84700 examples at 24.0 eps, loss = 6.751\n",
      "    [batch 852]: seen 85200 examples at 24.1 eps, loss = 6.750\n",
      "    [batch 855]: seen 85500 examples at 24.1 eps, loss = 6.750\n",
      "    [batch 860]: seen 86000 examples at 24.1 eps, loss = 6.748\n",
      "    [batch 865]: seen 86500 examples at 24.2 eps, loss = 6.749\n",
      "    [batch 869]: seen 86900 examples at 24.2 eps, loss = 6.747\n",
      "    [batch 872]: seen 87200 examples at 24.2 eps, loss = 6.745\n",
      "    [batch 877]: seen 87700 examples at 24.3 eps, loss = 6.743\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-879\n",
      "    [batch 881]: seen 88100 examples at 24.3 eps, loss = 6.743\n",
      "    [batch 883]: seen 88300 examples at 24.3 eps, loss = 6.741\n",
      "    [batch 886]: seen 88600 examples at 24.3 eps, loss = 6.740\n",
      "    [batch 893]: seen 89300 examples at 24.4 eps, loss = 6.742\n",
      "    [batch 896]: seen 89600 examples at 24.4 eps, loss = 6.739\n",
      "    [batch 899]: seen 89900 examples at 24.4 eps, loss = 6.738\n",
      "    [batch 903]: seen 90300 examples at 24.4 eps, loss = 6.736\n",
      "    [batch 906]: seen 90600 examples at 24.4 eps, loss = 6.734\n",
      "    [batch 913]: seen 91300 examples at 24.5 eps, loss = 6.734\n",
      "    [batch 917]: seen 91700 examples at 24.6 eps, loss = 6.734\n",
      "    [batch 920]: seen 92000 examples at 24.5 eps, loss = 6.731\n",
      "    [batch 925]: seen 92500 examples at 24.6 eps, loss = 6.729\n",
      "    [batch 932]: seen 93200 examples at 24.7 eps, loss = 6.732\n",
      "    [batch 939]: seen 93900 examples at 24.8 eps, loss = 6.734\n",
      "    [batch 946]: seen 94600 examples at 24.9 eps, loss = 6.729\n",
      "    [batch 949]: seen 94900 examples at 24.9 eps, loss = 6.727\n",
      "    [batch 954]: seen 95400 examples at 25.0 eps, loss = 6.725\n",
      "    [batch 957]: seen 95700 examples at 25.0 eps, loss = 6.724\n",
      "    [batch 960]: seen 96000 examples at 25.0 eps, loss = 6.722\n",
      "    [batch 963]: seen 96300 examples at 25.0 eps, loss = 6.721\n",
      "    [batch 966]: seen 96600 examples at 24.9 eps, loss = 6.719\n",
      "    [batch 969]: seen 96900 examples at 24.9 eps, loss = 6.718\n",
      "    [batch 973]: seen 97300 examples at 25.0 eps, loss = 6.717\n",
      "    [batch 976]: seen 97600 examples at 25.0 eps, loss = 6.715\n",
      "    [batch 981]: seen 98100 examples at 25.0 eps, loss = 6.714\n",
      "    [batch 984]: seen 98400 examples at 25.0 eps, loss = 6.713\n",
      "    [batch 990]: seen 99000 examples at 25.1 eps, loss = 6.713\n",
      "    [batch 993]: seen 99300 examples at 25.1 eps, loss = 6.712\n",
      "    [batch 998]: seen 99800 examples at 25.1 eps, loss = 6.710\n",
      "    [batch 1000]: seen 100000 examples at 25.1 eps, loss = 6.710\n",
      "    [batch 1005]: seen 100500 examples at 25.1 eps, loss = 6.711\n",
      "    [batch 1012]: seen 101200 examples at 25.3 eps, loss = 6.729\n",
      "    [batch 1019]: seen 101900 examples at 25.4 eps, loss = 6.726\n",
      "    [batch 1026]: seen 102600 examples at 25.5 eps, loss = 6.726\n",
      "    [batch 1033]: seen 103300 examples at 25.6 eps, loss = 6.721\n",
      "    [batch 1040]: seen 104000 examples at 25.7 eps, loss = 6.718\n",
      "    [batch 1047]: seen 104700 examples at 25.8 eps, loss = 6.712\n",
      "    [batch 1052]: seen 105200 examples at 25.8 eps, loss = 6.707\n",
      "    [batch 1055]: seen 105500 examples at 25.8 eps, loss = 6.707\n",
      "    [batch 1058]: seen 105800 examples at 25.8 eps, loss = 6.704\n",
      "    [batch 1061]: seen 106100 examples at 25.8 eps, loss = 6.701\n",
      "    [batch 1064]: seen 106400 examples at 25.8 eps, loss = 6.699\n",
      "    [batch 1066]: seen 106600 examples at 25.8 eps, loss = 6.697\n",
      "    [batch 1069]: seen 106900 examples at 25.8 eps, loss = 6.695\n",
      "    [batch 1072]: seen 107200 examples at 25.8 eps, loss = 6.693\n",
      "    [batch 1075]: seen 107500 examples at 25.7 eps, loss = 6.691\n",
      "    [batch 1078]: seen 107800 examples at 25.7 eps, loss = 6.689\n",
      "    [batch 1081]: seen 108100 examples at 25.7 eps, loss = 6.686\n",
      "    [batch 1084]: seen 108400 examples at 25.7 eps, loss = 6.686\n",
      "    [batch 1086]: seen 108600 examples at 25.7 eps, loss = 6.684\n",
      "    [batch 1089]: seen 108900 examples at 25.7 eps, loss = 6.682\n",
      "    [batch 1092]: seen 109200 examples at 25.7 eps, loss = 6.680\n",
      "    [batch 1098]: seen 109800 examples at 25.8 eps, loss = 6.680\n",
      "    [batch 1101]: seen 110100 examples at 25.8 eps, loss = 6.679\n",
      "    [batch 1104]: seen 110400 examples at 25.8 eps, loss = 6.678\n",
      "    [batch 1107]: seen 110700 examples at 25.8 eps, loss = 6.676\n",
      "    [batch 1110]: seen 111000 examples at 25.8 eps, loss = 6.675\n",
      "    [batch 1112]: seen 111200 examples at 25.7 eps, loss = 6.675\n",
      "    [batch 1116]: seen 111600 examples at 25.8 eps, loss = 6.673\n",
      "    [batch 1119]: seen 111900 examples at 25.7 eps, loss = 6.669\n",
      "    [batch 1122]: seen 112200 examples at 25.7 eps, loss = 6.667\n",
      "    [batch 1127]: seen 112700 examples at 25.8 eps, loss = 6.666\n",
      "    [batch 1130]: seen 113000 examples at 25.8 eps, loss = 6.665\n",
      "    [batch 1133]: seen 113300 examples at 25.8 eps, loss = 6.663\n",
      "    [batch 1136]: seen 113600 examples at 25.8 eps, loss = 6.662\n",
      "    [batch 1139]: seen 113900 examples at 25.8 eps, loss = 6.661\n",
      "    [batch 1142]: seen 114200 examples at 25.8 eps, loss = 6.660\n",
      "    [batch 1145]: seen 114500 examples at 25.8 eps, loss = 6.658\n",
      "    [batch 1148]: seen 114800 examples at 25.8 eps, loss = 6.656\n",
      "    [batch 1151]: seen 115100 examples at 25.8 eps, loss = 6.655\n",
      "    [batch 1155]: seen 115500 examples at 25.8 eps, loss = 6.654\n",
      "    [batch 1158]: seen 115800 examples at 25.8 eps, loss = 6.651\n",
      "    [batch 1162]: seen 116200 examples at 25.8 eps, loss = 6.651\n",
      "    [batch 1167]: seen 116700 examples at 25.8 eps, loss = 6.652\n",
      "    [batch 1174]: seen 117400 examples at 25.9 eps, loss = 6.655\n",
      "    [batch 1181]: seen 118100 examples at 26.0 eps, loss = 6.656\n",
      "    [batch 1188]: seen 118800 examples at 26.1 eps, loss = 6.656\n",
      "    [batch 1195]: seen 119500 examples at 26.2 eps, loss = 6.654\n",
      "    [batch 1202]: seen 120200 examples at 26.3 eps, loss = 6.651\n",
      "    [batch 1206]: seen 120600 examples at 26.3 eps, loss = 6.649\n",
      "    [batch 1213]: seen 121300 examples at 26.4 eps, loss = 6.649\n",
      "    [batch 1218]: seen 121800 examples at 26.5 eps, loss = 6.649\n",
      "    [batch 1221]: seen 122100 examples at 26.5 eps, loss = 6.646\n",
      "    [batch 1224]: seen 122400 examples at 26.4 eps, loss = 6.644\n",
      "    [batch 1227]: seen 122700 examples at 26.4 eps, loss = 6.643\n",
      "    [batch 1230]: seen 123000 examples at 26.4 eps, loss = 6.640\n",
      "    [batch 1233]: seen 123300 examples at 26.4 eps, loss = 6.639\n",
      "    [batch 1238]: seen 123800 examples at 26.5 eps, loss = 6.639\n",
      "    [batch 1242]: seen 124200 examples at 26.5 eps, loss = 6.636\n",
      "    [batch 1249]: seen 124900 examples at 26.6 eps, loss = 6.638\n",
      "    [batch 1256]: seen 125600 examples at 26.7 eps, loss = 6.636\n",
      "    [batch 1259]: seen 125900 examples at 26.7 eps, loss = 6.635\n",
      "    [batch 1262]: seen 126200 examples at 26.6 eps, loss = 6.633\n",
      "    [batch 1265]: seen 126500 examples at 26.6 eps, loss = 6.633\n",
      "    [batch 1269]: seen 126900 examples at 26.7 eps, loss = 6.630\n",
      "    [batch 1272]: seen 127200 examples at 26.6 eps, loss = 6.627\n",
      "    [batch 1274]: seen 127400 examples at 26.6 eps, loss = 6.626\n",
      "    [batch 1277]: seen 127700 examples at 26.6 eps, loss = 6.624\n",
      "    [batch 1281]: seen 128100 examples at 26.6 eps, loss = 6.622\n",
      "    [batch 1286]: seen 128600 examples at 26.7 eps, loss = 6.621\n",
      "    [batch 1289]: seen 128900 examples at 26.7 eps, loss = 6.620\n",
      "    [batch 1296]: seen 129600 examples at 26.8 eps, loss = 6.621\n",
      "    [batch 1300]: seen 130000 examples at 26.8 eps, loss = 6.619\n",
      "    [batch 1303]: seen 130300 examples at 26.8 eps, loss = 6.618\n",
      "    [batch 1306]: seen 130600 examples at 26.8 eps, loss = 6.615\n",
      "    [batch 1308]: seen 130800 examples at 26.7 eps, loss = 6.614\n",
      "    [batch 1311]: seen 131100 examples at 26.7 eps, loss = 6.612\n",
      "    [batch 1316]: seen 131600 examples at 26.8 eps, loss = 6.611\n",
      "    [batch 1319]: seen 131900 examples at 26.7 eps, loss = 6.610\n",
      "    [batch 1322]: seen 132200 examples at 26.7 eps, loss = 6.609\n",
      "    [batch 1325]: seen 132500 examples at 26.7 eps, loss = 6.607\n",
      "    [batch 1330]: seen 133000 examples at 26.8 eps, loss = 6.608\n",
      "    [batch 1337]: seen 133700 examples at 26.9 eps, loss = 6.606\n",
      "    [batch 1341]: seen 134100 examples at 26.9 eps, loss = 6.606\n",
      "    [batch 1344]: seen 134400 examples at 26.9 eps, loss = 6.605\n",
      "    [batch 1347]: seen 134700 examples at 26.9 eps, loss = 6.603\n",
      "    [batch 1350]: seen 135000 examples at 26.8 eps, loss = 6.600\n",
      "    [batch 1354]: seen 135400 examples at 26.9 eps, loss = 6.598\n",
      "    [batch 1358]: seen 135800 examples at 26.9 eps, loss = 6.597\n",
      "    [batch 1361]: seen 136100 examples at 26.9 eps, loss = 6.595\n",
      "    [batch 1364]: seen 136400 examples at 26.9 eps, loss = 6.594\n",
      "    [batch 1367]: seen 136700 examples at 26.9 eps, loss = 6.594\n",
      "    [batch 1369]: seen 136900 examples at 26.9 eps, loss = 6.592\n",
      "    [batch 1372]: seen 137200 examples at 26.8 eps, loss = 6.590\n",
      "    [batch 1375]: seen 137500 examples at 26.8 eps, loss = 6.589\n",
      "    [batch 1378]: seen 137800 examples at 26.8 eps, loss = 6.589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1381]: seen 138100 examples at 26.8 eps, loss = 6.588\n",
      "    [batch 1384]: seen 138400 examples at 26.8 eps, loss = 6.587\n",
      "    [batch 1391]: seen 139100 examples at 26.9 eps, loss = 6.590\n",
      "    [batch 1398]: seen 139800 examples at 27.0 eps, loss = 6.588\n",
      "    [batch 1403]: seen 140300 examples at 27.0 eps, loss = 6.586\n",
      "    [batch 1406]: seen 140600 examples at 27.0 eps, loss = 6.586\n",
      "    [batch 1411]: seen 141100 examples at 27.1 eps, loss = 6.586\n",
      "    [batch 1418]: seen 141800 examples at 27.2 eps, loss = 6.586\n",
      "    [batch 1423]: seen 142300 examples at 27.2 eps, loss = 6.584\n",
      "    [batch 1430]: seen 143000 examples at 27.3 eps, loss = 6.585\n",
      "    [batch 1437]: seen 143700 examples at 27.4 eps, loss = 6.585\n",
      "    [batch 1440]: seen 144000 examples at 27.4 eps, loss = 6.583\n",
      "    [batch 1443]: seen 144300 examples at 27.3 eps, loss = 6.582\n",
      "    [batch 1446]: seen 144600 examples at 27.3 eps, loss = 6.581\n",
      "    [batch 1449]: seen 144900 examples at 27.3 eps, loss = 6.580\n",
      "    [batch 1452]: seen 145200 examples at 27.3 eps, loss = 6.578\n",
      "    [batch 1459]: seen 145900 examples at 27.4 eps, loss = 6.579\n",
      "    [batch 1466]: seen 146600 examples at 27.4 eps, loss = 6.577\n",
      "    [batch 1468]: seen 146800 examples at 27.4 eps, loss = 6.576\n",
      "    [batch 1470]: seen 147000 examples at 27.4 eps, loss = 6.575\n",
      "    [batch 1473]: seen 147300 examples at 27.4 eps, loss = 6.573\n",
      "    [batch 1476]: seen 147600 examples at 27.4 eps, loss = 6.569\n",
      "    [batch 1479]: seen 147900 examples at 27.4 eps, loss = 6.567\n",
      "    [batch 1484]: seen 148400 examples at 27.4 eps, loss = 6.566\n",
      "    [batch 1488]: seen 148800 examples at 27.4 eps, loss = 6.565\n",
      "    [batch 1491]: seen 149100 examples at 27.4 eps, loss = 6.564\n",
      "    [batch 1496]: seen 149600 examples at 27.5 eps, loss = 6.563\n",
      "    [batch 1503]: seen 150300 examples at 27.5 eps, loss = 6.563\n",
      "    [batch 1505]: seen 150500 examples at 27.5 eps, loss = 6.562\n",
      "    [batch 1508]: seen 150800 examples at 27.5 eps, loss = 6.561\n",
      "    [batch 1511]: seen 151100 examples at 27.5 eps, loss = 6.559\n",
      "    [batch 1514]: seen 151400 examples at 27.5 eps, loss = 6.558\n",
      "    [batch 1518]: seen 151800 examples at 27.5 eps, loss = 6.557\n",
      "    [batch 1523]: seen 152300 examples at 27.6 eps, loss = 6.557\n",
      "    [batch 1526]: seen 152600 examples at 27.6 eps, loss = 6.555\n",
      "    [batch 1530]: seen 153000 examples at 27.6 eps, loss = 6.555\n",
      "    [batch 1533]: seen 153300 examples at 27.5 eps, loss = 6.551\n",
      "    [batch 1536]: seen 153600 examples at 27.5 eps, loss = 6.550\n",
      "    [batch 1539]: seen 153900 examples at 27.5 eps, loss = 6.548\n",
      "    [batch 1542]: seen 154200 examples at 27.5 eps, loss = 6.548\n",
      "    [batch 1548]: seen 154800 examples at 27.6 eps, loss = 6.548\n",
      "    [batch 1551]: seen 155100 examples at 27.6 eps, loss = 6.546\n",
      "    [batch 1558]: seen 155800 examples at 27.6 eps, loss = 6.546\n",
      "    [batch 1561]: seen 156100 examples at 27.6 eps, loss = 6.543\n",
      "    [batch 1564]: seen 156400 examples at 27.6 eps, loss = 6.542\n",
      "    [batch 1566]: seen 156600 examples at 27.6 eps, loss = 6.541\n",
      "    [batch 1569]: seen 156900 examples at 27.6 eps, loss = 6.538\n",
      "    [batch 1572]: seen 157200 examples at 27.6 eps, loss = 6.535\n",
      "    [batch 1577]: seen 157700 examples at 27.6 eps, loss = 6.535\n",
      "    [batch 1580]: seen 158000 examples at 27.6 eps, loss = 6.534\n",
      "    [batch 1583]: seen 158300 examples at 27.6 eps, loss = 6.533\n",
      "    [batch 1586]: seen 158600 examples at 27.6 eps, loss = 6.533\n",
      "    [batch 1588]: seen 158800 examples at 27.6 eps, loss = 6.532\n",
      "    [batch 1591]: seen 159100 examples at 27.5 eps, loss = 6.530\n",
      "    [batch 1594]: seen 159400 examples at 27.5 eps, loss = 6.529\n",
      "    [batch 1597]: seen 159700 examples at 27.5 eps, loss = 6.527\n",
      "    [batch 1600]: seen 160000 examples at 27.5 eps, loss = 6.526\n",
      "    [batch 1603]: seen 160300 examples at 27.5 eps, loss = 6.523\n",
      "    [batch 1607]: seen 160700 examples at 27.5 eps, loss = 6.522\n",
      "    [batch 1610]: seen 161000 examples at 27.5 eps, loss = 6.521\n",
      "    [batch 1613]: seen 161300 examples at 27.5 eps, loss = 6.520\n",
      "    [batch 1616]: seen 161600 examples at 27.5 eps, loss = 6.518\n",
      "    [batch 1619]: seen 161900 examples at 27.5 eps, loss = 6.517\n",
      "    [batch 1622]: seen 162200 examples at 27.4 eps, loss = 6.513\n",
      "    [batch 1626]: seen 162600 examples at 27.5 eps, loss = 6.511\n",
      "    [batch 1629]: seen 162900 examples at 27.5 eps, loss = 6.510\n",
      "    [batch 1632]: seen 163200 examples at 27.5 eps, loss = 6.507\n",
      "    [batch 1635]: seen 163500 examples at 27.5 eps, loss = 6.506\n",
      "    [batch 1638]: seen 163800 examples at 27.5 eps, loss = 6.503\n",
      "    [batch 1645]: seen 164500 examples at 27.5 eps, loss = 6.504\n",
      "    [batch 1649]: seen 164900 examples at 27.5 eps, loss = 6.502\n",
      "    [batch 1652]: seen 165200 examples at 27.5 eps, loss = 6.499\n",
      "    [batch 1657]: seen 165700 examples at 27.6 eps, loss = 6.499\n",
      "    [batch 1663]: seen 166300 examples at 27.6 eps, loss = 6.498\n",
      "    [batch 1666]: seen 166600 examples at 27.6 eps, loss = 6.497\n",
      "    [batch 1669]: seen 166900 examples at 27.6 eps, loss = 6.497\n",
      "    [batch 1676]: seen 167600 examples at 27.7 eps, loss = 6.507\n",
      "    [batch 1683]: seen 168300 examples at 27.7 eps, loss = 6.507\n",
      "    [batch 1690]: seen 169000 examples at 27.8 eps, loss = 6.502\n",
      "    [batch 1697]: seen 169700 examples at 27.9 eps, loss = 6.500\n",
      "    [batch 1704]: seen 170400 examples at 27.9 eps, loss = 6.496\n",
      "    [batch 1708]: seen 170800 examples at 27.9 eps, loss = 6.495\n",
      "    [batch 1711]: seen 171100 examples at 27.9 eps, loss = 6.492\n",
      "    [batch 1714]: seen 171400 examples at 27.9 eps, loss = 6.490\n",
      "    [batch 1717]: seen 171700 examples at 27.9 eps, loss = 6.489\n",
      "    [batch 1720]: seen 172000 examples at 27.9 eps, loss = 6.487\n",
      "    [batch 1723]: seen 172300 examples at 27.9 eps, loss = 6.485\n",
      "    [batch 1726]: seen 172600 examples at 27.9 eps, loss = 6.484\n",
      "    [batch 1730]: seen 173000 examples at 27.9 eps, loss = 6.483\n",
      "    [batch 1733]: seen 173300 examples at 27.9 eps, loss = 6.480\n",
      "    [batch 1738]: seen 173800 examples at 27.9 eps, loss = 6.480\n",
      "    [batch 1741]: seen 174100 examples at 27.9 eps, loss = 6.478\n",
      "    [batch 1744]: seen 174400 examples at 27.9 eps, loss = 6.476\n",
      "    [batch 1747]: seen 174700 examples at 27.9 eps, loss = 6.474\n",
      "    [batch 1750]: seen 175000 examples at 27.9 eps, loss = 6.473\n",
      "    [batch 1757]: seen 175700 examples at 28.0 eps, loss = 6.474\n",
      "    [batch 1763]: seen 176300 examples at 28.0 eps, loss = 6.473\n",
      "    [batch 1767]: seen 176700 examples at 28.0 eps, loss = 6.471\n",
      "    [batch 1770]: seen 177000 examples at 28.0 eps, loss = 6.469\n",
      "    [batch 1773]: seen 177300 examples at 28.0 eps, loss = 6.466\n",
      "    [batch 1776]: seen 177600 examples at 28.0 eps, loss = 6.465\n",
      "    [batch 1779]: seen 177900 examples at 27.9 eps, loss = 6.462\n",
      "    [batch 1782]: seen 178200 examples at 27.9 eps, loss = 6.462\n",
      "    [batch 1787]: seen 178700 examples at 28.0 eps, loss = 6.460\n",
      "    [batch 1789]: seen 178900 examples at 28.0 eps, loss = 6.458\n",
      "    [batch 1792]: seen 179200 examples at 28.0 eps, loss = 6.456\n",
      "    [batch 1795]: seen 179500 examples at 27.9 eps, loss = 6.455\n",
      "    [batch 1798]: seen 179800 examples at 27.9 eps, loss = 6.453\n",
      "    [batch 1802]: seen 180200 examples at 28.0 eps, loss = 6.451\n",
      "    [batch 1805]: seen 180500 examples at 28.0 eps, loss = 6.450\n",
      "    [batch 1808]: seen 180800 examples at 28.0 eps, loss = 6.449\n",
      "    [batch 1812]: seen 181200 examples at 28.0 eps, loss = 6.447\n",
      "    [batch 1814]: seen 181400 examples at 28.0 eps, loss = 6.446\n",
      "    [batch 1817]: seen 181700 examples at 27.9 eps, loss = 6.445\n",
      "    [batch 1820]: seen 182000 examples at 27.9 eps, loss = 6.442\n",
      "    [batch 1823]: seen 182300 examples at 27.9 eps, loss = 6.441\n",
      "    [batch 1826]: seen 182600 examples at 27.9 eps, loss = 6.438\n",
      "    [batch 1829]: seen 182900 examples at 27.9 eps, loss = 6.435\n",
      "    [batch 1836]: seen 183600 examples at 28.0 eps, loss = 6.435\n",
      "    [batch 1839]: seen 183900 examples at 28.0 eps, loss = 6.433\n",
      "    [batch 1842]: seen 184200 examples at 27.9 eps, loss = 6.432\n",
      "    [batch 1845]: seen 184500 examples at 27.9 eps, loss = 6.429\n",
      "    [batch 1849]: seen 184900 examples at 27.9 eps, loss = 6.427\n",
      "    [batch 1854]: seen 185400 examples at 28.0 eps, loss = 6.425\n",
      "    [batch 1857]: seen 185700 examples at 27.9 eps, loss = 6.424\n",
      "    [batch 1862]: seen 186200 examples at 28.0 eps, loss = 6.424\n",
      "    [batch 1866]: seen 186600 examples at 28.0 eps, loss = 6.423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [END] Training complete: Total examples : 187100; Total time: 1:51:16\n",
      "[EPOCH 1] Complete. Avg Loss: 6.424464537108169; Best Loss: 6.4225969953155\n",
      "[EPOCH 2] Starting training..\n",
      "    [batch 7]: seen 700 examples at 52.0 eps, loss = 6.421\n",
      "    [batch 12]: seen 1200 examples at 49.8 eps, loss = 6.422\n",
      "    [batch 19]: seen 1900 examples at 55.5 eps, loss = 6.424\n",
      "    [batch 26]: seen 2600 examples at 58.5 eps, loss = 6.425\n",
      "    [batch 33]: seen 3300 examples at 60.5 eps, loss = 6.425\n",
      "    [batch 40]: seen 4000 examples at 61.8 eps, loss = 6.425\n",
      "    [batch 47]: seen 4700 examples at 62.8 eps, loss = 6.421\n",
      "    [batch 49]: seen 4900 examples at 57.6 eps, loss = 6.421\n",
      "    [batch 52]: seen 5200 examples at 52.4 eps, loss = 6.420\n",
      "    [batch 56]: seen 5600 examples at 50.1 eps, loss = 6.418\n",
      "    [batch 61]: seen 6100 examples at 49.9 eps, loss = 6.417\n",
      "    [batch 67]: seen 6700 examples at 49.9 eps, loss = 6.416\n",
      "    [batch 70]: seen 7000 examples at 48.2 eps, loss = 6.415\n",
      "    [batch 73]: seen 7300 examples at 45.8 eps, loss = 6.414\n",
      "    [batch 76]: seen 7600 examples at 43.7 eps, loss = 6.413\n",
      "    [batch 78]: seen 7800 examples at 42.4 eps, loss = 6.411\n",
      "    [batch 81]: seen 8100 examples at 40.9 eps, loss = 6.408\n",
      "    [batch 86]: seen 8600 examples at 40.6 eps, loss = 6.406\n",
      "    [batch 93]: seen 9300 examples at 41.8 eps, loss = 6.407\n",
      "    [batch 96]: seen 9600 examples at 41.2 eps, loss = 6.404\n",
      "    [batch 99]: seen 9900 examples at 40.0 eps, loss = 6.401\n",
      "    [batch 102]: seen 10200 examples at 39.4 eps, loss = 6.401\n",
      "    [batch 105]: seen 10500 examples at 38.9 eps, loss = 6.400\n",
      "    [batch 108]: seen 10800 examples at 38.0 eps, loss = 6.397\n",
      "    [batch 113]: seen 11300 examples at 38.3 eps, loss = 6.398\n",
      "    [batch 119]: seen 11900 examples at 38.7 eps, loss = 6.395\n",
      "    [batch 122]: seen 12200 examples at 38.0 eps, loss = 6.394\n",
      "    [batch 129]: seen 12900 examples at 38.9 eps, loss = 6.395\n",
      "    [batch 135]: seen 13500 examples at 39.3 eps, loss = 6.394\n",
      "    [batch 138]: seen 13800 examples at 38.6 eps, loss = 6.391\n",
      "    [batch 141]: seen 14100 examples at 37.9 eps, loss = 6.389\n",
      "    [batch 144]: seen 14400 examples at 37.6 eps, loss = 6.389\n",
      "    [batch 147]: seen 14700 examples at 36.9 eps, loss = 6.385\n",
      "    [batch 150]: seen 15000 examples at 36.4 eps, loss = 6.383\n",
      "    [batch 153]: seen 15300 examples at 35.9 eps, loss = 6.381\n",
      "    [batch 157]: seen 15700 examples at 35.7 eps, loss = 6.380\n",
      "    [batch 160]: seen 16000 examples at 35.5 eps, loss = 6.379\n",
      "    [batch 163]: seen 16300 examples at 35.3 eps, loss = 6.380\n",
      "    [batch 168]: seen 16800 examples at 35.4 eps, loss = 6.376\n",
      "    [batch 171]: seen 17100 examples at 35.2 eps, loss = 6.375\n",
      "    [batch 173]: seen 17300 examples at 34.9 eps, loss = 6.373\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-2045\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-2045\n",
      "    [batch 176]: seen 17600 examples at 34.6 eps, loss = 6.373\n",
      "    [batch 181]: seen 18100 examples at 34.6 eps, loss = 6.372\n",
      "    [batch 186]: seen 18600 examples at 34.9 eps, loss = 6.373\n",
      "    [batch 189]: seen 18900 examples at 34.5 eps, loss = 6.370\n",
      "    [batch 192]: seen 19200 examples at 34.4 eps, loss = 6.368\n",
      "    [batch 195]: seen 19500 examples at 34.2 eps, loss = 6.368\n",
      "    [batch 198]: seen 19800 examples at 33.9 eps, loss = 6.367\n",
      "    [batch 201]: seen 20100 examples at 33.8 eps, loss = 6.365\n",
      "    [batch 205]: seen 20500 examples at 33.7 eps, loss = 6.364\n",
      "    [batch 208]: seen 20800 examples at 33.4 eps, loss = 6.363\n",
      "    [batch 213]: seen 21300 examples at 33.7 eps, loss = 6.362\n",
      "    [batch 216]: seen 21600 examples at 33.6 eps, loss = 6.361\n",
      "    [batch 221]: seen 22100 examples at 33.8 eps, loss = 6.360\n",
      "    [batch 224]: seen 22400 examples at 33.7 eps, loss = 6.359\n",
      "    [batch 229]: seen 22900 examples at 33.7 eps, loss = 6.356\n",
      "    [batch 232]: seen 23200 examples at 33.5 eps, loss = 6.352\n",
      "    [batch 234]: seen 23400 examples at 33.3 eps, loss = 6.350\n",
      "    [batch 237]: seen 23700 examples at 33.2 eps, loss = 6.350\n",
      "    [batch 240]: seen 24000 examples at 33.1 eps, loss = 6.348\n",
      "    [batch 243]: seen 24300 examples at 32.8 eps, loss = 6.346\n",
      "    [batch 246]: seen 24600 examples at 32.6 eps, loss = 6.344\n",
      "    [batch 249]: seen 24900 examples at 32.4 eps, loss = 6.343\n",
      "    [batch 254]: seen 25400 examples at 32.5 eps, loss = 6.342\n",
      "    [batch 256]: seen 25600 examples at 32.3 eps, loss = 6.341\n",
      "    [batch 261]: seen 26100 examples at 32.5 eps, loss = 6.341\n",
      "    [batch 265]: seen 26500 examples at 32.5 eps, loss = 6.339\n",
      "    [batch 268]: seen 26800 examples at 32.3 eps, loss = 6.337\n",
      "    [batch 273]: seen 27300 examples at 32.5 eps, loss = 6.337\n",
      "    [batch 276]: seen 27600 examples at 32.4 eps, loss = 6.335\n",
      "    [batch 279]: seen 27900 examples at 32.4 eps, loss = 6.335\n",
      "    [batch 284]: seen 28400 examples at 32.4 eps, loss = 6.333\n",
      "    [batch 287]: seen 28700 examples at 32.3 eps, loss = 6.333\n",
      "    [batch 294]: seen 29400 examples at 32.8 eps, loss = 6.333\n",
      "    [batch 297]: seen 29700 examples at 32.7 eps, loss = 6.331\n",
      "    [batch 302]: seen 30200 examples at 32.8 eps, loss = 6.331\n",
      "    [batch 305]: seen 30500 examples at 32.8 eps, loss = 6.331\n",
      "    [batch 309]: seen 30900 examples at 32.8 eps, loss = 6.329\n",
      "    [batch 313]: seen 31300 examples at 32.8 eps, loss = 6.328\n",
      "    [batch 316]: seen 31600 examples at 32.6 eps, loss = 6.325\n",
      "    [batch 319]: seen 31900 examples at 32.4 eps, loss = 6.323\n",
      "    [batch 322]: seen 32200 examples at 32.4 eps, loss = 6.323\n",
      "    [batch 324]: seen 32400 examples at 32.2 eps, loss = 6.321\n",
      "    [batch 327]: seen 32700 examples at 32.2 eps, loss = 6.321\n",
      "    [batch 332]: seen 33200 examples at 32.2 eps, loss = 6.320\n",
      "    [batch 337]: seen 33700 examples at 32.4 eps, loss = 6.322\n",
      "    [batch 343]: seen 34300 examples at 32.6 eps, loss = 6.319\n",
      "    [batch 346]: seen 34600 examples at 32.4 eps, loss = 6.318\n",
      "    [batch 349]: seen 34900 examples at 32.3 eps, loss = 6.316\n",
      "    [batch 352]: seen 35200 examples at 32.1 eps, loss = 6.314\n",
      "    [batch 356]: seen 35600 examples at 32.1 eps, loss = 6.314\n",
      "    [batch 359]: seen 35900 examples at 32.1 eps, loss = 6.311\n",
      "    [batch 362]: seen 36200 examples at 31.9 eps, loss = 6.310\n",
      "    [batch 366]: seen 36600 examples at 31.9 eps, loss = 6.309\n",
      "    [batch 369]: seen 36900 examples at 31.8 eps, loss = 6.307\n",
      "    [batch 372]: seen 37200 examples at 31.8 eps, loss = 6.307\n",
      "    [batch 378]: seen 37800 examples at 31.9 eps, loss = 6.306\n",
      "    [batch 381]: seen 38100 examples at 31.9 eps, loss = 6.305\n",
      "    [batch 385]: seen 38500 examples at 31.9 eps, loss = 6.304\n",
      "    [batch 392]: seen 39200 examples at 32.1 eps, loss = 6.304\n",
      "    [batch 396]: seen 39600 examples at 32.1 eps, loss = 6.301\n",
      "    [batch 399]: seen 39900 examples at 32.1 eps, loss = 6.299\n",
      "    [batch 402]: seen 40200 examples at 32.0 eps, loss = 6.300\n",
      "    [batch 407]: seen 40700 examples at 32.2 eps, loss = 6.300\n",
      "    [batch 412]: seen 41200 examples at 32.3 eps, loss = 6.298\n",
      "    [batch 415]: seen 41500 examples at 32.2 eps, loss = 6.297\n",
      "    [batch 422]: seen 42200 examples at 32.4 eps, loss = 6.297\n",
      "    [batch 425]: seen 42500 examples at 32.4 eps, loss = 6.295\n",
      "    [batch 428]: seen 42800 examples at 32.3 eps, loss = 6.293\n",
      "    [batch 431]: seen 43100 examples at 32.1 eps, loss = 6.292\n",
      "    [batch 435]: seen 43500 examples at 32.1 eps, loss = 6.290\n",
      "    [batch 438]: seen 43800 examples at 32.0 eps, loss = 6.287\n",
      "    [batch 441]: seen 44100 examples at 32.0 eps, loss = 6.286\n",
      "    [batch 444]: seen 44400 examples at 31.9 eps, loss = 6.284\n",
      "    [batch 447]: seen 44700 examples at 31.9 eps, loss = 6.283\n",
      "    [batch 454]: seen 45400 examples at 32.2 eps, loss = 6.286\n",
      "    [batch 459]: seen 45900 examples at 32.3 eps, loss = 6.282\n",
      "    [batch 461]: seen 46100 examples at 32.2 eps, loss = 6.280\n",
      "    [batch 464]: seen 46400 examples at 32.1 eps, loss = 6.278\n",
      "    [batch 467]: seen 46700 examples at 32.1 eps, loss = 6.275\n",
      "    [batch 474]: seen 47400 examples at 32.3 eps, loss = 6.277\n",
      "    [batch 481]: seen 48100 examples at 32.6 eps, loss = 6.280\n",
      "    [batch 488]: seen 48800 examples at 32.8 eps, loss = 6.278\n",
      "    [batch 495]: seen 49500 examples at 33.0 eps, loss = 6.275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 498]: seen 49800 examples at 32.9 eps, loss = 6.274\n",
      "    [batch 501]: seen 50100 examples at 32.9 eps, loss = 6.274\n",
      "    [batch 506]: seen 50600 examples at 33.0 eps, loss = 6.275\n",
      "    [batch 511]: seen 51100 examples at 33.1 eps, loss = 6.276\n",
      "    [batch 518]: seen 51800 examples at 33.3 eps, loss = 6.274\n",
      "    [batch 523]: seen 52300 examples at 33.4 eps, loss = 6.274\n",
      "    [batch 526]: seen 52600 examples at 33.4 eps, loss = 6.271\n",
      "    [batch 528]: seen 52800 examples at 33.3 eps, loss = 6.268\n",
      "    [batch 531]: seen 53100 examples at 33.3 eps, loss = 6.266\n",
      "    [batch 536]: seen 53600 examples at 33.3 eps, loss = 6.263\n",
      "    [batch 539]: seen 53900 examples at 33.2 eps, loss = 6.261\n",
      "    [batch 546]: seen 54600 examples at 33.5 eps, loss = 6.263\n",
      "    [batch 551]: seen 55100 examples at 33.5 eps, loss = 6.259\n",
      "    [batch 555]: seen 55500 examples at 33.5 eps, loss = 6.259\n",
      "    [batch 558]: seen 55800 examples at 33.4 eps, loss = 6.256\n",
      "    [batch 562]: seen 56200 examples at 33.4 eps, loss = 6.255\n",
      "    [batch 565]: seen 56500 examples at 33.2 eps, loss = 6.253\n",
      "    [batch 568]: seen 56800 examples at 33.1 eps, loss = 6.253\n",
      "    [batch 571]: seen 57100 examples at 33.1 eps, loss = 6.251\n",
      "    [batch 574]: seen 57400 examples at 33.1 eps, loss = 6.250\n",
      "    [batch 577]: seen 57700 examples at 33.0 eps, loss = 6.247\n",
      "    [batch 580]: seen 58000 examples at 32.9 eps, loss = 6.245\n",
      "    [batch 583]: seen 58300 examples at 32.8 eps, loss = 6.244\n",
      "    [batch 585]: seen 58500 examples at 32.7 eps, loss = 6.241\n",
      "    [batch 590]: seen 59000 examples at 32.8 eps, loss = 6.242\n",
      "    [batch 593]: seen 59300 examples at 32.8 eps, loss = 6.240\n",
      "    [batch 598]: seen 59800 examples at 32.8 eps, loss = 6.239\n",
      "    [batch 601]: seen 60100 examples at 32.7 eps, loss = 6.238\n",
      "    [batch 604]: seen 60400 examples at 32.7 eps, loss = 6.238\n",
      "    [batch 609]: seen 60900 examples at 32.7 eps, loss = 6.237\n",
      "    [batch 612]: seen 61200 examples at 32.6 eps, loss = 6.234\n",
      "    [batch 614]: seen 61400 examples at 32.6 eps, loss = 6.233\n",
      "    [batch 617]: seen 61700 examples at 32.5 eps, loss = 6.232\n",
      "    [batch 622]: seen 62200 examples at 32.6 eps, loss = 6.233\n",
      "    [batch 629]: seen 62900 examples at 32.8 eps, loss = 6.232\n",
      "    [batch 634]: seen 63400 examples at 32.9 eps, loss = 6.230\n",
      "    [batch 637]: seen 63700 examples at 32.8 eps, loss = 6.228\n",
      "    [batch 641]: seen 64100 examples at 32.8 eps, loss = 6.226\n",
      "    [batch 644]: seen 64400 examples at 32.7 eps, loss = 6.223\n",
      "    [batch 647]: seen 64700 examples at 32.6 eps, loss = 6.221\n",
      "    [batch 649]: seen 64900 examples at 32.6 eps, loss = 6.218\n",
      "    [batch 652]: seen 65200 examples at 32.5 eps, loss = 6.216\n",
      "    [batch 655]: seen 65500 examples at 32.5 eps, loss = 6.215\n",
      "    [batch 660]: seen 66000 examples at 32.5 eps, loss = 6.214\n",
      "    [batch 663]: seen 66300 examples at 32.5 eps, loss = 6.212\n",
      "    [batch 666]: seen 66600 examples at 32.4 eps, loss = 6.210\n",
      "    [batch 669]: seen 66900 examples at 32.4 eps, loss = 6.208\n",
      "    [batch 672]: seen 67200 examples at 32.3 eps, loss = 6.206\n",
      "    [batch 675]: seen 67500 examples at 32.2 eps, loss = 6.203\n",
      "    [batch 679]: seen 67900 examples at 32.2 eps, loss = 6.201\n",
      "    [batch 684]: seen 68400 examples at 32.3 eps, loss = 6.201\n",
      "    [batch 691]: seen 69100 examples at 32.5 eps, loss = 6.202\n",
      "    [batch 698]: seen 69800 examples at 32.7 eps, loss = 6.201\n",
      "    [batch 701]: seen 70100 examples at 32.6 eps, loss = 6.198\n",
      "    [batch 704]: seen 70400 examples at 32.6 eps, loss = 6.195\n",
      "    [batch 709]: seen 70900 examples at 32.6 eps, loss = 6.193\n",
      "    [batch 712]: seen 71200 examples at 32.6 eps, loss = 6.192\n",
      "    [batch 716]: seen 71600 examples at 32.5 eps, loss = 6.191\n",
      "    [batch 718]: seen 71800 examples at 32.5 eps, loss = 6.190\n",
      "    [batch 721]: seen 72100 examples at 32.5 eps, loss = 6.188\n",
      "    [batch 726]: seen 72600 examples at 32.5 eps, loss = 6.188\n",
      "    [batch 730]: seen 73000 examples at 32.5 eps, loss = 6.185\n",
      "    [batch 733]: seen 73300 examples at 32.5 eps, loss = 6.184\n",
      "    [batch 736]: seen 73600 examples at 32.4 eps, loss = 6.179\n",
      "    [batch 741]: seen 74100 examples at 32.5 eps, loss = 6.177\n",
      "    [batch 744]: seen 74400 examples at 32.4 eps, loss = 6.174\n",
      "    [batch 746]: seen 74600 examples at 32.3 eps, loss = 6.173\n",
      "    [batch 750]: seen 75000 examples at 32.3 eps, loss = 6.171\n",
      "    [batch 753]: seen 75300 examples at 32.3 eps, loss = 6.170\n",
      "    [batch 758]: seen 75800 examples at 32.4 eps, loss = 6.169\n",
      "    [batch 761]: seen 76100 examples at 32.4 eps, loss = 6.168\n",
      "    [batch 764]: seen 76400 examples at 32.3 eps, loss = 6.168\n",
      "    [batch 767]: seen 76700 examples at 32.3 eps, loss = 6.167\n",
      "    [batch 771]: seen 77100 examples at 32.3 eps, loss = 6.165\n",
      "    [batch 774]: seen 77400 examples at 32.2 eps, loss = 6.162\n",
      "    [batch 776]: seen 77600 examples at 32.2 eps, loss = 6.162\n",
      "    [batch 779]: seen 77900 examples at 32.2 eps, loss = 6.161\n",
      "    [batch 784]: seen 78400 examples at 32.2 eps, loss = 6.160\n",
      "    [batch 787]: seen 78700 examples at 32.1 eps, loss = 6.156\n",
      "    [batch 791]: seen 79100 examples at 32.1 eps, loss = 6.154\n",
      "    [batch 796]: seen 79600 examples at 32.2 eps, loss = 6.153\n",
      "    [batch 803]: seen 80300 examples at 32.3 eps, loss = 6.156\n",
      "    [batch 808]: seen 80800 examples at 32.4 eps, loss = 6.152\n",
      "    [batch 813]: seen 81300 examples at 32.4 eps, loss = 6.150\n",
      "    [batch 816]: seen 81600 examples at 32.4 eps, loss = 6.148\n",
      "    [batch 823]: seen 82300 examples at 32.5 eps, loss = 6.151\n",
      "    [batch 828]: seen 82800 examples at 32.6 eps, loss = 6.148\n",
      "    [batch 831]: seen 83100 examples at 32.5 eps, loss = 6.147\n",
      "    [batch 834]: seen 83400 examples at 32.5 eps, loss = 6.145\n",
      "    [batch 837]: seen 83700 examples at 32.4 eps, loss = 6.141\n",
      "    [batch 840]: seen 84000 examples at 32.4 eps, loss = 6.139\n",
      "    [batch 843]: seen 84300 examples at 32.4 eps, loss = 6.138\n",
      "    [batch 846]: seen 84600 examples at 32.3 eps, loss = 6.137\n",
      "    [batch 848]: seen 84800 examples at 32.2 eps, loss = 6.133\n",
      "    [batch 851]: seen 85100 examples at 32.2 eps, loss = 6.132\n",
      "    [batch 854]: seen 85400 examples at 32.2 eps, loss = 6.130\n",
      "    [batch 857]: seen 85700 examples at 32.1 eps, loss = 6.127\n",
      "    [batch 860]: seen 86000 examples at 32.1 eps, loss = 6.127\n",
      "    [batch 866]: seen 86600 examples at 32.2 eps, loss = 6.126\n",
      "    [batch 869]: seen 86900 examples at 32.1 eps, loss = 6.125\n",
      "    [batch 874]: seen 87400 examples at 32.2 eps, loss = 6.128\n",
      "    [batch 881]: seen 88100 examples at 32.3 eps, loss = 6.134\n",
      "    [batch 888]: seen 88800 examples at 32.5 eps, loss = 6.130\n",
      "    [batch 895]: seen 89500 examples at 32.6 eps, loss = 6.129\n",
      "    [batch 902]: seen 90200 examples at 32.7 eps, loss = 6.125\n",
      "    [batch 907]: seen 90700 examples at 32.8 eps, loss = 6.121\n",
      "    [batch 911]: seen 91100 examples at 32.8 eps, loss = 6.120\n",
      "    [batch 914]: seen 91400 examples at 32.7 eps, loss = 6.117\n",
      "    [batch 919]: seen 91900 examples at 32.7 eps, loss = 6.118\n",
      "    [batch 924]: seen 92400 examples at 32.8 eps, loss = 6.114\n",
      "    [batch 929]: seen 92900 examples at 32.9 eps, loss = 6.114\n",
      "    [batch 933]: seen 93300 examples at 32.9 eps, loss = 6.111\n",
      "    [batch 936]: seen 93600 examples at 32.8 eps, loss = 6.111\n",
      "    [batch 943]: seen 94300 examples at 33.0 eps, loss = 6.110\n",
      "    [batch 946]: seen 94600 examples at 32.9 eps, loss = 6.107\n",
      "    [batch 951]: seen 95100 examples at 33.0 eps, loss = 6.106\n",
      "    [batch 954]: seen 95400 examples at 32.9 eps, loss = 6.102\n",
      "    [batch 957]: seen 95700 examples at 32.9 eps, loss = 6.101\n",
      "    [batch 964]: seen 96400 examples at 33.0 eps, loss = 6.106\n",
      "    [batch 971]: seen 97100 examples at 33.1 eps, loss = 6.107\n",
      "    [batch 978]: seen 97800 examples at 33.2 eps, loss = 6.103\n",
      "    [batch 982]: seen 98200 examples at 33.2 eps, loss = 6.100\n",
      "    [batch 986]: seen 98600 examples at 33.2 eps, loss = 6.098\n",
      "    [batch 989]: seen 98900 examples at 33.2 eps, loss = 6.096\n",
      "    [batch 992]: seen 99200 examples at 33.1 eps, loss = 6.093\n",
      "    [batch 997]: seen 99700 examples at 33.1 eps, loss = 6.092\n",
      "    [batch 999]: seen 99900 examples at 33.1 eps, loss = 6.091\n",
      "    [batch 1002]: seen 100200 examples at 33.0 eps, loss = 6.090\n",
      "    [batch 1005]: seen 100500 examples at 33.0 eps, loss = 6.086\n",
      "    [batch 1008]: seen 100800 examples at 32.9 eps, loss = 6.085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1015]: seen 101500 examples at 33.1 eps, loss = 6.085\n",
      "    [batch 1018]: seen 101800 examples at 33.0 eps, loss = 6.082\n",
      "    [batch 1023]: seen 102300 examples at 33.1 eps, loss = 6.081\n",
      "    [batch 1027]: seen 102700 examples at 33.1 eps, loss = 6.080\n",
      "    [batch 1034]: seen 103400 examples at 33.2 eps, loss = 6.083\n",
      "    [batch 1041]: seen 104100 examples at 33.3 eps, loss = 6.080\n",
      "    [batch 1045]: seen 104500 examples at 33.3 eps, loss = 6.077\n",
      "    [batch 1050]: seen 105000 examples at 33.3 eps, loss = 6.075\n",
      "    [batch 1054]: seen 105400 examples at 33.3 eps, loss = 6.073\n",
      "    [batch 1057]: seen 105700 examples at 33.3 eps, loss = 6.068\n",
      "    [batch 1060]: seen 106000 examples at 33.2 eps, loss = 6.066\n",
      "    [batch 1063]: seen 106300 examples at 33.2 eps, loss = 6.065\n",
      "    [batch 1066]: seen 106600 examples at 33.2 eps, loss = 6.062\n",
      "    [batch 1069]: seen 106900 examples at 33.1 eps, loss = 6.060\n",
      "    [batch 1072]: seen 107200 examples at 33.1 eps, loss = 6.059\n",
      "    [batch 1079]: seen 107900 examples at 33.2 eps, loss = 6.058\n",
      "    [batch 1082]: seen 108200 examples at 33.1 eps, loss = 6.053\n",
      "    [batch 1087]: seen 108700 examples at 33.2 eps, loss = 6.055\n",
      "    [batch 1092]: seen 109200 examples at 33.2 eps, loss = 6.052\n",
      "    [batch 1095]: seen 109500 examples at 33.2 eps, loss = 6.050\n",
      "    [batch 1098]: seen 109800 examples at 33.1 eps, loss = 6.049\n",
      "    [batch 1103]: seen 110300 examples at 33.2 eps, loss = 6.048\n",
      "    [batch 1106]: seen 110600 examples at 33.2 eps, loss = 6.047\n",
      "    [batch 1109]: seen 110900 examples at 33.1 eps, loss = 6.046\n",
      "    [batch 1116]: seen 111600 examples at 33.2 eps, loss = 6.045\n",
      "    [batch 1119]: seen 111900 examples at 33.2 eps, loss = 6.042\n",
      "    [batch 1123]: seen 112300 examples at 33.2 eps, loss = 6.041\n",
      "    [batch 1128]: seen 112800 examples at 33.2 eps, loss = 6.039\n",
      "    [batch 1131]: seen 113100 examples at 33.2 eps, loss = 6.038\n",
      "    [batch 1136]: seen 113600 examples at 33.2 eps, loss = 6.037\n",
      "    [batch 1139]: seen 113900 examples at 33.2 eps, loss = 6.035\n",
      "    [batch 1144]: seen 114400 examples at 33.2 eps, loss = 6.036\n",
      "    [batch 1147]: seen 114700 examples at 33.2 eps, loss = 6.033\n",
      "    [batch 1150]: seen 115000 examples at 33.1 eps, loss = 6.032\n",
      "    [batch 1153]: seen 115300 examples at 33.1 eps, loss = 6.031\n",
      "    [batch 1158]: seen 115800 examples at 33.1 eps, loss = 6.028\n",
      "    [batch 1161]: seen 116100 examples at 33.1 eps, loss = 6.025\n",
      "    [batch 1165]: seen 116500 examples at 33.1 eps, loss = 6.022\n",
      "    [batch 1168]: seen 116800 examples at 33.1 eps, loss = 6.021\n",
      "    [batch 1171]: seen 117100 examples at 33.0 eps, loss = 6.018\n",
      "    [batch 1174]: seen 117400 examples at 33.0 eps, loss = 6.018\n",
      "    [batch 1177]: seen 117700 examples at 33.0 eps, loss = 6.017\n",
      "    [batch 1180]: seen 118000 examples at 33.0 eps, loss = 6.016\n",
      "    [batch 1183]: seen 118300 examples at 33.0 eps, loss = 6.012\n",
      "    [batch 1188]: seen 118800 examples at 33.0 eps, loss = 6.013\n",
      "    [batch 1195]: seen 119500 examples at 33.1 eps, loss = 6.020\n",
      "    [batch 1202]: seen 120200 examples at 33.2 eps, loss = 6.026\n",
      "    [batch 1209]: seen 120900 examples at 33.3 eps, loss = 6.023\n",
      "    [batch 1216]: seen 121600 examples at 33.4 eps, loss = 6.017\n",
      "    [batch 1223]: seen 122300 examples at 33.5 eps, loss = 6.012\n",
      "    [batch 1226]: seen 122600 examples at 33.4 eps, loss = 6.009\n",
      "    [batch 1229]: seen 122900 examples at 33.4 eps, loss = 6.007\n",
      "    [batch 1232]: seen 123200 examples at 33.4 eps, loss = 6.004\n",
      "    [batch 1235]: seen 123500 examples at 33.3 eps, loss = 6.000\n",
      "    [batch 1237]: seen 123700 examples at 33.3 eps, loss = 5.996\n",
      "    [batch 1242]: seen 124200 examples at 33.3 eps, loss = 5.993\n",
      "    [batch 1245]: seen 124500 examples at 33.2 eps, loss = 5.989\n",
      "    [batch 1248]: seen 124800 examples at 33.2 eps, loss = 5.985\n",
      "    [batch 1251]: seen 125100 examples at 33.2 eps, loss = 5.983\n",
      "    [batch 1254]: seen 125400 examples at 33.2 eps, loss = 5.982\n",
      "    [batch 1259]: seen 125900 examples at 33.2 eps, loss = 5.979\n",
      "    [batch 1264]: seen 126400 examples at 33.2 eps, loss = 5.982\n",
      "    [batch 1271]: seen 127100 examples at 33.3 eps, loss = 5.980\n",
      "    [batch 1274]: seen 127400 examples at 33.3 eps, loss = 5.976\n",
      "    [batch 1277]: seen 127700 examples at 33.3 eps, loss = 5.975\n",
      "    [batch 1281]: seen 128100 examples at 33.3 eps, loss = 5.973\n",
      "    [batch 1286]: seen 128600 examples at 33.3 eps, loss = 5.977\n",
      "    [batch 1293]: seen 129300 examples at 33.4 eps, loss = 5.975\n",
      "    [batch 1297]: seen 129700 examples at 33.4 eps, loss = 5.971\n",
      "    [batch 1301]: seen 130100 examples at 33.4 eps, loss = 5.968\n",
      "    [batch 1304]: seen 130400 examples at 33.4 eps, loss = 5.966\n",
      "    [batch 1307]: seen 130700 examples at 33.4 eps, loss = 5.963\n",
      "    [batch 1310]: seen 131000 examples at 33.3 eps, loss = 5.961\n",
      "    [batch 1312]: seen 131200 examples at 33.3 eps, loss = 5.958\n",
      "    [batch 1315]: seen 131500 examples at 33.3 eps, loss = 5.957\n",
      "    [batch 1318]: seen 131800 examples at 33.3 eps, loss = 5.955\n",
      "    [batch 1321]: seen 132100 examples at 33.2 eps, loss = 5.952\n",
      "    [batch 1324]: seen 132400 examples at 33.2 eps, loss = 5.950\n",
      "    [batch 1327]: seen 132700 examples at 33.1 eps, loss = 5.946\n",
      "    [batch 1330]: seen 133000 examples at 33.1 eps, loss = 5.946\n",
      "    [batch 1333]: seen 133300 examples at 33.1 eps, loss = 5.944\n",
      "    [batch 1336]: seen 133600 examples at 33.1 eps, loss = 5.943\n",
      "    [batch 1339]: seen 133900 examples at 33.1 eps, loss = 5.940\n",
      "    [batch 1342]: seen 134200 examples at 33.0 eps, loss = 5.937\n",
      "    [batch 1345]: seen 134500 examples at 33.0 eps, loss = 5.936\n",
      "    [batch 1348]: seen 134800 examples at 33.0 eps, loss = 5.934\n",
      "    [batch 1353]: seen 135300 examples at 33.0 eps, loss = 5.936\n",
      "    [batch 1360]: seen 136000 examples at 33.1 eps, loss = 5.933\n",
      "    [batch 1363]: seen 136300 examples at 33.1 eps, loss = 5.931\n",
      "    [batch 1365]: seen 136500 examples at 33.0 eps, loss = 5.929\n",
      "    [batch 1369]: seen 136900 examples at 33.0 eps, loss = 5.929\n",
      "    [batch 1372]: seen 137200 examples at 33.0 eps, loss = 5.925\n",
      "    [batch 1377]: seen 137700 examples at 33.0 eps, loss = 5.925\n",
      "    [batch 1382]: seen 138200 examples at 33.1 eps, loss = 5.923\n",
      "    [batch 1386]: seen 138600 examples at 33.1 eps, loss = 5.921\n",
      "    [batch 1393]: seen 139300 examples at 33.2 eps, loss = 5.923\n",
      "    [batch 1397]: seen 139700 examples at 33.1 eps, loss = 5.921\n",
      "    [batch 1400]: seen 140000 examples at 33.1 eps, loss = 5.917\n",
      "    [batch 1403]: seen 140300 examples at 33.1 eps, loss = 5.913\n",
      "    [batch 1406]: seen 140600 examples at 33.1 eps, loss = 5.911\n",
      "    [batch 1413]: seen 141300 examples at 33.2 eps, loss = 5.915\n",
      "    [batch 1420]: seen 142000 examples at 33.2 eps, loss = 5.914\n",
      "    [batch 1425]: seen 142500 examples at 33.3 eps, loss = 5.910\n",
      "    [batch 1428]: seen 142800 examples at 33.3 eps, loss = 5.908\n",
      "    [batch 1431]: seen 143100 examples at 33.2 eps, loss = 5.905\n",
      "    [batch 1434]: seen 143400 examples at 33.2 eps, loss = 5.904\n",
      "    [batch 1439]: seen 143900 examples at 33.2 eps, loss = 5.903\n",
      "    [batch 1442]: seen 144200 examples at 33.2 eps, loss = 5.901\n",
      "    [batch 1445]: seen 144500 examples at 33.2 eps, loss = 5.897\n",
      "    [batch 1450]: seen 145000 examples at 33.2 eps, loss = 5.897\n",
      "    [batch 1453]: seen 145300 examples at 33.2 eps, loss = 5.894\n",
      "    [batch 1458]: seen 145800 examples at 33.2 eps, loss = 5.893\n",
      "    [batch 1463]: seen 146300 examples at 33.2 eps, loss = 5.893\n",
      "    [batch 1466]: seen 146600 examples at 33.2 eps, loss = 5.892\n",
      "    [batch 1470]: seen 147000 examples at 33.2 eps, loss = 5.888\n",
      "    [batch 1473]: seen 147300 examples at 33.2 eps, loss = 5.886\n",
      "    [batch 1480]: seen 148000 examples at 33.3 eps, loss = 5.893\n",
      "    [batch 1487]: seen 148700 examples at 33.4 eps, loss = 5.887\n",
      "    [batch 1491]: seen 149100 examples at 33.4 eps, loss = 5.882\n",
      "    [batch 1496]: seen 149600 examples at 33.4 eps, loss = 5.881\n",
      "    [batch 1501]: seen 150100 examples at 33.4 eps, loss = 5.880\n",
      "    [batch 1504]: seen 150400 examples at 33.4 eps, loss = 5.878\n",
      "    [batch 1507]: seen 150700 examples at 33.4 eps, loss = 5.876\n",
      "    [batch 1510]: seen 151000 examples at 33.3 eps, loss = 5.871\n",
      "    [batch 1515]: seen 151500 examples at 33.4 eps, loss = 5.872\n",
      "    [batch 1518]: seen 151800 examples at 33.3 eps, loss = 5.867\n",
      "    [batch 1521]: seen 152100 examples at 33.3 eps, loss = 5.863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1524]: seen 152400 examples at 33.3 eps, loss = 5.860\n",
      "    [batch 1527]: seen 152700 examples at 33.2 eps, loss = 5.855\n",
      "    [batch 1530]: seen 153000 examples at 33.2 eps, loss = 5.851\n",
      "    [batch 1533]: seen 153300 examples at 33.2 eps, loss = 5.846\n",
      "    [batch 1536]: seen 153600 examples at 33.1 eps, loss = 5.844\n",
      "    [batch 1539]: seen 153900 examples at 33.1 eps, loss = 5.839\n",
      "    [batch 1542]: seen 154200 examples at 33.1 eps, loss = 5.834\n",
      "    [batch 1545]: seen 154500 examples at 33.0 eps, loss = 5.830\n",
      "    [batch 1548]: seen 154800 examples at 33.0 eps, loss = 5.828\n",
      "    [batch 1551]: seen 155100 examples at 33.0 eps, loss = 5.823\n",
      "    [batch 1555]: seen 155500 examples at 33.0 eps, loss = 5.822\n",
      "    [batch 1558]: seen 155800 examples at 33.0 eps, loss = 5.818\n",
      "    [batch 1561]: seen 156100 examples at 32.9 eps, loss = 5.813\n",
      "    [batch 1565]: seen 156500 examples at 32.9 eps, loss = 5.811\n",
      "    [batch 1568]: seen 156800 examples at 32.9 eps, loss = 5.809\n",
      "    [batch 1570]: seen 157000 examples at 32.9 eps, loss = 5.805\n",
      "    [batch 1573]: seen 157300 examples at 32.8 eps, loss = 5.800\n",
      "    [batch 1577]: seen 157700 examples at 32.8 eps, loss = 5.798\n",
      "    [batch 1580]: seen 158000 examples at 32.8 eps, loss = 5.795\n",
      "    [batch 1583]: seen 158300 examples at 32.8 eps, loss = 5.793\n",
      "    [batch 1588]: seen 158800 examples at 32.8 eps, loss = 5.792\n",
      "    [batch 1591]: seen 159100 examples at 32.8 eps, loss = 5.785\n",
      "    [batch 1595]: seen 159500 examples at 32.8 eps, loss = 5.783\n",
      "    [batch 1598]: seen 159800 examples at 32.8 eps, loss = 5.778\n",
      "    [batch 1601]: seen 160100 examples at 32.7 eps, loss = 5.773\n",
      "    [batch 1604]: seen 160400 examples at 32.7 eps, loss = 5.769\n",
      "    [batch 1607]: seen 160700 examples at 32.7 eps, loss = 5.766\n",
      "    [batch 1612]: seen 161200 examples at 32.7 eps, loss = 5.763\n",
      "    [batch 1615]: seen 161500 examples at 32.7 eps, loss = 5.759\n",
      "    [batch 1618]: seen 161800 examples at 32.6 eps, loss = 5.754\n",
      "    [batch 1621]: seen 162100 examples at 32.6 eps, loss = 5.752\n",
      "    [batch 1624]: seen 162400 examples at 32.6 eps, loss = 5.746\n",
      "    [batch 1627]: seen 162700 examples at 32.5 eps, loss = 5.740\n",
      "    [batch 1630]: seen 163000 examples at 32.5 eps, loss = 5.738\n",
      "    [batch 1633]: seen 163300 examples at 32.5 eps, loss = 5.736\n",
      "    [batch 1636]: seen 163600 examples at 32.5 eps, loss = 5.733\n",
      "    [batch 1643]: seen 164300 examples at 32.5 eps, loss = 5.737\n",
      "    [batch 1650]: seen 165000 examples at 32.6 eps, loss = 5.735\n",
      "    [batch 1653]: seen 165300 examples at 32.6 eps, loss = 5.731\n",
      "    [batch 1656]: seen 165600 examples at 32.6 eps, loss = 5.728\n",
      "    [batch 1659]: seen 165900 examples at 32.6 eps, loss = 5.726\n",
      "    [batch 1663]: seen 166300 examples at 32.6 eps, loss = 5.723\n",
      "    [batch 1666]: seen 166600 examples at 32.6 eps, loss = 5.721\n",
      "    [batch 1671]: seen 167100 examples at 32.6 eps, loss = 5.720\n",
      "    [batch 1674]: seen 167400 examples at 32.6 eps, loss = 5.718\n",
      "    [batch 1677]: seen 167700 examples at 32.5 eps, loss = 5.713\n",
      "    [batch 1679]: seen 167900 examples at 32.5 eps, loss = 5.708\n",
      "    [batch 1684]: seen 168400 examples at 32.5 eps, loss = 5.708\n",
      "    [batch 1687]: seen 168700 examples at 32.5 eps, loss = 5.701\n",
      "    [batch 1690]: seen 169000 examples at 32.5 eps, loss = 5.696\n",
      "    [batch 1695]: seen 169500 examples at 32.6 eps, loss = 5.695\n",
      "    [batch 1698]: seen 169800 examples at 32.5 eps, loss = 5.693\n",
      "    [batch 1701]: seen 170100 examples at 32.5 eps, loss = 5.690\n",
      "    [batch 1704]: seen 170400 examples at 32.5 eps, loss = 5.683\n",
      "    [batch 1707]: seen 170700 examples at 32.5 eps, loss = 5.681\n",
      "    [batch 1710]: seen 171000 examples at 32.5 eps, loss = 5.679\n",
      "    [batch 1714]: seen 171400 examples at 32.5 eps, loss = 5.678\n",
      "    [batch 1717]: seen 171700 examples at 32.4 eps, loss = 5.676\n",
      "    [batch 1720]: seen 172000 examples at 32.4 eps, loss = 5.671\n",
      "    [batch 1723]: seen 172300 examples at 32.4 eps, loss = 5.667\n",
      "    [batch 1726]: seen 172600 examples at 32.4 eps, loss = 5.658\n",
      "    [batch 1729]: seen 172900 examples at 32.4 eps, loss = 5.657\n",
      "    [batch 1731]: seen 173100 examples at 32.3 eps, loss = 5.654\n",
      "    [batch 1734]: seen 173400 examples at 32.3 eps, loss = 5.652\n",
      "    [batch 1737]: seen 173700 examples at 32.3 eps, loss = 5.648\n",
      "    [batch 1742]: seen 174200 examples at 32.3 eps, loss = 5.647\n",
      "    [batch 1745]: seen 174500 examples at 32.3 eps, loss = 5.644\n",
      "    [batch 1748]: seen 174800 examples at 32.2 eps, loss = 5.640\n",
      "    [batch 1752]: seen 175200 examples at 32.2 eps, loss = 5.639\n",
      "    [batch 1755]: seen 175500 examples at 32.2 eps, loss = 5.637\n",
      "    [batch 1761]: seen 176100 examples at 32.3 eps, loss = 5.635\n",
      "    [batch 1764]: seen 176400 examples at 32.2 eps, loss = 5.633\n",
      "    [batch 1767]: seen 176700 examples at 32.2 eps, loss = 5.630\n",
      "    [batch 1772]: seen 177200 examples at 32.3 eps, loss = 5.628\n",
      "    [batch 1776]: seen 177600 examples at 32.3 eps, loss = 5.623\n",
      "    [batch 1780]: seen 178000 examples at 32.3 eps, loss = 5.619\n",
      "    [batch 1783]: seen 178300 examples at 32.3 eps, loss = 5.618\n",
      "    [batch 1786]: seen 178600 examples at 32.2 eps, loss = 5.615\n",
      "    [batch 1788]: seen 178800 examples at 32.2 eps, loss = 5.610\n",
      "    [batch 1791]: seen 179100 examples at 32.2 eps, loss = 5.610\n",
      "    [batch 1794]: seen 179400 examples at 32.2 eps, loss = 5.608\n",
      "    [batch 1797]: seen 179700 examples at 32.2 eps, loss = 5.603\n",
      "    [batch 1800]: seen 180000 examples at 32.2 eps, loss = 5.599\n",
      "    [batch 1803]: seen 180300 examples at 32.2 eps, loss = 5.598\n",
      "    [batch 1806]: seen 180600 examples at 32.1 eps, loss = 5.598\n",
      "    [batch 1809]: seen 180900 examples at 32.1 eps, loss = 5.596\n",
      "    [batch 1814]: seen 181400 examples at 32.2 eps, loss = 5.598\n",
      "    [batch 1818]: seen 181800 examples at 32.2 eps, loss = 5.594\n",
      "    [batch 1821]: seen 182100 examples at 32.1 eps, loss = 5.593\n",
      "    [batch 1824]: seen 182400 examples at 32.1 eps, loss = 5.588\n",
      "    [batch 1827]: seen 182700 examples at 32.1 eps, loss = 5.584\n",
      "    [batch 1832]: seen 183200 examples at 32.1 eps, loss = 5.583\n",
      "    [batch 1835]: seen 183500 examples at 32.1 eps, loss = 5.580\n",
      "    [batch 1838]: seen 183800 examples at 32.1 eps, loss = 5.578\n",
      "    [batch 1842]: seen 184200 examples at 32.1 eps, loss = 5.574\n",
      "    [batch 1844]: seen 184400 examples at 32.1 eps, loss = 5.571\n",
      "    [batch 1847]: seen 184700 examples at 32.1 eps, loss = 5.567\n",
      "    [batch 1854]: seen 185400 examples at 32.1 eps, loss = 5.567\n",
      "    [batch 1860]: seen 186000 examples at 32.2 eps, loss = 5.565\n",
      "    [batch 1863]: seen 186300 examples at 32.1 eps, loss = 5.561\n",
      "    [batch 1866]: seen 186600 examples at 32.1 eps, loss = 5.555\n",
      "    [batch 1869]: seen 186900 examples at 32.1 eps, loss = 5.551\n",
      "    [END] Training complete: Total examples : 187100; Total time: 1:37:17\n",
      "[EPOCH 2] Complete. Avg Loss: 5.548880518911135; Best Loss: 5.548880518911135\n",
      "[EPOCH 3] Starting training..\n",
      "    [batch 2]: seen 200 examples at 19.6 eps, loss = 5.546\n",
      "    [batch 5]: seen 500 examples at 20.4 eps, loss = 5.543\n",
      "    [batch 8]: seen 800 examples at 22.5 eps, loss = 5.543\n",
      "    [batch 11]: seen 1100 examples at 23.7 eps, loss = 5.541\n",
      "    [batch 14]: seen 1400 examples at 24.4 eps, loss = 5.539\n",
      "    [batch 17]: seen 1700 examples at 23.7 eps, loss = 5.535\n",
      "    [batch 20]: seen 2000 examples at 23.3 eps, loss = 5.533\n",
      "    [batch 22]: seen 2200 examples at 22.9 eps, loss = 5.531\n",
      "    [batch 25]: seen 2500 examples at 22.6 eps, loss = 5.526\n",
      "    [batch 28]: seen 2800 examples at 22.4 eps, loss = 5.523\n",
      "    [batch 32]: seen 3200 examples at 23.3 eps, loss = 5.518\n",
      "    [batch 35]: seen 3500 examples at 23.6 eps, loss = 5.515\n",
      "    [batch 38]: seen 3800 examples at 23.4 eps, loss = 5.509\n",
      "    [batch 43]: seen 4300 examples at 24.9 eps, loss = 5.515\n",
      "    [batch 48]: seen 4800 examples at 26.1 eps, loss = 5.508\n",
      "    [batch 50]: seen 5000 examples at 25.8 eps, loss = 5.502\n",
      "    [batch 53]: seen 5300 examples at 25.5 eps, loss = 5.498\n",
      "    [batch 56]: seen 5600 examples at 25.2 eps, loss = 5.493\n",
      "    [batch 61]: seen 6100 examples at 25.8 eps, loss = 5.490\n",
      "    [batch 64]: seen 6400 examples at 25.9 eps, loss = 5.484\n",
      "    [batch 68]: seen 6800 examples at 26.2 eps, loss = 5.483\n",
      "    [batch 71]: seen 7100 examples at 25.9 eps, loss = 5.477\n",
      "    [batch 74]: seen 7400 examples at 25.9 eps, loss = 5.475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 77]: seen 7700 examples at 25.7 eps, loss = 5.466\n",
      "    [batch 80]: seen 8000 examples at 25.5 eps, loss = 5.462\n",
      "    [batch 85]: seen 8500 examples at 26.2 eps, loss = 5.461\n",
      "    [batch 88]: seen 8800 examples at 26.2 eps, loss = 5.458\n",
      "    [batch 91]: seen 9100 examples at 26.2 eps, loss = 5.456\n",
      "    [batch 94]: seen 9400 examples at 26.3 eps, loss = 5.454\n",
      "    [batch 99]: seen 9900 examples at 26.6 eps, loss = 5.451\n",
      "    [batch 101]: seen 10100 examples at 26.5 eps, loss = 5.448\n",
      "    [batch 104]: seen 10400 examples at 26.2 eps, loss = 5.444\n",
      "    [batch 107]: seen 10700 examples at 26.3 eps, loss = 5.441\n",
      "    [batch 110]: seen 11000 examples at 26.1 eps, loss = 5.436\n",
      "    [batch 113]: seen 11300 examples at 26.1 eps, loss = 5.434\n",
      "    [batch 116]: seen 11600 examples at 26.2 eps, loss = 5.431\n",
      "    [batch 121]: seen 12100 examples at 26.7 eps, loss = 5.433\n",
      "    [batch 128]: seen 12800 examples at 27.6 eps, loss = 5.434\n",
      "    [batch 132]: seen 13200 examples at 27.7 eps, loss = 5.427\n",
      "    [batch 137]: seen 13700 examples at 27.9 eps, loss = 5.426\n",
      "    [batch 140]: seen 14000 examples at 27.7 eps, loss = 5.423\n",
      "    [batch 147]: seen 14700 examples at 28.5 eps, loss = 5.424\n",
      "    [batch 153]: seen 15300 examples at 29.0 eps, loss = 5.423\n",
      "    [batch 156]: seen 15600 examples at 29.0 eps, loss = 5.421\n",
      "    [batch 159]: seen 15900 examples at 28.8 eps, loss = 5.416\n",
      "    [batch 162]: seen 16200 examples at 28.7 eps, loss = 5.410\n",
      "    [batch 167]: seen 16700 examples at 29.1 eps, loss = 5.414\n",
      "    [batch 172]: seen 17200 examples at 29.4 eps, loss = 5.409\n",
      "    [batch 175]: seen 17500 examples at 29.2 eps, loss = 5.405\n",
      "    [batch 178]: seen 17800 examples at 29.1 eps, loss = 5.404\n",
      "    [batch 181]: seen 18100 examples at 29.1 eps, loss = 5.404\n",
      "    [batch 184]: seen 18400 examples at 28.9 eps, loss = 5.400\n",
      "    [batch 187]: seen 18700 examples at 28.8 eps, loss = 5.395\n",
      "    [batch 191]: seen 19100 examples at 28.8 eps, loss = 5.391\n",
      "    [batch 194]: seen 19400 examples at 28.8 eps, loss = 5.388\n",
      "    [batch 198]: seen 19800 examples at 28.9 eps, loss = 5.382\n",
      "    [batch 203]: seen 20300 examples at 29.1 eps, loss = 5.384\n",
      "    [batch 210]: seen 21000 examples at 29.7 eps, loss = 5.382\n",
      "    [batch 212]: seen 21200 examples at 29.6 eps, loss = 5.378\n",
      "    [batch 215]: seen 21500 examples at 29.4 eps, loss = 5.374\n",
      "    [batch 218]: seen 21800 examples at 29.2 eps, loss = 5.367\n",
      "    [batch 221]: seen 22100 examples at 29.2 eps, loss = 5.365\n",
      "    [batch 226]: seen 22600 examples at 29.3 eps, loss = 5.363\n",
      "    [batch 229]: seen 22900 examples at 29.2 eps, loss = 5.357\n",
      "    [batch 232]: seen 23200 examples at 29.2 eps, loss = 5.354\n",
      "    [batch 234]: seen 23400 examples at 29.0 eps, loss = 5.352\n",
      "    [batch 237]: seen 23700 examples at 29.0 eps, loss = 5.350\n",
      "    [batch 240]: seen 24000 examples at 29.0 eps, loss = 5.348\n",
      "    [batch 243]: seen 24300 examples at 28.8 eps, loss = 5.340\n",
      "    [batch 250]: seen 25000 examples at 29.3 eps, loss = 5.341\n",
      "    [batch 254]: seen 25400 examples at 29.4 eps, loss = 5.338\n",
      "    [batch 258]: seen 25800 examples at 29.4 eps, loss = 5.336\n",
      "    [batch 263]: seen 26300 examples at 29.6 eps, loss = 5.337\n",
      "    [batch 269]: seen 26900 examples at 29.9 eps, loss = 5.335\n",
      "    [batch 272]: seen 27200 examples at 29.7 eps, loss = 5.332\n",
      "    [batch 279]: seen 27900 examples at 30.2 eps, loss = 5.333\n",
      "    [batch 281]: seen 28100 examples at 30.1 eps, loss = 5.332\n",
      "    [batch 285]: seen 28500 examples at 30.1 eps, loss = 5.329\n",
      "    [batch 288]: seen 28800 examples at 30.0 eps, loss = 5.322\n",
      "    [batch 292]: seen 29200 examples at 30.0 eps, loss = 5.321\n",
      "    [batch 295]: seen 29500 examples at 30.0 eps, loss = 5.318\n",
      "    [batch 298]: seen 29800 examples at 29.9 eps, loss = 5.315\n",
      "    [batch 304]: seen 30400 examples at 30.2 eps, loss = 5.314\n",
      "    [batch 307]: seen 30700 examples at 30.0 eps, loss = 5.310\n",
      "    [batch 309]: seen 30900 examples at 29.9 eps, loss = 5.306\n",
      "    [batch 312]: seen 31200 examples at 29.8 eps, loss = 5.305\n",
      "    [batch 317]: seen 31700 examples at 29.9 eps, loss = 5.303\n",
      "    [batch 320]: seen 32000 examples at 29.9 eps, loss = 5.299\n",
      "    [batch 324]: seen 32400 examples at 29.9 eps, loss = 5.297\n",
      "    [batch 327]: seen 32700 examples at 29.9 eps, loss = 5.296\n",
      "    [batch 330]: seen 33000 examples at 29.7 eps, loss = 5.291\n",
      "    [batch 333]: seen 33300 examples at 29.6 eps, loss = 5.289\n",
      "    [batch 338]: seen 33800 examples at 29.8 eps, loss = 5.286\n",
      "    [batch 341]: seen 34100 examples at 29.7 eps, loss = 5.283\n",
      "    [batch 344]: seen 34400 examples at 29.6 eps, loss = 5.280\n",
      "    [batch 347]: seen 34700 examples at 29.5 eps, loss = 5.274\n",
      "    [batch 350]: seen 35000 examples at 29.4 eps, loss = 5.276\n",
      "    [batch 355]: seen 35500 examples at 29.5 eps, loss = 5.271\n",
      "    [batch 358]: seen 35800 examples at 29.4 eps, loss = 5.265\n",
      "    [batch 362]: seen 36200 examples at 29.4 eps, loss = 5.261\n",
      "    [batch 365]: seen 36500 examples at 29.3 eps, loss = 5.258\n",
      "    [batch 368]: seen 36800 examples at 29.2 eps, loss = 5.253\n",
      "    [batch 375]: seen 37500 examples at 29.6 eps, loss = 5.262\n",
      "    [batch 382]: seen 38200 examples at 29.9 eps, loss = 5.262\n",
      "    [batch 389]: seen 38900 examples at 30.2 eps, loss = 5.259\n",
      "    [batch 394]: seen 39400 examples at 30.2 eps, loss = 5.251\n",
      "    [batch 397]: seen 39700 examples at 30.2 eps, loss = 5.251\n",
      "    [batch 400]: seen 40000 examples at 30.2 eps, loss = 5.249\n",
      "    [batch 403]: seen 40300 examples at 30.2 eps, loss = 5.245\n",
      "    [batch 405]: seen 40500 examples at 30.1 eps, loss = 5.244\n",
      "    [batch 408]: seen 40800 examples at 30.1 eps, loss = 5.243\n",
      "    [batch 413]: seen 41300 examples at 30.1 eps, loss = 5.241\n",
      "    [batch 416]: seen 41600 examples at 30.0 eps, loss = 5.235\n",
      "    [batch 419]: seen 41900 examples at 30.0 eps, loss = 5.234\n",
      "    [batch 424]: seen 42400 examples at 30.1 eps, loss = 5.233\n",
      "    [batch 427]: seen 42700 examples at 30.0 eps, loss = 5.228\n",
      "    [batch 430]: seen 43000 examples at 30.0 eps, loss = 5.232\n",
      "    [batch 437]: seen 43700 examples at 30.3 eps, loss = 5.228\n",
      "    [batch 440]: seen 44000 examples at 30.3 eps, loss = 5.222\n",
      "    [batch 443]: seen 44300 examples at 30.2 eps, loss = 5.214\n",
      "    [batch 446]: seen 44600 examples at 30.1 eps, loss = 5.211\n",
      "    [batch 449]: seen 44900 examples at 30.1 eps, loss = 5.207\n",
      "    [batch 456]: seen 45600 examples at 30.3 eps, loss = 5.210\n",
      "    [batch 459]: seen 45900 examples at 30.2 eps, loss = 5.202\n",
      "    [batch 462]: seen 46200 examples at 30.2 eps, loss = 5.197\n",
      "    [batch 468]: seen 46800 examples at 30.3 eps, loss = 5.195\n",
      "    [batch 470]: seen 47000 examples at 30.2 eps, loss = 5.193\n",
      "    [batch 474]: seen 47400 examples at 30.3 eps, loss = 5.191\n",
      "    [batch 477]: seen 47700 examples at 30.2 eps, loss = 5.183\n",
      "    [batch 480]: seen 48000 examples at 30.2 eps, loss = 5.181\n",
      "    [batch 483]: seen 48300 examples at 30.1 eps, loss = 5.178\n",
      "    [batch 487]: seen 48700 examples at 30.1 eps, loss = 5.175\n",
      "    [batch 490]: seen 49000 examples at 30.1 eps, loss = 5.173\n",
      "    [batch 493]: seen 49300 examples at 30.0 eps, loss = 5.170\n",
      "    [batch 496]: seen 49600 examples at 30.0 eps, loss = 5.167\n",
      "    [batch 499]: seen 49900 examples at 29.9 eps, loss = 5.160\n",
      "    [batch 506]: seen 50600 examples at 30.2 eps, loss = 5.160\n",
      "    [batch 513]: seen 51300 examples at 30.4 eps, loss = 5.159\n",
      "    [batch 516]: seen 51600 examples at 30.3 eps, loss = 5.157\n",
      "    [batch 519]: seen 51900 examples at 30.3 eps, loss = 5.154\n",
      "    [batch 526]: seen 52600 examples at 30.5 eps, loss = 5.162\n",
      "    [batch 533]: seen 53300 examples at 30.8 eps, loss = 5.156\n",
      "    [batch 536]: seen 53600 examples at 30.7 eps, loss = 5.151\n",
      "    [batch 540]: seen 54000 examples at 30.8 eps, loss = 5.150\n",
      "    [batch 543]: seen 54300 examples at 30.7 eps, loss = 5.144\n",
      "    [batch 545]: seen 54500 examples at 30.6 eps, loss = 5.141\n",
      "    [batch 551]: seen 55100 examples at 30.7 eps, loss = 5.141\n",
      "    [batch 556]: seen 55600 examples at 30.8 eps, loss = 5.144\n",
      "    [batch 563]: seen 56300 examples at 31.1 eps, loss = 5.141\n",
      "    [batch 566]: seen 56600 examples at 31.0 eps, loss = 5.137\n",
      "    [batch 569]: seen 56900 examples at 30.9 eps, loss = 5.130\n",
      "    [batch 574]: seen 57400 examples at 30.9 eps, loss = 5.129\n",
      "    [batch 577]: seen 57700 examples at 30.9 eps, loss = 5.125\n",
      "    [batch 580]: seen 58000 examples at 30.8 eps, loss = 5.121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 583]: seen 58300 examples at 30.8 eps, loss = 5.118\n",
      "    [batch 589]: seen 58900 examples at 30.9 eps, loss = 5.117\n",
      "    [batch 592]: seen 59200 examples at 30.9 eps, loss = 5.114\n",
      "    [batch 595]: seen 59500 examples at 30.8 eps, loss = 5.107\n",
      "    [batch 598]: seen 59800 examples at 30.7 eps, loss = 5.103\n",
      "    [batch 605]: seen 60500 examples at 30.9 eps, loss = 5.102\n",
      "    [batch 609]: seen 60900 examples at 30.9 eps, loss = 5.098\n",
      "    [batch 613]: seen 61300 examples at 30.9 eps, loss = 5.097\n",
      "    [batch 618]: seen 61800 examples at 31.0 eps, loss = 5.095\n",
      "    [batch 622]: seen 62200 examples at 31.0 eps, loss = 5.093\n",
      "    [batch 625]: seen 62500 examples at 30.9 eps, loss = 5.092\n",
      "    [batch 630]: seen 63000 examples at 31.0 eps, loss = 5.090\n",
      "    [batch 635]: seen 63500 examples at 31.1 eps, loss = 5.091\n",
      "    [batch 640]: seen 64000 examples at 31.1 eps, loss = 5.089\n",
      "    [batch 647]: seen 64700 examples at 31.3 eps, loss = 5.094\n",
      "    [batch 654]: seen 65400 examples at 31.5 eps, loss = 5.098\n",
      "    [batch 661]: seen 66100 examples at 31.7 eps, loss = 5.092\n",
      "    [batch 668]: seen 66800 examples at 31.9 eps, loss = 5.090\n",
      "    [batch 672]: seen 67200 examples at 31.9 eps, loss = 5.085\n",
      "    [batch 677]: seen 67700 examples at 32.0 eps, loss = 5.082\n",
      "    [batch 681]: seen 68100 examples at 32.0 eps, loss = 5.078\n",
      "    [batch 684]: seen 68400 examples at 31.9 eps, loss = 5.075\n",
      "    [batch 686]: seen 68600 examples at 31.8 eps, loss = 5.071\n",
      "    [batch 689]: seen 68900 examples at 31.8 eps, loss = 5.065\n",
      "    [batch 694]: seen 69400 examples at 31.8 eps, loss = 5.064\n",
      "    [batch 701]: seen 70100 examples at 32.0 eps, loss = 5.065\n",
      "    [batch 704]: seen 70400 examples at 31.9 eps, loss = 5.061\n",
      "    [batch 708]: seen 70800 examples at 31.9 eps, loss = 5.059\n",
      "    [batch 712]: seen 71200 examples at 31.9 eps, loss = 5.058\n",
      "    [batch 715]: seen 71500 examples at 31.9 eps, loss = 5.052\n",
      "    [batch 718]: seen 71800 examples at 31.8 eps, loss = 5.052\n",
      "    [batch 720]: seen 72000 examples at 31.8 eps, loss = 5.050\n",
      "    [batch 725]: seen 72500 examples at 31.9 eps, loss = 5.056\n",
      "    [batch 732]: seen 73200 examples at 32.0 eps, loss = 5.056\n",
      "    [batch 739]: seen 73900 examples at 32.2 eps, loss = 5.053\n",
      "    [batch 745]: seen 74500 examples at 32.3 eps, loss = 5.049\n",
      "    [batch 750]: seen 75000 examples at 32.3 eps, loss = 5.048\n",
      "    [batch 753]: seen 75300 examples at 32.3 eps, loss = 5.045\n",
      "    [batch 756]: seen 75600 examples at 32.2 eps, loss = 5.042\n",
      "    [batch 759]: seen 75900 examples at 32.2 eps, loss = 5.039\n",
      "    [batch 763]: seen 76300 examples at 32.2 eps, loss = 5.036\n",
      "    [batch 766]: seen 76600 examples at 32.2 eps, loss = 5.033\n",
      "    [batch 768]: seen 76800 examples at 32.1 eps, loss = 5.031\n",
      "    [batch 773]: seen 77300 examples at 32.1 eps, loss = 5.028\n",
      "    [batch 776]: seen 77600 examples at 32.1 eps, loss = 5.023\n",
      "    [batch 779]: seen 77900 examples at 32.0 eps, loss = 5.019\n",
      "    [batch 783]: seen 78300 examples at 32.0 eps, loss = 5.019\n",
      "    [batch 786]: seen 78600 examples at 32.0 eps, loss = 5.016\n",
      "    [batch 791]: seen 79100 examples at 32.1 eps, loss = 5.016\n",
      "    [batch 794]: seen 79400 examples at 32.1 eps, loss = 5.015\n",
      "    [batch 799]: seen 79900 examples at 32.1 eps, loss = 5.013\n",
      "    [batch 804]: seen 80400 examples at 32.1 eps, loss = 5.011\n",
      "    [batch 807]: seen 80700 examples at 32.1 eps, loss = 5.006\n",
      "    [batch 810]: seen 81000 examples at 32.0 eps, loss = 5.004\n",
      "    [batch 814]: seen 81400 examples at 32.0 eps, loss = 5.003\n",
      "    [batch 819]: seen 81900 examples at 32.1 eps, loss = 5.003\n",
      "    [batch 824]: seen 82400 examples at 32.1 eps, loss = 5.001\n",
      "    [batch 829]: seen 82900 examples at 32.2 eps, loss = 5.005\n",
      "    [batch 834]: seen 83400 examples at 32.2 eps, loss = 4.998\n",
      "    [batch 839]: seen 83900 examples at 32.2 eps, loss = 4.998\n",
      "    [batch 841]: seen 84100 examples at 32.2 eps, loss = 4.996\n",
      "    [batch 844]: seen 84400 examples at 32.2 eps, loss = 4.995\n",
      "    [batch 847]: seen 84700 examples at 32.2 eps, loss = 4.997\n",
      "    [batch 854]: seen 85400 examples at 32.3 eps, loss = 4.995\n",
      "    [batch 859]: seen 85900 examples at 32.4 eps, loss = 4.993\n",
      "    [batch 862]: seen 86200 examples at 32.3 eps, loss = 4.991\n",
      "    [batch 865]: seen 86500 examples at 32.3 eps, loss = 4.988\n",
      "    [batch 870]: seen 87000 examples at 32.4 eps, loss = 4.988\n",
      "    [batch 875]: seen 87500 examples at 32.4 eps, loss = 4.987\n",
      "    [batch 882]: seen 88200 examples at 32.6 eps, loss = 4.987\n",
      "    [batch 888]: seen 88800 examples at 32.6 eps, loss = 4.985\n",
      "    [batch 892]: seen 89200 examples at 32.6 eps, loss = 4.984\n",
      "    [batch 895]: seen 89500 examples at 32.6 eps, loss = 4.980\n",
      "    [batch 897]: seen 89700 examples at 32.6 eps, loss = 4.972\n",
      "    [batch 900]: seen 90000 examples at 32.6 eps, loss = 4.966\n",
      "    [batch 903]: seen 90300 examples at 32.5 eps, loss = 4.964\n",
      "    [batch 906]: seen 90600 examples at 32.4 eps, loss = 4.959\n",
      "    [batch 911]: seen 91100 examples at 32.5 eps, loss = 4.960\n",
      "    [batch 916]: seen 91600 examples at 32.5 eps, loss = 4.955\n",
      "    [batch 919]: seen 91900 examples at 32.5 eps, loss = 4.952\n",
      "    [batch 921]: seen 92100 examples at 32.4 eps, loss = 4.947\n",
      "    [batch 924]: seen 92400 examples at 32.4 eps, loss = 4.944\n",
      "    [batch 927]: seen 92700 examples at 32.3 eps, loss = 4.940\n",
      "    [batch 932]: seen 93200 examples at 32.3 eps, loss = 4.939\n",
      "    [batch 935]: seen 93500 examples at 32.3 eps, loss = 4.935\n",
      "    [batch 938]: seen 93800 examples at 32.3 eps, loss = 4.931\n",
      "    [batch 942]: seen 94200 examples at 32.3 eps, loss = 4.930\n",
      "    [batch 949]: seen 94900 examples at 32.4 eps, loss = 4.928\n",
      "    [batch 951]: seen 95100 examples at 32.3 eps, loss = 4.924\n",
      "    [batch 956]: seen 95600 examples at 32.4 eps, loss = 4.925\n",
      "    [batch 959]: seen 95900 examples at 32.4 eps, loss = 4.918\n",
      "    [batch 963]: seen 96300 examples at 32.4 eps, loss = 4.916\n",
      "    [batch 966]: seen 96600 examples at 32.3 eps, loss = 4.911\n",
      "    [batch 969]: seen 96900 examples at 32.2 eps, loss = 4.908\n",
      "    [batch 972]: seen 97200 examples at 32.2 eps, loss = 4.902\n",
      "    [batch 976]: seen 97600 examples at 32.2 eps, loss = 4.901\n",
      "    [batch 981]: seen 98100 examples at 32.3 eps, loss = 4.902\n",
      "    [batch 986]: seen 98600 examples at 32.3 eps, loss = 4.904\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-4724\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-4724\n",
      "    [batch 992]: seen 99200 examples at 32.4 eps, loss = 4.898\n",
      "    [batch 997]: seen 99700 examples at 32.4 eps, loss = 4.899\n",
      "    [batch 1004]: seen 100400 examples at 32.5 eps, loss = 4.900\n",
      "    [batch 1010]: seen 101000 examples at 32.6 eps, loss = 4.895\n",
      "    [batch 1015]: seen 101500 examples at 32.7 eps, loss = 4.895\n",
      "    [batch 1020]: seen 102000 examples at 32.7 eps, loss = 4.893\n",
      "    [batch 1023]: seen 102300 examples at 32.7 eps, loss = 4.892\n",
      "    [batch 1026]: seen 102600 examples at 32.7 eps, loss = 4.888\n",
      "    [batch 1031]: seen 103100 examples at 32.7 eps, loss = 4.892\n",
      "    [batch 1036]: seen 103600 examples at 32.8 eps, loss = 4.888\n",
      "    [batch 1043]: seen 104300 examples at 32.9 eps, loss = 4.886\n",
      "    [batch 1046]: seen 104600 examples at 32.8 eps, loss = 4.883\n",
      "    [batch 1051]: seen 105100 examples at 32.9 eps, loss = 4.881\n",
      "    [batch 1058]: seen 105800 examples at 33.0 eps, loss = 4.881\n",
      "    [batch 1063]: seen 106300 examples at 33.0 eps, loss = 4.879\n",
      "    [batch 1068]: seen 106800 examples at 33.1 eps, loss = 4.878\n",
      "    [batch 1071]: seen 107100 examples at 33.0 eps, loss = 4.872\n",
      "    [batch 1074]: seen 107400 examples at 33.0 eps, loss = 4.871\n",
      "    [batch 1077]: seen 107700 examples at 33.0 eps, loss = 4.870\n",
      "    [batch 1080]: seen 108000 examples at 32.9 eps, loss = 4.864\n",
      "    [batch 1083]: seen 108300 examples at 32.9 eps, loss = 4.861\n",
      "    [batch 1086]: seen 108600 examples at 32.9 eps, loss = 4.859\n",
      "    [batch 1090]: seen 109000 examples at 32.9 eps, loss = 4.858\n",
      "    [batch 1095]: seen 109500 examples at 32.9 eps, loss = 4.857\n",
      "    [batch 1098]: seen 109800 examples at 32.8 eps, loss = 4.856\n",
      "    [batch 1101]: seen 110100 examples at 32.8 eps, loss = 4.853\n",
      "    [batch 1108]: seen 110800 examples at 32.9 eps, loss = 4.855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1112]: seen 111200 examples at 32.9 eps, loss = 4.849\n",
      "    [batch 1115]: seen 111500 examples at 32.9 eps, loss = 4.845\n",
      "    [batch 1119]: seen 111900 examples at 32.9 eps, loss = 4.844\n",
      "    [batch 1122]: seen 112200 examples at 32.8 eps, loss = 4.838\n",
      "    [batch 1125]: seen 112500 examples at 32.8 eps, loss = 4.837\n",
      "    [batch 1129]: seen 112900 examples at 32.8 eps, loss = 4.837\n",
      "    [batch 1132]: seen 113200 examples at 32.7 eps, loss = 4.833\n",
      "    [batch 1135]: seen 113500 examples at 32.7 eps, loss = 4.831\n",
      "    [batch 1139]: seen 113900 examples at 32.7 eps, loss = 4.828\n",
      "    [batch 1144]: seen 114400 examples at 32.7 eps, loss = 4.829\n",
      "    [batch 1149]: seen 114900 examples at 32.7 eps, loss = 4.825\n",
      "    [batch 1153]: seen 115300 examples at 32.7 eps, loss = 4.823\n",
      "    [batch 1155]: seen 115500 examples at 32.7 eps, loss = 4.821\n",
      "    [batch 1160]: seen 116000 examples at 32.7 eps, loss = 4.820\n",
      "    [batch 1163]: seen 116300 examples at 32.7 eps, loss = 4.818\n",
      "    [batch 1168]: seen 116800 examples at 32.7 eps, loss = 4.817\n",
      "    [batch 1173]: seen 117300 examples at 32.8 eps, loss = 4.815\n",
      "    [batch 1176]: seen 117600 examples at 32.7 eps, loss = 4.812\n",
      "    [batch 1179]: seen 117900 examples at 32.7 eps, loss = 4.808\n",
      "    [batch 1186]: seen 118600 examples at 32.8 eps, loss = 4.808\n",
      "    [batch 1190]: seen 119000 examples at 32.8 eps, loss = 4.805\n",
      "    [batch 1195]: seen 119500 examples at 32.8 eps, loss = 4.804\n",
      "    [batch 1198]: seen 119800 examples at 32.8 eps, loss = 4.803\n",
      "    [batch 1203]: seen 120300 examples at 32.8 eps, loss = 4.803\n",
      "    [batch 1210]: seen 121000 examples at 32.9 eps, loss = 4.803\n",
      "    [batch 1215]: seen 121500 examples at 33.0 eps, loss = 4.804\n",
      "    [batch 1222]: seen 122200 examples at 33.0 eps, loss = 4.799\n",
      "    [batch 1227]: seen 122700 examples at 33.1 eps, loss = 4.800\n",
      "    [batch 1234]: seen 123400 examples at 33.2 eps, loss = 4.801\n",
      "    [batch 1237]: seen 123700 examples at 33.2 eps, loss = 4.796\n",
      "    [batch 1240]: seen 124000 examples at 33.1 eps, loss = 4.796\n",
      "    [batch 1244]: seen 124400 examples at 33.1 eps, loss = 4.794\n",
      "    [batch 1251]: seen 125100 examples at 33.2 eps, loss = 4.795\n",
      "    [batch 1258]: seen 125800 examples at 33.3 eps, loss = 4.793\n",
      "    [batch 1261]: seen 126100 examples at 33.3 eps, loss = 4.791\n",
      "    [batch 1263]: seen 126300 examples at 33.2 eps, loss = 4.787\n",
      "    [batch 1267]: seen 126700 examples at 33.2 eps, loss = 4.786\n",
      "    [batch 1274]: seen 127400 examples at 33.3 eps, loss = 4.790\n",
      "    [batch 1281]: seen 128100 examples at 33.4 eps, loss = 4.793\n",
      "    [batch 1288]: seen 128800 examples at 33.5 eps, loss = 4.791\n",
      "    [batch 1295]: seen 129500 examples at 33.6 eps, loss = 4.787\n",
      "    [batch 1300]: seen 130000 examples at 33.6 eps, loss = 4.783\n",
      "    [batch 1303]: seen 130300 examples at 33.6 eps, loss = 4.783\n",
      "    [batch 1309]: seen 130900 examples at 33.7 eps, loss = 4.780\n",
      "    [batch 1312]: seen 131200 examples at 33.6 eps, loss = 4.776\n",
      "    [batch 1317]: seen 131700 examples at 33.7 eps, loss = 4.778\n",
      "    [batch 1323]: seen 132300 examples at 33.7 eps, loss = 4.773\n",
      "    [batch 1330]: seen 133000 examples at 33.8 eps, loss = 4.774\n",
      "    [batch 1333]: seen 133300 examples at 33.8 eps, loss = 4.772\n",
      "    [batch 1336]: seen 133600 examples at 33.8 eps, loss = 4.771\n",
      "    [batch 1339]: seen 133900 examples at 33.7 eps, loss = 4.767\n",
      "    [batch 1344]: seen 134400 examples at 33.7 eps, loss = 4.765\n",
      "    [batch 1347]: seen 134700 examples at 33.7 eps, loss = 4.761\n",
      "    [batch 1354]: seen 135400 examples at 33.8 eps, loss = 4.761\n",
      "    [batch 1359]: seen 135900 examples at 33.8 eps, loss = 4.759\n",
      "    [batch 1364]: seen 136400 examples at 33.9 eps, loss = 4.760\n",
      "    [batch 1367]: seen 136700 examples at 33.9 eps, loss = 4.757\n",
      "    [batch 1372]: seen 137200 examples at 33.9 eps, loss = 4.753\n",
      "    [batch 1375]: seen 137500 examples at 33.8 eps, loss = 4.746\n",
      "    [batch 1382]: seen 138200 examples at 33.9 eps, loss = 4.753\n",
      "    [batch 1389]: seen 138900 examples at 34.0 eps, loss = 4.749\n",
      "    [batch 1396]: seen 139600 examples at 34.1 eps, loss = 4.751\n",
      "    [batch 1403]: seen 140300 examples at 34.2 eps, loss = 4.750\n",
      "    [batch 1407]: seen 140700 examples at 34.2 eps, loss = 4.743\n",
      "    [batch 1410]: seen 141000 examples at 34.1 eps, loss = 4.741\n",
      "    [batch 1413]: seen 141300 examples at 34.1 eps, loss = 4.739\n",
      "    [batch 1416]: seen 141600 examples at 34.1 eps, loss = 4.737\n",
      "    [batch 1419]: seen 141900 examples at 34.1 eps, loss = 4.734\n",
      "    [batch 1422]: seen 142200 examples at 34.0 eps, loss = 4.730\n",
      "    [batch 1429]: seen 142900 examples at 34.1 eps, loss = 4.726\n",
      "    [batch 1433]: seen 143300 examples at 34.1 eps, loss = 4.726\n",
      "    [batch 1436]: seen 143600 examples at 34.1 eps, loss = 4.725\n",
      "    [batch 1443]: seen 144300 examples at 34.2 eps, loss = 4.726\n",
      "    [batch 1448]: seen 144800 examples at 34.2 eps, loss = 4.725\n",
      "    [batch 1455]: seen 145500 examples at 34.3 eps, loss = 4.722\n",
      "    [batch 1460]: seen 146000 examples at 34.3 eps, loss = 4.723\n",
      "    [batch 1465]: seen 146500 examples at 34.3 eps, loss = 4.719\n",
      "    [batch 1468]: seen 146800 examples at 34.3 eps, loss = 4.720\n",
      "    [batch 1474]: seen 147400 examples at 34.3 eps, loss = 4.717\n",
      "    [batch 1479]: seen 147900 examples at 34.4 eps, loss = 4.715\n",
      "    [batch 1482]: seen 148200 examples at 34.4 eps, loss = 4.714\n",
      "    [batch 1489]: seen 148900 examples at 34.4 eps, loss = 4.718\n",
      "    [batch 1496]: seen 149600 examples at 34.5 eps, loss = 4.717\n",
      "    [batch 1503]: seen 150300 examples at 34.6 eps, loss = 4.715\n",
      "    [batch 1510]: seen 151000 examples at 34.7 eps, loss = 4.718\n",
      "    [batch 1517]: seen 151700 examples at 34.8 eps, loss = 4.714\n",
      "    [batch 1524]: seen 152400 examples at 34.8 eps, loss = 4.716\n",
      "    [batch 1531]: seen 153100 examples at 34.9 eps, loss = 4.716\n",
      "    [batch 1537]: seen 153700 examples at 35.0 eps, loss = 4.711\n",
      "    [batch 1542]: seen 154200 examples at 35.0 eps, loss = 4.714\n",
      "    [batch 1549]: seen 154900 examples at 35.1 eps, loss = 4.710\n",
      "    [batch 1556]: seen 155600 examples at 35.1 eps, loss = 4.713\n",
      "    [batch 1563]: seen 156300 examples at 35.2 eps, loss = 4.715\n",
      "    [batch 1568]: seen 156800 examples at 35.2 eps, loss = 4.704\n",
      "    [batch 1571]: seen 157100 examples at 35.2 eps, loss = 4.703\n",
      "    [batch 1574]: seen 157400 examples at 35.2 eps, loss = 4.700\n",
      "    [batch 1578]: seen 157800 examples at 35.2 eps, loss = 4.698\n",
      "    [batch 1583]: seen 158300 examples at 35.2 eps, loss = 4.698\n",
      "    [batch 1586]: seen 158600 examples at 35.2 eps, loss = 4.696\n",
      "    [batch 1591]: seen 159100 examples at 35.2 eps, loss = 4.693\n",
      "    [batch 1596]: seen 159600 examples at 35.2 eps, loss = 4.690\n",
      "    [batch 1599]: seen 159900 examples at 35.2 eps, loss = 4.689\n",
      "    [batch 1602]: seen 160200 examples at 35.2 eps, loss = 4.687\n",
      "    [batch 1607]: seen 160700 examples at 35.2 eps, loss = 4.690\n",
      "    [batch 1614]: seen 161400 examples at 35.3 eps, loss = 4.691\n",
      "    [batch 1621]: seen 162100 examples at 35.3 eps, loss = 4.687\n",
      "    [batch 1627]: seen 162700 examples at 35.4 eps, loss = 4.685\n",
      "    [batch 1630]: seen 163000 examples at 35.3 eps, loss = 4.681\n",
      "    [batch 1637]: seen 163700 examples at 35.4 eps, loss = 4.680\n",
      "    [batch 1639]: seen 163900 examples at 35.4 eps, loss = 4.676\n",
      "    [batch 1644]: seen 164400 examples at 35.4 eps, loss = 4.678\n",
      "    [batch 1650]: seen 165000 examples at 35.4 eps, loss = 4.675\n",
      "    [batch 1653]: seen 165300 examples at 35.4 eps, loss = 4.674\n",
      "    [batch 1656]: seen 165600 examples at 35.4 eps, loss = 4.673\n",
      "    [batch 1659]: seen 165900 examples at 35.4 eps, loss = 4.671\n",
      "    [batch 1662]: seen 166200 examples at 35.3 eps, loss = 4.669\n",
      "    [batch 1669]: seen 166900 examples at 35.4 eps, loss = 4.671\n",
      "    [batch 1674]: seen 167400 examples at 35.4 eps, loss = 4.668\n",
      "    [batch 1677]: seen 167700 examples at 35.4 eps, loss = 4.667\n",
      "    [batch 1680]: seen 168000 examples at 35.4 eps, loss = 4.660\n",
      "    [batch 1686]: seen 168600 examples at 35.4 eps, loss = 4.660\n",
      "    [batch 1689]: seen 168900 examples at 35.4 eps, loss = 4.655\n",
      "    [batch 1693]: seen 169300 examples at 35.4 eps, loss = 4.653\n",
      "    [batch 1696]: seen 169600 examples at 35.4 eps, loss = 4.652\n",
      "    [batch 1701]: seen 170100 examples at 35.4 eps, loss = 4.652\n",
      "    [batch 1704]: seen 170400 examples at 35.4 eps, loss = 4.650\n",
      "    [batch 1707]: seen 170700 examples at 35.4 eps, loss = 4.648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1710]: seen 171000 examples at 35.3 eps, loss = 4.647\n",
      "    [batch 1713]: seen 171300 examples at 35.3 eps, loss = 4.642\n",
      "    [batch 1718]: seen 171800 examples at 35.3 eps, loss = 4.644\n",
      "    [batch 1725]: seen 172500 examples at 35.4 eps, loss = 4.647\n",
      "    [batch 1732]: seen 173200 examples at 35.4 eps, loss = 4.645\n",
      "    [batch 1739]: seen 173900 examples at 35.5 eps, loss = 4.646\n",
      "    [batch 1746]: seen 174600 examples at 35.6 eps, loss = 4.642\n",
      "    [batch 1751]: seen 175100 examples at 35.6 eps, loss = 4.644\n",
      "    [batch 1758]: seen 175800 examples at 35.7 eps, loss = 4.643\n",
      "    [batch 1765]: seen 176500 examples at 35.7 eps, loss = 4.640\n",
      "    [batch 1768]: seen 176800 examples at 35.7 eps, loss = 4.639\n",
      "    [batch 1771]: seen 177100 examples at 35.6 eps, loss = 4.636\n",
      "    [batch 1774]: seen 177400 examples at 35.6 eps, loss = 4.634\n",
      "    [batch 1781]: seen 178100 examples at 35.7 eps, loss = 4.638\n",
      "    [batch 1788]: seen 178800 examples at 35.8 eps, loss = 4.642\n",
      "    [batch 1795]: seen 179500 examples at 35.8 eps, loss = 4.638\n",
      "    [batch 1802]: seen 180200 examples at 35.9 eps, loss = 4.638\n",
      "    [batch 1809]: seen 180900 examples at 35.9 eps, loss = 4.632\n",
      "    [batch 1812]: seen 181200 examples at 35.9 eps, loss = 4.628\n",
      "    [batch 1819]: seen 181900 examples at 36.0 eps, loss = 4.628\n",
      "    [batch 1822]: seen 182200 examples at 35.9 eps, loss = 4.622\n",
      "    [batch 1826]: seen 182600 examples at 35.9 eps, loss = 4.619\n",
      "    [batch 1831]: seen 183100 examples at 35.9 eps, loss = 4.623\n",
      "    [batch 1838]: seen 183800 examples at 36.0 eps, loss = 4.619\n",
      "    [batch 1841]: seen 184100 examples at 36.0 eps, loss = 4.613\n",
      "    [batch 1844]: seen 184400 examples at 35.9 eps, loss = 4.611\n",
      "    [batch 1847]: seen 184700 examples at 35.9 eps, loss = 4.605\n",
      "    [batch 1850]: seen 185000 examples at 35.9 eps, loss = 4.603\n",
      "    [batch 1855]: seen 185500 examples at 35.9 eps, loss = 4.605\n",
      "    [batch 1861]: seen 186100 examples at 35.9 eps, loss = 4.599\n",
      "    [batch 1863]: seen 186300 examples at 35.9 eps, loss = 4.596\n",
      "    [batch 1868]: seen 186800 examples at 35.9 eps, loss = 4.594\n",
      "    [END] Training complete: Total examples : 187100; Total time: 1:26:50\n",
      "[EPOCH 3] Complete. Avg Loss: 4.596100863578203; Best Loss: 4.594230183726878\n",
      "[EPOCH 4] Starting training..\n",
      "    [batch 7]: seen 700 examples at 69.1 eps, loss = 4.597\n",
      "    [batch 14]: seen 1400 examples at 69.0 eps, loss = 4.598\n",
      "    [batch 19]: seen 1900 examples at 61.5 eps, loss = 4.594\n",
      "    [batch 24]: seen 2400 examples at 53.6 eps, loss = 4.592\n",
      "    [batch 27]: seen 2700 examples at 48.4 eps, loss = 4.590\n",
      "    [batch 30]: seen 3000 examples at 44.9 eps, loss = 4.591\n",
      "    [batch 35]: seen 3500 examples at 45.3 eps, loss = 4.589\n",
      "    [batch 41]: seen 4100 examples at 45.9 eps, loss = 4.588\n",
      "    [batch 44]: seen 4400 examples at 42.5 eps, loss = 4.587\n",
      "    [batch 47]: seen 4700 examples at 40.8 eps, loss = 4.585\n",
      "    [batch 50]: seen 5000 examples at 38.6 eps, loss = 4.582\n",
      "    [batch 53]: seen 5300 examples at 37.7 eps, loss = 4.580\n",
      "    [batch 56]: seen 5600 examples at 36.2 eps, loss = 4.577\n",
      "    [batch 61]: seen 6100 examples at 36.9 eps, loss = 4.577\n",
      "    [batch 64]: seen 6400 examples at 35.6 eps, loss = 4.574\n",
      "    [batch 68]: seen 6800 examples at 35.4 eps, loss = 4.570\n",
      "    [batch 71]: seen 7100 examples at 34.3 eps, loss = 4.563\n",
      "    [batch 75]: seen 7500 examples at 34.2 eps, loss = 4.561\n",
      "    [batch 80]: seen 8000 examples at 34.8 eps, loss = 4.566\n",
      "    [batch 87]: seen 8700 examples at 36.2 eps, loss = 4.577\n",
      "    [batch 94]: seen 9400 examples at 37.6 eps, loss = 4.577\n",
      "    [batch 101]: seen 10100 examples at 38.8 eps, loss = 4.581\n",
      "    [batch 108]: seen 10800 examples at 39.9 eps, loss = 4.573\n",
      "    [batch 115]: seen 11500 examples at 41.0 eps, loss = 4.574\n",
      "    [batch 122]: seen 12200 examples at 41.9 eps, loss = 4.572\n",
      "    [batch 129]: seen 12900 examples at 42.9 eps, loss = 4.569\n",
      "    [batch 136]: seen 13600 examples at 43.2 eps, loss = 4.560\n",
      "    [batch 143]: seen 14300 examples at 43.6 eps, loss = 4.558\n",
      "    [batch 146]: seen 14600 examples at 43.1 eps, loss = 4.556\n",
      "    [batch 153]: seen 15300 examples at 43.8 eps, loss = 4.556\n",
      "    [batch 156]: seen 15600 examples at 43.3 eps, loss = 4.554\n",
      "    [batch 160]: seen 16000 examples at 43.0 eps, loss = 4.554\n",
      "    [batch 163]: seen 16300 examples at 42.1 eps, loss = 4.551\n",
      "    [batch 166]: seen 16600 examples at 41.7 eps, loss = 4.549\n",
      "    [batch 173]: seen 17300 examples at 42.4 eps, loss = 4.549\n",
      "    [batch 176]: seen 17600 examples at 41.9 eps, loss = 4.545\n",
      "    [batch 181]: seen 18100 examples at 42.1 eps, loss = 4.546\n",
      "    [batch 188]: seen 18800 examples at 42.7 eps, loss = 4.548\n",
      "    [batch 195]: seen 19500 examples at 43.3 eps, loss = 4.551\n",
      "    [batch 202]: seen 20200 examples at 43.9 eps, loss = 4.545\n",
      "    [batch 207]: seen 20700 examples at 43.6 eps, loss = 4.543\n",
      "    [batch 210]: seen 21000 examples at 43.3 eps, loss = 4.542\n",
      "    [batch 215]: seen 21500 examples at 43.1 eps, loss = 4.541\n",
      "    [batch 218]: seen 21800 examples at 42.7 eps, loss = 4.538\n",
      "    [batch 225]: seen 22500 examples at 43.2 eps, loss = 4.540\n",
      "    [batch 230]: seen 23000 examples at 43.3 eps, loss = 4.539\n",
      "    [batch 237]: seen 23700 examples at 43.8 eps, loss = 4.539\n",
      "    [batch 244]: seen 24400 examples at 44.0 eps, loss = 4.534\n",
      "    [batch 249]: seen 24900 examples at 44.0 eps, loss = 4.535\n",
      "    [batch 254]: seen 25400 examples at 43.9 eps, loss = 4.529\n",
      "    [batch 257]: seen 25700 examples at 43.5 eps, loss = 4.526\n",
      "    [batch 264]: seen 26400 examples at 44.0 eps, loss = 4.527\n",
      "    [batch 266]: seen 26600 examples at 43.6 eps, loss = 4.525\n",
      "    [batch 273]: seen 27300 examples at 44.0 eps, loss = 4.526\n",
      "    [batch 280]: seen 28000 examples at 44.4 eps, loss = 4.528\n",
      "    [batch 285]: seen 28500 examples at 44.4 eps, loss = 4.522\n",
      "    [batch 290]: seen 29000 examples at 44.5 eps, loss = 4.521\n",
      "    [batch 293]: seen 29300 examples at 44.2 eps, loss = 4.515\n",
      "    [batch 299]: seen 29900 examples at 44.3 eps, loss = 4.514\n",
      "    [batch 302]: seen 30200 examples at 44.0 eps, loss = 4.514\n",
      "    [batch 309]: seen 30900 examples at 44.4 eps, loss = 4.514\n",
      "    [batch 312]: seen 31200 examples at 43.9 eps, loss = 4.508\n",
      "    [batch 315]: seen 31500 examples at 43.7 eps, loss = 4.507\n",
      "    [batch 318]: seen 31800 examples at 43.4 eps, loss = 4.504\n",
      "    [batch 321]: seen 32100 examples at 43.1 eps, loss = 4.502\n",
      "    [batch 328]: seen 32800 examples at 43.5 eps, loss = 4.508\n",
      "    [batch 334]: seen 33400 examples at 43.6 eps, loss = 4.502\n",
      "    [batch 339]: seen 33900 examples at 43.4 eps, loss = 4.498\n",
      "    [batch 344]: seen 34400 examples at 43.5 eps, loss = 4.502\n",
      "    [batch 349]: seen 34900 examples at 43.6 eps, loss = 4.497\n",
      "    [batch 354]: seen 35400 examples at 43.6 eps, loss = 4.496\n",
      "    [batch 358]: seen 35800 examples at 43.4 eps, loss = 4.494\n",
      "    [batch 363]: seen 36300 examples at 43.5 eps, loss = 4.494\n",
      "    [batch 366]: seen 36600 examples at 43.3 eps, loss = 4.493\n",
      "    [batch 369]: seen 36900 examples at 43.1 eps, loss = 4.490\n",
      "    [batch 376]: seen 37600 examples at 43.4 eps, loss = 4.493\n",
      "    [batch 381]: seen 38100 examples at 43.4 eps, loss = 4.491\n",
      "    [batch 385]: seen 38500 examples at 43.3 eps, loss = 4.487\n",
      "    [batch 388]: seen 38800 examples at 43.0 eps, loss = 4.485\n",
      "    [batch 395]: seen 39500 examples at 43.2 eps, loss = 4.484\n",
      "    [batch 400]: seen 40000 examples at 43.1 eps, loss = 4.482\n",
      "    [batch 403]: seen 40300 examples at 42.7 eps, loss = 4.480\n",
      "    [batch 408]: seen 40800 examples at 42.8 eps, loss = 4.488\n",
      "    [batch 415]: seen 41500 examples at 43.0 eps, loss = 4.487\n",
      "    [batch 422]: seen 42200 examples at 43.3 eps, loss = 4.492\n",
      "    [batch 429]: seen 42900 examples at 43.6 eps, loss = 4.492\n",
      "    [batch 436]: seen 43600 examples at 43.8 eps, loss = 4.493\n",
      "    [batch 443]: seen 44300 examples at 44.1 eps, loss = 4.493\n",
      "    [batch 450]: seen 45000 examples at 44.3 eps, loss = 4.490\n",
      "    [batch 457]: seen 45700 examples at 44.6 eps, loss = 4.485\n",
      "    [batch 463]: seen 46300 examples at 44.6 eps, loss = 4.479\n",
      "    [batch 466]: seen 46600 examples at 44.5 eps, loss = 4.478\n",
      "    [batch 469]: seen 46900 examples at 44.3 eps, loss = 4.476\n",
      "    [batch 476]: seen 47600 examples at 44.5 eps, loss = 4.484\n",
      "    [batch 483]: seen 48300 examples at 44.8 eps, loss = 4.482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 490]: seen 49000 examples at 45.0 eps, loss = 4.478\n",
      "    [batch 495]: seen 49500 examples at 45.0 eps, loss = 4.478\n",
      "    [batch 500]: seen 50000 examples at 44.9 eps, loss = 4.474\n",
      "    [batch 502]: seen 50200 examples at 44.7 eps, loss = 4.471\n",
      "    [batch 505]: seen 50500 examples at 44.4 eps, loss = 4.469\n",
      "    [batch 508]: seen 50800 examples at 44.2 eps, loss = 4.467\n",
      "    [batch 511]: seen 51100 examples at 43.9 eps, loss = 4.464\n",
      "    [batch 518]: seen 51800 examples at 44.1 eps, loss = 4.468\n",
      "    [batch 523]: seen 52300 examples at 44.2 eps, loss = 4.463\n",
      "    [batch 526]: seen 52600 examples at 44.0 eps, loss = 4.460\n",
      "    [batch 531]: seen 53100 examples at 44.0 eps, loss = 4.459\n",
      "    [batch 538]: seen 53800 examples at 44.2 eps, loss = 4.465\n",
      "    [batch 545]: seen 54500 examples at 44.5 eps, loss = 4.469\n",
      "    [batch 552]: seen 55200 examples at 44.7 eps, loss = 4.468\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-6140\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-6140\n",
      "    [batch 559]: seen 55900 examples at 44.8 eps, loss = 4.467\n",
      "    [batch 566]: seen 56600 examples at 45.0 eps, loss = 4.460\n",
      "    [batch 569]: seen 56900 examples at 44.8 eps, loss = 4.458\n",
      "    [batch 576]: seen 57600 examples at 45.0 eps, loss = 4.461\n",
      "    [batch 583]: seen 58300 examples at 45.2 eps, loss = 4.462\n",
      "    [batch 590]: seen 59000 examples at 45.4 eps, loss = 4.464\n",
      "    [batch 597]: seen 59700 examples at 45.6 eps, loss = 4.459\n",
      "    [batch 602]: seen 60200 examples at 45.6 eps, loss = 4.457\n",
      "    [batch 607]: seen 60700 examples at 45.6 eps, loss = 4.460\n",
      "    [batch 612]: seen 61200 examples at 45.5 eps, loss = 4.452\n",
      "    [batch 615]: seen 61500 examples at 45.2 eps, loss = 4.450\n",
      "    [batch 622]: seen 62200 examples at 45.4 eps, loss = 4.451\n",
      "    [batch 625]: seen 62500 examples at 45.3 eps, loss = 4.448\n",
      "    [batch 630]: seen 63000 examples at 45.3 eps, loss = 4.448\n",
      "    [batch 637]: seen 63700 examples at 45.3 eps, loss = 4.448\n",
      "    [batch 644]: seen 64400 examples at 45.5 eps, loss = 4.456\n",
      "    [batch 651]: seen 65100 examples at 45.7 eps, loss = 4.454\n",
      "    [batch 656]: seen 65600 examples at 45.7 eps, loss = 4.447\n",
      "    [batch 659]: seen 65900 examples at 45.4 eps, loss = 4.444\n",
      "    [batch 664]: seen 66400 examples at 45.5 eps, loss = 4.443\n",
      "    [batch 671]: seen 67100 examples at 45.6 eps, loss = 4.445\n",
      "    [batch 675]: seen 67500 examples at 45.5 eps, loss = 4.441\n",
      "    [batch 677]: seen 67700 examples at 45.3 eps, loss = 4.439\n",
      "    [batch 680]: seen 68000 examples at 45.2 eps, loss = 4.435\n",
      "    [batch 683]: seen 68300 examples at 45.1 eps, loss = 4.435\n",
      "    [batch 689]: seen 68900 examples at 45.1 eps, loss = 4.433\n",
      "    [batch 692]: seen 69200 examples at 44.9 eps, loss = 4.428\n",
      "    [batch 695]: seen 69500 examples at 44.8 eps, loss = 4.424\n",
      "    [batch 698]: seen 69800 examples at 44.6 eps, loss = 4.423\n",
      "    [batch 704]: seen 70400 examples at 44.7 eps, loss = 4.421\n",
      "    [batch 708]: seen 70800 examples at 44.6 eps, loss = 4.419\n",
      "    [EXCEPTION]:  OOM when allocating tensor with shape[100,50017] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/final_distribution/concat_98 = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/final_distribution/mul_98, projection/final_distribution/zeros, Training/gradients/b_count)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2271 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'projection/final_distribution/concat_98', defined at:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n",
      "    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-d8a78f380874>\", line 1, in <module>\n",
      "    train(hps,30)\n",
      "  File \"<ipython-input-5-a4c1f7b896ea>\", line 3, in train\n",
      "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/training_util.py\", line 37, in training_init\n",
      "    lm.BuildCoreGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 140, in BuildCoreGraph\n",
      "    self.BuildProjectionGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 177, in BuildProjectionGraph\n",
      "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 177, in <listcomp>\n",
      "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1189, in concat\n",
      "    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 953, in concat_v2\n",
      "    \"ConcatV2\", values=values, axis=axis, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,50017] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/final_distribution/concat_98 = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/final_distribution/mul_98, projection/final_distribution/zeros, Training/gradients/b_count)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2271 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      " ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-6291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-6291\n",
      "    [batch 710]: seen 71000 examples at 43.5 eps, loss = 4.415\n",
      "    [batch 715]: seen 71500 examples at 43.5 eps, loss = 4.422\n",
      "    [batch 722]: seen 72200 examples at 43.7 eps, loss = 4.417\n",
      "    [batch 729]: seen 72900 examples at 43.8 eps, loss = 4.423\n",
      "    [batch 736]: seen 73600 examples at 44.0 eps, loss = 4.424\n",
      "    [batch 743]: seen 74300 examples at 44.1 eps, loss = 4.422\n",
      "    [batch 750]: seen 75000 examples at 44.3 eps, loss = 4.424\n",
      "    [batch 757]: seen 75700 examples at 44.4 eps, loss = 4.425\n",
      "    [batch 764]: seen 76400 examples at 44.6 eps, loss = 4.423\n",
      "    [batch 771]: seen 77100 examples at 44.7 eps, loss = 4.418\n",
      "    [batch 778]: seen 77800 examples at 44.9 eps, loss = 4.413\n",
      "    [batch 785]: seen 78500 examples at 45.0 eps, loss = 4.417\n",
      "    [batch 792]: seen 79200 examples at 45.1 eps, loss = 4.419\n",
      "    [batch 799]: seen 79900 examples at 45.3 eps, loss = 4.424\n",
      "    [batch 806]: seen 80600 examples at 45.4 eps, loss = 4.428\n",
      "    [batch 813]: seen 81300 examples at 45.5 eps, loss = 4.424\n",
      "    [batch 820]: seen 82000 examples at 45.7 eps, loss = 4.422\n",
      "    [batch 827]: seen 82700 examples at 45.8 eps, loss = 4.420\n",
      "    [batch 834]: seen 83400 examples at 45.9 eps, loss = 4.416\n",
      "    [batch 841]: seen 84100 examples at 46.1 eps, loss = 4.421\n",
      "    [batch 848]: seen 84800 examples at 46.2 eps, loss = 4.423\n",
      "    [batch 855]: seen 85500 examples at 46.3 eps, loss = 4.425\n",
      "    [batch 862]: seen 86200 examples at 46.4 eps, loss = 4.420\n",
      "    [batch 869]: seen 86900 examples at 46.6 eps, loss = 4.417\n",
      "    [batch 874]: seen 87400 examples at 46.6 eps, loss = 4.412\n",
      "    [batch 877]: seen 87700 examples at 46.4 eps, loss = 4.409\n",
      "    [batch 880]: seen 88000 examples at 46.3 eps, loss = 4.407\n",
      "    [batch 883]: seen 88300 examples at 46.1 eps, loss = 4.404\n",
      "    [batch 888]: seen 88800 examples at 46.1 eps, loss = 4.403\n",
      "    [batch 895]: seen 89500 examples at 46.2 eps, loss = 4.405\n",
      "    [batch 898]: seen 89800 examples at 46.1 eps, loss = 4.402\n",
      "    [batch 901]: seen 90100 examples at 45.9 eps, loss = 4.400\n",
      "    [batch 908]: seen 90800 examples at 46.0 eps, loss = 4.407\n",
      "    [batch 915]: seen 91500 examples at 46.1 eps, loss = 4.407\n",
      "    [batch 922]: seen 92200 examples at 46.3 eps, loss = 4.407\n",
      "    [batch 929]: seen 92900 examples at 46.4 eps, loss = 4.402\n",
      "    [batch 934]: seen 93400 examples at 46.4 eps, loss = 4.399\n",
      "    [batch 937]: seen 93700 examples at 46.3 eps, loss = 4.399\n",
      "    [batch 944]: seen 94400 examples at 46.4 eps, loss = 4.399\n",
      "    [batch 951]: seen 95100 examples at 46.5 eps, loss = 4.404\n",
      "    [batch 958]: seen 95800 examples at 46.6 eps, loss = 4.403\n",
      "    [batch 963]: seen 96300 examples at 46.5 eps, loss = 4.393\n",
      "    [batch 966]: seen 96600 examples at 46.4 eps, loss = 4.386\n",
      "    [batch 973]: seen 97300 examples at 46.5 eps, loss = 4.387\n",
      "    [batch 978]: seen 97800 examples at 46.5 eps, loss = 4.391\n",
      "    [batch 985]: seen 98500 examples at 46.6 eps, loss = 4.389\n",
      "    [batch 992]: seen 99200 examples at 46.7 eps, loss = 4.386\n",
      "    [batch 995]: seen 99500 examples at 46.5 eps, loss = 4.380\n",
      "    [batch 998]: seen 99800 examples at 46.3 eps, loss = 4.379\n",
      "    [batch 1003]: seen 100300 examples at 46.3 eps, loss = 4.377\n",
      "    [batch 1005]: seen 100500 examples at 46.2 eps, loss = 4.374\n",
      "    [batch 1012]: seen 101200 examples at 46.2 eps, loss = 4.374\n",
      "    [batch 1019]: seen 101900 examples at 46.3 eps, loss = 4.376\n",
      "    [batch 1024]: seen 102400 examples at 46.2 eps, loss = 4.373\n",
      "    [batch 1031]: seen 103100 examples at 46.3 eps, loss = 4.378\n",
      "    [batch 1038]: seen 103800 examples at 46.4 eps, loss = 4.380\n",
      "    [batch 1045]: seen 104500 examples at 46.5 eps, loss = 4.380\n",
      "    [batch 1052]: seen 105200 examples at 46.6 eps, loss = 4.378\n",
      "    [batch 1059]: seen 105900 examples at 46.7 eps, loss = 4.386\n",
      "    [batch 1066]: seen 106600 examples at 46.8 eps, loss = 4.392\n",
      "    [batch 1073]: seen 107300 examples at 46.9 eps, loss = 4.387\n",
      "    [batch 1080]: seen 108000 examples at 47.0 eps, loss = 4.379\n",
      "    [batch 1087]: seen 108700 examples at 47.1 eps, loss = 4.380\n",
      "    [batch 1094]: seen 109400 examples at 47.2 eps, loss = 4.381\n",
      "    [batch 1101]: seen 110100 examples at 47.3 eps, loss = 4.377\n",
      "    [batch 1108]: seen 110800 examples at 47.4 eps, loss = 4.377\n",
      "    [batch 1113]: seen 111300 examples at 47.4 eps, loss = 4.372\n",
      "    [batch 1118]: seen 111800 examples at 47.3 eps, loss = 4.370\n",
      "    [batch 1121]: seen 112100 examples at 47.3 eps, loss = 4.370\n",
      "    [batch 1124]: seen 112400 examples at 47.2 eps, loss = 4.367\n",
      "    [batch 1128]: seen 112800 examples at 47.1 eps, loss = 4.363\n",
      "    [batch 1133]: seen 113300 examples at 47.1 eps, loss = 4.370\n",
      "    [batch 1140]: seen 114000 examples at 47.2 eps, loss = 4.370\n",
      "    [batch 1147]: seen 114700 examples at 47.3 eps, loss = 4.368\n",
      "    [batch 1152]: seen 115200 examples at 47.3 eps, loss = 4.360\n",
      "    [batch 1157]: seen 115700 examples at 47.2 eps, loss = 4.359\n",
      "    [batch 1160]: seen 116000 examples at 47.0 eps, loss = 4.355\n",
      "    [batch 1164]: seen 116400 examples at 47.0 eps, loss = 4.349\n",
      "    [batch 1167]: seen 116700 examples at 46.8 eps, loss = 4.344\n",
      "    [batch 1170]: seen 117000 examples at 46.7 eps, loss = 4.341\n",
      "    [batch 1173]: seen 117300 examples at 46.6 eps, loss = 4.339\n",
      "    [batch 1178]: seen 117800 examples at 46.6 eps, loss = 4.339\n",
      "    [batch 1185]: seen 118500 examples at 46.7 eps, loss = 4.343\n",
      "    [batch 1190]: seen 119000 examples at 46.6 eps, loss = 4.335\n",
      "    [batch 1192]: seen 119200 examples at 46.5 eps, loss = 4.331\n",
      "    [batch 1195]: seen 119500 examples at 46.4 eps, loss = 4.330\n",
      "    [batch 1198]: seen 119800 examples at 46.4 eps, loss = 4.329\n",
      "    [batch 1203]: seen 120300 examples at 46.3 eps, loss = 4.327\n",
      "    [batch 1206]: seen 120600 examples at 46.2 eps, loss = 4.325\n",
      "    [batch 1213]: seen 121300 examples at 46.3 eps, loss = 4.330\n",
      "    [batch 1220]: seen 122000 examples at 46.4 eps, loss = 4.326\n",
      "    [batch 1224]: seen 122400 examples at 46.3 eps, loss = 4.323\n",
      "    [batch 1229]: seen 122900 examples at 46.3 eps, loss = 4.325\n",
      "    [batch 1236]: seen 123600 examples at 46.4 eps, loss = 4.329\n",
      "    [batch 1243]: seen 124300 examples at 46.5 eps, loss = 4.326\n",
      "    [batch 1250]: seen 125000 examples at 46.6 eps, loss = 4.323\n",
      "    [batch 1255]: seen 125500 examples at 46.6 eps, loss = 4.321\n",
      "    [batch 1258]: seen 125800 examples at 46.5 eps, loss = 4.320\n",
      "    [batch 1263]: seen 126300 examples at 46.5 eps, loss = 4.318\n",
      "    [batch 1265]: seen 126500 examples at 46.4 eps, loss = 4.315\n",
      "    [batch 1268]: seen 126800 examples at 46.2 eps, loss = 4.313\n",
      "    [batch 1275]: seen 127500 examples at 46.3 eps, loss = 4.330\n",
      "    [batch 1282]: seen 128200 examples at 46.4 eps, loss = 4.330\n",
      "    [batch 1289]: seen 128900 examples at 46.5 eps, loss = 4.331\n",
      "    [batch 1296]: seen 129600 examples at 46.6 eps, loss = 4.324\n",
      "    [batch 1303]: seen 130300 examples at 46.6 eps, loss = 4.330\n",
      "    [batch 1310]: seen 131000 examples at 46.7 eps, loss = 4.326\n",
      "    [batch 1317]: seen 131700 examples at 46.8 eps, loss = 4.322\n",
      "    [batch 1324]: seen 132400 examples at 46.9 eps, loss = 4.320\n",
      "    [batch 1331]: seen 133100 examples at 47.0 eps, loss = 4.324\n",
      "    [batch 1338]: seen 133800 examples at 47.0 eps, loss = 4.326\n",
      "    [batch 1345]: seen 134500 examples at 47.1 eps, loss = 4.326\n",
      "    [batch 1352]: seen 135200 examples at 47.2 eps, loss = 4.329\n",
      "    [batch 1359]: seen 135900 examples at 47.3 eps, loss = 4.325\n",
      "    [batch 1366]: seen 136600 examples at 47.3 eps, loss = 4.321\n",
      "    [batch 1373]: seen 137300 examples at 47.4 eps, loss = 4.316\n",
      "    [batch 1380]: seen 138000 examples at 47.5 eps, loss = 4.314\n",
      "    [batch 1385]: seen 138500 examples at 47.4 eps, loss = 4.312\n",
      "    [batch 1392]: seen 139200 examples at 47.5 eps, loss = 4.319\n",
      "    [batch 1399]: seen 139900 examples at 47.6 eps, loss = 4.316\n",
      "    [batch 1406]: seen 140600 examples at 47.7 eps, loss = 4.320\n",
      "    [batch 1413]: seen 141300 examples at 47.7 eps, loss = 4.318\n",
      "    [batch 1420]: seen 142000 examples at 47.8 eps, loss = 4.315\n",
      "    [batch 1427]: seen 142700 examples at 47.9 eps, loss = 4.318\n",
      "    [batch 1434]: seen 143400 examples at 48.0 eps, loss = 4.316\n",
      "    [batch 1441]: seen 144100 examples at 48.0 eps, loss = 4.315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1448]: seen 144800 examples at 48.0 eps, loss = 4.312\n",
      "    [batch 1455]: seen 145500 examples at 48.1 eps, loss = 4.315\n",
      "    [batch 1460]: seen 146000 examples at 48.1 eps, loss = 4.311\n",
      "    [batch 1465]: seen 146500 examples at 48.0 eps, loss = 4.309\n",
      "    [batch 1472]: seen 147200 examples at 48.1 eps, loss = 4.311\n",
      "    [batch 1479]: seen 147900 examples at 48.1 eps, loss = 4.315\n",
      "    [batch 1486]: seen 148600 examples at 48.2 eps, loss = 4.310\n",
      "    [batch 1491]: seen 149100 examples at 48.2 eps, loss = 4.307\n",
      "    [batch 1498]: seen 149800 examples at 48.2 eps, loss = 4.313\n",
      "    [batch 1505]: seen 150500 examples at 48.3 eps, loss = 4.309\n",
      "    [batch 1510]: seen 151000 examples at 48.3 eps, loss = 4.309\n",
      "    [batch 1517]: seen 151700 examples at 48.4 eps, loss = 4.308\n",
      "    [batch 1524]: seen 152400 examples at 48.4 eps, loss = 4.308\n",
      "    [batch 1529]: seen 152900 examples at 48.4 eps, loss = 4.305\n",
      "    [batch 1536]: seen 153600 examples at 48.5 eps, loss = 4.309\n",
      "    [batch 1543]: seen 154300 examples at 48.5 eps, loss = 4.307\n",
      "    [batch 1548]: seen 154800 examples at 48.5 eps, loss = 4.308\n",
      "    [batch 1555]: seen 155500 examples at 48.6 eps, loss = 4.309\n",
      "    [batch 1560]: seen 156000 examples at 48.6 eps, loss = 4.304\n",
      "    [batch 1567]: seen 156700 examples at 48.7 eps, loss = 4.309\n",
      "    [batch 1572]: seen 157200 examples at 48.6 eps, loss = 4.303\n",
      "    [batch 1575]: seen 157500 examples at 48.5 eps, loss = 4.300\n",
      "    [batch 1578]: seen 157800 examples at 48.4 eps, loss = 4.298\n",
      "    [batch 1583]: seen 158300 examples at 48.4 eps, loss = 4.303\n",
      "    [batch 1590]: seen 159000 examples at 48.5 eps, loss = 4.304\n",
      "    [batch 1597]: seen 159700 examples at 48.5 eps, loss = 4.302\n",
      "    [batch 1604]: seen 160400 examples at 48.5 eps, loss = 4.296\n",
      "    [batch 1607]: seen 160700 examples at 48.5 eps, loss = 4.294\n",
      "    [batch 1611]: seen 161100 examples at 48.4 eps, loss = 4.292\n",
      "    [batch 1614]: seen 161400 examples at 48.3 eps, loss = 4.290\n",
      "    [batch 1618]: seen 161800 examples at 48.2 eps, loss = 4.286\n",
      "    [batch 1621]: seen 162100 examples at 48.2 eps, loss = 4.283\n",
      "    [batch 1625]: seen 162500 examples at 48.1 eps, loss = 4.281\n",
      "    [batch 1628]: seen 162800 examples at 48.0 eps, loss = 4.282\n",
      "    [batch 1635]: seen 163500 examples at 48.1 eps, loss = 4.287\n",
      "    [batch 1642]: seen 164200 examples at 48.2 eps, loss = 4.282\n",
      "    [batch 1646]: seen 164600 examples at 48.1 eps, loss = 4.278\n",
      "    [batch 1651]: seen 165100 examples at 48.1 eps, loss = 4.277\n",
      "    [batch 1654]: seen 165400 examples at 48.0 eps, loss = 4.277\n",
      "    [batch 1659]: seen 165900 examples at 48.0 eps, loss = 4.284\n",
      "    [batch 1666]: seen 166600 examples at 48.1 eps, loss = 4.294\n",
      "    [batch 1673]: seen 167300 examples at 48.1 eps, loss = 4.295\n",
      "    [batch 1680]: seen 168000 examples at 48.2 eps, loss = 4.287\n",
      "    [batch 1687]: seen 168700 examples at 48.3 eps, loss = 4.284\n",
      "    [batch 1694]: seen 169400 examples at 48.3 eps, loss = 4.293\n",
      "    [batch 1701]: seen 170100 examples at 48.4 eps, loss = 4.287\n",
      "    [batch 1708]: seen 170800 examples at 48.4 eps, loss = 4.285\n",
      "    [batch 1715]: seen 171500 examples at 48.5 eps, loss = 4.285\n",
      "    [batch 1722]: seen 172200 examples at 48.6 eps, loss = 4.276\n",
      "    [batch 1725]: seen 172500 examples at 48.5 eps, loss = 4.272\n",
      "    [batch 1727]: seen 172700 examples at 48.4 eps, loss = 4.269\n",
      "    [batch 1730]: seen 173000 examples at 48.3 eps, loss = 4.266\n",
      "    [batch 1735]: seen 173500 examples at 48.3 eps, loss = 4.269\n",
      "    [batch 1739]: seen 173900 examples at 48.2 eps, loss = 4.264\n",
      "    [batch 1742]: seen 174200 examples at 48.1 eps, loss = 4.265\n",
      "    [batch 1749]: seen 174900 examples at 48.2 eps, loss = 4.272\n",
      "    [batch 1756]: seen 175600 examples at 48.3 eps, loss = 4.271\n",
      "    [batch 1763]: seen 176300 examples at 48.3 eps, loss = 4.274\n",
      "    [batch 1770]: seen 177000 examples at 48.4 eps, loss = 4.272\n",
      "    [batch 1777]: seen 177700 examples at 48.4 eps, loss = 4.273\n",
      "    [batch 1784]: seen 178400 examples at 48.5 eps, loss = 4.267\n",
      "    [batch 1791]: seen 179100 examples at 48.5 eps, loss = 4.263\n",
      "    [batch 1798]: seen 179800 examples at 48.6 eps, loss = 4.266\n",
      "    [batch 1803]: seen 180300 examples at 48.6 eps, loss = 4.263\n",
      "    [batch 1806]: seen 180600 examples at 48.5 eps, loss = 4.261\n",
      "    [batch 1809]: seen 180900 examples at 48.4 eps, loss = 4.258\n",
      "    [batch 1812]: seen 181200 examples at 48.4 eps, loss = 4.256\n",
      "    [batch 1819]: seen 181900 examples at 48.4 eps, loss = 4.256\n",
      "    [batch 1822]: seen 182200 examples at 48.3 eps, loss = 4.253\n",
      "    [batch 1825]: seen 182500 examples at 48.2 eps, loss = 4.252\n",
      "    [batch 1829]: seen 182900 examples at 48.2 eps, loss = 4.250\n",
      "    [batch 1836]: seen 183600 examples at 48.3 eps, loss = 4.253\n",
      "    [batch 1843]: seen 184300 examples at 48.3 eps, loss = 4.254\n",
      "    [batch 1850]: seen 185000 examples at 48.4 eps, loss = 4.255\n",
      "    [batch 1855]: seen 185500 examples at 48.4 eps, loss = 4.250\n",
      "    [batch 1858]: seen 185800 examples at 48.3 eps, loss = 4.248\n",
      "    [batch 1863]: seen 186300 examples at 48.3 eps, loss = 4.246\n",
      "    [batch 1866]: seen 186600 examples at 48.2 eps, loss = 4.246\n",
      "    [END] Training complete: Total examples : 187000; Total time: 1:04:37\n",
      "[EPOCH 4] Complete. Avg Loss: 4.250057381535374; Best Loss: 4.242721534591369\n",
      "[EPOCH 5] Starting training..\n",
      "    [batch 7]: seen 700 examples at 69.0 eps, loss = 4.259\n",
      "    [batch 14]: seen 1400 examples at 69.0 eps, loss = 4.259\n",
      "    [batch 21]: seen 2100 examples at 69.0 eps, loss = 4.260\n",
      "    [batch 28]: seen 2800 examples at 68.8 eps, loss = 4.256\n",
      "    [batch 35]: seen 3500 examples at 68.8 eps, loss = 4.253\n",
      "    [batch 42]: seen 4200 examples at 68.9 eps, loss = 4.247\n",
      "    [batch 49]: seen 4900 examples at 68.9 eps, loss = 4.248\n",
      "    [batch 56]: seen 5600 examples at 68.9 eps, loss = 4.250\n",
      "    [batch 63]: seen 6300 examples at 68.9 eps, loss = 4.253\n",
      "    [batch 70]: seen 7000 examples at 68.9 eps, loss = 4.254\n",
      "    [batch 77]: seen 7700 examples at 68.9 eps, loss = 4.249\n",
      "    [batch 84]: seen 8400 examples at 68.9 eps, loss = 4.250\n",
      "    [batch 91]: seen 9100 examples at 68.9 eps, loss = 4.248\n",
      "    [batch 98]: seen 9800 examples at 68.9 eps, loss = 4.251\n",
      "    [batch 105]: seen 10500 examples at 68.9 eps, loss = 4.251\n",
      "    [batch 112]: seen 11200 examples at 68.9 eps, loss = 4.256\n",
      "    [batch 119]: seen 11900 examples at 68.9 eps, loss = 4.255\n",
      "    [batch 126]: seen 12600 examples at 68.9 eps, loss = 4.256\n",
      "    [batch 133]: seen 13300 examples at 68.9 eps, loss = 4.251\n",
      "    [batch 140]: seen 14000 examples at 68.9 eps, loss = 4.251\n",
      "    [batch 147]: seen 14700 examples at 68.9 eps, loss = 4.251\n",
      "    [batch 154]: seen 15400 examples at 68.9 eps, loss = 4.244\n",
      "    [batch 158]: seen 15800 examples at 67.0 eps, loss = 4.241\n",
      "    [batch 161]: seen 16100 examples at 65.1 eps, loss = 4.238\n",
      "    [batch 165]: seen 16500 examples at 63.5 eps, loss = 4.235\n",
      "    [batch 168]: seen 16800 examples at 62.0 eps, loss = 4.232\n",
      "    [batch 171]: seen 17100 examples at 60.6 eps, loss = 4.231\n",
      "    [batch 174]: seen 17400 examples at 59.4 eps, loss = 4.230\n",
      "    [batch 177]: seen 17700 examples at 58.2 eps, loss = 4.224\n",
      "    [batch 182]: seen 18200 examples at 57.9 eps, loss = 4.224\n",
      "    [batch 187]: seen 18700 examples at 57.5 eps, loss = 4.225\n",
      "    [batch 194]: seen 19400 examples at 57.9 eps, loss = 4.226\n",
      "    [batch 199]: seen 19900 examples at 57.0 eps, loss = 4.219\n",
      "    [batch 206]: seen 20600 examples at 57.3 eps, loss = 4.229\n",
      "    [batch 213]: seen 21300 examples at 57.7 eps, loss = 4.224\n",
      "    [batch 218]: seen 21800 examples at 56.9 eps, loss = 4.218\n",
      "    [batch 223]: seen 22300 examples at 56.1 eps, loss = 4.217\n",
      "    [batch 227]: seen 22700 examples at 55.3 eps, loss = 4.215\n",
      "    [batch 234]: seen 23400 examples at 55.2 eps, loss = 4.214\n",
      "    [batch 239]: seen 23900 examples at 55.0 eps, loss = 4.216\n",
      "    [batch 246]: seen 24600 examples at 55.4 eps, loss = 4.217\n",
      "    [batch 253]: seen 25300 examples at 55.7 eps, loss = 4.214\n",
      "    [batch 256]: seen 25600 examples at 55.0 eps, loss = 4.213\n",
      "    [batch 259]: seen 25900 examples at 54.4 eps, loss = 4.211\n",
      "    [batch 265]: seen 26500 examples at 54.2 eps, loss = 4.211\n",
      "    [batch 270]: seen 27000 examples at 53.8 eps, loss = 4.209\n",
      "    [batch 273]: seen 27300 examples at 53.2 eps, loss = 4.207\n",
      "    [batch 276]: seen 27600 examples at 52.6 eps, loss = 4.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 279]: seen 27900 examples at 52.1 eps, loss = 4.204\n",
      "    [batch 285]: seen 28500 examples at 52.0 eps, loss = 4.203\n",
      "    [batch 289]: seen 28900 examples at 51.6 eps, loss = 4.199\n",
      "    [batch 294]: seen 29400 examples at 51.5 eps, loss = 4.201\n",
      "    [batch 300]: seen 30000 examples at 51.5 eps, loss = 4.199\n",
      "    [batch 307]: seen 30700 examples at 51.8 eps, loss = 4.204\n",
      "    [batch 314]: seen 31400 examples at 52.1 eps, loss = 4.199\n",
      "    [batch 321]: seen 32100 examples at 52.3 eps, loss = 4.205\n",
      "    [batch 328]: seen 32800 examples at 52.6 eps, loss = 4.199\n",
      "    [batch 331]: seen 33100 examples at 52.2 eps, loss = 4.197\n",
      "    [batch 338]: seen 33800 examples at 52.2 eps, loss = 4.196\n",
      "    [batch 345]: seen 34500 examples at 52.4 eps, loss = 4.197\n",
      "    [batch 348]: seen 34800 examples at 52.0 eps, loss = 4.196\n",
      "    [batch 355]: seen 35500 examples at 52.3 eps, loss = 4.203\n",
      "    [batch 362]: seen 36200 examples at 52.5 eps, loss = 4.198\n",
      "    [batch 365]: seen 36500 examples at 52.1 eps, loss = 4.195\n",
      "    [batch 369]: seen 36900 examples at 51.8 eps, loss = 4.194\n",
      "    [batch 372]: seen 37200 examples at 51.3 eps, loss = 4.193\n",
      "    [batch 375]: seen 37500 examples at 50.8 eps, loss = 4.186\n",
      "    [batch 378]: seen 37800 examples at 50.4 eps, loss = 4.183\n",
      "    [batch 382]: seen 38200 examples at 50.1 eps, loss = 4.183\n",
      "    [batch 387]: seen 38700 examples at 50.1 eps, loss = 4.188\n",
      "    [batch 394]: seen 39400 examples at 50.3 eps, loss = 4.197\n",
      "    [batch 401]: seen 40100 examples at 50.6 eps, loss = 4.197\n",
      "    [batch 408]: seen 40800 examples at 50.8 eps, loss = 4.195\n",
      "    [batch 415]: seen 41500 examples at 51.0 eps, loss = 4.187\n",
      "    [batch 422]: seen 42200 examples at 51.2 eps, loss = 4.185\n",
      "    [batch 429]: seen 42900 examples at 51.5 eps, loss = 4.183\n",
      "    [batch 436]: seen 43600 examples at 51.7 eps, loss = 4.188\n",
      "    [batch 443]: seen 44300 examples at 51.9 eps, loss = 4.192\n",
      "    [batch 450]: seen 45000 examples at 52.1 eps, loss = 4.187\n",
      "    [batch 457]: seen 45700 examples at 52.3 eps, loss = 4.184\n",
      "    [batch 462]: seen 46200 examples at 52.0 eps, loss = 4.180\n",
      "    [batch 465]: seen 46500 examples at 51.7 eps, loss = 4.177\n",
      "    [batch 468]: seen 46800 examples at 51.2 eps, loss = 4.172\n",
      "    [batch 471]: seen 47100 examples at 51.0 eps, loss = 4.171\n",
      "    [batch 476]: seen 47600 examples at 50.9 eps, loss = 4.170\n",
      "    [batch 483]: seen 48300 examples at 51.1 eps, loss = 4.173\n",
      "    [batch 488]: seen 48800 examples at 50.9 eps, loss = 4.168\n",
      "    [batch 491]: seen 49100 examples at 50.4 eps, loss = 4.166\n",
      "    [batch 496]: seen 49600 examples at 50.4 eps, loss = 4.166\n",
      "    [batch 503]: seen 50300 examples at 50.6 eps, loss = 4.169\n",
      "    [batch 510]: seen 51000 examples at 50.8 eps, loss = 4.169\n",
      "    [batch 517]: seen 51700 examples at 50.9 eps, loss = 4.167\n",
      "    [batch 524]: seen 52400 examples at 51.1 eps, loss = 4.168\n",
      "    [batch 531]: seen 53100 examples at 51.3 eps, loss = 4.166\n",
      "    [batch 536]: seen 53600 examples at 51.1 eps, loss = 4.162\n",
      "    [batch 541]: seen 54100 examples at 50.9 eps, loss = 4.160\n",
      "    [batch 546]: seen 54600 examples at 50.9 eps, loss = 4.161\n",
      "    [batch 551]: seen 55100 examples at 50.8 eps, loss = 4.160\n",
      "    [batch 556]: seen 55600 examples at 50.8 eps, loss = 4.163\n",
      "    [batch 563]: seen 56300 examples at 51.0 eps, loss = 4.175\n",
      "    [batch 570]: seen 57000 examples at 51.1 eps, loss = 4.172\n",
      "    [batch 577]: seen 57700 examples at 51.3 eps, loss = 4.169\n",
      "    [batch 584]: seen 58400 examples at 51.4 eps, loss = 4.161\n",
      "    [batch 589]: seen 58900 examples at 51.4 eps, loss = 4.160\n",
      "    [batch 594]: seen 59400 examples at 51.4 eps, loss = 4.157\n",
      "    [batch 597]: seen 59700 examples at 51.1 eps, loss = 4.155\n",
      "    [batch 602]: seen 60200 examples at 51.1 eps, loss = 4.155\n",
      "    [batch 607]: seen 60700 examples at 51.0 eps, loss = 4.157\n",
      "    [batch 611]: seen 61100 examples at 50.9 eps, loss = 4.153\n",
      "    [batch 616]: seen 61600 examples at 50.8 eps, loss = 4.158\n",
      "    [batch 623]: seen 62300 examples at 51.0 eps, loss = 4.158\n",
      "    [batch 630]: seen 63000 examples at 51.1 eps, loss = 4.157\n",
      "    [batch 637]: seen 63700 examples at 51.3 eps, loss = 4.158\n",
      "    [batch 644]: seen 64400 examples at 51.4 eps, loss = 4.158\n",
      "    [batch 651]: seen 65100 examples at 51.5 eps, loss = 4.159\n",
      "    [batch 658]: seen 65800 examples at 51.7 eps, loss = 4.169\n",
      "    [batch 665]: seen 66500 examples at 51.8 eps, loss = 4.171\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-8064\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-8064\n",
      "    [batch 671]: seen 67100 examples at 51.9 eps, loss = 4.166\n",
      "    [batch 678]: seen 67800 examples at 52.0 eps, loss = 4.159\n",
      "    [batch 685]: seen 68500 examples at 52.1 eps, loss = 4.155\n",
      "    [batch 688]: seen 68800 examples at 51.9 eps, loss = 4.152\n",
      "    [batch 693]: seen 69300 examples at 51.9 eps, loss = 4.151\n",
      "    [batch 696]: seen 69600 examples at 51.7 eps, loss = 4.149\n",
      "    [batch 703]: seen 70300 examples at 51.8 eps, loss = 4.151\n",
      "    [batch 710]: seen 71000 examples at 51.9 eps, loss = 4.154\n",
      "    [batch 717]: seen 71700 examples at 52.1 eps, loss = 4.154\n",
      "    [batch 724]: seen 72400 examples at 52.2 eps, loss = 4.158\n",
      "    [batch 731]: seen 73100 examples at 52.3 eps, loss = 4.156\n",
      "    [batch 738]: seen 73800 examples at 52.4 eps, loss = 4.156\n",
      "    [batch 745]: seen 74500 examples at 52.6 eps, loss = 4.154\n",
      "    [batch 752]: seen 75200 examples at 52.7 eps, loss = 4.157\n",
      "    [batch 759]: seen 75900 examples at 52.8 eps, loss = 4.169\n",
      "    [batch 766]: seen 76600 examples at 52.9 eps, loss = 4.166\n",
      "    [batch 773]: seen 77300 examples at 53.0 eps, loss = 4.166\n",
      "    [batch 780]: seen 78000 examples at 53.1 eps, loss = 4.168\n",
      "    [batch 787]: seen 78700 examples at 53.2 eps, loss = 4.166\n",
      "    [batch 794]: seen 79400 examples at 53.3 eps, loss = 4.164\n",
      "    [batch 801]: seen 80100 examples at 53.4 eps, loss = 4.164\n",
      "    [batch 808]: seen 80800 examples at 53.5 eps, loss = 4.160\n",
      "    [batch 815]: seen 81500 examples at 53.7 eps, loss = 4.158\n",
      "    [batch 822]: seen 82200 examples at 53.8 eps, loss = 4.154\n",
      "    [batch 829]: seen 82900 examples at 53.9 eps, loss = 4.157\n",
      "    [batch 836]: seen 83600 examples at 54.0 eps, loss = 4.159\n",
      "    [batch 843]: seen 84300 examples at 54.0 eps, loss = 4.160\n",
      "    [batch 850]: seen 85000 examples at 54.1 eps, loss = 4.156\n",
      "    [batch 857]: seen 85700 examples at 54.2 eps, loss = 4.154\n",
      "    [batch 864]: seen 86400 examples at 54.3 eps, loss = 4.154\n",
      "    [batch 871]: seen 87100 examples at 54.4 eps, loss = 4.154\n",
      "    [batch 878]: seen 87800 examples at 54.5 eps, loss = 4.160\n",
      "    [batch 885]: seen 88500 examples at 54.6 eps, loss = 4.160\n",
      "    [batch 892]: seen 89200 examples at 54.7 eps, loss = 4.161\n",
      "    [batch 899]: seen 89900 examples at 54.8 eps, loss = 4.160\n",
      "    [batch 906]: seen 90600 examples at 54.9 eps, loss = 4.164\n",
      "    [batch 913]: seen 91300 examples at 55.0 eps, loss = 4.159\n",
      "    [batch 920]: seen 92000 examples at 55.0 eps, loss = 4.159\n",
      "    [batch 927]: seen 92700 examples at 55.1 eps, loss = 4.157\n",
      "    [batch 934]: seen 93400 examples at 55.2 eps, loss = 4.153\n",
      "    [batch 941]: seen 94100 examples at 55.3 eps, loss = 4.155\n",
      "    [batch 948]: seen 94800 examples at 55.4 eps, loss = 4.156\n",
      "    [batch 955]: seen 95500 examples at 55.5 eps, loss = 4.151\n",
      "    [batch 962]: seen 96200 examples at 55.5 eps, loss = 4.149\n",
      "    [batch 969]: seen 96900 examples at 55.6 eps, loss = 4.153\n",
      "    [batch 976]: seen 97600 examples at 55.7 eps, loss = 4.154\n",
      "    [batch 983]: seen 98300 examples at 55.8 eps, loss = 4.153\n",
      "    [batch 990]: seen 99000 examples at 55.8 eps, loss = 4.151\n",
      "    [batch 997]: seen 99700 examples at 55.8 eps, loss = 4.148\n",
      "    [batch 1000]: seen 100000 examples at 55.6 eps, loss = 4.149\n",
      "    [batch 1007]: seen 100700 examples at 55.7 eps, loss = 4.155\n",
      "    [batch 1014]: seen 101400 examples at 55.8 eps, loss = 4.152\n",
      "    [batch 1021]: seen 102100 examples at 55.9 eps, loss = 4.155\n",
      "    [batch 1028]: seen 102800 examples at 55.9 eps, loss = 4.156\n",
      "    [batch 1035]: seen 103500 examples at 56.0 eps, loss = 4.154\n",
      "    [batch 1042]: seen 104200 examples at 56.1 eps, loss = 4.152\n",
      "    [batch 1049]: seen 104900 examples at 56.1 eps, loss = 4.149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1056]: seen 105600 examples at 56.1 eps, loss = 4.144\n",
      "    [batch 1061]: seen 106100 examples at 56.0 eps, loss = 4.143\n",
      "    [batch 1064]: seen 106400 examples at 55.8 eps, loss = 4.141\n",
      "    [batch 1071]: seen 107100 examples at 55.8 eps, loss = 4.143\n",
      "    [batch 1075]: seen 107500 examples at 55.7 eps, loss = 4.141\n",
      "    [batch 1078]: seen 107800 examples at 55.5 eps, loss = 4.142\n",
      "    [batch 1084]: seen 108400 examples at 55.5 eps, loss = 4.138\n",
      "    [batch 1091]: seen 109100 examples at 55.6 eps, loss = 4.141\n",
      "    [batch 1096]: seen 109600 examples at 55.5 eps, loss = 4.137\n",
      "    [batch 1099]: seen 109900 examples at 55.4 eps, loss = 4.137\n",
      "    [batch 1106]: seen 110600 examples at 55.4 eps, loss = 4.152\n",
      "    [batch 1113]: seen 111300 examples at 55.5 eps, loss = 4.155\n",
      "    [batch 1120]: seen 112000 examples at 55.6 eps, loss = 4.154\n",
      "    [batch 1127]: seen 112700 examples at 55.6 eps, loss = 4.151\n",
      "    [batch 1134]: seen 113400 examples at 55.7 eps, loss = 4.148\n",
      "    [batch 1141]: seen 114100 examples at 55.8 eps, loss = 4.145\n",
      "    [batch 1148]: seen 114800 examples at 55.8 eps, loss = 4.145\n",
      "    [batch 1155]: seen 115500 examples at 55.9 eps, loss = 4.146\n",
      "    [batch 1162]: seen 116200 examples at 56.0 eps, loss = 4.146\n",
      "    [batch 1169]: seen 116900 examples at 56.0 eps, loss = 4.148\n",
      "    [batch 1176]: seen 117600 examples at 56.1 eps, loss = 4.142\n",
      "    [batch 1183]: seen 118300 examples at 56.2 eps, loss = 4.137\n",
      "    [batch 1186]: seen 118600 examples at 56.0 eps, loss = 4.134\n",
      "    [batch 1189]: seen 118900 examples at 55.8 eps, loss = 4.129\n",
      "    [batch 1193]: seen 119300 examples at 55.6 eps, loss = 4.128\n",
      "    [batch 1198]: seen 119800 examples at 55.6 eps, loss = 4.127\n",
      "    [batch 1205]: seen 120500 examples at 55.6 eps, loss = 4.130\n",
      "    [batch 1212]: seen 121200 examples at 55.7 eps, loss = 4.130\n",
      "    [batch 1219]: seen 121900 examples at 55.8 eps, loss = 4.131\n",
      "    [batch 1226]: seen 122600 examples at 55.8 eps, loss = 4.135\n",
      "    [batch 1233]: seen 123300 examples at 55.9 eps, loss = 4.133\n",
      "    [batch 1239]: seen 123900 examples at 55.8 eps, loss = 4.126\n",
      "    [batch 1243]: seen 124300 examples at 55.7 eps, loss = 4.126\n",
      "    [batch 1248]: seen 124800 examples at 55.6 eps, loss = 4.125\n",
      "    [batch 1253]: seen 125300 examples at 55.6 eps, loss = 4.130\n",
      "    [batch 1260]: seen 126000 examples at 55.6 eps, loss = 4.129\n",
      "    [batch 1267]: seen 126700 examples at 55.7 eps, loss = 4.128\n",
      "    [batch 1274]: seen 127400 examples at 55.7 eps, loss = 4.131\n",
      "    [batch 1281]: seen 128100 examples at 55.8 eps, loss = 4.138\n",
      "    [batch 1288]: seen 128800 examples at 55.8 eps, loss = 4.143\n",
      "    [batch 1295]: seen 129500 examples at 55.9 eps, loss = 4.133\n",
      "    [batch 1302]: seen 130200 examples at 56.0 eps, loss = 4.132\n",
      "    [batch 1309]: seen 130900 examples at 56.0 eps, loss = 4.126\n",
      "    [batch 1315]: seen 131500 examples at 56.0 eps, loss = 4.123\n",
      "    [batch 1322]: seen 132200 examples at 56.0 eps, loss = 4.126\n",
      "    [batch 1327]: seen 132700 examples at 56.0 eps, loss = 4.122\n",
      "    [batch 1330]: seen 133000 examples at 55.8 eps, loss = 4.119\n",
      "    [batch 1333]: seen 133300 examples at 55.6 eps, loss = 4.117\n",
      "    [batch 1336]: seen 133600 examples at 55.5 eps, loss = 4.115\n",
      "    [batch 1343]: seen 134300 examples at 55.5 eps, loss = 4.118\n",
      "    [batch 1350]: seen 135000 examples at 55.6 eps, loss = 4.120\n",
      "    [batch 1357]: seen 135700 examples at 55.6 eps, loss = 4.118\n",
      "    [batch 1364]: seen 136400 examples at 55.7 eps, loss = 4.121\n",
      "    [batch 1371]: seen 137100 examples at 55.7 eps, loss = 4.118\n",
      "    [batch 1378]: seen 137800 examples at 55.8 eps, loss = 4.121\n",
      "    [batch 1385]: seen 138500 examples at 55.8 eps, loss = 4.122\n",
      "    [batch 1392]: seen 139200 examples at 55.9 eps, loss = 4.120\n",
      "    [batch 1399]: seen 139900 examples at 56.0 eps, loss = 4.118\n",
      "    [batch 1404]: seen 140400 examples at 55.9 eps, loss = 4.115\n",
      "    [batch 1411]: seen 141100 examples at 56.0 eps, loss = 4.117\n",
      "    [batch 1418]: seen 141800 examples at 56.0 eps, loss = 4.116\n",
      "    [batch 1425]: seen 142500 examples at 56.1 eps, loss = 4.118\n",
      "    [batch 1432]: seen 143200 examples at 56.1 eps, loss = 4.120\n",
      "    [batch 1439]: seen 143900 examples at 56.2 eps, loss = 4.121\n",
      "    [batch 1446]: seen 144600 examples at 56.2 eps, loss = 4.129\n",
      "    [batch 1453]: seen 145300 examples at 56.3 eps, loss = 4.125\n",
      "    [batch 1460]: seen 146000 examples at 56.3 eps, loss = 4.125\n",
      "    [batch 1467]: seen 146700 examples at 56.4 eps, loss = 4.128\n",
      "    [batch 1474]: seen 147400 examples at 56.4 eps, loss = 4.130\n",
      "    [batch 1481]: seen 148100 examples at 56.5 eps, loss = 4.124\n",
      "    [batch 1488]: seen 148800 examples at 56.5 eps, loss = 4.125\n",
      "    [batch 1495]: seen 149500 examples at 56.6 eps, loss = 4.124\n",
      "    [batch 1502]: seen 150200 examples at 56.6 eps, loss = 4.121\n",
      "    [batch 1509]: seen 150900 examples at 56.7 eps, loss = 4.126\n",
      "    [batch 1516]: seen 151600 examples at 56.7 eps, loss = 4.131\n",
      "    [batch 1523]: seen 152300 examples at 56.8 eps, loss = 4.126\n",
      "    [batch 1530]: seen 153000 examples at 56.8 eps, loss = 4.124\n",
      "    [batch 1537]: seen 153700 examples at 56.8 eps, loss = 4.121\n",
      "    [batch 1544]: seen 154400 examples at 56.9 eps, loss = 4.119\n",
      "    [batch 1548]: seen 154800 examples at 56.8 eps, loss = 4.113\n",
      "    [batch 1553]: seen 155300 examples at 56.7 eps, loss = 4.113\n",
      "    [batch 1559]: seen 155900 examples at 56.7 eps, loss = 4.110\n",
      "    [batch 1566]: seen 156600 examples at 56.7 eps, loss = 4.111\n",
      "    [batch 1569]: seen 156900 examples at 56.6 eps, loss = 4.108\n",
      "    [batch 1575]: seen 157500 examples at 56.6 eps, loss = 4.107\n",
      "    [batch 1580]: seen 158000 examples at 56.6 eps, loss = 4.109\n",
      "    [batch 1587]: seen 158700 examples at 56.6 eps, loss = 4.110\n",
      "    [batch 1594]: seen 159400 examples at 56.6 eps, loss = 4.111\n",
      "    [batch 1601]: seen 160100 examples at 56.7 eps, loss = 4.114\n",
      "    [batch 1608]: seen 160800 examples at 56.7 eps, loss = 4.120\n",
      "    [batch 1615]: seen 161500 examples at 56.8 eps, loss = 4.118\n",
      "    [batch 1622]: seen 162200 examples at 56.8 eps, loss = 4.117\n",
      "    [batch 1629]: seen 162900 examples at 56.9 eps, loss = 4.112\n",
      "    [batch 1636]: seen 163600 examples at 56.9 eps, loss = 4.107\n",
      "    [batch 1643]: seen 164300 examples at 56.9 eps, loss = 4.109\n",
      "    [batch 1646]: seen 164600 examples at 56.8 eps, loss = 4.104\n",
      "    [batch 1650]: seen 165000 examples at 56.7 eps, loss = 4.103\n",
      "    [batch 1653]: seen 165300 examples at 56.6 eps, loss = 4.099\n",
      "    [batch 1657]: seen 165700 examples at 56.5 eps, loss = 4.096\n",
      "    [batch 1662]: seen 166200 examples at 56.4 eps, loss = 4.095\n",
      "    [batch 1665]: seen 166500 examples at 56.3 eps, loss = 4.093\n",
      "    [batch 1672]: seen 167200 examples at 56.3 eps, loss = 4.097\n",
      "    [batch 1679]: seen 167900 examples at 56.4 eps, loss = 4.099\n",
      "    [batch 1686]: seen 168600 examples at 56.4 eps, loss = 4.095\n",
      "    [batch 1690]: seen 169000 examples at 56.3 eps, loss = 4.089\n",
      "    [batch 1694]: seen 169400 examples at 56.2 eps, loss = 4.087\n",
      "    [batch 1701]: seen 170100 examples at 56.3 eps, loss = 4.092\n",
      "    [batch 1708]: seen 170800 examples at 56.3 eps, loss = 4.094\n",
      "    [batch 1715]: seen 171500 examples at 56.4 eps, loss = 4.098\n",
      "    [batch 1722]: seen 172200 examples at 56.4 eps, loss = 4.104\n",
      "    [batch 1729]: seen 172900 examples at 56.4 eps, loss = 4.100\n",
      "    [batch 1736]: seen 173600 examples at 56.5 eps, loss = 4.099\n",
      "    [batch 1743]: seen 174300 examples at 56.5 eps, loss = 4.090\n",
      "    [batch 1749]: seen 174900 examples at 56.5 eps, loss = 4.085\n",
      "    [batch 1752]: seen 175200 examples at 56.4 eps, loss = 4.084\n",
      "    [batch 1759]: seen 175900 examples at 56.4 eps, loss = 4.091\n",
      "    [batch 1766]: seen 176600 examples at 56.5 eps, loss = 4.089\n",
      "    [batch 1772]: seen 177200 examples at 56.4 eps, loss = 4.084\n",
      "    [batch 1775]: seen 177500 examples at 56.3 eps, loss = 4.081\n",
      "    [batch 1782]: seen 178200 examples at 56.3 eps, loss = 4.087\n",
      "    [batch 1789]: seen 178900 examples at 56.4 eps, loss = 4.088\n",
      "    [batch 1796]: seen 179600 examples at 56.4 eps, loss = 4.091\n",
      "    [batch 1803]: seen 180300 examples at 56.4 eps, loss = 4.094\n",
      "    [batch 1810]: seen 181000 examples at 56.5 eps, loss = 4.096\n",
      "    [batch 1817]: seen 181700 examples at 56.5 eps, loss = 4.089\n",
      "    [batch 1824]: seen 182400 examples at 56.6 eps, loss = 4.083\n",
      "    [batch 1829]: seen 182900 examples at 56.5 eps, loss = 4.079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1833]: seen 183300 examples at 56.4 eps, loss = 4.076\n",
      "    [batch 1836]: seen 183600 examples at 56.3 eps, loss = 4.076\n",
      "    [batch 1841]: seen 184100 examples at 56.2 eps, loss = 4.074\n",
      "    [batch 1848]: seen 184800 examples at 56.2 eps, loss = 4.076\n",
      "    [batch 1855]: seen 185500 examples at 56.3 eps, loss = 4.076\n",
      "    [batch 1858]: seen 185800 examples at 56.2 eps, loss = 4.072\n",
      "    [batch 1861]: seen 186100 examples at 56.0 eps, loss = 4.068\n",
      "    [batch 1868]: seen 186800 examples at 56.1 eps, loss = 4.071\n",
      "    [END] Training complete: Total examples : 187100; Total time: 0:55:36\n",
      "[EPOCH 5] Complete. Avg Loss: 4.073161114532455; Best Loss: 4.067813832117995\n",
      "[EPOCH 6] Starting training..\n",
      "    [batch 7]: seen 700 examples at 68.8 eps, loss = 4.075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d8a78f380874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a4c1f7b896ea>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(hps, epochs, restore)\u001b[0m\n\u001b[1;32m     40\u001b[0m                                                             \u001b[0mhps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                                             \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                                                             avg_loss)\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcurr_best\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266/final_0/W266_Final/model_4/training_util.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(lm, session, batches, summary_writer, train_dir, train_step, saver, hps, best_loss, avg_loss)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunTrainStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266/final_0/W266_Final/model_4/training_util.py\u001b[0m in \u001b[0;36mrunTrainStep\u001b[0;34m(lm, session, batch)\u001b[0m\n\u001b[1;32m     87\u001b[0m     }\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(hps,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training session 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start):\n",
    "    \n",
    "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        \n",
    "    with tf.Session(graph=lm.graph) as session:\n",
    "        \n",
    "        session.run(initializer)\n",
    "        saver = tf.train.Saver(max_to_keep=3) # keep 3 checkpoints at a time\n",
    "        sv = tf.train.Supervisor(logdir=train_dir,\n",
    "                                 is_chief=True,\n",
    "                                 saver=saver,\n",
    "                                 summary_op=None,\n",
    "                                 save_summaries_secs=30, \n",
    "                                 save_model_secs=30, \n",
    "                                 global_step=lm.global_step)    \n",
    "\n",
    "        summary_writer = sv.summary_writer\n",
    "        \n",
    "        if restore:\n",
    "            ckpt_path = util.load_ckpt(saver, session,hps,hps.log_root)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for i in range(epoch_start,epoch_start+epochs):\n",
    "            print(f\"[EPOCH {i+1}] Starting training..\")\n",
    "            random.shuffle(batches)\n",
    "            avg_loss,curr_best,train_step = tutil.run_epoch(lm,\n",
    "                                                            session,\n",
    "                                                            batches,\n",
    "                                                            summary_writer,\n",
    "                                                            train_dir,\n",
    "                                                            train_step,\n",
    "                                                            saver,\n",
    "                                                            hps,\n",
    "                                                            best_loss,\n",
    "                                                            avg_loss)\n",
    "            \n",
    "            if(best_loss is None or curr_best < best_loss):\n",
    "                best_loss = curr_best\n",
    "            print(f\"[EPOCH {i+1}] Complete. Avg Loss: {avg_loss}; Best Loss: {best_loss}\")\n",
    "        sv.stop()\n",
    "        time_total = tutil.pretty_timedelta(since=start_time)\n",
    "        print(f\"[END] Training complete: Best Loss: {best_loss}; Total time: {time_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 252\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 110\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 2 1/2 76\n",
      "\n",
      "\n",
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: 16:03\n",
      "INFO:tensorflow:Fetching data..\n",
      "INFO:tensorflow:Creating batches..\n",
      "INFO:tensorflow:[TOTAL Batches]  : 1872\n",
      "INFO:tensorflow:[TOTAL Examples] : 187193\n",
      "INFO:tensorflow:Creating batches..COMPLETE\n",
      "INFO:tensorflow:Building core graph...\n",
      "INFO:tensorflow:Adding attention_decoder timestep 0 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 1 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 2 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 3 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 4 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 5 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 6 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 7 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 8 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 9 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 10 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 11 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 12 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 13 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 14 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 15 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 16 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 17 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 18 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 19 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 20 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 21 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 22 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 23 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 24 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 25 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 26 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 27 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 28 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 29 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 30 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 31 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 32 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 33 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 34 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 35 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 36 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 37 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 38 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 39 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 40 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 41 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 42 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 43 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 44 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 45 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 46 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 47 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 48 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 49 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 50 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 51 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 52 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 53 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 54 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 55 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 56 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 57 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 58 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 59 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 60 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 61 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 62 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 63 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 64 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 65 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 66 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 67 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 68 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 69 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 70 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 71 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 72 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 73 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 74 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 75 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 76 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 77 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 78 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 79 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 80 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 81 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 82 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 83 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 84 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 85 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 86 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 87 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 88 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 89 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 90 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 91 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 92 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 93 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 94 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 95 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 96 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 97 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 98 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 99 of 100\n",
      "INFO:tensorflow:Building projection graph...\n",
      "INFO:tensorflow:Building projection graph...COMPLETE\n",
      "INFO:tensorflow:Building Loss graph...\n",
      "INFO:tensorflow:Building Loss graph...COMPLETE\n",
      "INFO:tensorflow:Building core graph...COMPLETE\n",
      "INFO:tensorflow:Building train graph...\n",
      "INFO:tensorflow:Building train graph...COMPLETE\n",
      "INFO:tensorflow:Building summary graph...\n",
      "INFO:tensorflow:Building summary graph...COMPLETE\n",
      "WARNING:tensorflow:From <ipython-input-5-17e3ed376236>:18: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-9260\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-9260\n",
      "[EPOCH 6] Starting training..\n",
      "    [batch 1]: seen 100 examples at 3.6 eps, loss = 4.073\n",
      "    [batch 8]: seen 800 examples at 21.2 eps, loss = 4.072\n",
      "    [batch 15]: seen 1500 examples at 31.4 eps, loss = 4.070\n",
      "    [batch 20]: seen 2000 examples at 34.0 eps, loss = 4.068\n",
      "    [batch 24]: seen 2400 examples at 33.6 eps, loss = 4.065\n",
      "    [batch 27]: seen 2700 examples at 32.6 eps, loss = 4.064\n",
      "    [batch 34]: seen 3400 examples at 36.6 eps, loss = 4.068\n",
      "    [batch 39]: seen 3900 examples at 37.6 eps, loss = 4.065\n",
      "    [batch 43]: seen 4300 examples at 37.0 eps, loss = 4.062\n",
      "    [batch 50]: seen 5000 examples at 39.6 eps, loss = 4.063\n",
      "    [batch 55]: seen 5500 examples at 40.1 eps, loss = 4.060\n",
      "    [batch 58]: seen 5800 examples at 39.1 eps, loss = 4.058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 65]: seen 6500 examples at 41.0 eps, loss = 4.059\n",
      "    [batch 68]: seen 6800 examples at 39.3 eps, loss = 4.055\n",
      "    [batch 73]: seen 7300 examples at 39.7 eps, loss = 4.054\n",
      "    [batch 75]: seen 7500 examples at 38.6 eps, loss = 4.053\n",
      "    [batch 80]: seen 8000 examples at 39.0 eps, loss = 4.056\n",
      "    [batch 87]: seen 8700 examples at 40.4 eps, loss = 4.055\n",
      "    [batch 94]: seen 9400 examples at 41.7 eps, loss = 4.057\n",
      "    [batch 101]: seen 10100 examples at 42.9 eps, loss = 4.058\n",
      "    [batch 108]: seen 10800 examples at 44.0 eps, loss = 4.060\n",
      "    [batch 115]: seen 11500 examples at 45.0 eps, loss = 4.065\n",
      "    [batch 122]: seen 12200 examples at 45.9 eps, loss = 4.061\n",
      "    [batch 129]: seen 12900 examples at 46.8 eps, loss = 4.062\n",
      "    [batch 136]: seen 13600 examples at 47.6 eps, loss = 4.064\n",
      "    [batch 143]: seen 14300 examples at 48.3 eps, loss = 4.068\n",
      "    [batch 150]: seen 15000 examples at 49.0 eps, loss = 4.069\n",
      "    [batch 157]: seen 15700 examples at 49.7 eps, loss = 4.066\n",
      "    [batch 164]: seen 16400 examples at 50.3 eps, loss = 4.063\n",
      "    [batch 171]: seen 17100 examples at 50.9 eps, loss = 4.057\n",
      "    [batch 178]: seen 17800 examples at 51.4 eps, loss = 4.056\n",
      "    [batch 185]: seen 18500 examples at 51.9 eps, loss = 4.060\n",
      "    [batch 192]: seen 19200 examples at 52.4 eps, loss = 4.056\n",
      "    [batch 199]: seen 19900 examples at 52.8 eps, loss = 4.056\n",
      "    [batch 206]: seen 20600 examples at 53.3 eps, loss = 4.056\n",
      "    [batch 213]: seen 21300 examples at 53.7 eps, loss = 4.057\n",
      "    [batch 220]: seen 22000 examples at 54.1 eps, loss = 4.056\n",
      "    [batch 227]: seen 22700 examples at 54.4 eps, loss = 4.056\n",
      "    [batch 232]: seen 23200 examples at 54.3 eps, loss = 4.051\n",
      "    [batch 239]: seen 23900 examples at 54.6 eps, loss = 4.053\n",
      "    [batch 246]: seen 24600 examples at 54.9 eps, loss = 4.061\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-9492\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-9492\n",
      "    [batch 252]: seen 25200 examples at 55.0 eps, loss = 4.058\n",
      "    [batch 259]: seen 25900 examples at 55.3 eps, loss = 4.057\n",
      "    [batch 266]: seen 26600 examples at 55.6 eps, loss = 4.055\n",
      "    [batch 273]: seen 27300 examples at 55.9 eps, loss = 4.058\n",
      "    [batch 278]: seen 27800 examples at 55.3 eps, loss = 4.050\n",
      "    [batch 283]: seen 28300 examples at 54.8 eps, loss = 4.049\n",
      "    [batch 286]: seen 28600 examples at 54.2 eps, loss = 4.048\n",
      "    [batch 291]: seen 29100 examples at 54.1 eps, loss = 4.046\n",
      "    [batch 294]: seen 29400 examples at 53.5 eps, loss = 4.046\n",
      "    [batch 301]: seen 30100 examples at 53.8 eps, loss = 4.049\n",
      "    [batch 308]: seen 30800 examples at 54.1 eps, loss = 4.050\n",
      "    [batch 315]: seen 31500 examples at 54.3 eps, loss = 4.048\n",
      "    [batch 322]: seen 32200 examples at 54.6 eps, loss = 4.046\n",
      "    [batch 329]: seen 32900 examples at 54.8 eps, loss = 4.046\n",
      "    [batch 336]: seen 33600 examples at 55.1 eps, loss = 4.049\n",
      "    [batch 343]: seen 34300 examples at 55.3 eps, loss = 4.056\n",
      "    [batch 350]: seen 35000 examples at 55.5 eps, loss = 4.059\n",
      "    [batch 357]: seen 35700 examples at 55.8 eps, loss = 4.064\n",
      "    [batch 364]: seen 36400 examples at 56.0 eps, loss = 4.057\n",
      "    [batch 371]: seen 37100 examples at 56.2 eps, loss = 4.056\n",
      "    [batch 378]: seen 37800 examples at 56.4 eps, loss = 4.057\n",
      "    [batch 385]: seen 38500 examples at 56.6 eps, loss = 4.057\n",
      "    [batch 392]: seen 39200 examples at 56.8 eps, loss = 4.058\n",
      "    [batch 399]: seen 39900 examples at 56.9 eps, loss = 4.066\n",
      "    [batch 406]: seen 40600 examples at 57.1 eps, loss = 4.068\n",
      "    [batch 413]: seen 41300 examples at 57.3 eps, loss = 4.063\n",
      "    [batch 420]: seen 42000 examples at 57.5 eps, loss = 4.062\n",
      "    [batch 427]: seen 42700 examples at 57.6 eps, loss = 4.057\n",
      "    [batch 434]: seen 43400 examples at 57.8 eps, loss = 4.050\n",
      "    [batch 441]: seen 44100 examples at 57.9 eps, loss = 4.049\n",
      "    [batch 448]: seen 44800 examples at 58.1 eps, loss = 4.047\n",
      "    [batch 455]: seen 45500 examples at 58.2 eps, loss = 4.047\n",
      "    [batch 460]: seen 46000 examples at 58.1 eps, loss = 4.044\n",
      "    [batch 463]: seen 46300 examples at 57.7 eps, loss = 4.043\n",
      "    [batch 466]: seen 46600 examples at 57.2 eps, loss = 4.041\n",
      "    [batch 469]: seen 46900 examples at 56.8 eps, loss = 4.039\n",
      "    [batch 472]: seen 47200 examples at 56.4 eps, loss = 4.038\n",
      "    [batch 479]: seen 47900 examples at 56.5 eps, loss = 4.041\n",
      "    [batch 483]: seen 48300 examples at 56.2 eps, loss = 4.034\n",
      "    [batch 488]: seen 48800 examples at 56.1 eps, loss = 4.034\n",
      "    [batch 491]: seen 49100 examples at 55.7 eps, loss = 4.031\n",
      "    [batch 496]: seen 49600 examples at 55.4 eps, loss = 4.028\n",
      "    [batch 499]: seen 49900 examples at 54.8 eps, loss = 4.024\n",
      "    [batch 502]: seen 50200 examples at 54.3 eps, loss = 4.022\n",
      "    [batch 509]: seen 50900 examples at 54.4 eps, loss = 4.025\n",
      "    [batch 516]: seen 51600 examples at 54.6 eps, loss = 4.026\n",
      "    [batch 523]: seen 52300 examples at 54.8 eps, loss = 4.031\n",
      "    [batch 530]: seen 53000 examples at 54.9 eps, loss = 4.026\n",
      "    [batch 537]: seen 53700 examples at 55.1 eps, loss = 4.023\n",
      "    [batch 542]: seen 54200 examples at 55.0 eps, loss = 4.022\n",
      "    [batch 545]: seen 54500 examples at 54.7 eps, loss = 4.020\n",
      "    [batch 550]: seen 55000 examples at 54.4 eps, loss = 4.017\n",
      "    [batch 553]: seen 55300 examples at 53.9 eps, loss = 4.015\n",
      "    [batch 556]: seen 55600 examples at 53.5 eps, loss = 4.011\n",
      "    [batch 563]: seen 56300 examples at 53.6 eps, loss = 4.014\n",
      "    [batch 570]: seen 57000 examples at 53.8 eps, loss = 4.018\n",
      "    [batch 577]: seen 57700 examples at 53.9 eps, loss = 4.023\n",
      "    [batch 584]: seen 58400 examples at 54.1 eps, loss = 4.024\n",
      "    [batch 591]: seen 59100 examples at 54.2 eps, loss = 4.022\n",
      "    [batch 598]: seen 59800 examples at 54.4 eps, loss = 4.028\n",
      "    [batch 605]: seen 60500 examples at 54.5 eps, loss = 4.023\n",
      "    [batch 612]: seen 61200 examples at 54.6 eps, loss = 4.025\n",
      "    [batch 619]: seen 61900 examples at 54.8 eps, loss = 4.023\n",
      "    [batch 626]: seen 62600 examples at 54.9 eps, loss = 4.019\n",
      "    [EXCEPTION]:  OOM when allocating tensor with shape[100,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/Softmax_98 = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/xw_plus_b_98)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'projection/Softmax_98', defined at:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-40345e958dd0>\", line 9, in <module>\n",
      "    train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)\n",
      "  File \"<ipython-input-5-17e3ed376236>\", line 3, in train_continue\n",
      "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/training_util.py\", line 37, in training_init\n",
      "    lm.BuildCoreGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 140, in BuildCoreGraph\n",
      "    self.BuildProjectionGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 169, in BuildProjectionGraph\n",
      "    vocab_distribution = [tf.nn.softmax(s) for s in self.vocab_scores]\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 169, in <listcomp>\n",
      "    vocab_distribution = [tf.nn.softmax(s) for s in self.vocab_scores]\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1738, in softmax\n",
      "    return _softmax(logits, gen_nn_ops.softmax, axis, name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1673, in _softmax\n",
      "    return compute_op(logits, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7097, in softmax\n",
      "    \"Softmax\", logits=logits, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/Softmax_98 = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/xw_plus_b_98)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      " ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-9798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-9798\n",
      "    [batch 630]: seen 63000 examples at 54.4 eps, loss = 4.014\n",
      "    [batch 637]: seen 63700 examples at 54.6 eps, loss = 4.014\n",
      "    [batch 644]: seen 64400 examples at 54.7 eps, loss = 4.015\n",
      "    [batch 651]: seen 65100 examples at 54.8 eps, loss = 4.014\n",
      "    [batch 656]: seen 65600 examples at 54.8 eps, loss = 4.011\n",
      "    [batch 660]: seen 66000 examples at 54.5 eps, loss = 4.010\n",
      "    [batch 667]: seen 66700 examples at 54.6 eps, loss = 4.011\n",
      "    [batch 672]: seen 67200 examples at 54.6 eps, loss = 4.010\n",
      "    [batch 679]: seen 67900 examples at 54.7 eps, loss = 4.013\n",
      "    [batch 686]: seen 68600 examples at 54.8 eps, loss = 4.010\n",
      "    [batch 691]: seen 69100 examples at 54.8 eps, loss = 4.011\n",
      "    [batch 696]: seen 69600 examples at 54.7 eps, loss = 4.009\n",
      "    [batch 700]: seen 70000 examples at 54.5 eps, loss = 4.008\n",
      "    [batch 705]: seen 70500 examples at 54.4 eps, loss = 4.008\n",
      "    [batch 709]: seen 70900 examples at 54.2 eps, loss = 4.006\n",
      "    [batch 713]: seen 71300 examples at 54.0 eps, loss = 4.005\n",
      "    [batch 716]: seen 71600 examples at 53.8 eps, loss = 4.003\n",
      "    [batch 723]: seen 72300 examples at 53.9 eps, loss = 4.005\n",
      "    [batch 730]: seen 73000 examples at 54.0 eps, loss = 4.009\n",
      "    [batch 737]: seen 73700 examples at 54.1 eps, loss = 4.011\n",
      "    [batch 744]: seen 74400 examples at 54.2 eps, loss = 4.018\n",
      "    [batch 751]: seen 75100 examples at 54.3 eps, loss = 4.017\n",
      "    [batch 758]: seen 75800 examples at 54.4 eps, loss = 4.018\n",
      "    [batch 765]: seen 76500 examples at 54.6 eps, loss = 4.018\n",
      "    [batch 772]: seen 77200 examples at 54.7 eps, loss = 4.018\n",
      "    [batch 779]: seen 77900 examples at 54.8 eps, loss = 4.012\n",
      "    [batch 786]: seen 78600 examples at 54.9 eps, loss = 4.004\n",
      "    [batch 791]: seen 79100 examples at 54.8 eps, loss = 4.004\n",
      "    [batch 798]: seen 79800 examples at 54.8 eps, loss = 4.002\n",
      "    [batch 801]: seen 80100 examples at 54.6 eps, loss = 4.000\n",
      "    [batch 806]: seen 80600 examples at 54.4 eps, loss = 3.999\n",
      "    [batch 808]: seen 80800 examples at 54.2 eps, loss = 3.998\n",
      "    [batch 813]: seen 81300 examples at 54.1 eps, loss = 3.998\n",
      "    [batch 816]: seen 81600 examples at 53.9 eps, loss = 3.997\n",
      "    [batch 819]: seen 81900 examples at 53.7 eps, loss = 3.995\n",
      "    [batch 822]: seen 82200 examples at 53.4 eps, loss = 3.994\n",
      "    [batch 825]: seen 82500 examples at 53.1 eps, loss = 3.990\n",
      "    [batch 832]: seen 83200 examples at 53.2 eps, loss = 3.993\n",
      "    [batch 839]: seen 83900 examples at 53.3 eps, loss = 4.001\n",
      "    [batch 846]: seen 84600 examples at 53.4 eps, loss = 4.004\n",
      "    [batch 853]: seen 85300 examples at 53.5 eps, loss = 4.004\n",
      "    [batch 860]: seen 86000 examples at 53.6 eps, loss = 4.001\n",
      "    [batch 867]: seen 86700 examples at 53.7 eps, loss = 4.001\n",
      "    [batch 874]: seen 87400 examples at 53.8 eps, loss = 4.004\n",
      "    [batch 881]: seen 88100 examples at 53.9 eps, loss = 4.002\n",
      "    [batch 888]: seen 88800 examples at 54.0 eps, loss = 4.003\n",
      "    [batch 895]: seen 89500 examples at 54.1 eps, loss = 4.005\n",
      "    [batch 902]: seen 90200 examples at 54.2 eps, loss = 4.007\n",
      "    [batch 909]: seen 90900 examples at 54.3 eps, loss = 4.012\n",
      "    [batch 916]: seen 91600 examples at 54.4 eps, loss = 4.010\n",
      "    [batch 923]: seen 92300 examples at 54.5 eps, loss = 4.004\n",
      "    [batch 930]: seen 93000 examples at 54.6 eps, loss = 4.002\n",
      "    [batch 937]: seen 93700 examples at 54.7 eps, loss = 4.001\n",
      "    [batch 944]: seen 94400 examples at 54.8 eps, loss = 4.009\n",
      "    [batch 951]: seen 95100 examples at 54.8 eps, loss = 4.006\n",
      "    [batch 958]: seen 95800 examples at 54.9 eps, loss = 4.007\n",
      "    [batch 965]: seen 96500 examples at 55.0 eps, loss = 4.003\n",
      "    [batch 972]: seen 97200 examples at 55.1 eps, loss = 4.003\n",
      "    [batch 979]: seen 97900 examples at 55.2 eps, loss = 4.005\n",
      "    [batch 986]: seen 98600 examples at 55.3 eps, loss = 4.009\n",
      "    [batch 993]: seen 99300 examples at 55.3 eps, loss = 4.012\n",
      "    [batch 1000]: seen 100000 examples at 55.4 eps, loss = 4.010\n",
      "    [batch 1007]: seen 100700 examples at 55.5 eps, loss = 4.013\n",
      "    [batch 1014]: seen 101400 examples at 55.6 eps, loss = 4.007\n",
      "    [batch 1021]: seen 102100 examples at 55.7 eps, loss = 4.009\n",
      "    [batch 1028]: seen 102800 examples at 55.7 eps, loss = 4.012\n",
      "    [batch 1035]: seen 103500 examples at 55.8 eps, loss = 4.012\n",
      "    [batch 1042]: seen 104200 examples at 55.9 eps, loss = 4.014\n",
      "    [batch 1049]: seen 104900 examples at 56.0 eps, loss = 4.012\n",
      "    [batch 1056]: seen 105600 examples at 56.0 eps, loss = 4.006\n",
      "    [batch 1063]: seen 106300 examples at 56.1 eps, loss = 4.010\n",
      "    [batch 1070]: seen 107000 examples at 56.2 eps, loss = 4.009\n",
      "    [batch 1077]: seen 107700 examples at 56.2 eps, loss = 4.009\n",
      "    [batch 1084]: seen 108400 examples at 56.3 eps, loss = 4.003\n",
      "    [batch 1091]: seen 109100 examples at 56.4 eps, loss = 4.005\n",
      "    [batch 1098]: seen 109800 examples at 56.4 eps, loss = 4.006\n",
      "    [batch 1105]: seen 110500 examples at 56.5 eps, loss = 4.001\n",
      "    [batch 1112]: seen 111200 examples at 56.6 eps, loss = 4.001\n",
      "    [batch 1119]: seen 111900 examples at 56.6 eps, loss = 4.004\n",
      "    [batch 1126]: seen 112600 examples at 56.7 eps, loss = 4.013\n",
      "    [batch 1133]: seen 113300 examples at 56.8 eps, loss = 4.006\n",
      "    [batch 1140]: seen 114000 examples at 56.8 eps, loss = 4.003\n",
      "    [batch 1147]: seen 114700 examples at 56.9 eps, loss = 4.002\n",
      "    [batch 1154]: seen 115400 examples at 57.0 eps, loss = 4.007\n",
      "    [batch 1161]: seen 116100 examples at 57.0 eps, loss = 4.005\n",
      "    [batch 1168]: seen 116800 examples at 57.1 eps, loss = 4.004\n",
      "    [batch 1175]: seen 117500 examples at 57.2 eps, loss = 4.000\n",
      "    [batch 1182]: seen 118200 examples at 57.2 eps, loss = 4.002\n",
      "    [batch 1189]: seen 118900 examples at 57.3 eps, loss = 4.001\n",
      "    [batch 1196]: seen 119600 examples at 57.3 eps, loss = 3.998\n",
      "    [batch 1203]: seen 120300 examples at 57.4 eps, loss = 3.993\n",
      "    [batch 1210]: seen 121000 examples at 57.5 eps, loss = 3.992\n",
      "    [batch 1214]: seen 121400 examples at 57.3 eps, loss = 3.989\n",
      "    [batch 1217]: seen 121700 examples at 57.2 eps, loss = 3.989\n",
      "    [batch 1222]: seen 122200 examples at 57.1 eps, loss = 3.990\n",
      "    [batch 1227]: seen 122700 examples at 57.0 eps, loss = 3.990\n",
      "    [batch 1232]: seen 123200 examples at 56.9 eps, loss = 3.985\n",
      "    [batch 1235]: seen 123500 examples at 56.7 eps, loss = 3.984\n",
      "    [batch 1240]: seen 124000 examples at 56.6 eps, loss = 3.984\n",
      "    [batch 1246]: seen 124600 examples at 56.6 eps, loss = 3.983\n",
      "    [batch 1253]: seen 125300 examples at 56.6 eps, loss = 3.985\n",
      "    [batch 1260]: seen 126000 examples at 56.7 eps, loss = 3.988\n",
      "    [batch 1266]: seen 126600 examples at 56.7 eps, loss = 3.982\n",
      "    [batch 1269]: seen 126900 examples at 56.5 eps, loss = 3.979\n",
      "    [batch 1274]: seen 127400 examples at 56.5 eps, loss = 3.979\n",
      "    [batch 1277]: seen 127700 examples at 56.3 eps, loss = 3.976\n",
      "    [batch 1282]: seen 128200 examples at 56.2 eps, loss = 3.980\n",
      "    [batch 1289]: seen 128900 examples at 56.3 eps, loss = 3.980\n",
      "    [batch 1296]: seen 129600 examples at 56.3 eps, loss = 3.982\n",
      "    [batch 1303]: seen 130300 examples at 56.4 eps, loss = 3.980\n",
      "    [batch 1310]: seen 131000 examples at 56.5 eps, loss = 3.979\n",
      "    [batch 1317]: seen 131700 examples at 56.5 eps, loss = 3.986\n",
      "    [batch 1324]: seen 132400 examples at 56.6 eps, loss = 3.999\n",
      "    [batch 1331]: seen 133100 examples at 56.6 eps, loss = 3.997\n",
      "    [batch 1338]: seen 133800 examples at 56.7 eps, loss = 3.993\n",
      "    [batch 1345]: seen 134500 examples at 56.7 eps, loss = 3.993\n",
      "    [batch 1352]: seen 135200 examples at 56.8 eps, loss = 3.993\n",
      "    [batch 1359]: seen 135900 examples at 56.8 eps, loss = 3.994\n",
      "    [batch 1366]: seen 136600 examples at 56.9 eps, loss = 3.995\n",
      "    [batch 1373]: seen 137300 examples at 57.0 eps, loss = 3.992\n",
      "    [batch 1380]: seen 138000 examples at 57.0 eps, loss = 3.993\n",
      "    [batch 1387]: seen 138700 examples at 57.1 eps, loss = 3.994\n",
      "    [batch 1394]: seen 139400 examples at 57.1 eps, loss = 3.999\n",
      "    [batch 1401]: seen 140100 examples at 57.2 eps, loss = 3.995\n",
      "    [batch 1408]: seen 140800 examples at 57.2 eps, loss = 3.984\n",
      "    [batch 1415]: seen 141500 examples at 57.3 eps, loss = 3.989\n",
      "    [batch 1422]: seen 142200 examples at 57.3 eps, loss = 3.983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1429]: seen 142900 examples at 57.3 eps, loss = 3.975\n",
      "    [batch 1436]: seen 143600 examples at 57.3 eps, loss = 3.984\n",
      "    [batch 1443]: seen 144300 examples at 57.4 eps, loss = 3.978\n",
      "    [batch 1449]: seen 144900 examples at 57.3 eps, loss = 3.975\n",
      "    [batch 1452]: seen 145200 examples at 57.2 eps, loss = 3.975\n",
      "    [batch 1455]: seen 145500 examples at 57.1 eps, loss = 3.972\n",
      "    [batch 1462]: seen 146200 examples at 57.1 eps, loss = 3.972\n",
      "    [batch 1465]: seen 146500 examples at 57.0 eps, loss = 3.970\n",
      "    [batch 1472]: seen 147200 examples at 57.1 eps, loss = 3.977\n",
      "    [batch 1479]: seen 147900 examples at 57.1 eps, loss = 3.974\n",
      "    [batch 1486]: seen 148600 examples at 57.2 eps, loss = 3.975\n",
      "    [batch 1494]: seen 149400 examples at 57.2 eps, loss = 3.970\n",
      "    [batch 1497]: seen 149700 examples at 57.1 eps, loss = 3.969\n",
      "    [batch 1504]: seen 150400 examples at 57.1 eps, loss = 3.974\n",
      "    [batch 1511]: seen 151100 examples at 57.2 eps, loss = 3.977\n",
      "    [batch 1519]: seen 151900 examples at 57.2 eps, loss = 3.973\n",
      "    [batch 1527]: seen 152700 examples at 57.3 eps, loss = 3.978\n",
      "    [batch 1535]: seen 153500 examples at 57.3 eps, loss = 3.974\n",
      "    [batch 1543]: seen 154300 examples at 57.4 eps, loss = 3.978\n",
      "    [batch 1551]: seen 155100 examples at 57.5 eps, loss = 3.981\n",
      "    [batch 1558]: seen 155800 examples at 57.5 eps, loss = 3.988\n",
      "    [batch 1566]: seen 156600 examples at 57.6 eps, loss = 3.990\n",
      "    [batch 1574]: seen 157400 examples at 57.6 eps, loss = 3.990\n",
      "    [batch 1582]: seen 158200 examples at 57.7 eps, loss = 3.989\n",
      "    [batch 1589]: seen 158900 examples at 57.7 eps, loss = 3.991\n",
      "    [EXCEPTION]:  OOM when allocating tensor with shape[100,50017] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/final_distribution/concat_98 = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/final_distribution/mul_98, projection/final_distribution/zeros, Training/gradients/b_count)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'projection/final_distribution/concat_98', defined at:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-40345e958dd0>\", line 9, in <module>\n",
      "    train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)\n",
      "  File \"<ipython-input-5-17e3ed376236>\", line 3, in train_continue\n",
      "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/training_util.py\", line 37, in training_init\n",
      "    lm.BuildCoreGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 140, in BuildCoreGraph\n",
      "    self.BuildProjectionGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 177, in BuildProjectionGraph\n",
      "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 177, in <listcomp>\n",
      "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1189, in concat\n",
      "    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 953, in concat_v2\n",
      "    \"ConcatV2\", values=values, axis=axis, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,50017] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/final_distribution/concat_98 = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/final_distribution/mul_98, projection/final_distribution/zeros, Training/gradients/b_count)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      " ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-10665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-10665\n",
      "    [batch 1591]: seen 159100 examples at 57.5 eps, loss = 3.989\n",
      "    [batch 1598]: seen 159800 examples at 57.5 eps, loss = 3.989\n",
      "    [batch 1606]: seen 160600 examples at 57.6 eps, loss = 3.982\n",
      "    [batch 1614]: seen 161400 examples at 57.6 eps, loss = 3.980\n",
      "    [batch 1622]: seen 162200 examples at 57.7 eps, loss = 3.981\n",
      "    [batch 1629]: seen 162900 examples at 57.7 eps, loss = 3.985\n",
      "    [batch 1637]: seen 163700 examples at 57.8 eps, loss = 3.980\n",
      "    [batch 1645]: seen 164500 examples at 57.8 eps, loss = 3.979\n",
      "    [batch 1653]: seen 165300 examples at 57.9 eps, loss = 3.979\n",
      "    [batch 1661]: seen 166100 examples at 57.9 eps, loss = 3.982\n",
      "    [batch 1669]: seen 166900 examples at 58.0 eps, loss = 3.982\n",
      "    [batch 1677]: seen 167700 examples at 58.0 eps, loss = 3.983\n",
      "    [batch 1685]: seen 168500 examples at 58.1 eps, loss = 3.985\n",
      "    [batch 1693]: seen 169300 examples at 58.1 eps, loss = 3.987\n",
      "    [batch 1701]: seen 170100 examples at 58.2 eps, loss = 3.985\n",
      "    [batch 1709]: seen 170900 examples at 58.2 eps, loss = 3.981\n",
      "    [batch 1717]: seen 171700 examples at 58.3 eps, loss = 3.982\n",
      "    [batch 1725]: seen 172500 examples at 58.3 eps, loss = 3.978\n",
      "    [batch 1733]: seen 173300 examples at 58.3 eps, loss = 3.979\n",
      "    [batch 1741]: seen 174100 examples at 58.4 eps, loss = 3.989\n",
      "    [batch 1749]: seen 174900 examples at 58.4 eps, loss = 3.995\n",
      "    [batch 1756]: seen 175600 examples at 58.5 eps, loss = 3.988\n",
      "    [batch 1764]: seen 176400 examples at 58.5 eps, loss = 3.982\n",
      "    [batch 1772]: seen 177200 examples at 58.6 eps, loss = 3.978\n",
      "    [batch 1780]: seen 178000 examples at 58.6 eps, loss = 3.972\n",
      "    [batch 1788]: seen 178800 examples at 58.6 eps, loss = 3.972\n",
      "    [batch 1795]: seen 179500 examples at 58.7 eps, loss = 3.970\n",
      "    [batch 1798]: seen 179800 examples at 58.6 eps, loss = 3.967\n",
      "    [batch 1803]: seen 180300 examples at 58.5 eps, loss = 3.967\n",
      "    [batch 1811]: seen 181100 examples at 58.6 eps, loss = 3.969\n",
      "    [batch 1818]: seen 181800 examples at 58.6 eps, loss = 3.967\n",
      "    [batch 1825]: seen 182500 examples at 58.7 eps, loss = 3.967\n",
      "    [batch 1830]: seen 183000 examples at 58.6 eps, loss = 3.967\n",
      "    [batch 1837]: seen 183700 examples at 58.6 eps, loss = 3.971\n",
      "    [batch 1844]: seen 184400 examples at 58.7 eps, loss = 3.978\n",
      "    [batch 1852]: seen 185200 examples at 58.7 eps, loss = 3.972\n",
      "    [batch 1860]: seen 186000 examples at 58.7 eps, loss = 3.964\n",
      "    [batch 1865]: seen 186500 examples at 58.7 eps, loss = 3.965\n",
      "    [batch 1868]: seen 186800 examples at 58.6 eps, loss = 3.961\n",
      "    [END] Training complete: Total examples : 186900; Total time: 0:53:15\n",
      "[EPOCH 6] Complete. Avg Loss: 3.960545786230878; Best Loss: 3.960545786230878\n",
      "[EPOCH 7] Starting training..\n",
      "    [batch 3]: seen 300 examples at 21.0 eps, loss = 3.958\n",
      "    [batch 7]: seen 700 examples at 26.2 eps, loss = 3.955\n",
      "    [batch 12]: seen 1200 examples at 29.6 eps, loss = 3.954\n",
      "    [batch 15]: seen 1500 examples at 29.2 eps, loss = 3.954\n",
      "    [batch 20]: seen 2000 examples at 32.3 eps, loss = 3.956\n",
      "    [batch 28]: seen 2800 examples at 38.2 eps, loss = 3.954\n",
      "    [batch 36]: seen 3600 examples at 42.5 eps, loss = 3.958\n",
      "    [batch 42]: seen 4200 examples at 43.5 eps, loss = 3.951\n",
      "    [batch 47]: seen 4700 examples at 43.9 eps, loss = 3.952\n",
      "    [batch 55]: seen 5500 examples at 46.4 eps, loss = 3.953\n",
      "    [batch 63]: seen 6300 examples at 48.5 eps, loss = 3.955\n",
      "    [batch 66]: seen 6600 examples at 46.9 eps, loss = 3.949\n",
      "    [batch 71]: seen 7100 examples at 46.9 eps, loss = 3.949\n",
      "    [batch 74]: seen 7400 examples at 45.4 eps, loss = 3.948\n",
      "    [batch 79]: seen 7900 examples at 45.6 eps, loss = 3.950\n",
      "    [batch 87]: seen 8700 examples at 47.1 eps, loss = 3.952\n",
      "    [batch 94]: seen 9400 examples at 48.3 eps, loss = 3.949\n",
      "    [batch 102]: seen 10200 examples at 49.5 eps, loss = 3.950\n",
      "    [batch 109]: seen 10900 examples at 50.4 eps, loss = 3.953\n",
      "    [batch 114]: seen 11400 examples at 50.3 eps, loss = 3.946\n",
      "    [batch 119]: seen 11900 examples at 50.2 eps, loss = 3.946\n",
      "    [batch 125]: seen 12500 examples at 50.2 eps, loss = 3.945\n",
      "    [batch 130]: seen 13000 examples at 49.5 eps, loss = 3.943\n",
      "    [batch 135]: seen 13500 examples at 48.8 eps, loss = 3.942\n",
      "    [batch 138]: seen 13800 examples at 47.5 eps, loss = 3.939\n",
      "    [batch 141]: seen 14100 examples at 46.7 eps, loss = 3.936\n",
      "    [batch 144]: seen 14400 examples at 46.1 eps, loss = 3.935\n",
      "    [batch 149]: seen 14900 examples at 45.5 eps, loss = 3.932\n",
      "    [batch 153]: seen 15300 examples at 45.1 eps, loss = 3.929\n",
      "    [batch 156]: seen 15600 examples at 44.1 eps, loss = 3.926\n",
      "    [batch 161]: seen 16100 examples at 44.2 eps, loss = 3.928\n",
      "    [batch 169]: seen 16900 examples at 45.0 eps, loss = 3.930\n",
      "    [batch 176]: seen 17600 examples at 45.6 eps, loss = 3.932\n",
      "    [batch 184]: seen 18400 examples at 46.3 eps, loss = 3.932\n",
      "    [batch 192]: seen 19200 examples at 47.0 eps, loss = 3.937\n",
      "    [batch 200]: seen 20000 examples at 47.6 eps, loss = 3.936\n",
      "    [batch 208]: seen 20800 examples at 48.2 eps, loss = 3.936\n",
      "    [batch 215]: seen 21500 examples at 48.7 eps, loss = 3.933\n",
      "    [batch 223]: seen 22300 examples at 49.3 eps, loss = 3.933\n",
      "    [batch 231]: seen 23100 examples at 49.8 eps, loss = 3.936\n",
      "    [batch 239]: seen 23900 examples at 50.3 eps, loss = 3.931\n",
      "    [batch 246]: seen 24600 examples at 50.7 eps, loss = 3.932\n",
      "    [batch 254]: seen 25400 examples at 51.1 eps, loss = 3.934\n",
      "    [batch 262]: seen 26200 examples at 51.5 eps, loss = 3.942\n",
      "    [batch 269]: seen 26900 examples at 51.9 eps, loss = 3.940\n",
      "    [batch 277]: seen 27700 examples at 52.3 eps, loss = 3.943\n",
      "    [batch 285]: seen 28500 examples at 52.7 eps, loss = 3.940\n",
      "    [batch 293]: seen 29300 examples at 53.0 eps, loss = 3.942\n",
      "    [batch 301]: seen 30100 examples at 53.4 eps, loss = 3.944\n",
      "    [batch 309]: seen 30900 examples at 53.7 eps, loss = 3.948\n",
      "    [batch 317]: seen 31700 examples at 54.0 eps, loss = 3.940\n",
      "    [batch 325]: seen 32500 examples at 54.3 eps, loss = 3.941\n",
      "    [batch 333]: seen 33300 examples at 54.6 eps, loss = 3.939\n",
      "    [batch 341]: seen 34100 examples at 54.9 eps, loss = 3.939\n",
      "    [batch 349]: seen 34900 examples at 55.2 eps, loss = 3.935\n",
      "    [batch 357]: seen 35700 examples at 55.4 eps, loss = 3.941\n",
      "    [batch 365]: seen 36500 examples at 55.7 eps, loss = 3.941\n",
      "    [batch 373]: seen 37300 examples at 56.0 eps, loss = 3.950\n",
      "    [batch 381]: seen 38100 examples at 56.2 eps, loss = 3.947\n",
      "    [batch 389]: seen 38900 examples at 56.4 eps, loss = 3.949\n",
      "    [batch 397]: seen 39700 examples at 56.6 eps, loss = 3.944\n",
      "    [batch 405]: seen 40500 examples at 56.9 eps, loss = 3.945\n",
      "    [batch 413]: seen 41300 examples at 57.1 eps, loss = 3.946\n",
      "    [batch 421]: seen 42100 examples at 57.3 eps, loss = 3.939\n",
      "    [batch 429]: seen 42900 examples at 57.5 eps, loss = 3.942\n",
      "    [batch 437]: seen 43700 examples at 57.7 eps, loss = 3.941\n",
      "    [batch 445]: seen 44500 examples at 57.8 eps, loss = 3.941\n",
      "    [batch 453]: seen 45300 examples at 58.0 eps, loss = 3.939\n",
      "    [batch 460]: seen 46000 examples at 58.2 eps, loss = 3.941\n",
      "    [batch 468]: seen 46800 examples at 58.3 eps, loss = 3.936\n",
      "    [batch 475]: seen 47500 examples at 58.5 eps, loss = 3.929\n",
      "    [batch 483]: seen 48300 examples at 58.6 eps, loss = 3.933\n",
      "    [batch 489]: seen 48900 examples at 58.5 eps, loss = 3.925\n",
      "    [batch 494]: seen 49400 examples at 58.4 eps, loss = 3.925\n",
      "    [batch 501]: seen 50100 examples at 58.5 eps, loss = 3.925\n",
      "    [batch 509]: seen 50900 examples at 58.7 eps, loss = 3.930\n",
      "    [batch 516]: seen 51600 examples at 58.8 eps, loss = 3.936\n",
      "    [batch 524]: seen 52400 examples at 59.0 eps, loss = 3.934\n",
      "    [batch 532]: seen 53200 examples at 59.1 eps, loss = 3.935\n",
      "    [batch 539]: seen 53900 examples at 59.2 eps, loss = 3.933\n",
      "    [batch 547]: seen 54700 examples at 59.4 eps, loss = 3.933\n",
      "    [batch 555]: seen 55500 examples at 59.5 eps, loss = 3.928\n",
      "    [batch 563]: seen 56300 examples at 59.4 eps, loss = 3.924\n",
      "    [batch 568]: seen 56800 examples at 59.3 eps, loss = 3.926\n",
      "    [batch 576]: seen 57600 examples at 59.4 eps, loss = 3.931\n",
      "    [batch 583]: seen 58300 examples at 59.5 eps, loss = 3.935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 590]: seen 59000 examples at 59.6 eps, loss = 3.930\n",
      "    [batch 598]: seen 59800 examples at 59.7 eps, loss = 3.927\n",
      "    [batch 606]: seen 60600 examples at 59.9 eps, loss = 3.929\n",
      "    [batch 614]: seen 61400 examples at 60.0 eps, loss = 3.931\n",
      "    [batch 622]: seen 62200 examples at 60.1 eps, loss = 3.931\n",
      "    [batch 629]: seen 62900 examples at 60.2 eps, loss = 3.928\n",
      "    [batch 637]: seen 63700 examples at 60.3 eps, loss = 3.931\n",
      "    [batch 645]: seen 64500 examples at 60.4 eps, loss = 3.931\n",
      "    [batch 653]: seen 65300 examples at 60.5 eps, loss = 3.931\n",
      "    [batch 659]: seen 65900 examples at 60.4 eps, loss = 3.923\n",
      "    [batch 664]: seen 66400 examples at 60.3 eps, loss = 3.924\n",
      "    [batch 672]: seen 67200 examples at 60.4 eps, loss = 3.923\n",
      "    [batch 675]: seen 67500 examples at 60.0 eps, loss = 3.919\n",
      "    [batch 679]: seen 67900 examples at 59.7 eps, loss = 3.917\n",
      "    [batch 683]: seen 68300 examples at 59.4 eps, loss = 3.915\n",
      "    [batch 688]: seen 68800 examples at 59.1 eps, loss = 3.914\n",
      "    [batch 691]: seen 69100 examples at 58.7 eps, loss = 3.911\n",
      "    [batch 698]: seen 69800 examples at 58.8 eps, loss = 3.917\n",
      "    [batch 705]: seen 70500 examples at 58.9 eps, loss = 3.914\n",
      "    [batch 713]: seen 71300 examples at 59.0 eps, loss = 3.918\n",
      "    [batch 721]: seen 72100 examples at 59.1 eps, loss = 3.925\n",
      "    [batch 729]: seen 72900 examples at 59.2 eps, loss = 3.925\n",
      "    [batch 736]: seen 73600 examples at 59.3 eps, loss = 3.922\n",
      "    [batch 743]: seen 74300 examples at 59.3 eps, loss = 3.918\n",
      "    [batch 750]: seen 75000 examples at 59.4 eps, loss = 3.916\n",
      "    [batch 758]: seen 75800 examples at 59.5 eps, loss = 3.913\n",
      "    [batch 766]: seen 76600 examples at 59.6 eps, loss = 3.916\n",
      "    [batch 774]: seen 77400 examples at 59.7 eps, loss = 3.914\n",
      "    [batch 779]: seen 77900 examples at 59.6 eps, loss = 3.910\n",
      "    [batch 782]: seen 78200 examples at 59.3 eps, loss = 3.905\n",
      "    [batch 787]: seen 78700 examples at 59.3 eps, loss = 3.904\n",
      "    [batch 790]: seen 79000 examples at 59.0 eps, loss = 3.905\n",
      "    [batch 797]: seen 79700 examples at 59.1 eps, loss = 3.917\n",
      "    [batch 804]: seen 80400 examples at 59.2 eps, loss = 3.917\n",
      "    [batch 812]: seen 81200 examples at 59.3 eps, loss = 3.919\n",
      "    [batch 820]: seen 82000 examples at 59.3 eps, loss = 3.911\n",
      "    [batch 828]: seen 82800 examples at 59.4 eps, loss = 3.914\n",
      "    [batch 835]: seen 83500 examples at 59.5 eps, loss = 3.916\n",
      "    [batch 843]: seen 84300 examples at 59.6 eps, loss = 3.923\n",
      "    [batch 851]: seen 85100 examples at 59.7 eps, loss = 3.920\n",
      "    [batch 858]: seen 85800 examples at 59.7 eps, loss = 3.916\n",
      "    [batch 866]: seen 86600 examples at 59.8 eps, loss = 3.917\n",
      "    [batch 874]: seen 87400 examples at 59.9 eps, loss = 3.913\n",
      "    [batch 882]: seen 88200 examples at 60.0 eps, loss = 3.918\n",
      "    [batch 889]: seen 88900 examples at 60.1 eps, loss = 3.917\n",
      "    [batch 896]: seen 89600 examples at 60.1 eps, loss = 3.919\n",
      "    [batch 904]: seen 90400 examples at 60.2 eps, loss = 3.916\n",
      "    [batch 911]: seen 91100 examples at 60.3 eps, loss = 3.912\n",
      "    [batch 919]: seen 91900 examples at 60.3 eps, loss = 3.915\n",
      "    [batch 926]: seen 92600 examples at 60.4 eps, loss = 3.912\n",
      "    [batch 934]: seen 93400 examples at 60.5 eps, loss = 3.914\n",
      "    [batch 941]: seen 94100 examples at 60.5 eps, loss = 3.908\n",
      "    [batch 949]: seen 94900 examples at 60.6 eps, loss = 3.905\n",
      "    [batch 956]: seen 95600 examples at 60.7 eps, loss = 3.908\n",
      "    [batch 964]: seen 96400 examples at 60.7 eps, loss = 3.907\n",
      "    [batch 969]: seen 96900 examples at 60.6 eps, loss = 3.903\n",
      "    [batch 974]: seen 97400 examples at 60.6 eps, loss = 3.904\n",
      "    [batch 977]: seen 97700 examples at 60.3 eps, loss = 3.902\n",
      "    [batch 982]: seen 98200 examples at 60.2 eps, loss = 3.904\n",
      "    [batch 987]: seen 98700 examples at 60.0 eps, loss = 3.899\n",
      "    [batch 994]: seen 99400 examples at 60.1 eps, loss = 3.902\n",
      "    [batch 1001]: seen 100100 examples at 60.1 eps, loss = 3.902\n",
      "    [batch 1007]: seen 100700 examples at 60.1 eps, loss = 3.898\n",
      "    [batch 1012]: seen 101200 examples at 60.0 eps, loss = 3.898\n",
      "    [batch 1017]: seen 101700 examples at 59.9 eps, loss = 3.897\n",
      "    [batch 1022]: seen 102200 examples at 59.9 eps, loss = 3.895\n",
      "    [batch 1025]: seen 102500 examples at 59.6 eps, loss = 3.895\n",
      "    [batch 1033]: seen 103300 examples at 59.7 eps, loss = 3.897\n",
      "    [batch 1038]: seen 103800 examples at 59.6 eps, loss = 3.894\n",
      "    [batch 1046]: seen 104600 examples at 59.7 eps, loss = 3.896\n",
      "    [batch 1054]: seen 105400 examples at 59.8 eps, loss = 3.895\n",
      "    [batch 1062]: seen 106200 examples at 59.8 eps, loss = 3.906\n",
      "    [batch 1070]: seen 107000 examples at 59.9 eps, loss = 3.906\n",
      "    [batch 1078]: seen 107800 examples at 60.0 eps, loss = 3.904\n",
      "    [batch 1085]: seen 108500 examples at 60.0 eps, loss = 3.911\n",
      "    [batch 1093]: seen 109300 examples at 60.1 eps, loss = 3.914\n",
      "    [batch 1101]: seen 110100 examples at 60.2 eps, loss = 3.913\n",
      "    [batch 1108]: seen 110800 examples at 60.2 eps, loss = 3.915\n",
      "    [batch 1116]: seen 111600 examples at 60.3 eps, loss = 3.916\n",
      "    [batch 1124]: seen 112400 examples at 60.3 eps, loss = 3.917\n",
      "    [batch 1132]: seen 113200 examples at 60.4 eps, loss = 3.914\n",
      "    [batch 1139]: seen 113900 examples at 60.4 eps, loss = 3.911\n",
      "    [batch 1146]: seen 114600 examples at 60.5 eps, loss = 3.908\n",
      "    [batch 1154]: seen 115400 examples at 60.5 eps, loss = 3.908\n",
      "    [batch 1162]: seen 116200 examples at 60.6 eps, loss = 3.911\n",
      "    [batch 1169]: seen 116900 examples at 60.7 eps, loss = 3.909\n",
      "    [batch 1176]: seen 117600 examples at 60.7 eps, loss = 3.902\n",
      "    [batch 1184]: seen 118400 examples at 60.8 eps, loss = 3.899\n",
      "    [batch 1192]: seen 119200 examples at 60.8 eps, loss = 3.896\n",
      "    [batch 1199]: seen 119900 examples at 60.9 eps, loss = 3.899\n",
      "    [batch 1207]: seen 120700 examples at 60.9 eps, loss = 3.893\n",
      "    [batch 1215]: seen 121500 examples at 61.0 eps, loss = 3.894\n",
      "    [batch 1220]: seen 122000 examples at 60.9 eps, loss = 3.890\n",
      "    [batch 1227]: seen 122700 examples at 60.9 eps, loss = 3.898\n",
      "    [batch 1235]: seen 123500 examples at 61.0 eps, loss = 3.905\n",
      "    [batch 1242]: seen 124200 examples at 61.0 eps, loss = 3.905\n",
      "    [batch 1249]: seen 124900 examples at 61.1 eps, loss = 3.908\n",
      "    [batch 1257]: seen 125700 examples at 61.1 eps, loss = 3.908\n",
      "    [batch 1265]: seen 126500 examples at 61.2 eps, loss = 3.912\n",
      "    [batch 1272]: seen 127200 examples at 61.2 eps, loss = 3.911\n",
      "    [batch 1279]: seen 127900 examples at 61.3 eps, loss = 3.914\n",
      "    [batch 1287]: seen 128700 examples at 61.3 eps, loss = 3.910\n",
      "    [batch 1295]: seen 129500 examples at 61.4 eps, loss = 3.907\n",
      "    [batch 1303]: seen 130300 examples at 61.4 eps, loss = 3.906\n",
      "    [batch 1311]: seen 131100 examples at 61.5 eps, loss = 3.902\n",
      "    [batch 1319]: seen 131900 examples at 61.5 eps, loss = 3.901\n",
      "    [batch 1327]: seen 132700 examples at 61.5 eps, loss = 3.899\n",
      "    [batch 1334]: seen 133400 examples at 61.6 eps, loss = 3.897\n",
      "    [batch 1341]: seen 134100 examples at 61.6 eps, loss = 3.899\n",
      "    [batch 1348]: seen 134800 examples at 61.7 eps, loss = 3.894\n",
      "    [batch 1355]: seen 135500 examples at 61.7 eps, loss = 3.896\n",
      "    [batch 1363]: seen 136300 examples at 61.7 eps, loss = 3.895\n",
      "    [batch 1371]: seen 137100 examples at 61.8 eps, loss = 3.890\n",
      "    [batch 1374]: seen 137400 examples at 61.5 eps, loss = 3.887\n",
      "    [batch 1379]: seen 137900 examples at 61.4 eps, loss = 3.886\n",
      "    [batch 1386]: seen 138600 examples at 61.4 eps, loss = 3.888\n",
      "    [batch 1394]: seen 139400 examples at 61.4 eps, loss = 3.892\n",
      "    [batch 1402]: seen 140200 examples at 61.5 eps, loss = 3.889\n",
      "    [batch 1410]: seen 141000 examples at 61.5 eps, loss = 3.888\n",
      "    [batch 1417]: seen 141700 examples at 61.6 eps, loss = 3.886\n",
      "    [batch 1422]: seen 142200 examples at 61.5 eps, loss = 3.886\n",
      "    [batch 1429]: seen 142900 examples at 61.5 eps, loss = 3.891\n",
      "    [batch 1436]: seen 143600 examples at 61.6 eps, loss = 3.887\n",
      "    [batch 1443]: seen 144300 examples at 61.6 eps, loss = 3.891\n",
      "    [batch 1450]: seen 145000 examples at 61.6 eps, loss = 3.890\n",
      "    [batch 1458]: seen 145800 examples at 61.7 eps, loss = 3.892\n",
      "    [batch 1466]: seen 146600 examples at 61.7 eps, loss = 3.894\n",
      "    [batch 1474]: seen 147400 examples at 61.8 eps, loss = 3.894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1482]: seen 148200 examples at 61.8 eps, loss = 3.894\n",
      "    [batch 1490]: seen 149000 examples at 61.8 eps, loss = 3.891\n",
      "    [batch 1498]: seen 149800 examples at 61.9 eps, loss = 3.896\n",
      "    [batch 1506]: seen 150600 examples at 61.9 eps, loss = 3.895\n",
      "    [batch 1513]: seen 151300 examples at 61.9 eps, loss = 3.894\n",
      "    [batch 1521]: seen 152100 examples at 62.0 eps, loss = 3.897\n",
      "    [batch 1528]: seen 152800 examples at 62.0 eps, loss = 3.898\n",
      "    [batch 1536]: seen 153600 examples at 62.1 eps, loss = 3.899\n",
      "    [batch 1544]: seen 154400 examples at 62.1 eps, loss = 3.893\n",
      "    [batch 1552]: seen 155200 examples at 62.1 eps, loss = 3.895\n",
      "    [batch 1560]: seen 156000 examples at 62.2 eps, loss = 3.893\n",
      "    [batch 1568]: seen 156800 examples at 62.2 eps, loss = 3.888\n",
      "    [batch 1575]: seen 157500 examples at 62.2 eps, loss = 3.889\n",
      "    [batch 1582]: seen 158200 examples at 62.3 eps, loss = 3.891\n",
      "    [batch 1590]: seen 159000 examples at 62.3 eps, loss = 3.892\n",
      "    [batch 1597]: seen 159700 examples at 62.3 eps, loss = 3.894\n",
      "    [batch 1605]: seen 160500 examples at 62.4 eps, loss = 3.902\n",
      "    [batch 1612]: seen 161200 examples at 62.4 eps, loss = 3.900\n",
      "    [batch 1620]: seen 162000 examples at 62.4 eps, loss = 3.895\n",
      "    [batch 1628]: seen 162800 examples at 62.5 eps, loss = 3.892\n",
      "    [batch 1636]: seen 163600 examples at 62.5 eps, loss = 3.895\n",
      "    [batch 1644]: seen 164400 examples at 62.5 eps, loss = 3.888\n",
      "    [batch 1652]: seen 165200 examples at 62.6 eps, loss = 3.890\n",
      "    [batch 1659]: seen 165900 examples at 62.6 eps, loss = 3.890\n",
      "    [batch 1667]: seen 166700 examples at 62.5 eps, loss = 3.885\n",
      "    [batch 1672]: seen 167200 examples at 62.5 eps, loss = 3.884\n",
      "    [batch 1675]: seen 167500 examples at 62.3 eps, loss = 3.881\n",
      "    [batch 1678]: seen 167800 examples at 62.1 eps, loss = 3.878\n",
      "    [batch 1685]: seen 168500 examples at 62.1 eps, loss = 3.881\n",
      "    [batch 1693]: seen 169300 examples at 62.2 eps, loss = 3.882\n",
      "    [batch 1701]: seen 170100 examples at 62.2 eps, loss = 3.884\n",
      "    [batch 1708]: seen 170800 examples at 62.2 eps, loss = 3.882\n",
      "    [batch 1716]: seen 171600 examples at 62.3 eps, loss = 3.883\n",
      "    [batch 1724]: seen 172400 examples at 62.3 eps, loss = 3.881\n",
      "    [batch 1729]: seen 172900 examples at 62.2 eps, loss = 3.877\n",
      "    [batch 1736]: seen 173600 examples at 62.2 eps, loss = 3.881\n",
      "    [batch 1744]: seen 174400 examples at 62.2 eps, loss = 3.879\n",
      "    [batch 1748]: seen 174800 examples at 62.1 eps, loss = 3.877\n",
      "    [batch 1753]: seen 175300 examples at 62.1 eps, loss = 3.877\n",
      "    [batch 1758]: seen 175800 examples at 62.0 eps, loss = 3.875\n",
      "    [batch 1761]: seen 176100 examples at 61.9 eps, loss = 3.877\n",
      "    [batch 1769]: seen 176900 examples at 61.9 eps, loss = 3.881\n",
      "    [batch 1777]: seen 177700 examples at 61.9 eps, loss = 3.886\n",
      "    [batch 1785]: seen 178500 examples at 62.0 eps, loss = 3.886\n",
      "    [batch 1793]: seen 179300 examples at 62.0 eps, loss = 3.881\n",
      "    [batch 1801]: seen 180100 examples at 62.0 eps, loss = 3.875\n",
      "    [batch 1804]: seen 180400 examples at 61.9 eps, loss = 3.873\n",
      "    [batch 1807]: seen 180700 examples at 61.8 eps, loss = 3.872\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-12750\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-12750\n",
      "    [batch 1814]: seen 181400 examples at 61.8 eps, loss = 3.873\n",
      "    [batch 1822]: seen 182200 examples at 61.8 eps, loss = 3.876\n",
      "    [batch 1830]: seen 183000 examples at 61.8 eps, loss = 3.878\n",
      "    [batch 1837]: seen 183700 examples at 61.8 eps, loss = 3.877\n",
      "    [batch 1845]: seen 184500 examples at 61.9 eps, loss = 3.877\n",
      "    [batch 1852]: seen 185200 examples at 61.9 eps, loss = 3.881\n",
      "    [batch 1859]: seen 185900 examples at 61.9 eps, loss = 3.877\n",
      "    [batch 1867]: seen 186700 examples at 62.0 eps, loss = 3.879\n",
      "    [END] Training complete: Total examples : 187100; Total time: 0:50:18\n",
      "[EPOCH 7] Complete. Avg Loss: 3.8762482256207895; Best Loss: 3.8702905267524192\n",
      "[EPOCH 8] Starting training..\n",
      "    [batch 8]: seen 800 examples at 70.1 eps, loss = 3.874\n",
      "    [batch 15]: seen 1500 examples at 70.0 eps, loss = 3.875\n",
      "    [batch 23]: seen 2300 examples at 70.0 eps, loss = 3.876\n",
      "    [batch 31]: seen 3100 examples at 70.1 eps, loss = 3.872\n",
      "    [batch 34]: seen 3400 examples at 61.6 eps, loss = 3.870\n",
      "    [batch 37]: seen 3700 examples at 56.0 eps, loss = 3.869\n",
      "    [batch 41]: seen 4100 examples at 52.3 eps, loss = 3.866\n",
      "    [batch 44]: seen 4400 examples at 49.2 eps, loss = 3.866\n",
      "    [batch 47]: seen 4700 examples at 45.3 eps, loss = 3.863\n",
      "    [batch 50]: seen 5000 examples at 43.6 eps, loss = 3.860\n",
      "    [batch 53]: seen 5300 examples at 40.9 eps, loss = 3.856\n",
      "    [batch 56]: seen 5600 examples at 39.8 eps, loss = 3.853\n",
      "    [batch 59]: seen 5900 examples at 38.1 eps, loss = 3.851\n",
      "    [batch 62]: seen 6200 examples at 37.4 eps, loss = 3.850\n",
      "    [batch 70]: seen 7000 examples at 38.8 eps, loss = 3.849\n",
      "    [batch 77]: seen 7700 examples at 40.4 eps, loss = 3.854\n",
      "    [batch 85]: seen 8500 examples at 42.1 eps, loss = 3.856\n",
      "    [batch 92]: seen 9200 examples at 43.4 eps, loss = 3.852\n",
      "    [batch 97]: seen 9700 examples at 43.6 eps, loss = 3.849\n",
      "    [batch 104]: seen 10400 examples at 44.1 eps, loss = 3.848\n",
      "    [batch 111]: seen 11100 examples at 45.2 eps, loss = 3.852\n",
      "    [batch 119]: seen 11900 examples at 46.3 eps, loss = 3.857\n",
      "    [batch 126]: seen 12600 examples at 47.2 eps, loss = 3.861\n",
      "    [batch 133]: seen 13300 examples at 48.0 eps, loss = 3.859\n",
      "    [batch 141]: seen 14100 examples at 48.9 eps, loss = 3.859\n",
      "    [batch 149]: seen 14900 examples at 49.7 eps, loss = 3.856\n",
      "    [batch 156]: seen 15600 examples at 50.3 eps, loss = 3.850\n",
      "    [batch 163]: seen 16300 examples at 50.9 eps, loss = 3.850\n",
      "    [batch 171]: seen 17100 examples at 51.6 eps, loss = 3.851\n",
      "    [batch 179]: seen 17900 examples at 52.2 eps, loss = 3.852\n",
      "    [batch 187]: seen 18700 examples at 52.8 eps, loss = 3.856\n",
      "    [batch 194]: seen 19400 examples at 53.3 eps, loss = 3.855\n",
      "    [batch 202]: seen 20200 examples at 53.8 eps, loss = 3.861\n",
      "    [batch 210]: seen 21000 examples at 54.3 eps, loss = 3.857\n",
      "    [batch 217]: seen 21700 examples at 54.7 eps, loss = 3.854\n",
      "    [batch 224]: seen 22400 examples at 55.0 eps, loss = 3.849\n",
      "    [batch 228]: seen 22800 examples at 54.4 eps, loss = 3.847\n",
      "    [batch 233]: seen 23300 examples at 54.2 eps, loss = 3.849\n",
      "    [batch 239]: seen 23900 examples at 54.1 eps, loss = 3.846\n",
      "    [batch 244]: seen 24400 examples at 54.0 eps, loss = 3.843\n",
      "    [batch 249]: seen 24900 examples at 53.7 eps, loss = 3.846\n",
      "    [batch 257]: seen 25700 examples at 54.1 eps, loss = 3.845\n",
      "    [batch 262]: seen 26200 examples at 53.6 eps, loss = 3.838\n",
      "    [batch 265]: seen 26500 examples at 52.7 eps, loss = 3.834\n",
      "    [batch 268]: seen 26800 examples at 51.8 eps, loss = 3.830\n",
      "    [batch 273]: seen 27300 examples at 51.7 eps, loss = 3.831\n",
      "    [batch 281]: seen 28100 examples at 52.1 eps, loss = 3.834\n",
      "    [batch 289]: seen 28900 examples at 52.5 eps, loss = 3.836\n",
      "    [batch 296]: seen 29600 examples at 52.8 eps, loss = 3.834\n",
      "    [batch 304]: seen 30400 examples at 53.2 eps, loss = 3.838\n",
      "    [batch 311]: seen 31100 examples at 53.4 eps, loss = 3.837\n",
      "    [batch 319]: seen 31900 examples at 53.8 eps, loss = 3.840\n",
      "    [batch 327]: seen 32700 examples at 54.1 eps, loss = 3.841\n",
      "    [batch 335]: seen 33500 examples at 54.4 eps, loss = 3.838\n",
      "    [batch 343]: seen 34300 examples at 54.7 eps, loss = 3.843\n",
      "    [batch 351]: seen 35100 examples at 54.9 eps, loss = 3.846\n",
      "    [batch 359]: seen 35900 examples at 55.2 eps, loss = 3.850\n",
      "    [batch 367]: seen 36700 examples at 55.5 eps, loss = 3.846\n",
      "    [EXCEPTION]:  OOM when allocating tensor with shape[100,50017] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/final_distribution/concat_98 = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/final_distribution/mul_98, projection/final_distribution/zeros, Training/gradients/b_count)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'projection/final_distribution/concat_98', defined at:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-40345e958dd0>\", line 9, in <module>\n",
      "    train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)\n",
      "  File \"<ipython-input-5-17e3ed376236>\", line 3, in train_continue\n",
      "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/training_util.py\", line 37, in training_init\n",
      "    lm.BuildCoreGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 140, in BuildCoreGraph\n",
      "    self.BuildProjectionGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 177, in BuildProjectionGraph\n",
      "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 177, in <listcomp>\n",
      "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1189, in concat\n",
      "    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 953, in concat_v2\n",
      "    \"ConcatV2\", values=values, axis=axis, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,50017] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/final_distribution/concat_98 = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/final_distribution/mul_98, projection/final_distribution/zeros, Training/gradients/b_count)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      " ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13077\n",
      "    [batch 373]: seen 37300 examples at 54.7 eps, loss = 3.846\n",
      "    [batch 381]: seen 38100 examples at 54.9 eps, loss = 3.847\n",
      "    [batch 389]: seen 38900 examples at 55.2 eps, loss = 3.850\n",
      "    [batch 397]: seen 39700 examples at 55.4 eps, loss = 3.848\n",
      "    [batch 405]: seen 40500 examples at 55.6 eps, loss = 3.853\n",
      "    [batch 413]: seen 41300 examples at 55.9 eps, loss = 3.849\n",
      "    [batch 421]: seen 42100 examples at 56.1 eps, loss = 3.848\n",
      "    [batch 429]: seen 42900 examples at 56.3 eps, loss = 3.848\n",
      "    [batch 437]: seen 43700 examples at 56.5 eps, loss = 3.846\n",
      "    [batch 445]: seen 44500 examples at 56.7 eps, loss = 3.841\n",
      "    [batch 453]: seen 45300 examples at 56.9 eps, loss = 3.848\n",
      "    [batch 461]: seen 46100 examples at 57.1 eps, loss = 3.855\n",
      "    [batch 468]: seen 46800 examples at 57.2 eps, loss = 3.846\n",
      "    [batch 476]: seen 47600 examples at 57.4 eps, loss = 3.848\n",
      "    [batch 484]: seen 48400 examples at 57.6 eps, loss = 3.843\n",
      "    [batch 491]: seen 49100 examples at 57.7 eps, loss = 3.848\n",
      "    [batch 499]: seen 49900 examples at 57.9 eps, loss = 3.849\n",
      "    [batch 506]: seen 50600 examples at 58.0 eps, loss = 3.849\n",
      "    [batch 513]: seen 51300 examples at 58.2 eps, loss = 3.850\n",
      "    [batch 521]: seen 52100 examples at 58.3 eps, loss = 3.853\n",
      "    [batch 529]: seen 52900 examples at 58.5 eps, loss = 3.858\n",
      "    [batch 537]: seen 53700 examples at 58.6 eps, loss = 3.857\n",
      "    [batch 545]: seen 54500 examples at 58.8 eps, loss = 3.856\n",
      "    [batch 553]: seen 55300 examples at 58.9 eps, loss = 3.855\n",
      "    [batch 560]: seen 56000 examples at 59.0 eps, loss = 3.854\n",
      "    [batch 568]: seen 56800 examples at 59.1 eps, loss = 3.851\n",
      "    [batch 576]: seen 57600 examples at 59.3 eps, loss = 3.854\n",
      "    [batch 584]: seen 58400 examples at 59.4 eps, loss = 3.853\n",
      "    [batch 591]: seen 59100 examples at 59.5 eps, loss = 3.854\n",
      "    [batch 599]: seen 59900 examples at 59.6 eps, loss = 3.854\n",
      "    [batch 607]: seen 60700 examples at 59.7 eps, loss = 3.857\n",
      "    [batch 615]: seen 61500 examples at 59.9 eps, loss = 3.857\n",
      "    [batch 622]: seen 62200 examples at 60.0 eps, loss = 3.854\n",
      "    [batch 630]: seen 63000 examples at 60.1 eps, loss = 3.850\n",
      "    [batch 637]: seen 63700 examples at 60.2 eps, loss = 3.850\n",
      "    [batch 645]: seen 64500 examples at 60.3 eps, loss = 3.854\n",
      "    [batch 653]: seen 65300 examples at 60.4 eps, loss = 3.850\n",
      "    [batch 661]: seen 66100 examples at 60.5 eps, loss = 3.852\n",
      "    [batch 668]: seen 66800 examples at 60.6 eps, loss = 3.848\n",
      "    [batch 675]: seen 67500 examples at 60.6 eps, loss = 3.848\n",
      "    [batch 683]: seen 68300 examples at 60.7 eps, loss = 3.847\n",
      "    [batch 691]: seen 69100 examples at 60.8 eps, loss = 3.843\n",
      "    [batch 699]: seen 69900 examples at 60.9 eps, loss = 3.840\n",
      "    [batch 707]: seen 70700 examples at 61.0 eps, loss = 3.844\n",
      "    [batch 715]: seen 71500 examples at 61.1 eps, loss = 3.844\n",
      "    [batch 723]: seen 72300 examples at 61.2 eps, loss = 3.849\n",
      "    [batch 731]: seen 73100 examples at 61.3 eps, loss = 3.855\n",
      "    [batch 739]: seen 73900 examples at 61.4 eps, loss = 3.855\n",
      "    [batch 746]: seen 74600 examples at 61.4 eps, loss = 3.853\n",
      "    [batch 754]: seen 75400 examples at 61.5 eps, loss = 3.855\n",
      "    [batch 762]: seen 76200 examples at 61.6 eps, loss = 3.853\n",
      "    [batch 770]: seen 77000 examples at 61.7 eps, loss = 3.851\n",
      "    [batch 778]: seen 77800 examples at 61.7 eps, loss = 3.853\n",
      "    [batch 785]: seen 78500 examples at 61.8 eps, loss = 3.850\n",
      "    [batch 793]: seen 79300 examples at 61.9 eps, loss = 3.847\n",
      "    [batch 800]: seen 80000 examples at 61.9 eps, loss = 3.846\n",
      "    [batch 808]: seen 80800 examples at 62.0 eps, loss = 3.848\n",
      "    [batch 815]: seen 81500 examples at 62.1 eps, loss = 3.845\n",
      "    [batch 823]: seen 82300 examples at 62.1 eps, loss = 3.842\n",
      "    [batch 831]: seen 83100 examples at 62.2 eps, loss = 3.835\n",
      "    [batch 839]: seen 83900 examples at 62.3 eps, loss = 3.837\n",
      "    [batch 846]: seen 84600 examples at 62.3 eps, loss = 3.840\n",
      "    [batch 854]: seen 85400 examples at 62.4 eps, loss = 3.842\n",
      "    [batch 862]: seen 86200 examples at 62.5 eps, loss = 3.841\n",
      "    [batch 869]: seen 86900 examples at 62.5 eps, loss = 3.838\n",
      "    [batch 877]: seen 87700 examples at 62.6 eps, loss = 3.837\n",
      "    [batch 885]: seen 88500 examples at 62.6 eps, loss = 3.837\n",
      "    [batch 893]: seen 89300 examples at 62.7 eps, loss = 3.838\n",
      "    [batch 901]: seen 90100 examples at 62.8 eps, loss = 3.838\n",
      "    [batch 909]: seen 90900 examples at 62.8 eps, loss = 3.844\n",
      "    [batch 917]: seen 91700 examples at 62.9 eps, loss = 3.838\n",
      "    [batch 925]: seen 92500 examples at 62.9 eps, loss = 3.840\n",
      "    [batch 933]: seen 93300 examples at 63.0 eps, loss = 3.843\n",
      "    [batch 941]: seen 94100 examples at 63.0 eps, loss = 3.842\n",
      "    [batch 949]: seen 94900 examples at 63.1 eps, loss = 3.846\n",
      "    [batch 957]: seen 95700 examples at 63.1 eps, loss = 3.851\n",
      "    [batch 965]: seen 96500 examples at 63.2 eps, loss = 3.844\n",
      "    [batch 972]: seen 97200 examples at 63.2 eps, loss = 3.845\n",
      "    [batch 979]: seen 97900 examples at 63.3 eps, loss = 3.843\n",
      "    [EXCEPTION]:  OOM when allocating tensor with shape[100,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/Softmax_98 = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/xw_plus_b_98)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'projection/Softmax_98', defined at:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-40345e958dd0>\", line 9, in <module>\n",
      "    train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)\n",
      "  File \"<ipython-input-5-17e3ed376236>\", line 3, in train_continue\n",
      "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/training_util.py\", line 37, in training_init\n",
      "    lm.BuildCoreGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 140, in BuildCoreGraph\n",
      "    self.BuildProjectionGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 169, in BuildProjectionGraph\n",
      "    vocab_distribution = [tf.nn.softmax(s) for s in self.vocab_scores]\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 169, in <listcomp>\n",
      "    vocab_distribution = [tf.nn.softmax(s) for s in self.vocab_scores]\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1738, in softmax\n",
      "    return _softmax(logits, gen_nn_ops.softmax, axis, name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1673, in _softmax\n",
      "    return compute_op(logits, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7097, in softmax\n",
      "    \"Softmax\", logits=logits, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/Softmax_98 = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/xw_plus_b_98)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      " ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13077\n",
      "    [batch 980]: seen 98000 examples at 62.8 eps, loss = 3.843\n",
      "    [batch 988]: seen 98800 examples at 62.9 eps, loss = 3.843\n",
      "    [batch 996]: seen 99600 examples at 62.9 eps, loss = 3.846\n",
      "    [batch 1004]: seen 100400 examples at 63.0 eps, loss = 3.850\n",
      "    [batch 1012]: seen 101200 examples at 63.0 eps, loss = 3.851\n",
      "    [batch 1020]: seen 102000 examples at 63.1 eps, loss = 3.851\n",
      "    [batch 1028]: seen 102800 examples at 63.1 eps, loss = 3.852\n",
      "    [batch 1036]: seen 103600 examples at 63.2 eps, loss = 3.849\n",
      "    [batch 1044]: seen 104400 examples at 63.2 eps, loss = 3.854\n",
      "    [batch 1052]: seen 105200 examples at 63.3 eps, loss = 3.851\n",
      "    [batch 1060]: seen 106000 examples at 63.3 eps, loss = 3.852\n",
      "    [batch 1068]: seen 106800 examples at 63.4 eps, loss = 3.850\n",
      "    [batch 1076]: seen 107600 examples at 63.4 eps, loss = 3.851\n",
      "    [batch 1084]: seen 108400 examples at 63.5 eps, loss = 3.850\n",
      "    [batch 1091]: seen 109100 examples at 63.5 eps, loss = 3.854\n",
      "    [batch 1099]: seen 109900 examples at 63.6 eps, loss = 3.859\n",
      "    [batch 1106]: seen 110600 examples at 63.6 eps, loss = 3.855\n",
      "    [batch 1113]: seen 111300 examples at 63.6 eps, loss = 3.853\n",
      "    [batch 1120]: seen 112000 examples at 63.7 eps, loss = 3.851\n",
      "    [batch 1128]: seen 112800 examples at 63.7 eps, loss = 3.848\n",
      "    [batch 1136]: seen 113600 examples at 63.7 eps, loss = 3.846\n",
      "    [batch 1144]: seen 114400 examples at 63.8 eps, loss = 3.846\n",
      "    [batch 1152]: seen 115200 examples at 63.8 eps, loss = 3.847\n",
      "    [batch 1160]: seen 116000 examples at 63.9 eps, loss = 3.850\n",
      "    [batch 1168]: seen 116800 examples at 63.9 eps, loss = 3.848\n",
      "    [batch 1175]: seen 117500 examples at 63.9 eps, loss = 3.843\n",
      "    [batch 1183]: seen 118300 examples at 64.0 eps, loss = 3.840\n",
      "    [batch 1191]: seen 119100 examples at 64.0 eps, loss = 3.836\n",
      "    [batch 1199]: seen 119900 examples at 64.1 eps, loss = 3.841\n",
      "    [batch 1207]: seen 120700 examples at 64.1 eps, loss = 3.843\n",
      "    [batch 1214]: seen 121400 examples at 64.1 eps, loss = 3.847\n",
      "    [batch 1222]: seen 122200 examples at 64.2 eps, loss = 3.855\n",
      "    [batch 1230]: seen 123000 examples at 64.2 eps, loss = 3.851\n",
      "    [batch 1238]: seen 123800 examples at 64.2 eps, loss = 3.850\n",
      "    [batch 1245]: seen 124500 examples at 64.3 eps, loss = 3.850\n",
      "    [batch 1253]: seen 125300 examples at 64.3 eps, loss = 3.845\n",
      "    [batch 1261]: seen 126100 examples at 64.3 eps, loss = 3.848\n",
      "    [batch 1269]: seen 126900 examples at 64.4 eps, loss = 3.848\n",
      "    [batch 1276]: seen 127600 examples at 64.4 eps, loss = 3.856\n",
      "    [batch 1284]: seen 128400 examples at 64.4 eps, loss = 3.865\n",
      "    [batch 1291]: seen 129100 examples at 64.4 eps, loss = 3.862\n",
      "    [batch 1298]: seen 129800 examples at 64.5 eps, loss = 3.860\n",
      "    [batch 1305]: seen 130500 examples at 64.5 eps, loss = 3.864\n",
      "    [EXCEPTION]:  OOM when allocating tensor with shape[100,50019] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/final_distribution/concat_98 = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/final_distribution/mul_98, projection/final_distribution/zeros, Training/gradients/b_count)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'projection/final_distribution/concat_98', defined at:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-40345e958dd0>\", line 9, in <module>\n",
      "    train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)\n",
      "  File \"<ipython-input-5-17e3ed376236>\", line 3, in train_continue\n",
      "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/training_util.py\", line 37, in training_init\n",
      "    lm.BuildCoreGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 140, in BuildCoreGraph\n",
      "    self.BuildProjectionGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 177, in BuildProjectionGraph\n",
      "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 177, in <listcomp>\n",
      "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1189, in concat\n",
      "    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 953, in concat_v2\n",
      "    \"ConcatV2\", values=values, axis=axis, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,50019] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/final_distribution/concat_98 = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/final_distribution/mul_98, projection/final_distribution/zeros, Training/gradients/b_count)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      " ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13077\n",
      "    [batch 1309]: seen 130900 examples at 64.2 eps, loss = 3.862\n",
      "    [batch 1316]: seen 131600 examples at 64.2 eps, loss = 3.865\n",
      "    [batch 1324]: seen 132400 examples at 64.2 eps, loss = 3.862\n",
      "    [batch 1332]: seen 133200 examples at 64.3 eps, loss = 3.859\n",
      "    [batch 1340]: seen 134000 examples at 64.3 eps, loss = 3.858\n",
      "    [batch 1347]: seen 134700 examples at 64.3 eps, loss = 3.858\n",
      "    [batch 1355]: seen 135500 examples at 64.3 eps, loss = 3.853\n",
      "    [batch 1362]: seen 136200 examples at 64.4 eps, loss = 3.855\n",
      "    [batch 1370]: seen 137000 examples at 64.4 eps, loss = 3.855\n",
      "    [batch 1378]: seen 137800 examples at 64.4 eps, loss = 3.854\n",
      "    [batch 1386]: seen 138600 examples at 64.5 eps, loss = 3.849\n",
      "    [batch 1393]: seen 139300 examples at 64.5 eps, loss = 3.849\n",
      "    [batch 1401]: seen 140100 examples at 64.5 eps, loss = 3.843\n",
      "    [batch 1409]: seen 140900 examples at 64.5 eps, loss = 3.843\n",
      "    [batch 1416]: seen 141600 examples at 64.6 eps, loss = 3.846\n",
      "    [batch 1423]: seen 142300 examples at 64.6 eps, loss = 3.845\n",
      "    [batch 1431]: seen 143100 examples at 64.6 eps, loss = 3.846\n",
      "    [batch 1439]: seen 143900 examples at 64.7 eps, loss = 3.858\n",
      "    [batch 1447]: seen 144700 examples at 64.7 eps, loss = 3.862\n",
      "    [batch 1455]: seen 145500 examples at 64.7 eps, loss = 3.861\n",
      "    [batch 1462]: seen 146200 examples at 64.7 eps, loss = 3.857\n",
      "    [batch 1470]: seen 147000 examples at 64.8 eps, loss = 3.856\n",
      "    [batch 1478]: seen 147800 examples at 64.8 eps, loss = 3.853\n",
      "    [batch 1485]: seen 148500 examples at 64.8 eps, loss = 3.854\n",
      "    [batch 1493]: seen 149300 examples at 64.8 eps, loss = 3.854\n",
      "    [batch 1501]: seen 150100 examples at 64.9 eps, loss = 3.852\n",
      "    [batch 1509]: seen 150900 examples at 64.9 eps, loss = 3.849\n",
      "    [batch 1517]: seen 151700 examples at 64.9 eps, loss = 3.851\n",
      "    [batch 1525]: seen 152500 examples at 64.9 eps, loss = 3.852\n",
      "    [batch 1533]: seen 153300 examples at 65.0 eps, loss = 3.850\n",
      "    [batch 1540]: seen 154000 examples at 65.0 eps, loss = 3.848\n",
      "    [batch 1547]: seen 154700 examples at 65.0 eps, loss = 3.846\n",
      "    [batch 1555]: seen 155500 examples at 65.0 eps, loss = 3.848\n",
      "    [batch 1563]: seen 156300 examples at 65.1 eps, loss = 3.851\n",
      "    [batch 1571]: seen 157100 examples at 65.1 eps, loss = 3.852\n",
      "    [batch 1579]: seen 157900 examples at 65.1 eps, loss = 3.853\n",
      "    [batch 1587]: seen 158700 examples at 65.1 eps, loss = 3.852\n",
      "    [batch 1595]: seen 159500 examples at 65.1 eps, loss = 3.849\n",
      "    [batch 1602]: seen 160200 examples at 65.2 eps, loss = 3.849\n",
      "    [batch 1610]: seen 161000 examples at 65.2 eps, loss = 3.846\n",
      "    [batch 1617]: seen 161700 examples at 65.2 eps, loss = 3.849\n",
      "    [batch 1625]: seen 162500 examples at 65.2 eps, loss = 3.846\n",
      "    [batch 1633]: seen 163300 examples at 65.3 eps, loss = 3.856\n",
      "    [batch 1641]: seen 164100 examples at 65.3 eps, loss = 3.854\n",
      "    [batch 1648]: seen 164800 examples at 65.3 eps, loss = 3.850\n",
      "    [batch 1656]: seen 165600 examples at 65.3 eps, loss = 3.850\n",
      "    [batch 1664]: seen 166400 examples at 65.3 eps, loss = 3.849\n",
      "    [batch 1671]: seen 167100 examples at 65.4 eps, loss = 3.845\n",
      "    [batch 1679]: seen 167900 examples at 65.4 eps, loss = 3.842\n",
      "    [batch 1687]: seen 168700 examples at 65.4 eps, loss = 3.839\n",
      "    [batch 1695]: seen 169500 examples at 65.4 eps, loss = 3.840\n",
      "    [batch 1703]: seen 170300 examples at 65.4 eps, loss = 3.837\n",
      "    [batch 1711]: seen 171100 examples at 65.5 eps, loss = 3.836\n",
      "    [batch 1719]: seen 171900 examples at 65.5 eps, loss = 3.835\n",
      "    [batch 1726]: seen 172600 examples at 65.5 eps, loss = 3.837\n",
      "    [batch 1733]: seen 173300 examples at 65.5 eps, loss = 3.837\n",
      "    [batch 1740]: seen 174000 examples at 65.5 eps, loss = 3.842\n",
      "    [batch 1748]: seen 174800 examples at 65.5 eps, loss = 3.839\n",
      "    [batch 1755]: seen 175500 examples at 65.6 eps, loss = 3.840\n",
      "    [batch 1763]: seen 176300 examples at 65.6 eps, loss = 3.837\n",
      "    [batch 1770]: seen 177000 examples at 65.6 eps, loss = 3.844\n",
      "    [batch 1778]: seen 177800 examples at 65.6 eps, loss = 3.840\n",
      "    [batch 1786]: seen 178600 examples at 65.6 eps, loss = 3.838\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13077\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13077\n",
      "    [batch 1793]: seen 179300 examples at 65.6 eps, loss = 3.835\n",
      "    [batch 1801]: seen 180100 examples at 65.6 eps, loss = 3.837\n",
      "    [batch 1809]: seen 180900 examples at 65.7 eps, loss = 3.843\n",
      "    [batch 1816]: seen 181600 examples at 65.7 eps, loss = 3.841\n",
      "    [batch 1823]: seen 182300 examples at 65.7 eps, loss = 3.839\n",
      "    [batch 1831]: seen 183100 examples at 65.7 eps, loss = 3.840\n",
      "    [batch 1839]: seen 183900 examples at 65.7 eps, loss = 3.838\n",
      "    [batch 1847]: seen 184700 examples at 65.7 eps, loss = 3.843\n",
      "    [batch 1855]: seen 185500 examples at 65.8 eps, loss = 3.842\n",
      "    [batch 1862]: seen 186200 examples at 65.8 eps, loss = 3.841\n",
      "    [END] Training complete: Total examples : 186800; Total time: 0:47:19\n",
      "[EPOCH 8] Complete. Avg Loss: 3.8376867660753584; Best Loss: 3.8284044591975817\n",
      "[EPOCH 9] Starting training..\n",
      "    [batch 7]: seen 700 examples at 69.9 eps, loss = 3.838\n",
      "    [batch 15]: seen 1500 examples at 70.1 eps, loss = 3.841\n",
      "    [batch 22]: seen 2200 examples at 70.0 eps, loss = 3.840\n",
      "    [batch 30]: seen 3000 examples at 70.0 eps, loss = 3.843\n",
      "    [batch 38]: seen 3800 examples at 70.1 eps, loss = 3.838\n",
      "    [batch 45]: seen 4500 examples at 70.0 eps, loss = 3.830\n",
      "    [batch 52]: seen 5200 examples at 70.0 eps, loss = 3.835\n",
      "    [batch 60]: seen 6000 examples at 70.0 eps, loss = 3.835\n",
      "    [batch 67]: seen 6700 examples at 70.0 eps, loss = 3.836\n",
      "    [batch 75]: seen 7500 examples at 70.0 eps, loss = 3.834\n",
      "    [batch 82]: seen 8200 examples at 68.1 eps, loss = 3.828\n",
      "    [batch 85]: seen 8500 examples at 64.7 eps, loss = 3.828\n",
      "    [batch 88]: seen 8800 examples at 61.8 eps, loss = 3.827\n",
      "    [batch 93]: seen 9300 examples at 60.9 eps, loss = 3.826\n",
      "    [batch 99]: seen 9900 examples at 60.1 eps, loss = 3.826\n",
      "    [batch 106]: seen 10600 examples at 60.7 eps, loss = 3.831\n",
      "    [batch 114]: seen 11400 examples at 61.3 eps, loss = 3.831\n",
      "    [batch 122]: seen 12200 examples at 61.8 eps, loss = 3.830\n",
      "    [batch 130]: seen 13000 examples at 62.2 eps, loss = 3.830\n",
      "    [batch 138]: seen 13800 examples at 62.6 eps, loss = 3.829\n",
      "    [batch 143]: seen 14300 examples at 60.9 eps, loss = 3.823\n",
      "    [batch 150]: seen 15000 examples at 61.3 eps, loss = 3.829\n",
      "    [batch 158]: seen 15800 examples at 61.7 eps, loss = 3.833\n",
      "    [batch 165]: seen 16500 examples at 62.0 eps, loss = 3.832\n",
      "    [batch 172]: seen 17200 examples at 62.3 eps, loss = 3.831\n",
      "    [batch 180]: seen 18000 examples at 62.6 eps, loss = 3.827\n",
      "    [batch 188]: seen 18800 examples at 62.9 eps, loss = 3.832\n",
      "    [batch 196]: seen 19600 examples at 63.2 eps, loss = 3.833\n",
      "    [batch 204]: seen 20400 examples at 63.4 eps, loss = 3.842\n",
      "    [batch 212]: seen 21200 examples at 63.6 eps, loss = 3.842\n",
      "    [batch 220]: seen 22000 examples at 63.8 eps, loss = 3.842\n",
      "    [batch 227]: seen 22700 examples at 64.0 eps, loss = 3.837\n",
      "    [batch 235]: seen 23500 examples at 64.2 eps, loss = 3.841\n",
      "    [batch 243]: seen 24300 examples at 64.4 eps, loss = 3.838\n",
      "    [batch 251]: seen 25100 examples at 64.6 eps, loss = 3.840\n",
      "    [batch 258]: seen 25800 examples at 64.7 eps, loss = 3.837\n",
      "    [batch 266]: seen 26600 examples at 64.8 eps, loss = 3.839\n",
      "    [batch 274]: seen 27400 examples at 65.0 eps, loss = 3.841\n",
      "    [batch 282]: seen 28200 examples at 65.1 eps, loss = 3.847\n",
      "    [batch 290]: seen 29000 examples at 65.2 eps, loss = 3.848\n",
      "    [batch 297]: seen 29700 examples at 65.3 eps, loss = 3.850\n",
      "    [batch 304]: seen 30400 examples at 65.4 eps, loss = 3.851\n",
      "    [batch 312]: seen 31200 examples at 65.6 eps, loss = 3.848\n",
      "    [batch 320]: seen 32000 examples at 65.7 eps, loss = 3.848\n",
      "    [batch 327]: seen 32700 examples at 65.8 eps, loss = 3.844\n",
      "    [batch 335]: seen 33500 examples at 65.8 eps, loss = 3.844\n",
      "    [batch 342]: seen 34200 examples at 65.9 eps, loss = 3.838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 349]: seen 34900 examples at 66.0 eps, loss = 3.838\n",
      "    [batch 356]: seen 35600 examples at 66.1 eps, loss = 3.831\n",
      "    [batch 363]: seen 36300 examples at 66.1 eps, loss = 3.836\n",
      "    [batch 371]: seen 37100 examples at 66.2 eps, loss = 3.834\n",
      "    [batch 379]: seen 37900 examples at 66.3 eps, loss = 3.836\n",
      "    [batch 386]: seen 38600 examples at 66.4 eps, loss = 3.830\n",
      "    [batch 394]: seen 39400 examples at 66.4 eps, loss = 3.830\n",
      "    [batch 399]: seen 39900 examples at 65.7 eps, loss = 3.822\n",
      "    [batch 404]: seen 40400 examples at 65.4 eps, loss = 3.822\n",
      "    [batch 407]: seen 40700 examples at 64.8 eps, loss = 3.821\n",
      "    [batch 411]: seen 41100 examples at 64.2 eps, loss = 3.817\n",
      "    [batch 414]: seen 41400 examples at 63.2 eps, loss = 3.814\n",
      "    [batch 419]: seen 41900 examples at 63.0 eps, loss = 3.814\n",
      "    [batch 424]: seen 42400 examples at 62.7 eps, loss = 3.816\n",
      "    [batch 432]: seen 43200 examples at 62.9 eps, loss = 3.822\n",
      "    [batch 439]: seen 43900 examples at 63.0 eps, loss = 3.821\n",
      "    [batch 447]: seen 44700 examples at 63.1 eps, loss = 3.825\n",
      "    [EXCEPTION]:  OOM when allocating tensor with shape[100,50022] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/final_distribution/concat_98 = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/final_distribution/mul_98, projection/final_distribution/zeros, Training/gradients/b_count)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'projection/final_distribution/concat_98', defined at:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-40345e958dd0>\", line 9, in <module>\n",
      "    train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)\n",
      "  File \"<ipython-input-5-17e3ed376236>\", line 3, in train_continue\n",
      "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/training_util.py\", line 37, in training_init\n",
      "    lm.BuildCoreGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 140, in BuildCoreGraph\n",
      "    self.BuildProjectionGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 177, in BuildProjectionGraph\n",
      "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 177, in <listcomp>\n",
      "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1189, in concat\n",
      "    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 953, in concat_v2\n",
      "    \"ConcatV2\", values=values, axis=axis, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,50022] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/final_distribution/concat_98 = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/final_distribution/mul_98, projection/final_distribution/zeros, Training/gradients/b_count)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      " ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13574\n",
      "    [batch 449]: seen 44900 examples at 62.1 eps, loss = 3.824\n",
      "    [batch 457]: seen 45700 examples at 62.3 eps, loss = 3.820\n",
      "    [batch 465]: seen 46500 examples at 62.4 eps, loss = 3.823\n",
      "    [batch 473]: seen 47300 examples at 62.5 eps, loss = 3.820\n",
      "    [batch 481]: seen 48100 examples at 62.6 eps, loss = 3.824\n",
      "    [batch 488]: seen 48800 examples at 62.7 eps, loss = 3.821\n",
      "    [batch 496]: seen 49600 examples at 62.8 eps, loss = 3.819\n",
      "    [batch 504]: seen 50400 examples at 62.9 eps, loss = 3.818\n",
      "    [batch 511]: seen 51100 examples at 63.0 eps, loss = 3.820\n",
      "    [batch 519]: seen 51900 examples at 63.1 eps, loss = 3.823\n",
      "    [batch 527]: seen 52700 examples at 63.2 eps, loss = 3.821\n",
      "    [batch 535]: seen 53500 examples at 63.3 eps, loss = 3.826\n",
      "    [batch 543]: seen 54300 examples at 63.4 eps, loss = 3.835\n",
      "    [batch 551]: seen 55100 examples at 63.5 eps, loss = 3.833\n",
      "    [batch 559]: seen 55900 examples at 63.6 eps, loss = 3.832\n",
      "    [batch 566]: seen 56600 examples at 63.6 eps, loss = 3.834\n",
      "    [batch 574]: seen 57400 examples at 63.7 eps, loss = 3.834\n",
      "    [batch 581]: seen 58100 examples at 63.8 eps, loss = 3.838\n",
      "    [batch 589]: seen 58900 examples at 63.9 eps, loss = 3.840\n",
      "    [batch 597]: seen 59700 examples at 63.9 eps, loss = 3.834\n",
      "    [batch 605]: seen 60500 examples at 64.0 eps, loss = 3.829\n",
      "    [batch 613]: seen 61300 examples at 64.1 eps, loss = 3.831\n",
      "    [batch 621]: seen 62100 examples at 64.2 eps, loss = 3.838\n",
      "    [batch 629]: seen 62900 examples at 64.2 eps, loss = 3.839\n",
      "    [batch 637]: seen 63700 examples at 64.3 eps, loss = 3.841\n",
      "    [batch 645]: seen 64500 examples at 64.4 eps, loss = 3.845\n",
      "    [batch 652]: seen 65200 examples at 64.4 eps, loss = 3.842\n",
      "    [batch 660]: seen 66000 examples at 64.5 eps, loss = 3.841\n",
      "    [batch 668]: seen 66800 examples at 64.5 eps, loss = 3.844\n",
      "    [batch 675]: seen 67500 examples at 64.6 eps, loss = 3.841\n",
      "    [batch 683]: seen 68300 examples at 64.7 eps, loss = 3.841\n",
      "    [batch 690]: seen 69000 examples at 64.7 eps, loss = 3.845\n",
      "    [batch 697]: seen 69700 examples at 64.7 eps, loss = 3.841\n",
      "    [batch 705]: seen 70500 examples at 64.8 eps, loss = 3.837\n",
      "    [batch 713]: seen 71300 examples at 64.9 eps, loss = 3.832\n",
      "    [batch 721]: seen 72100 examples at 64.9 eps, loss = 3.831\n",
      "    [batch 729]: seen 72900 examples at 65.0 eps, loss = 3.831\n",
      "    [batch 737]: seen 73700 examples at 65.0 eps, loss = 3.826\n",
      "    [batch 745]: seen 74500 examples at 65.1 eps, loss = 3.829\n",
      "    [batch 753]: seen 75300 examples at 65.1 eps, loss = 3.835\n",
      "    [batch 760]: seen 76000 examples at 65.2 eps, loss = 3.836\n",
      "    [batch 767]: seen 76700 examples at 65.2 eps, loss = 3.836\n",
      "    [batch 774]: seen 77400 examples at 65.2 eps, loss = 3.837\n",
      "    [batch 782]: seen 78200 examples at 65.3 eps, loss = 3.839\n",
      "    [batch 790]: seen 79000 examples at 65.3 eps, loss = 3.841\n",
      "    [batch 798]: seen 79800 examples at 65.4 eps, loss = 3.840\n",
      "    [batch 806]: seen 80600 examples at 65.4 eps, loss = 3.837\n",
      "    [batch 814]: seen 81400 examples at 65.5 eps, loss = 3.839\n",
      "    [batch 821]: seen 82100 examples at 65.5 eps, loss = 3.839\n",
      "    [batch 829]: seen 82900 examples at 65.5 eps, loss = 3.837\n",
      "    [batch 837]: seen 83700 examples at 65.6 eps, loss = 3.833\n",
      "    [batch 845]: seen 84500 examples at 65.6 eps, loss = 3.836\n",
      "    [batch 853]: seen 85300 examples at 65.7 eps, loss = 3.830\n",
      "    [batch 860]: seen 86000 examples at 65.7 eps, loss = 3.829\n",
      "    [batch 868]: seen 86800 examples at 65.7 eps, loss = 3.828\n",
      "    [batch 876]: seen 87600 examples at 65.8 eps, loss = 3.824\n",
      "    [batch 884]: seen 88400 examples at 65.8 eps, loss = 3.824\n",
      "    [batch 892]: seen 89200 examples at 65.8 eps, loss = 3.830\n",
      "    [batch 900]: seen 90000 examples at 65.9 eps, loss = 3.829\n",
      "    [batch 908]: seen 90800 examples at 65.9 eps, loss = 3.832\n",
      "    [batch 915]: seen 91500 examples at 65.9 eps, loss = 3.834\n",
      "    [batch 922]: seen 92200 examples at 66.0 eps, loss = 3.836\n",
      "    [batch 930]: seen 93000 examples at 66.0 eps, loss = 3.837\n",
      "    [batch 938]: seen 93800 examples at 66.0 eps, loss = 3.839\n",
      "    [batch 945]: seen 94500 examples at 66.1 eps, loss = 3.839\n",
      "    [batch 953]: seen 95300 examples at 66.1 eps, loss = 3.837\n",
      "    [batch 960]: seen 96000 examples at 66.1 eps, loss = 3.836\n",
      "    [batch 968]: seen 96800 examples at 66.1 eps, loss = 3.838\n",
      "    [batch 976]: seen 97600 examples at 66.2 eps, loss = 3.835\n",
      "    [batch 984]: seen 98400 examples at 66.2 eps, loss = 3.839\n",
      "    [batch 992]: seen 99200 examples at 66.2 eps, loss = 3.843\n",
      "    [batch 1000]: seen 100000 examples at 66.3 eps, loss = 3.845\n",
      "    [batch 1008]: seen 100800 examples at 66.3 eps, loss = 3.843\n",
      "    [batch 1016]: seen 101600 examples at 66.3 eps, loss = 3.840\n",
      "    [batch 1024]: seen 102400 examples at 66.4 eps, loss = 3.836\n",
      "    [batch 1032]: seen 103200 examples at 66.4 eps, loss = 3.831\n",
      "    [batch 1040]: seen 104000 examples at 66.4 eps, loss = 3.832\n",
      "    [batch 1047]: seen 104700 examples at 66.4 eps, loss = 3.832\n",
      "    [batch 1055]: seen 105500 examples at 66.5 eps, loss = 3.827\n",
      "    [batch 1063]: seen 106300 examples at 66.5 eps, loss = 3.821\n",
      "    [batch 1071]: seen 107100 examples at 66.5 eps, loss = 3.822\n",
      "    [batch 1079]: seen 107900 examples at 66.5 eps, loss = 3.823\n",
      "    [batch 1086]: seen 108600 examples at 66.6 eps, loss = 3.818\n",
      "    [batch 1093]: seen 109300 examples at 66.6 eps, loss = 3.816\n",
      "    [batch 1101]: seen 110100 examples at 66.6 eps, loss = 3.819\n",
      "    [batch 1109]: seen 110900 examples at 66.6 eps, loss = 3.815\n",
      "    [batch 1117]: seen 111700 examples at 66.6 eps, loss = 3.818\n",
      "    [EXCEPTION]:  OOM when allocating tensor with shape[100,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/Softmax_98 = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/xw_plus_b_98)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'projection/Softmax_98', defined at:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-40345e958dd0>\", line 9, in <module>\n",
      "    train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)\n",
      "  File \"<ipython-input-5-17e3ed376236>\", line 3, in train_continue\n",
      "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/training_util.py\", line 37, in training_init\n",
      "    lm.BuildCoreGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 140, in BuildCoreGraph\n",
      "    self.BuildProjectionGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 169, in BuildProjectionGraph\n",
      "    vocab_distribution = [tf.nn.softmax(s) for s in self.vocab_scores]\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 169, in <listcomp>\n",
      "    vocab_distribution = [tf.nn.softmax(s) for s in self.vocab_scores]\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1738, in softmax\n",
      "    return _softmax(logits, gen_nn_ops.softmax, axis, name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1673, in _softmax\n",
      "    return compute_op(logits, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7097, in softmax\n",
      "    \"Softmax\", logits=logits, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/Softmax_98 = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/xw_plus_b_98)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      " ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13574\n",
      "    [batch 1118]: seen 111800 examples at 66.2 eps, loss = 3.816\n",
      "    [batch 1123]: seen 112300 examples at 66.1 eps, loss = 3.814\n",
      "    [batch 1126]: seen 112600 examples at 65.9 eps, loss = 3.812\n",
      "    [batch 1128]: seen 112800 examples at 65.6 eps, loss = 3.811\n",
      "    [batch 1135]: seen 113500 examples at 65.6 eps, loss = 3.813\n",
      "    [batch 1138]: seen 113800 examples at 65.4 eps, loss = 3.809\n",
      "    [batch 1141]: seen 114100 examples at 65.1 eps, loss = 3.806\n",
      "    [batch 1144]: seen 114400 examples at 64.9 eps, loss = 3.805\n",
      "    [batch 1152]: seen 115200 examples at 64.9 eps, loss = 3.810\n",
      "    [batch 1159]: seen 115900 examples at 65.0 eps, loss = 3.805\n",
      "    [batch 1164]: seen 116400 examples at 64.9 eps, loss = 3.806\n",
      "    [batch 1172]: seen 117200 examples at 64.9 eps, loss = 3.812\n",
      "    [batch 1179]: seen 117900 examples at 64.9 eps, loss = 3.813\n",
      "    [batch 1187]: seen 118700 examples at 65.0 eps, loss = 3.810\n",
      "    [batch 1195]: seen 119500 examples at 65.0 eps, loss = 3.816\n",
      "    [batch 1202]: seen 120200 examples at 65.0 eps, loss = 3.816\n",
      "    [batch 1210]: seen 121000 examples at 65.0 eps, loss = 3.820\n",
      "    [batch 1218]: seen 121800 examples at 65.1 eps, loss = 3.826\n",
      "    [batch 1225]: seen 122500 examples at 65.1 eps, loss = 3.827\n",
      "    [batch 1233]: seen 123300 examples at 65.1 eps, loss = 3.826\n",
      "    [batch 1241]: seen 124100 examples at 65.2 eps, loss = 3.822\n",
      "    [batch 1248]: seen 124800 examples at 65.2 eps, loss = 3.825\n",
      "    [batch 1255]: seen 125500 examples at 65.2 eps, loss = 3.828\n",
      "    [batch 1263]: seen 126300 examples at 65.2 eps, loss = 3.834\n",
      "    [batch 1271]: seen 127100 examples at 65.3 eps, loss = 3.831\n",
      "    [batch 1279]: seen 127900 examples at 65.3 eps, loss = 3.831\n",
      "    [batch 1287]: seen 128700 examples at 65.3 eps, loss = 3.835\n",
      "    [batch 1295]: seen 129500 examples at 65.3 eps, loss = 3.838\n",
      "    [batch 1303]: seen 130300 examples at 65.4 eps, loss = 3.832\n",
      "    [EXCEPTION]:  OOM when allocating tensor with shape[100,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/Softmax_98 = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/xw_plus_b_98)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'projection/Softmax_98', defined at:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-40345e958dd0>\", line 9, in <module>\n",
      "    train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)\n",
      "  File \"<ipython-input-5-17e3ed376236>\", line 3, in train_continue\n",
      "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/training_util.py\", line 37, in training_init\n",
      "    lm.BuildCoreGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 140, in BuildCoreGraph\n",
      "    self.BuildProjectionGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 169, in BuildProjectionGraph\n",
      "    vocab_distribution = [tf.nn.softmax(s) for s in self.vocab_scores]\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 169, in <listcomp>\n",
      "    vocab_distribution = [tf.nn.softmax(s) for s in self.vocab_scores]\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1738, in softmax\n",
      "    return _softmax(logits, gen_nn_ops.softmax, axis, name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1673, in _softmax\n",
      "    return compute_op(logits, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7097, in softmax\n",
      "    \"Softmax\", logits=logits, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/Softmax_98 = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/xw_plus_b_98)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      " ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13619\n",
      "    [batch 1308]: seen 130800 examples at 65.0 eps, loss = 3.830\n",
      "    [batch 1316]: seen 131600 examples at 65.1 eps, loss = 3.839\n",
      "    [batch 1324]: seen 132400 examples at 65.1 eps, loss = 3.848\n",
      "    [batch 1332]: seen 133200 examples at 65.1 eps, loss = 3.850\n",
      "    [batch 1340]: seen 134000 examples at 65.1 eps, loss = 3.846\n",
      "    [batch 1348]: seen 134800 examples at 65.2 eps, loss = 3.846\n",
      "    [batch 1356]: seen 135600 examples at 65.2 eps, loss = 3.844\n",
      "    [batch 1364]: seen 136400 examples at 65.2 eps, loss = 3.846\n",
      "    [batch 1372]: seen 137200 examples at 65.3 eps, loss = 3.844\n",
      "    [batch 1380]: seen 138000 examples at 65.3 eps, loss = 3.846\n",
      "    [batch 1387]: seen 138700 examples at 65.3 eps, loss = 3.841\n",
      "    [batch 1395]: seen 139500 examples at 65.3 eps, loss = 3.836\n",
      "    [batch 1402]: seen 140200 examples at 65.3 eps, loss = 3.836\n",
      "    [batch 1409]: seen 140900 examples at 65.4 eps, loss = 3.834\n",
      "    [batch 1417]: seen 141700 examples at 65.4 eps, loss = 3.837\n",
      "    [batch 1425]: seen 142500 examples at 65.4 eps, loss = 3.834\n",
      "    [batch 1433]: seen 143300 examples at 65.4 eps, loss = 3.829\n",
      "    [batch 1441]: seen 144100 examples at 65.5 eps, loss = 3.835\n",
      "    [batch 1449]: seen 144900 examples at 65.5 eps, loss = 3.829\n",
      "    [batch 1456]: seen 145600 examples at 65.5 eps, loss = 3.827\n",
      "    [batch 1464]: seen 146400 examples at 65.5 eps, loss = 3.830\n",
      "    [batch 1471]: seen 147100 examples at 65.6 eps, loss = 3.826\n",
      "    [batch 1479]: seen 147900 examples at 65.6 eps, loss = 3.833\n",
      "    [batch 1486]: seen 148600 examples at 65.6 eps, loss = 3.836\n",
      "    [batch 1494]: seen 149400 examples at 65.6 eps, loss = 3.839\n",
      "    [batch 1502]: seen 150200 examples at 65.6 eps, loss = 3.838\n",
      "    [batch 1510]: seen 151000 examples at 65.7 eps, loss = 3.836\n",
      "    [batch 1518]: seen 151800 examples at 65.7 eps, loss = 3.829\n",
      "    [batch 1526]: seen 152600 examples at 65.7 eps, loss = 3.831\n",
      "    [batch 1533]: seen 153300 examples at 65.7 eps, loss = 3.828\n",
      "    [batch 1541]: seen 154100 examples at 65.7 eps, loss = 3.825\n",
      "    [batch 1548]: seen 154800 examples at 65.8 eps, loss = 3.825\n",
      "    [batch 1556]: seen 155600 examples at 65.8 eps, loss = 3.828\n",
      "    [batch 1563]: seen 156300 examples at 65.8 eps, loss = 3.830\n",
      "    [batch 1571]: seen 157100 examples at 65.8 eps, loss = 3.828\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13619\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-13619\n",
      "    [batch 1577]: seen 157700 examples at 65.8 eps, loss = 3.835\n",
      "    [batch 1584]: seen 158400 examples at 65.8 eps, loss = 3.839\n",
      "    [batch 1592]: seen 159200 examples at 65.8 eps, loss = 3.839\n",
      "    [batch 1600]: seen 160000 examples at 65.9 eps, loss = 3.832\n",
      "    [batch 1608]: seen 160800 examples at 65.9 eps, loss = 3.828\n",
      "    [batch 1616]: seen 161600 examples at 65.9 eps, loss = 3.826\n",
      "    [batch 1623]: seen 162300 examples at 65.9 eps, loss = 3.825\n",
      "    [batch 1630]: seen 163000 examples at 65.9 eps, loss = 3.829\n",
      "    [batch 1637]: seen 163700 examples at 65.9 eps, loss = 3.833\n",
      "    [batch 1645]: seen 164500 examples at 66.0 eps, loss = 3.825\n",
      "    [batch 1653]: seen 165300 examples at 66.0 eps, loss = 3.829\n",
      "    [batch 1660]: seen 166000 examples at 66.0 eps, loss = 3.831\n",
      "    [batch 1668]: seen 166800 examples at 66.0 eps, loss = 3.828\n",
      "    [batch 1675]: seen 167500 examples at 66.0 eps, loss = 3.829\n",
      "    [batch 1683]: seen 168300 examples at 66.0 eps, loss = 3.831\n",
      "    [batch 1691]: seen 169100 examples at 66.1 eps, loss = 3.832\n",
      "    [batch 1699]: seen 169900 examples at 66.1 eps, loss = 3.824\n",
      "    [batch 1707]: seen 170700 examples at 66.1 eps, loss = 3.823\n",
      "    [batch 1715]: seen 171500 examples at 66.1 eps, loss = 3.819\n",
      "    [batch 1723]: seen 172300 examples at 66.1 eps, loss = 3.823\n",
      "    [batch 1731]: seen 173100 examples at 66.2 eps, loss = 3.830\n",
      "    [batch 1739]: seen 173900 examples at 66.2 eps, loss = 3.827\n",
      "    [batch 1747]: seen 174700 examples at 66.2 eps, loss = 3.831\n",
      "    [batch 1755]: seen 175500 examples at 66.2 eps, loss = 3.833\n",
      "    [batch 1762]: seen 176200 examples at 66.2 eps, loss = 3.832\n",
      "    [batch 1769]: seen 176900 examples at 66.2 eps, loss = 3.829\n",
      "    [batch 1777]: seen 177700 examples at 66.2 eps, loss = 3.824\n",
      "    [batch 1784]: seen 178400 examples at 66.3 eps, loss = 3.822\n",
      "    [batch 1792]: seen 179200 examples at 66.3 eps, loss = 3.816\n",
      "    [batch 1800]: seen 180000 examples at 66.3 eps, loss = 3.813\n",
      "    [batch 1808]: seen 180800 examples at 66.3 eps, loss = 3.818\n",
      "    [batch 1816]: seen 181600 examples at 66.3 eps, loss = 3.818\n",
      "    [batch 1824]: seen 182400 examples at 66.3 eps, loss = 3.820\n",
      "    [batch 1832]: seen 183200 examples at 66.4 eps, loss = 3.819\n",
      "    [batch 1839]: seen 183900 examples at 66.4 eps, loss = 3.818\n",
      "    [batch 1847]: seen 184700 examples at 66.4 eps, loss = 3.815\n",
      "    [batch 1855]: seen 185500 examples at 66.4 eps, loss = 3.810\n",
      "    [batch 1863]: seen 186300 examples at 66.4 eps, loss = 3.811\n",
      "    [END] Training complete: Total examples : 186800; Total time: 0:46:52\n",
      "[EPOCH 9] Complete. Avg Loss: 3.8133811317146953; Best Loss: 3.8025051756803\n",
      "[EPOCH 10] Starting training..\n",
      "    [batch 8]: seen 800 examples at 70.1 eps, loss = 3.819\n",
      "    [batch 16]: seen 1600 examples at 70.1 eps, loss = 3.823\n",
      "    [batch 23]: seen 2300 examples at 70.0 eps, loss = 3.823\n",
      "    [batch 30]: seen 3000 examples at 70.0 eps, loss = 3.827\n",
      "    [batch 38]: seen 3800 examples at 70.0 eps, loss = 3.825\n",
      "    [batch 45]: seen 4500 examples at 70.0 eps, loss = 3.823\n",
      "    [batch 53]: seen 5300 examples at 70.0 eps, loss = 3.819\n",
      "    [batch 60]: seen 6000 examples at 70.0 eps, loss = 3.817\n",
      "    [batch 68]: seen 6800 examples at 70.0 eps, loss = 3.822\n",
      "    [batch 76]: seen 7600 examples at 70.0 eps, loss = 3.820\n",
      "    [batch 84]: seen 8400 examples at 70.0 eps, loss = 3.817\n",
      "    [batch 92]: seen 9200 examples at 70.0 eps, loss = 3.816\n",
      "    [batch 100]: seen 10000 examples at 70.0 eps, loss = 3.817\n",
      "    [batch 107]: seen 10700 examples at 70.0 eps, loss = 3.817\n",
      "    [batch 114]: seen 11400 examples at 70.0 eps, loss = 3.812\n",
      "    [batch 122]: seen 12200 examples at 70.0 eps, loss = 3.808\n",
      "    [batch 130]: seen 13000 examples at 70.0 eps, loss = 3.809\n",
      "    [batch 138]: seen 13800 examples at 70.0 eps, loss = 3.808\n",
      "    [batch 146]: seen 14600 examples at 70.0 eps, loss = 3.806\n",
      "    [batch 153]: seen 15300 examples at 70.0 eps, loss = 3.810\n",
      "    [batch 161]: seen 16100 examples at 70.0 eps, loss = 3.811\n",
      "    [batch 168]: seen 16800 examples at 70.0 eps, loss = 3.810\n",
      "    [batch 175]: seen 17500 examples at 70.0 eps, loss = 3.818\n",
      "    [batch 183]: seen 18300 examples at 70.0 eps, loss = 3.826\n",
      "    [batch 190]: seen 19000 examples at 70.0 eps, loss = 3.829\n",
      "    [batch 198]: seen 19800 examples at 70.0 eps, loss = 3.820\n",
      "    [batch 205]: seen 20500 examples at 70.0 eps, loss = 3.818\n",
      "    [batch 213]: seen 21300 examples at 70.0 eps, loss = 3.821\n",
      "    [batch 221]: seen 22100 examples at 70.0 eps, loss = 3.822\n",
      "    [batch 229]: seen 22900 examples at 70.0 eps, loss = 3.823\n",
      "    [batch 236]: seen 23600 examples at 70.0 eps, loss = 3.830\n",
      "    [batch 244]: seen 24400 examples at 70.0 eps, loss = 3.830\n",
      "    [batch 252]: seen 25200 examples at 70.0 eps, loss = 3.827\n",
      "    [batch 260]: seen 26000 examples at 70.0 eps, loss = 3.824\n",
      "    [batch 268]: seen 26800 examples at 70.0 eps, loss = 3.820\n",
      "    [batch 275]: seen 27500 examples at 70.0 eps, loss = 3.819\n",
      "    [batch 283]: seen 28300 examples at 70.0 eps, loss = 3.819\n",
      "    [batch 291]: seen 29100 examples at 70.0 eps, loss = 3.818\n",
      "    [batch 298]: seen 29800 examples at 70.0 eps, loss = 3.823\n",
      "    [batch 306]: seen 30600 examples at 70.0 eps, loss = 3.822\n",
      "    [batch 314]: seen 31400 examples at 70.0 eps, loss = 3.821\n",
      "    [batch 322]: seen 32200 examples at 70.0 eps, loss = 3.817\n",
      "    [batch 330]: seen 33000 examples at 70.0 eps, loss = 3.812\n",
      "    [batch 338]: seen 33800 examples at 70.0 eps, loss = 3.813\n",
      "    [batch 345]: seen 34500 examples at 70.0 eps, loss = 3.812\n",
      "    [batch 353]: seen 35300 examples at 70.0 eps, loss = 3.815\n",
      "    [batch 360]: seen 36000 examples at 70.0 eps, loss = 3.817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 368]: seen 36800 examples at 70.0 eps, loss = 3.810\n",
      "    [batch 376]: seen 37600 examples at 70.0 eps, loss = 3.813\n",
      "    [batch 384]: seen 38400 examples at 70.0 eps, loss = 3.815\n",
      "    [batch 391]: seen 39100 examples at 70.0 eps, loss = 3.811\n",
      "    [batch 399]: seen 39900 examples at 70.0 eps, loss = 3.808\n",
      "    [batch 407]: seen 40700 examples at 70.0 eps, loss = 3.809\n",
      "    [batch 415]: seen 41500 examples at 70.0 eps, loss = 3.810\n",
      "    [batch 422]: seen 42200 examples at 70.0 eps, loss = 3.809\n",
      "    [batch 429]: seen 42900 examples at 70.0 eps, loss = 3.804\n",
      "    [batch 435]: seen 43500 examples at 69.7 eps, loss = 3.802\n",
      "    [batch 438]: seen 43800 examples at 68.9 eps, loss = 3.800\n",
      "    [batch 446]: seen 44600 examples at 69.0 eps, loss = 3.803\n",
      "    [batch 454]: seen 45400 examples at 69.0 eps, loss = 3.806\n",
      "    [batch 461]: seen 46100 examples at 69.0 eps, loss = 3.805\n",
      "    [batch 469]: seen 46900 examples at 69.0 eps, loss = 3.802\n",
      "    [batch 474]: seen 47400 examples at 68.7 eps, loss = 3.800\n",
      "    [batch 482]: seen 48200 examples at 68.7 eps, loss = 3.807\n",
      "    [batch 490]: seen 49000 examples at 68.7 eps, loss = 3.807\n",
      "    [batch 498]: seen 49800 examples at 68.8 eps, loss = 3.805\n",
      "    [batch 506]: seen 50600 examples at 68.8 eps, loss = 3.803\n",
      "    [batch 514]: seen 51400 examples at 68.8 eps, loss = 3.807\n",
      "    [batch 522]: seen 52200 examples at 68.8 eps, loss = 3.808\n",
      "    [batch 530]: seen 53000 examples at 68.8 eps, loss = 3.811\n",
      "    [batch 537]: seen 53700 examples at 68.9 eps, loss = 3.807\n",
      "    [batch 545]: seen 54500 examples at 68.9 eps, loss = 3.806\n",
      "    [batch 553]: seen 55300 examples at 68.9 eps, loss = 3.812\n",
      "    [batch 561]: seen 56100 examples at 68.9 eps, loss = 3.817\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-14383\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-14383\n",
      "    [batch 567]: seen 56700 examples at 68.8 eps, loss = 3.819\n",
      "    [batch 575]: seen 57500 examples at 68.8 eps, loss = 3.818\n",
      "    [batch 583]: seen 58300 examples at 68.8 eps, loss = 3.821\n",
      "    [batch 591]: seen 59100 examples at 68.8 eps, loss = 3.818\n",
      "    [batch 599]: seen 59900 examples at 68.9 eps, loss = 3.819\n",
      "    [batch 607]: seen 60700 examples at 68.9 eps, loss = 3.825\n",
      "    [batch 615]: seen 61500 examples at 68.9 eps, loss = 3.822\n",
      "    [batch 623]: seen 62300 examples at 68.9 eps, loss = 3.820\n",
      "    [batch 631]: seen 63100 examples at 68.9 eps, loss = 3.816\n",
      "    [batch 639]: seen 63900 examples at 68.9 eps, loss = 3.819\n",
      "    [batch 647]: seen 64700 examples at 68.9 eps, loss = 3.819\n",
      "    [batch 655]: seen 65500 examples at 69.0 eps, loss = 3.818\n",
      "    [batch 662]: seen 66200 examples at 69.0 eps, loss = 3.820\n",
      "    [batch 670]: seen 67000 examples at 69.0 eps, loss = 3.816\n",
      "    [batch 678]: seen 67800 examples at 69.0 eps, loss = 3.812\n",
      "    [batch 686]: seen 68600 examples at 69.0 eps, loss = 3.818\n",
      "    [batch 694]: seen 69400 examples at 69.0 eps, loss = 3.829\n",
      "    [batch 702]: seen 70200 examples at 69.0 eps, loss = 3.826\n",
      "    [batch 710]: seen 71000 examples at 69.0 eps, loss = 3.825\n",
      "    [batch 718]: seen 71800 examples at 69.1 eps, loss = 3.823\n",
      "    [batch 726]: seen 72600 examples at 69.1 eps, loss = 3.822\n",
      "    [batch 734]: seen 73400 examples at 69.1 eps, loss = 3.825\n",
      "    [batch 742]: seen 74200 examples at 69.1 eps, loss = 3.821\n",
      "    [batch 750]: seen 75000 examples at 69.1 eps, loss = 3.812\n",
      "    [batch 758]: seen 75800 examples at 69.1 eps, loss = 3.809\n",
      "    [batch 766]: seen 76600 examples at 69.1 eps, loss = 3.809\n",
      "    [batch 774]: seen 77400 examples at 69.1 eps, loss = 3.812\n",
      "    [batch 782]: seen 78200 examples at 69.1 eps, loss = 3.810\n",
      "    [batch 789]: seen 78900 examples at 69.1 eps, loss = 3.814\n",
      "    [batch 797]: seen 79700 examples at 69.2 eps, loss = 3.814\n",
      "    [batch 805]: seen 80500 examples at 69.2 eps, loss = 3.815\n",
      "    [batch 813]: seen 81300 examples at 69.2 eps, loss = 3.822\n",
      "    [batch 821]: seen 82100 examples at 69.2 eps, loss = 3.819\n",
      "    [batch 829]: seen 82900 examples at 69.2 eps, loss = 3.821\n",
      "    [batch 837]: seen 83700 examples at 69.2 eps, loss = 3.820\n",
      "    [batch 845]: seen 84500 examples at 69.2 eps, loss = 3.821\n",
      "    [batch 853]: seen 85300 examples at 69.2 eps, loss = 3.821\n",
      "    [batch 861]: seen 86100 examples at 69.2 eps, loss = 3.820\n",
      "    [batch 869]: seen 86900 examples at 69.2 eps, loss = 3.818\n",
      "    [batch 877]: seen 87700 examples at 69.2 eps, loss = 3.817\n",
      "    [batch 885]: seen 88500 examples at 69.2 eps, loss = 3.805\n",
      "    [batch 893]: seen 89300 examples at 69.3 eps, loss = 3.803\n",
      "    [batch 901]: seen 90100 examples at 69.3 eps, loss = 3.802\n",
      "    [batch 909]: seen 90900 examples at 69.3 eps, loss = 3.802\n",
      "    [batch 916]: seen 91600 examples at 69.3 eps, loss = 3.801\n",
      "    [batch 923]: seen 92300 examples at 69.3 eps, loss = 3.806\n",
      "    [batch 930]: seen 93000 examples at 69.3 eps, loss = 3.811\n",
      "    [batch 938]: seen 93800 examples at 69.3 eps, loss = 3.809\n",
      "    [batch 945]: seen 94500 examples at 69.3 eps, loss = 3.809\n",
      "    [batch 953]: seen 95300 examples at 69.3 eps, loss = 3.808\n",
      "    [batch 961]: seen 96100 examples at 69.3 eps, loss = 3.804\n",
      "    [batch 969]: seen 96900 examples at 69.3 eps, loss = 3.801\n",
      "    [batch 976]: seen 97600 examples at 69.3 eps, loss = 3.804\n",
      "    [batch 984]: seen 98400 examples at 69.3 eps, loss = 3.808\n",
      "    [batch 991]: seen 99100 examples at 69.3 eps, loss = 3.808\n",
      "    [batch 999]: seen 99900 examples at 69.3 eps, loss = 3.806\n",
      "    [batch 1006]: seen 100600 examples at 69.3 eps, loss = 3.806\n",
      "    [batch 1014]: seen 101400 examples at 69.3 eps, loss = 3.807\n",
      "    [batch 1022]: seen 102200 examples at 69.3 eps, loss = 3.804\n",
      "    [batch 1030]: seen 103000 examples at 69.4 eps, loss = 3.802\n",
      "    [batch 1035]: seen 103500 examples at 69.0 eps, loss = 3.799\n",
      "    [batch 1043]: seen 104300 examples at 69.1 eps, loss = 3.803\n",
      "    [batch 1051]: seen 105100 examples at 68.9 eps, loss = 3.797\n",
      "    [batch 1056]: seen 105600 examples at 68.8 eps, loss = 3.797\n",
      "    [batch 1058]: seen 105800 examples at 68.5 eps, loss = 3.794\n",
      "    [batch 1061]: seen 106100 examples at 68.2 eps, loss = 3.792\n",
      "    [batch 1064]: seen 106400 examples at 67.9 eps, loss = 3.789\n",
      "    [batch 1069]: seen 106900 examples at 67.7 eps, loss = 3.792\n",
      "    [batch 1077]: seen 107700 examples at 67.8 eps, loss = 3.792\n",
      "    [batch 1084]: seen 108400 examples at 67.8 eps, loss = 3.791\n",
      "    [batch 1090]: seen 109000 examples at 67.6 eps, loss = 3.788\n",
      "    [batch 1093]: seen 109300 examples at 67.2 eps, loss = 3.786\n",
      "    [batch 1101]: seen 110100 examples at 67.3 eps, loss = 3.793\n",
      "    [batch 1108]: seen 110800 examples at 67.3 eps, loss = 3.792\n",
      "    [batch 1116]: seen 111600 examples at 67.3 eps, loss = 3.796\n",
      "    [batch 1124]: seen 112400 examples at 67.3 eps, loss = 3.795\n",
      "    [batch 1131]: seen 113100 examples at 67.3 eps, loss = 3.792\n",
      "    [batch 1139]: seen 113900 examples at 67.3 eps, loss = 3.790\n",
      "    [batch 1146]: seen 114600 examples at 67.4 eps, loss = 3.790\n",
      "    [batch 1153]: seen 115300 examples at 67.4 eps, loss = 3.791\n",
      "    [batch 1161]: seen 116100 examples at 67.4 eps, loss = 3.789\n",
      "    [batch 1165]: seen 116500 examples at 67.1 eps, loss = 3.784\n",
      "    [batch 1170]: seen 117000 examples at 67.0 eps, loss = 3.784\n",
      "    [batch 1173]: seen 117300 examples at 66.8 eps, loss = 3.783\n",
      "    [batch 1178]: seen 117800 examples at 66.7 eps, loss = 3.780\n",
      "    [batch 1183]: seen 118300 examples at 66.5 eps, loss = 3.782\n",
      "    [batch 1191]: seen 119100 examples at 66.6 eps, loss = 3.782\n",
      "    [batch 1196]: seen 119600 examples at 66.4 eps, loss = 3.779\n",
      "    [batch 1199]: seen 119900 examples at 66.1 eps, loss = 3.777\n",
      "    [batch 1203]: seen 120300 examples at 65.9 eps, loss = 3.776\n",
      "    [batch 1211]: seen 121100 examples at 65.9 eps, loss = 3.780\n",
      "    [batch 1218]: seen 121800 examples at 65.9 eps, loss = 3.784\n",
      "    [batch 1225]: seen 122500 examples at 65.9 eps, loss = 3.782\n",
      "    [batch 1232]: seen 123200 examples at 66.0 eps, loss = 3.781\n",
      "    [batch 1240]: seen 124000 examples at 66.0 eps, loss = 3.780\n",
      "    [batch 1248]: seen 124800 examples at 66.0 eps, loss = 3.776\n",
      "    [batch 1256]: seen 125600 examples at 65.9 eps, loss = 3.776\n",
      "    [batch 1261]: seen 126100 examples at 65.8 eps, loss = 3.777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1266]: seen 126600 examples at 65.6 eps, loss = 3.775\n",
      "    [batch 1269]: seen 126900 examples at 65.3 eps, loss = 3.772\n",
      "    [batch 1274]: seen 127400 examples at 65.2 eps, loss = 3.774\n",
      "    [batch 1282]: seen 128200 examples at 65.2 eps, loss = 3.775\n",
      "    [batch 1290]: seen 129000 examples at 65.2 eps, loss = 3.774\n",
      "    [batch 1293]: seen 129300 examples at 65.0 eps, loss = 3.770\n",
      "    [batch 1295]: seen 129500 examples at 64.8 eps, loss = 3.768\n",
      "    [batch 1302]: seen 130200 examples at 64.8 eps, loss = 3.773\n",
      "    [batch 1310]: seen 131000 examples at 64.9 eps, loss = 3.775\n",
      "    [batch 1318]: seen 131800 examples at 64.9 eps, loss = 3.775\n",
      "    [batch 1325]: seen 132500 examples at 64.9 eps, loss = 3.773\n",
      "    [batch 1333]: seen 133300 examples at 64.9 eps, loss = 3.776\n",
      "    [batch 1340]: seen 134000 examples at 65.0 eps, loss = 3.780\n",
      "    [batch 1347]: seen 134700 examples at 65.0 eps, loss = 3.775\n",
      "    [batch 1355]: seen 135500 examples at 65.0 eps, loss = 3.779\n",
      "    [batch 1363]: seen 136300 examples at 65.0 eps, loss = 3.778\n",
      "    [batch 1370]: seen 137000 examples at 65.1 eps, loss = 3.779\n",
      "    [batch 1378]: seen 137800 examples at 65.1 eps, loss = 3.777\n",
      "    [batch 1386]: seen 138600 examples at 65.1 eps, loss = 3.775\n",
      "    [batch 1394]: seen 139400 examples at 65.1 eps, loss = 3.777\n",
      "    [batch 1402]: seen 140200 examples at 65.2 eps, loss = 3.775\n",
      "    [batch 1410]: seen 141000 examples at 65.2 eps, loss = 3.773\n",
      "    [batch 1418]: seen 141800 examples at 65.2 eps, loss = 3.770\n",
      "    [batch 1425]: seen 142500 examples at 65.2 eps, loss = 3.770\n",
      "    [EXCEPTION]:  OOM when allocating tensor with shape[100,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/Softmax_98 = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/xw_plus_b_98)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'projection/Softmax_98', defined at:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-40345e958dd0>\", line 9, in <module>\n",
      "    train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)\n",
      "  File \"<ipython-input-5-17e3ed376236>\", line 3, in train_continue\n",
      "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/training_util.py\", line 37, in training_init\n",
      "    lm.BuildCoreGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 140, in BuildCoreGraph\n",
      "    self.BuildProjectionGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 169, in BuildProjectionGraph\n",
      "    vocab_distribution = [tf.nn.softmax(s) for s in self.vocab_scores]\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 169, in <listcomp>\n",
      "    vocab_distribution = [tf.nn.softmax(s) for s in self.vocab_scores]\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1738, in softmax\n",
      "    return _softmax(logits, gen_nn_ops.softmax, axis, name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1673, in _softmax\n",
      "    return compute_op(logits, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7097, in softmax\n",
      "    \"Softmax\", logits=logits, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/Softmax_98 = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/xw_plus_b_98)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      " ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-15114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-15114\n",
      "    [batch 1433]: seen 143300 examples at 64.9 eps, loss = 3.774\n",
      "    [batch 1439]: seen 143900 examples at 64.9 eps, loss = 3.768\n",
      "    [batch 1447]: seen 144700 examples at 64.9 eps, loss = 3.770\n",
      "    [batch 1454]: seen 145400 examples at 64.9 eps, loss = 3.770\n",
      "    [batch 1461]: seen 146100 examples at 64.9 eps, loss = 3.769\n",
      "    [batch 1466]: seen 146600 examples at 64.9 eps, loss = 3.769\n",
      "    [batch 1474]: seen 147400 examples at 64.9 eps, loss = 3.771\n",
      "    [batch 1481]: seen 148100 examples at 64.9 eps, loss = 3.773\n",
      "    [batch 1489]: seen 148900 examples at 64.9 eps, loss = 3.776\n",
      "    [batch 1496]: seen 149600 examples at 65.0 eps, loss = 3.777\n",
      "    [batch 1504]: seen 150400 examples at 65.0 eps, loss = 3.777\n",
      "    [batch 1512]: seen 151200 examples at 65.0 eps, loss = 3.776\n",
      "    [batch 1520]: seen 152000 examples at 65.0 eps, loss = 3.776\n",
      "    [batch 1527]: seen 152700 examples at 65.1 eps, loss = 3.772\n",
      "    [batch 1535]: seen 153500 examples at 65.1 eps, loss = 3.776\n",
      "    [batch 1543]: seen 154300 examples at 65.1 eps, loss = 3.769\n",
      "    [batch 1551]: seen 155100 examples at 65.1 eps, loss = 3.772\n",
      "    [batch 1559]: seen 155900 examples at 65.2 eps, loss = 3.771\n",
      "    [batch 1566]: seen 156600 examples at 65.2 eps, loss = 3.770\n",
      "    [batch 1574]: seen 157400 examples at 65.2 eps, loss = 3.769\n",
      "    [batch 1582]: seen 158200 examples at 65.2 eps, loss = 3.772\n",
      "    [batch 1590]: seen 159000 examples at 65.2 eps, loss = 3.776\n",
      "    [batch 1598]: seen 159800 examples at 65.3 eps, loss = 3.777\n",
      "    [batch 1606]: seen 160600 examples at 65.3 eps, loss = 3.772\n",
      "    [batch 1614]: seen 161400 examples at 65.3 eps, loss = 3.770\n",
      "    [batch 1622]: seen 162200 examples at 65.2 eps, loss = 3.767\n",
      "    [batch 1626]: seen 162600 examples at 65.1 eps, loss = 3.766\n",
      "    [batch 1633]: seen 163300 examples at 65.1 eps, loss = 3.771\n",
      "    [batch 1641]: seen 164100 examples at 65.1 eps, loss = 3.769\n",
      "    [batch 1648]: seen 164800 examples at 65.1 eps, loss = 3.770\n",
      "    [batch 1656]: seen 165600 examples at 65.2 eps, loss = 3.773\n",
      "    [batch 1664]: seen 166400 examples at 65.2 eps, loss = 3.771\n",
      "    [batch 1671]: seen 167100 examples at 65.2 eps, loss = 3.778\n",
      "    [batch 1678]: seen 167800 examples at 65.2 eps, loss = 3.772\n",
      "    [batch 1685]: seen 168500 examples at 65.2 eps, loss = 3.766\n",
      "    [batch 1693]: seen 169300 examples at 65.2 eps, loss = 3.767\n",
      "    [batch 1701]: seen 170100 examples at 65.2 eps, loss = 3.768\n",
      "    [batch 1704]: seen 170400 examples at 65.0 eps, loss = 3.765\n",
      "    [batch 1709]: seen 170900 examples at 65.0 eps, loss = 3.764\n",
      "    [batch 1712]: seen 171200 examples at 64.8 eps, loss = 3.762\n",
      "    [batch 1715]: seen 171500 examples at 64.7 eps, loss = 3.759\n",
      "    [batch 1720]: seen 172000 examples at 64.6 eps, loss = 3.761\n",
      "    [batch 1728]: seen 172800 examples at 64.6 eps, loss = 3.763\n",
      "    [batch 1735]: seen 173500 examples at 64.6 eps, loss = 3.760\n",
      "    [batch 1743]: seen 174300 examples at 64.7 eps, loss = 3.761\n",
      "    [batch 1751]: seen 175100 examples at 64.7 eps, loss = 3.761\n",
      "    [batch 1759]: seen 175900 examples at 64.7 eps, loss = 3.763\n",
      "    [batch 1767]: seen 176700 examples at 64.7 eps, loss = 3.767\n",
      "    [batch 1775]: seen 177500 examples at 64.8 eps, loss = 3.764\n",
      "    [batch 1783]: seen 178300 examples at 64.8 eps, loss = 3.768\n",
      "    [batch 1790]: seen 179000 examples at 64.8 eps, loss = 3.769\n",
      "    [batch 1798]: seen 179800 examples at 64.8 eps, loss = 3.769\n",
      "    [batch 1806]: seen 180600 examples at 64.8 eps, loss = 3.773\n",
      "    [batch 1814]: seen 181400 examples at 64.9 eps, loss = 3.774\n",
      "    [batch 1822]: seen 182200 examples at 64.9 eps, loss = 3.769\n",
      "    [batch 1830]: seen 183000 examples at 64.9 eps, loss = 3.774\n",
      "    [batch 1838]: seen 183800 examples at 64.9 eps, loss = 3.771\n",
      "    [batch 1845]: seen 184500 examples at 64.9 eps, loss = 3.766\n",
      "    [batch 1853]: seen 185300 examples at 65.0 eps, loss = 3.765\n",
      "    [batch 1860]: seen 186000 examples at 65.0 eps, loss = 3.763\n",
      "    [batch 1868]: seen 186800 examples at 65.0 eps, loss = 3.769\n",
      "    [END] Training complete: Total examples : 187000; Total time: 0:47:56\n",
      "[EPOCH 10] Complete. Avg Loss: 3.7680904701269347; Best Loss: 3.7568204065927393\n",
      "[END] Training complete: Best Loss: 3.7568204065927393; Total time: 4:05:42\n"
     ]
    }
   ],
   "source": [
    "avg_loss = 4.07316\n",
    "best_loss = 4.06781\n",
    "curr_best = best_loss\n",
    "train_step = 9260\n",
    "epochs = 5\n",
    "restore = True\n",
    "epoch_start = 5\n",
    "\n",
    "train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train session 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 252\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 110\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 2 1/2 76\n",
      "\n",
      "\n",
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: 16:03\n",
      "INFO:tensorflow:Fetching data..\n",
      "INFO:tensorflow:Creating batches..\n",
      "INFO:tensorflow:[TOTAL Batches]  : 1872\n",
      "INFO:tensorflow:[TOTAL Examples] : 187193\n",
      "INFO:tensorflow:Creating batches..COMPLETE\n",
      "INFO:tensorflow:Building core graph...\n",
      "INFO:tensorflow:Adding attention_decoder timestep 0 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 1 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 2 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 3 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 4 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 5 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 6 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 7 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 8 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 9 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 10 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 11 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 12 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 13 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 14 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 15 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 16 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 17 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 18 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 19 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 20 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 21 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 22 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 23 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 24 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 25 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 26 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 27 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 28 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 29 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 30 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 31 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 32 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 33 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 34 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 35 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 36 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 37 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 38 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 39 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 40 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 41 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 42 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 43 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 44 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 45 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 46 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 47 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 48 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 49 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 50 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 51 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 52 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 53 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 54 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 55 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 56 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 57 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 58 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 59 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 60 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 61 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 62 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 63 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 64 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 65 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 66 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 67 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 68 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 69 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 70 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 71 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 72 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 73 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 74 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 75 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 76 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 77 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 78 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 79 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 80 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 81 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 82 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 83 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 84 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 85 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 86 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 87 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 88 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 89 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 90 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 91 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 92 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 93 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 94 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 95 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 96 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 97 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 98 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 99 of 100\n",
      "INFO:tensorflow:Building projection graph...\n",
      "INFO:tensorflow:Building projection graph...COMPLETE\n",
      "INFO:tensorflow:Building Loss graph...\n",
      "INFO:tensorflow:Building Loss graph...COMPLETE\n",
      "INFO:tensorflow:Building core graph...COMPLETE\n",
      "INFO:tensorflow:Building train graph...\n",
      "INFO:tensorflow:Building train graph...COMPLETE\n",
      "INFO:tensorflow:Building summary graph...\n",
      "INFO:tensorflow:Building summary graph...COMPLETE\n",
      "WARNING:tensorflow:From <ipython-input-5-17e3ed376236>:18: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-15398\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-15398\n",
      "[EPOCH 11] Starting training..\n",
      "    [batch 1]: seen 100 examples at 3.6 eps, loss = 3.767\n",
      "    [batch 8]: seen 800 examples at 21.0 eps, loss = 3.765\n",
      "    [batch 15]: seen 1500 examples at 31.1 eps, loss = 3.765\n",
      "    [batch 22]: seen 2200 examples at 37.6 eps, loss = 3.766\n",
      "    [batch 29]: seen 2900 examples at 42.2 eps, loss = 3.771\n",
      "    [batch 36]: seen 3600 examples at 45.7 eps, loss = 3.768\n",
      "    [batch 43]: seen 4300 examples at 48.3 eps, loss = 3.771\n",
      "    [batch 50]: seen 5000 examples at 50.4 eps, loss = 3.771\n",
      "    [batch 57]: seen 5700 examples at 52.1 eps, loss = 3.770\n",
      "    [batch 64]: seen 6400 examples at 53.5 eps, loss = 3.767\n",
      "    [batch 71]: seen 7100 examples at 54.7 eps, loss = 3.768\n",
      "    [batch 78]: seen 7800 examples at 55.7 eps, loss = 3.765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 85]: seen 8500 examples at 56.6 eps, loss = 3.763\n",
      "    [batch 92]: seen 9200 examples at 57.3 eps, loss = 3.764\n",
      "    [batch 99]: seen 9900 examples at 58.0 eps, loss = 3.761\n",
      "    [batch 106]: seen 10600 examples at 58.6 eps, loss = 3.758\n",
      "    [batch 113]: seen 11300 examples at 58.0 eps, loss = 3.757\n",
      "    [batch 120]: seen 12000 examples at 58.5 eps, loss = 3.758\n",
      "    [batch 127]: seen 12700 examples at 59.0 eps, loss = 3.766\n",
      "    [batch 134]: seen 13400 examples at 59.4 eps, loss = 3.762\n",
      "    [batch 141]: seen 14100 examples at 59.8 eps, loss = 3.763\n",
      "    [batch 148]: seen 14800 examples at 60.2 eps, loss = 3.763\n",
      "    [batch 155]: seen 15500 examples at 60.6 eps, loss = 3.758\n",
      "    [batch 162]: seen 16200 examples at 60.9 eps, loss = 3.759\n",
      "    [batch 169]: seen 16900 examples at 61.2 eps, loss = 3.758\n",
      "    [batch 176]: seen 17600 examples at 61.4 eps, loss = 3.759\n",
      "    [batch 182]: seen 18200 examples at 60.9 eps, loss = 3.756\n",
      "    [batch 189]: seen 18900 examples at 61.2 eps, loss = 3.759\n",
      "    [batch 195]: seen 19500 examples at 60.7 eps, loss = 3.756\n",
      "    [batch 200]: seen 20000 examples at 60.3 eps, loss = 3.756\n",
      "    [batch 204]: seen 20400 examples at 59.2 eps, loss = 3.755\n",
      "    [batch 207]: seen 20700 examples at 58.2 eps, loss = 3.754\n",
      "    [batch 212]: seen 21200 examples at 57.8 eps, loss = 3.755\n",
      "    [batch 219]: seen 21900 examples at 58.1 eps, loss = 3.761\n",
      "    [batch 226]: seen 22600 examples at 58.4 eps, loss = 3.761\n",
      "    [batch 233]: seen 23300 examples at 58.7 eps, loss = 3.761\n",
      "    [batch 240]: seen 24000 examples at 58.9 eps, loss = 3.759\n",
      "    [batch 247]: seen 24700 examples at 59.2 eps, loss = 3.757\n",
      "    [batch 254]: seen 25400 examples at 59.4 eps, loss = 3.757\n",
      "    [batch 259]: seen 25900 examples at 59.1 eps, loss = 3.754\n",
      "    [batch 266]: seen 26600 examples at 59.3 eps, loss = 3.753\n",
      "    [batch 273]: seen 27300 examples at 59.5 eps, loss = 3.755\n",
      "    [batch 276]: seen 27600 examples at 58.7 eps, loss = 3.751\n",
      "    [batch 280]: seen 28000 examples at 58.0 eps, loss = 3.748\n",
      "    [batch 285]: seen 28500 examples at 57.3 eps, loss = 3.747\n",
      "    [batch 290]: seen 29000 examples at 57.0 eps, loss = 3.751\n",
      "    [batch 297]: seen 29700 examples at 57.3 eps, loss = 3.752\n",
      "    [batch 304]: seen 30400 examples at 57.5 eps, loss = 3.755\n",
      "    [batch 311]: seen 31100 examples at 57.7 eps, loss = 3.759\n",
      "    [batch 318]: seen 31800 examples at 57.9 eps, loss = 3.761\n",
      "    [batch 325]: seen 32500 examples at 58.1 eps, loss = 3.759\n",
      "    [batch 332]: seen 33200 examples at 58.3 eps, loss = 3.763\n",
      "    [batch 339]: seen 33900 examples at 58.5 eps, loss = 3.765\n",
      "    [batch 346]: seen 34600 examples at 58.7 eps, loss = 3.771\n",
      "    [batch 353]: seen 35300 examples at 58.8 eps, loss = 3.770\n",
      "    [batch 360]: seen 36000 examples at 59.0 eps, loss = 3.773\n",
      "    [batch 367]: seen 36700 examples at 59.2 eps, loss = 3.774\n",
      "    [batch 374]: seen 37400 examples at 59.3 eps, loss = 3.772\n",
      "    [batch 381]: seen 38100 examples at 59.5 eps, loss = 3.769\n",
      "    [batch 388]: seen 38800 examples at 59.6 eps, loss = 3.768\n",
      "    [batch 395]: seen 39500 examples at 59.8 eps, loss = 3.768\n",
      "    [batch 402]: seen 40200 examples at 59.9 eps, loss = 3.765\n",
      "    [batch 409]: seen 40900 examples at 60.0 eps, loss = 3.766\n",
      "    [batch 416]: seen 41600 examples at 60.2 eps, loss = 3.761\n",
      "    [batch 423]: seen 42300 examples at 60.3 eps, loss = 3.760\n",
      "    [batch 430]: seen 43000 examples at 60.4 eps, loss = 3.756\n",
      "    [batch 437]: seen 43700 examples at 60.5 eps, loss = 3.756\n",
      "    [batch 444]: seen 44400 examples at 60.7 eps, loss = 3.762\n",
      "    [batch 451]: seen 45100 examples at 60.8 eps, loss = 3.759\n",
      "    [batch 458]: seen 45800 examples at 60.9 eps, loss = 3.763\n",
      "    [batch 465]: seen 46500 examples at 61.0 eps, loss = 3.761\n",
      "    [batch 472]: seen 47200 examples at 61.1 eps, loss = 3.760\n",
      "    [batch 479]: seen 47900 examples at 61.2 eps, loss = 3.764\n",
      "    [batch 486]: seen 48600 examples at 61.3 eps, loss = 3.765\n",
      "    [batch 493]: seen 49300 examples at 61.4 eps, loss = 3.768\n",
      "    [batch 500]: seen 50000 examples at 61.5 eps, loss = 3.767\n",
      "    [batch 507]: seen 50700 examples at 61.6 eps, loss = 3.767\n",
      "    [batch 514]: seen 51400 examples at 61.7 eps, loss = 3.766\n",
      "    [batch 521]: seen 52100 examples at 61.8 eps, loss = 3.763\n",
      "    [batch 528]: seen 52800 examples at 61.9 eps, loss = 3.762\n",
      "    [batch 535]: seen 53500 examples at 61.9 eps, loss = 3.761\n",
      "    [batch 542]: seen 54200 examples at 62.0 eps, loss = 3.761\n",
      "    [batch 549]: seen 54900 examples at 62.1 eps, loss = 3.761\n",
      "    [batch 556]: seen 55600 examples at 62.2 eps, loss = 3.757\n",
      "    [batch 563]: seen 56300 examples at 62.3 eps, loss = 3.758\n",
      "    [batch 570]: seen 57000 examples at 62.3 eps, loss = 3.757\n",
      "    [batch 577]: seen 57700 examples at 62.4 eps, loss = 3.751\n",
      "    [batch 584]: seen 58400 examples at 62.2 eps, loss = 3.747\n",
      "    [batch 589]: seen 58900 examples at 62.1 eps, loss = 3.746\n",
      "    [batch 592]: seen 59200 examples at 61.7 eps, loss = 3.744\n",
      "    [batch 599]: seen 59900 examples at 61.7 eps, loss = 3.746\n",
      "    [batch 602]: seen 60200 examples at 61.3 eps, loss = 3.740\n",
      "    [batch 605]: seen 60500 examples at 61.0 eps, loss = 3.739\n",
      "    [batch 610]: seen 61000 examples at 60.8 eps, loss = 3.743\n",
      "    [batch 617]: seen 61700 examples at 60.9 eps, loss = 3.741\n",
      "    [batch 624]: seen 62400 examples at 61.0 eps, loss = 3.740\n",
      "    [batch 629]: seen 62900 examples at 60.8 eps, loss = 3.740\n",
      "    [batch 634]: seen 63400 examples at 60.7 eps, loss = 3.741\n",
      "    [batch 641]: seen 64100 examples at 60.8 eps, loss = 3.739\n",
      "    [batch 648]: seen 64800 examples at 60.8 eps, loss = 3.743\n",
      "    [batch 655]: seen 65500 examples at 60.9 eps, loss = 3.738\n",
      "    [batch 658]: seen 65800 examples at 60.6 eps, loss = 3.736\n",
      "    [batch 662]: seen 66200 examples at 60.2 eps, loss = 3.735\n",
      "    [batch 667]: seen 66700 examples at 60.1 eps, loss = 3.736\n",
      "    [batch 671]: seen 67100 examples at 59.8 eps, loss = 3.734\n",
      "    [batch 678]: seen 67800 examples at 59.8 eps, loss = 3.735\n",
      "    [batch 683]: seen 68300 examples at 59.5 eps, loss = 3.734\n",
      "    [batch 686]: seen 68600 examples at 59.2 eps, loss = 3.735\n",
      "    [batch 691]: seen 69100 examples at 59.1 eps, loss = 3.734\n",
      "    [batch 696]: seen 69600 examples at 59.0 eps, loss = 3.737\n",
      "    [batch 703]: seen 70300 examples at 58.9 eps, loss = 3.731\n",
      "    [batch 706]: seen 70600 examples at 58.6 eps, loss = 3.731\n",
      "    [batch 711]: seen 71100 examples at 58.5 eps, loss = 3.731\n",
      "    [batch 716]: seen 71600 examples at 58.4 eps, loss = 3.734\n",
      "    [batch 723]: seen 72300 examples at 58.5 eps, loss = 3.732\n",
      "    [batch 728]: seen 72800 examples at 58.4 eps, loss = 3.729\n",
      "    [batch 735]: seen 73500 examples at 58.5 eps, loss = 3.734\n",
      "    [batch 742]: seen 74200 examples at 58.6 eps, loss = 3.736\n",
      "    [batch 749]: seen 74900 examples at 58.7 eps, loss = 3.737\n",
      "    [batch 756]: seen 75600 examples at 58.8 eps, loss = 3.739\n",
      "    [batch 763]: seen 76300 examples at 58.8 eps, loss = 3.736\n",
      "    [batch 770]: seen 77000 examples at 58.9 eps, loss = 3.736\n",
      "    [batch 777]: seen 77700 examples at 59.0 eps, loss = 3.740\n",
      "    [batch 784]: seen 78400 examples at 59.1 eps, loss = 3.748\n",
      "    [batch 791]: seen 79100 examples at 59.1 eps, loss = 3.749\n",
      "    [batch 798]: seen 79800 examples at 59.2 eps, loss = 3.749\n",
      "    [batch 805]: seen 80500 examples at 59.3 eps, loss = 3.745\n",
      "    [batch 812]: seen 81200 examples at 59.4 eps, loss = 3.746\n",
      "    [batch 819]: seen 81900 examples at 59.4 eps, loss = 3.746\n",
      "    [batch 826]: seen 82600 examples at 59.5 eps, loss = 3.744\n",
      "    [batch 833]: seen 83300 examples at 59.6 eps, loss = 3.743\n",
      "    [batch 840]: seen 84000 examples at 59.6 eps, loss = 3.737\n",
      "    [batch 847]: seen 84700 examples at 59.7 eps, loss = 3.740\n",
      "    [batch 854]: seen 85400 examples at 59.8 eps, loss = 3.736\n",
      "    [batch 861]: seen 86100 examples at 59.8 eps, loss = 3.734\n",
      "    [batch 868]: seen 86800 examples at 59.9 eps, loss = 3.738\n",
      "    [batch 875]: seen 87500 examples at 60.0 eps, loss = 3.739\n",
      "    [batch 882]: seen 88200 examples at 60.0 eps, loss = 3.736\n",
      "    [batch 889]: seen 88900 examples at 60.1 eps, loss = 3.736\n",
      "    [batch 896]: seen 89600 examples at 60.2 eps, loss = 3.744\n",
      "    [batch 903]: seen 90300 examples at 60.2 eps, loss = 3.746\n",
      "    [batch 910]: seen 91000 examples at 60.3 eps, loss = 3.748\n",
      "    [batch 917]: seen 91700 examples at 60.3 eps, loss = 3.749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 924]: seen 92400 examples at 60.4 eps, loss = 3.749\n",
      "    [batch 931]: seen 93100 examples at 60.4 eps, loss = 3.750\n",
      "    [batch 938]: seen 93800 examples at 60.5 eps, loss = 3.752\n",
      "    [batch 945]: seen 94500 examples at 60.6 eps, loss = 3.753\n",
      "    [batch 952]: seen 95200 examples at 60.6 eps, loss = 3.759\n",
      "    [batch 959]: seen 95900 examples at 60.7 eps, loss = 3.759\n",
      "    [batch 966]: seen 96600 examples at 60.7 eps, loss = 3.758\n",
      "    [batch 973]: seen 97300 examples at 60.8 eps, loss = 3.759\n",
      "    [batch 980]: seen 98000 examples at 60.8 eps, loss = 3.764\n",
      "    [batch 987]: seen 98700 examples at 60.9 eps, loss = 3.760\n",
      "    [batch 994]: seen 99400 examples at 60.9 eps, loss = 3.763\n",
      "    [batch 1001]: seen 100100 examples at 61.0 eps, loss = 3.755\n",
      "    [batch 1008]: seen 100800 examples at 61.0 eps, loss = 3.749\n",
      "    [batch 1015]: seen 101500 examples at 61.1 eps, loss = 3.751\n",
      "    [batch 1022]: seen 102200 examples at 61.1 eps, loss = 3.746\n",
      "    [batch 1029]: seen 102900 examples at 61.2 eps, loss = 3.747\n",
      "    [batch 1036]: seen 103600 examples at 61.2 eps, loss = 3.746\n",
      "    [batch 1043]: seen 104300 examples at 61.3 eps, loss = 3.743\n",
      "    [batch 1050]: seen 105000 examples at 61.3 eps, loss = 3.745\n",
      "    [batch 1057]: seen 105700 examples at 61.4 eps, loss = 3.748\n",
      "    [batch 1064]: seen 106400 examples at 61.4 eps, loss = 3.747\n",
      "    [batch 1071]: seen 107100 examples at 61.4 eps, loss = 3.748\n",
      "    [batch 1078]: seen 107800 examples at 61.5 eps, loss = 3.749\n",
      "    [batch 1085]: seen 108500 examples at 61.5 eps, loss = 3.748\n",
      "    [batch 1092]: seen 109200 examples at 61.6 eps, loss = 3.752\n",
      "    [batch 1099]: seen 109900 examples at 61.6 eps, loss = 3.757\n",
      "    [batch 1106]: seen 110600 examples at 61.7 eps, loss = 3.755\n",
      "    [batch 1113]: seen 111300 examples at 61.7 eps, loss = 3.758\n",
      "    [batch 1120]: seen 112000 examples at 61.7 eps, loss = 3.759\n",
      "    [batch 1127]: seen 112700 examples at 61.8 eps, loss = 3.758\n",
      "    [batch 1134]: seen 113400 examples at 61.8 eps, loss = 3.750\n",
      "    [batch 1141]: seen 114100 examples at 61.9 eps, loss = 3.749\n",
      "    [batch 1148]: seen 114800 examples at 61.9 eps, loss = 3.749\n",
      "    [batch 1155]: seen 115500 examples at 61.9 eps, loss = 3.749\n",
      "    [batch 1162]: seen 116200 examples at 62.0 eps, loss = 3.745\n",
      "    [batch 1169]: seen 116900 examples at 62.0 eps, loss = 3.743\n",
      "    [batch 1176]: seen 117600 examples at 62.1 eps, loss = 3.743\n",
      "    [batch 1183]: seen 118300 examples at 62.1 eps, loss = 3.740\n",
      "    [batch 1190]: seen 119000 examples at 62.1 eps, loss = 3.737\n",
      "    [batch 1197]: seen 119700 examples at 62.2 eps, loss = 3.735\n",
      "    [batch 1204]: seen 120400 examples at 62.2 eps, loss = 3.739\n",
      "    [batch 1211]: seen 121100 examples at 62.2 eps, loss = 3.735\n",
      "    [batch 1218]: seen 121800 examples at 62.3 eps, loss = 3.734\n",
      "    [batch 1225]: seen 122500 examples at 62.3 eps, loss = 3.732\n",
      "    [batch 1232]: seen 123200 examples at 62.3 eps, loss = 3.731\n",
      "    [batch 1239]: seen 123900 examples at 62.4 eps, loss = 3.731\n",
      "    [batch 1246]: seen 124600 examples at 62.4 eps, loss = 3.733\n",
      "    [batch 1253]: seen 125300 examples at 62.4 eps, loss = 3.730\n",
      "    [batch 1260]: seen 126000 examples at 62.5 eps, loss = 3.729\n",
      "    [batch 1267]: seen 126700 examples at 62.4 eps, loss = 3.728\n",
      "    [batch 1272]: seen 127200 examples at 62.3 eps, loss = 3.728\n",
      "    [batch 1274]: seen 127400 examples at 62.1 eps, loss = 3.727\n",
      "    [batch 1279]: seen 127900 examples at 62.0 eps, loss = 3.727\n",
      "    [batch 1284]: seen 128400 examples at 62.0 eps, loss = 3.728\n",
      "    [batch 1290]: seen 129000 examples at 61.9 eps, loss = 3.723\n",
      "    [batch 1297]: seen 129700 examples at 61.9 eps, loss = 3.728\n",
      "    [batch 1304]: seen 130400 examples at 62.0 eps, loss = 3.724\n",
      "    [batch 1311]: seen 131100 examples at 62.0 eps, loss = 3.726\n",
      "    [batch 1318]: seen 131800 examples at 62.0 eps, loss = 3.730\n",
      "    [batch 1325]: seen 132500 examples at 62.1 eps, loss = 3.731\n",
      "    [batch 1332]: seen 133200 examples at 62.1 eps, loss = 3.732\n",
      "    [batch 1339]: seen 133900 examples at 62.1 eps, loss = 3.731\n",
      "    [batch 1346]: seen 134600 examples at 62.2 eps, loss = 3.737\n",
      "    [batch 1353]: seen 135300 examples at 62.2 eps, loss = 3.740\n",
      "    [batch 1360]: seen 136000 examples at 62.2 eps, loss = 3.739\n",
      "    [batch 1367]: seen 136700 examples at 62.2 eps, loss = 3.733\n",
      "    [batch 1374]: seen 137400 examples at 62.3 eps, loss = 3.735\n",
      "    [batch 1381]: seen 138100 examples at 62.3 eps, loss = 3.740\n",
      "    [batch 1388]: seen 138800 examples at 62.3 eps, loss = 3.740\n",
      "    [batch 1395]: seen 139500 examples at 62.4 eps, loss = 3.737\n",
      "    [batch 1402]: seen 140200 examples at 62.4 eps, loss = 3.740\n",
      "    [batch 1409]: seen 140900 examples at 62.4 eps, loss = 3.746\n",
      "    [batch 1416]: seen 141600 examples at 62.5 eps, loss = 3.744\n",
      "    [batch 1423]: seen 142300 examples at 62.5 eps, loss = 3.739\n",
      "    [batch 1430]: seen 143000 examples at 62.5 eps, loss = 3.734\n",
      "    [batch 1437]: seen 143700 examples at 62.5 eps, loss = 3.732\n",
      "    [batch 1444]: seen 144400 examples at 62.6 eps, loss = 3.727\n",
      "    [batch 1451]: seen 145100 examples at 62.6 eps, loss = 3.732\n",
      "    [batch 1458]: seen 145800 examples at 62.6 eps, loss = 3.732\n",
      "    [batch 1465]: seen 146500 examples at 62.7 eps, loss = 3.732\n",
      "    [batch 1472]: seen 147200 examples at 62.7 eps, loss = 3.731\n",
      "    [batch 1479]: seen 147900 examples at 62.7 eps, loss = 3.734\n",
      "    [batch 1486]: seen 148600 examples at 62.7 eps, loss = 3.734\n",
      "    [batch 1493]: seen 149300 examples at 62.8 eps, loss = 3.732\n",
      "    [batch 1500]: seen 150000 examples at 62.8 eps, loss = 3.732\n",
      "    [batch 1507]: seen 150700 examples at 62.8 eps, loss = 3.742\n",
      "    [batch 1514]: seen 151400 examples at 62.9 eps, loss = 3.737\n",
      "    [batch 1521]: seen 152100 examples at 62.9 eps, loss = 3.736\n",
      "    [batch 1528]: seen 152800 examples at 62.9 eps, loss = 3.738\n",
      "    [batch 1535]: seen 153500 examples at 62.9 eps, loss = 3.743\n",
      "    [batch 1542]: seen 154200 examples at 63.0 eps, loss = 3.744\n",
      "    [batch 1549]: seen 154900 examples at 63.0 eps, loss = 3.743\n",
      "    [batch 1556]: seen 155600 examples at 63.0 eps, loss = 3.743\n",
      "    [batch 1563]: seen 156300 examples at 63.0 eps, loss = 3.744\n",
      "    [batch 1570]: seen 157000 examples at 63.1 eps, loss = 3.738\n",
      "    [batch 1577]: seen 157700 examples at 63.1 eps, loss = 3.735\n",
      "    [batch 1584]: seen 158400 examples at 63.1 eps, loss = 3.738\n",
      "    [batch 1591]: seen 159100 examples at 63.1 eps, loss = 3.738\n",
      "    [batch 1598]: seen 159800 examples at 63.2 eps, loss = 3.740\n",
      "    [batch 1605]: seen 160500 examples at 63.2 eps, loss = 3.740\n",
      "    [batch 1612]: seen 161200 examples at 63.2 eps, loss = 3.740\n",
      "    [batch 1619]: seen 161900 examples at 63.2 eps, loss = 3.738\n",
      "    [batch 1626]: seen 162600 examples at 63.3 eps, loss = 3.735\n",
      "    [batch 1633]: seen 163300 examples at 63.3 eps, loss = 3.734\n",
      "    [batch 1640]: seen 164000 examples at 63.3 eps, loss = 3.732\n",
      "    [batch 1647]: seen 164700 examples at 63.3 eps, loss = 3.728\n",
      "    [batch 1654]: seen 165400 examples at 63.4 eps, loss = 3.724\n",
      "    [batch 1661]: seen 166100 examples at 63.4 eps, loss = 3.728\n",
      "    [batch 1668]: seen 166800 examples at 63.4 eps, loss = 3.728\n",
      "    [batch 1675]: seen 167500 examples at 63.4 eps, loss = 3.730\n",
      "    [batch 1682]: seen 168200 examples at 63.5 eps, loss = 3.726\n",
      "    [batch 1687]: seen 168700 examples at 63.4 eps, loss = 3.723\n",
      "    [batch 1694]: seen 169400 examples at 63.4 eps, loss = 3.727\n",
      "    [batch 1701]: seen 170100 examples at 63.4 eps, loss = 3.724\n",
      "    [batch 1708]: seen 170800 examples at 63.5 eps, loss = 3.727\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-17085\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-17085\n",
      "    [batch 1714]: seen 171400 examples at 63.4 eps, loss = 3.729\n",
      "    [batch 1721]: seen 172100 examples at 63.5 eps, loss = 3.735\n",
      "    [batch 1728]: seen 172800 examples at 63.5 eps, loss = 3.734\n",
      "    [batch 1735]: seen 173500 examples at 63.5 eps, loss = 3.729\n",
      "    [batch 1742]: seen 174200 examples at 63.5 eps, loss = 3.729\n",
      "    [batch 1749]: seen 174900 examples at 63.6 eps, loss = 3.732\n",
      "    [batch 1756]: seen 175600 examples at 63.6 eps, loss = 3.735\n",
      "    [batch 1763]: seen 176300 examples at 63.6 eps, loss = 3.736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1770]: seen 177000 examples at 63.6 eps, loss = 3.739\n",
      "    [batch 1777]: seen 177700 examples at 63.6 eps, loss = 3.738\n",
      "    [batch 1784]: seen 178400 examples at 63.7 eps, loss = 3.738\n",
      "    [batch 1791]: seen 179100 examples at 63.7 eps, loss = 3.737\n",
      "    [batch 1798]: seen 179800 examples at 63.7 eps, loss = 3.737\n",
      "    [batch 1805]: seen 180500 examples at 63.7 eps, loss = 3.737\n",
      "    [batch 1812]: seen 181200 examples at 63.7 eps, loss = 3.735\n",
      "    [batch 1819]: seen 181900 examples at 63.8 eps, loss = 3.735\n",
      "    [batch 1826]: seen 182600 examples at 63.8 eps, loss = 3.731\n",
      "    [batch 1833]: seen 183300 examples at 63.8 eps, loss = 3.735\n",
      "    [batch 1840]: seen 184000 examples at 63.8 eps, loss = 3.734\n",
      "    [batch 1847]: seen 184700 examples at 63.8 eps, loss = 3.728\n",
      "    [batch 1853]: seen 185300 examples at 63.8 eps, loss = 3.722\n",
      "    [batch 1858]: seen 185800 examples at 63.7 eps, loss = 3.720\n",
      "    [batch 1861]: seen 186100 examples at 63.5 eps, loss = 3.719\n",
      "    [batch 1864]: seen 186400 examples at 63.4 eps, loss = 3.717\n",
      "    [batch 1871]: seen 187100 examples at 63.4 eps, loss = 3.725\n",
      "    [END] Training complete: Total examples : 187100; Total time: 0:49:11\n",
      "[EPOCH 11] Complete. Avg Loss: 3.7247077185751425; Best Loss: 3.7169681679715083\n",
      "[EPOCH 12] Starting training..\n",
      "    [batch 7]: seen 700 examples at 69.4 eps, loss = 3.732\n",
      "    [batch 14]: seen 1400 examples at 69.4 eps, loss = 3.731\n",
      "    [batch 21]: seen 2100 examples at 69.4 eps, loss = 3.729\n",
      "    [batch 28]: seen 2800 examples at 69.4 eps, loss = 3.726\n",
      "    [batch 35]: seen 3500 examples at 69.5 eps, loss = 3.727\n",
      "    [batch 42]: seen 4200 examples at 69.5 eps, loss = 3.723\n",
      "    [batch 49]: seen 4900 examples at 69.5 eps, loss = 3.721\n",
      "    [batch 56]: seen 5600 examples at 69.5 eps, loss = 3.722\n",
      "    [batch 63]: seen 6300 examples at 69.4 eps, loss = 3.729\n",
      "    [batch 70]: seen 7000 examples at 69.5 eps, loss = 3.728\n",
      "    [batch 77]: seen 7700 examples at 69.4 eps, loss = 3.725\n",
      "    [batch 84]: seen 8400 examples at 69.4 eps, loss = 3.722\n",
      "    [batch 91]: seen 9100 examples at 69.4 eps, loss = 3.722\n",
      "    [batch 98]: seen 9800 examples at 69.5 eps, loss = 3.721\n",
      "    [batch 105]: seen 10500 examples at 67.9 eps, loss = 3.716\n",
      "    [batch 108]: seen 10800 examples at 63.9 eps, loss = 3.713\n",
      "    [batch 113]: seen 11300 examples at 61.5 eps, loss = 3.709\n",
      "    [batch 116]: seen 11600 examples at 59.6 eps, loss = 3.708\n",
      "    [batch 122]: seen 12200 examples at 59.0 eps, loss = 3.706\n",
      "    [batch 127]: seen 12700 examples at 58.5 eps, loss = 3.706\n",
      "    [batch 133]: seen 13300 examples at 58.0 eps, loss = 3.704\n",
      "    [batch 136]: seen 13600 examples at 56.6 eps, loss = 3.701\n",
      "    [batch 141]: seen 14100 examples at 56.2 eps, loss = 3.703\n",
      "    [batch 146]: seen 14600 examples at 55.8 eps, loss = 3.701\n",
      "    [batch 150]: seen 15000 examples at 54.8 eps, loss = 3.698\n",
      "    [batch 153]: seen 15300 examples at 53.7 eps, loss = 3.697\n",
      "    [batch 158]: seen 15800 examples at 53.5 eps, loss = 3.696\n",
      "    [batch 163]: seen 16300 examples at 52.6 eps, loss = 3.693\n",
      "    [batch 168]: seen 16800 examples at 52.4 eps, loss = 3.693\n",
      "    [batch 175]: seen 17500 examples at 52.9 eps, loss = 3.693\n",
      "    [batch 182]: seen 18200 examples at 53.4 eps, loss = 3.691\n",
      "    [batch 186]: seen 18600 examples at 52.7 eps, loss = 3.689\n",
      "    [batch 193]: seen 19300 examples at 53.1 eps, loss = 3.692\n",
      "    [batch 200]: seen 20000 examples at 53.6 eps, loss = 3.694\n",
      "    [batch 207]: seen 20700 examples at 54.0 eps, loss = 3.704\n",
      "    [batch 214]: seen 21400 examples at 54.4 eps, loss = 3.706\n",
      "    [batch 221]: seen 22100 examples at 54.8 eps, loss = 3.705\n",
      "    [batch 228]: seen 22800 examples at 55.1 eps, loss = 3.700\n",
      "    [batch 235]: seen 23500 examples at 55.4 eps, loss = 3.696\n",
      "    [batch 242]: seen 24200 examples at 55.8 eps, loss = 3.692\n",
      "    [batch 249]: seen 24900 examples at 56.1 eps, loss = 3.690\n",
      "    [batch 256]: seen 25600 examples at 56.4 eps, loss = 3.693\n",
      "    [batch 263]: seen 26300 examples at 56.7 eps, loss = 3.695\n",
      "    [batch 270]: seen 27000 examples at 56.9 eps, loss = 3.696\n",
      "    [batch 277]: seen 27700 examples at 57.2 eps, loss = 3.693\n",
      "    [batch 284]: seen 28400 examples at 57.5 eps, loss = 3.693\n",
      "    [batch 291]: seen 29100 examples at 57.7 eps, loss = 3.693\n",
      "    [batch 298]: seen 29800 examples at 57.9 eps, loss = 3.696\n",
      "    [batch 305]: seen 30500 examples at 58.1 eps, loss = 3.699\n",
      "    [batch 312]: seen 31200 examples at 58.4 eps, loss = 3.699\n",
      "    [batch 319]: seen 31900 examples at 58.6 eps, loss = 3.698\n",
      "    [batch 326]: seen 32600 examples at 58.8 eps, loss = 3.698\n",
      "    [batch 333]: seen 33300 examples at 59.0 eps, loss = 3.694\n",
      "    [batch 340]: seen 34000 examples at 59.1 eps, loss = 3.700\n",
      "    [batch 347]: seen 34700 examples at 59.3 eps, loss = 3.700\n",
      "    [batch 354]: seen 35400 examples at 59.5 eps, loss = 3.698\n",
      "    [batch 361]: seen 36100 examples at 59.7 eps, loss = 3.696\n",
      "    [batch 368]: seen 36800 examples at 59.8 eps, loss = 3.699\n",
      "    [batch 375]: seen 37500 examples at 60.0 eps, loss = 3.697\n",
      "    [batch 382]: seen 38200 examples at 60.1 eps, loss = 3.698\n",
      "    [batch 389]: seen 38900 examples at 60.3 eps, loss = 3.697\n",
      "    [batch 396]: seen 39600 examples at 60.4 eps, loss = 3.697\n",
      "    [batch 403]: seen 40300 examples at 60.6 eps, loss = 3.695\n",
      "    [batch 410]: seen 41000 examples at 60.7 eps, loss = 3.698\n",
      "    [batch 417]: seen 41700 examples at 60.8 eps, loss = 3.691\n",
      "    [batch 424]: seen 42400 examples at 60.9 eps, loss = 3.699\n",
      "    [batch 431]: seen 43100 examples at 61.1 eps, loss = 3.699\n",
      "    [batch 438]: seen 43800 examples at 61.2 eps, loss = 3.695\n",
      "    [batch 445]: seen 44500 examples at 61.3 eps, loss = 3.698\n",
      "    [batch 452]: seen 45200 examples at 61.4 eps, loss = 3.696\n",
      "    [batch 459]: seen 45900 examples at 61.5 eps, loss = 3.700\n",
      "    [batch 466]: seen 46600 examples at 61.6 eps, loss = 3.698\n",
      "    [batch 473]: seen 47300 examples at 61.7 eps, loss = 3.696\n",
      "    [batch 480]: seen 48000 examples at 61.8 eps, loss = 3.697\n",
      "    [batch 487]: seen 48700 examples at 61.9 eps, loss = 3.699\n",
      "    [batch 494]: seen 49400 examples at 62.0 eps, loss = 3.699\n",
      "    [batch 501]: seen 50100 examples at 62.1 eps, loss = 3.693\n",
      "    [batch 508]: seen 50800 examples at 62.2 eps, loss = 3.693\n",
      "    [batch 515]: seen 51500 examples at 62.3 eps, loss = 3.694\n",
      "    [batch 522]: seen 52200 examples at 62.4 eps, loss = 3.694\n",
      "    [batch 529]: seen 52900 examples at 62.5 eps, loss = 3.696\n",
      "    [batch 536]: seen 53600 examples at 62.5 eps, loss = 3.694\n",
      "    [batch 543]: seen 54300 examples at 62.6 eps, loss = 3.698\n",
      "    [batch 550]: seen 55000 examples at 62.7 eps, loss = 3.699\n",
      "    [batch 557]: seen 55700 examples at 62.8 eps, loss = 3.702\n",
      "    [batch 564]: seen 56400 examples at 62.9 eps, loss = 3.700\n",
      "    [batch 571]: seen 57100 examples at 62.9 eps, loss = 3.703\n",
      "    [batch 578]: seen 57800 examples at 63.0 eps, loss = 3.702\n",
      "    [batch 585]: seen 58500 examples at 63.1 eps, loss = 3.703\n",
      "    [batch 592]: seen 59200 examples at 63.1 eps, loss = 3.708\n",
      "    [batch 599]: seen 59900 examples at 63.2 eps, loss = 3.706\n",
      "    [batch 606]: seen 60600 examples at 63.3 eps, loss = 3.706\n",
      "    [batch 613]: seen 61300 examples at 63.3 eps, loss = 3.712\n",
      "    [batch 620]: seen 62000 examples at 63.4 eps, loss = 3.717\n",
      "    [batch 627]: seen 62700 examples at 63.5 eps, loss = 3.711\n",
      "    [batch 634]: seen 63400 examples at 63.5 eps, loss = 3.711\n",
      "    [batch 641]: seen 64100 examples at 63.6 eps, loss = 3.709\n",
      "    [batch 648]: seen 64800 examples at 63.6 eps, loss = 3.710\n",
      "    [batch 655]: seen 65500 examples at 63.7 eps, loss = 3.709\n",
      "    [batch 662]: seen 66200 examples at 63.8 eps, loss = 3.707\n",
      "    [batch 669]: seen 66900 examples at 63.8 eps, loss = 3.704\n",
      "    [batch 676]: seen 67600 examples at 63.9 eps, loss = 3.704\n",
      "    [batch 683]: seen 68300 examples at 63.9 eps, loss = 3.701\n",
      "    [batch 690]: seen 69000 examples at 64.0 eps, loss = 3.703\n",
      "    [batch 697]: seen 69700 examples at 64.0 eps, loss = 3.702\n",
      "    [batch 704]: seen 70400 examples at 64.1 eps, loss = 3.700\n",
      "    [batch 711]: seen 71100 examples at 64.1 eps, loss = 3.697\n",
      "    [batch 718]: seen 71800 examples at 64.2 eps, loss = 3.694\n",
      "    [batch 725]: seen 72500 examples at 64.2 eps, loss = 3.692\n",
      "    [batch 732]: seen 73200 examples at 64.3 eps, loss = 3.694\n",
      "    [batch 739]: seen 73900 examples at 64.3 eps, loss = 3.694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 746]: seen 74600 examples at 64.3 eps, loss = 3.699\n",
      "    [batch 753]: seen 75300 examples at 64.4 eps, loss = 3.696\n",
      "    [batch 760]: seen 76000 examples at 64.4 eps, loss = 3.697\n",
      "    [batch 767]: seen 76700 examples at 64.5 eps, loss = 3.694\n",
      "    [batch 774]: seen 77400 examples at 64.5 eps, loss = 3.693\n",
      "    [batch 779]: seen 77900 examples at 64.4 eps, loss = 3.689\n",
      "    [batch 786]: seen 78600 examples at 64.4 eps, loss = 3.691\n",
      "    [batch 793]: seen 79300 examples at 64.5 eps, loss = 3.692\n",
      "    [batch 800]: seen 80000 examples at 64.5 eps, loss = 3.691\n",
      "    [batch 804]: seen 80400 examples at 64.2 eps, loss = 3.686\n",
      "    [batch 807]: seen 80700 examples at 63.7 eps, loss = 3.683\n",
      "    [batch 810]: seen 81000 examples at 63.4 eps, loss = 3.680\n",
      "    [batch 813]: seen 81300 examples at 63.1 eps, loss = 3.680\n",
      "    [batch 818]: seen 81800 examples at 62.9 eps, loss = 3.682\n",
      "    [batch 825]: seen 82500 examples at 63.0 eps, loss = 3.688\n",
      "    [batch 832]: seen 83200 examples at 63.0 eps, loss = 3.691\n",
      "    [batch 839]: seen 83900 examples at 63.1 eps, loss = 3.690\n",
      "    [batch 846]: seen 84600 examples at 63.1 eps, loss = 3.683\n",
      "    [batch 853]: seen 85300 examples at 63.2 eps, loss = 3.686\n",
      "    [batch 860]: seen 86000 examples at 63.2 eps, loss = 3.692\n",
      "    [batch 867]: seen 86700 examples at 63.3 eps, loss = 3.690\n",
      "    [batch 874]: seen 87400 examples at 63.3 eps, loss = 3.694\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-573ba27550bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurr_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-17e3ed376236>\u001b[0m in \u001b[0;36mtrain_continue\u001b[0;34m(hps, epochs, train_step, curr_best, best_loss, avg_loss, restore, epoch_start)\u001b[0m\n\u001b[1;32m     36\u001b[0m                                                             \u001b[0mhps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                                             \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                                                             avg_loss)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcurr_best\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266/final_0/W266_Final/model_4/training_util.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(lm, session, batches, summary_writer, train_dir, train_step, saver, hps, best_loss, avg_loss)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunTrainStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266/final_0/W266_Final/model_4/training_util.py\u001b[0m in \u001b[0;36mrunTrainStep\u001b[0;34m(lm, session, batch)\u001b[0m\n\u001b[1;32m     87\u001b[0m     }\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_loss = 3.7680904\n",
    "best_loss = 3.7568204\n",
    "curr_best = best_loss\n",
    "train_step = 15398\n",
    "epochs = 2\n",
    "restore = True\n",
    "epoch_start = 10\n",
    "\n",
    "train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train session 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 252\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 110\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 2 1/2 76\n",
      "\n",
      "\n",
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: 16:03\n",
      "INFO:tensorflow:Fetching data..\n",
      "INFO:tensorflow:Creating batches..\n",
      "INFO:tensorflow:[TOTAL Batches]  : 1872\n",
      "INFO:tensorflow:[TOTAL Examples] : 187193\n",
      "INFO:tensorflow:Creating batches..COMPLETE\n",
      "INFO:tensorflow:Building core graph...\n",
      "INFO:tensorflow:Adding attention_decoder timestep 0 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 1 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 2 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 3 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 4 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 5 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 6 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 7 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 8 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 9 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 10 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 11 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 12 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 13 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 14 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 15 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 16 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 17 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 18 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 19 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 20 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 21 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 22 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 23 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 24 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 25 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 26 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 27 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 28 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 29 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 30 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 31 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 32 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 33 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 34 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 35 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 36 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 37 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 38 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 39 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 40 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 41 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 42 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 43 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 44 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 45 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 46 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 47 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 48 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 49 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 50 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 51 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 52 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 53 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 54 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 55 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 56 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 57 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 58 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 59 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 60 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 61 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 62 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 63 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 64 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 65 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 66 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 67 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 68 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 69 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 70 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 71 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 72 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 73 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 74 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 75 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 76 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 77 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 78 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 79 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 80 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 81 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 82 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 83 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 84 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 85 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 86 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 87 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 88 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 89 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 90 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 91 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 92 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 93 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 94 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 95 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 96 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 97 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 98 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 99 of 100\n",
      "INFO:tensorflow:Building projection graph...\n",
      "INFO:tensorflow:Building projection graph...COMPLETE\n",
      "INFO:tensorflow:Building Loss graph...\n",
      "INFO:tensorflow:Building Loss graph...COMPLETE\n",
      "INFO:tensorflow:Building core graph...COMPLETE\n",
      "INFO:tensorflow:Building train graph...\n",
      "INFO:tensorflow:Building train graph...COMPLETE\n",
      "INFO:tensorflow:Building summary graph...\n",
      "INFO:tensorflow:Building summary graph...COMPLETE\n",
      "WARNING:tensorflow:From <ipython-input-5-17e3ed376236>:18: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-18059\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-18059\n",
      "[EPOCH 12] Starting training..\n",
      "    [batch 1]: seen 100 examples at 3.0 eps, loss = 3.692\n",
      "    [batch 6]: seen 600 examples at 13.6 eps, loss = 3.692\n",
      "    [batch 9]: seen 900 examples at 15.2 eps, loss = 3.690\n",
      "    [batch 12]: seen 1200 examples at 17.0 eps, loss = 3.690\n",
      "    [batch 17]: seen 1700 examples at 21.0 eps, loss = 3.689\n",
      "    [batch 22]: seen 2200 examples at 23.9 eps, loss = 3.688\n",
      "    [batch 29]: seen 2900 examples at 28.4 eps, loss = 3.688\n",
      "    [batch 34]: seen 3400 examples at 29.3 eps, loss = 3.685\n",
      "    [batch 37]: seen 3700 examples at 29.0 eps, loss = 3.681\n",
      "    [batch 40]: seen 4000 examples at 28.0 eps, loss = 3.677\n",
      "    [batch 43]: seen 4300 examples at 27.9 eps, loss = 3.676\n",
      "    [batch 46]: seen 4600 examples at 27.2 eps, loss = 3.673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 49]: seen 4900 examples at 27.2 eps, loss = 3.671\n",
      "    [batch 56]: seen 5600 examples at 29.4 eps, loss = 3.676\n",
      "    [batch 63]: seen 6300 examples at 31.4 eps, loss = 3.676\n",
      "    [batch 70]: seen 7000 examples at 33.2 eps, loss = 3.676\n",
      "    [batch 77]: seen 7700 examples at 34.9 eps, loss = 3.673\n",
      "    [batch 82]: seen 8200 examples at 35.4 eps, loss = 3.670\n",
      "    [batch 85]: seen 8500 examples at 35.0 eps, loss = 3.669\n",
      "    [batch 88]: seen 8800 examples at 34.2 eps, loss = 3.666\n",
      "    [batch 95]: seen 9500 examples at 35.5 eps, loss = 3.671\n",
      "    [batch 102]: seen 10200 examples at 36.7 eps, loss = 3.676\n",
      "    [batch 109]: seen 10900 examples at 37.9 eps, loss = 3.675\n",
      "    [batch 116]: seen 11600 examples at 38.9 eps, loss = 3.674\n",
      "    [batch 123]: seen 12300 examples at 39.9 eps, loss = 3.678\n",
      "    [batch 130]: seen 13000 examples at 40.8 eps, loss = 3.681\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-18147\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-18147\n",
      "    [batch 136]: seen 13600 examples at 41.4 eps, loss = 3.679\n",
      "    [batch 143]: seen 14300 examples at 42.2 eps, loss = 3.681\n",
      "    [batch 150]: seen 15000 examples at 43.0 eps, loss = 3.680\n",
      "    [batch 157]: seen 15700 examples at 43.7 eps, loss = 3.683\n",
      "    [batch 164]: seen 16400 examples at 44.4 eps, loss = 3.683\n",
      "    [batch 171]: seen 17100 examples at 45.1 eps, loss = 3.684\n",
      "    [batch 178]: seen 17800 examples at 45.7 eps, loss = 3.684\n",
      "    [batch 185]: seen 18500 examples at 46.3 eps, loss = 3.686\n",
      "    [batch 192]: seen 19200 examples at 46.9 eps, loss = 3.686\n",
      "    [batch 199]: seen 19900 examples at 47.4 eps, loss = 3.685\n",
      "    [batch 206]: seen 20600 examples at 47.9 eps, loss = 3.684\n",
      "    [batch 213]: seen 21300 examples at 48.4 eps, loss = 3.683\n",
      "    [batch 220]: seen 22000 examples at 48.9 eps, loss = 3.681\n",
      "    [batch 227]: seen 22700 examples at 49.3 eps, loss = 3.681\n",
      "    [batch 234]: seen 23400 examples at 49.8 eps, loss = 3.679\n",
      "    [batch 241]: seen 24100 examples at 50.2 eps, loss = 3.675\n",
      "    [batch 248]: seen 24800 examples at 50.6 eps, loss = 3.674\n",
      "    [batch 255]: seen 25500 examples at 50.9 eps, loss = 3.675\n",
      "    [batch 262]: seen 26200 examples at 51.3 eps, loss = 3.673\n",
      "    [batch 269]: seen 26900 examples at 51.6 eps, loss = 3.676\n",
      "    [batch 276]: seen 27600 examples at 52.0 eps, loss = 3.671\n",
      "    [batch 283]: seen 28300 examples at 52.3 eps, loss = 3.674\n",
      "    [batch 290]: seen 29000 examples at 52.6 eps, loss = 3.676\n",
      "    [batch 297]: seen 29700 examples at 52.9 eps, loss = 3.674\n",
      "    [batch 304]: seen 30400 examples at 53.2 eps, loss = 3.673\n",
      "    [batch 311]: seen 31100 examples at 53.5 eps, loss = 3.675\n",
      "    [batch 318]: seen 31800 examples at 53.7 eps, loss = 3.678\n",
      "    [batch 325]: seen 32500 examples at 54.0 eps, loss = 3.676\n",
      "    [batch 332]: seen 33200 examples at 54.3 eps, loss = 3.674\n",
      "    [batch 339]: seen 33900 examples at 54.5 eps, loss = 3.680\n",
      "    [batch 346]: seen 34600 examples at 54.7 eps, loss = 3.679\n",
      "    [batch 353]: seen 35300 examples at 55.0 eps, loss = 3.682\n",
      "    [batch 360]: seen 36000 examples at 55.2 eps, loss = 3.681\n",
      "    [batch 367]: seen 36700 examples at 55.4 eps, loss = 3.681\n",
      "    [batch 374]: seen 37400 examples at 55.6 eps, loss = 3.686\n",
      "    [batch 381]: seen 38100 examples at 55.8 eps, loss = 3.686\n",
      "    [batch 388]: seen 38800 examples at 56.0 eps, loss = 3.683\n",
      "    [batch 395]: seen 39500 examples at 56.2 eps, loss = 3.681\n",
      "    [batch 402]: seen 40200 examples at 56.4 eps, loss = 3.688\n",
      "    [batch 409]: seen 40900 examples at 56.6 eps, loss = 3.689\n",
      "    [batch 416]: seen 41600 examples at 56.7 eps, loss = 3.691\n",
      "    [batch 423]: seen 42300 examples at 56.9 eps, loss = 3.694\n",
      "    [batch 430]: seen 43000 examples at 57.1 eps, loss = 3.697\n",
      "    [batch 437]: seen 43700 examples at 57.2 eps, loss = 3.692\n",
      "    [batch 444]: seen 44400 examples at 57.4 eps, loss = 3.687\n",
      "    [batch 451]: seen 45100 examples at 57.5 eps, loss = 3.683\n",
      "    [batch 458]: seen 45800 examples at 57.7 eps, loss = 3.681\n",
      "    [batch 465]: seen 46500 examples at 57.8 eps, loss = 3.682\n",
      "    [batch 472]: seen 47200 examples at 58.0 eps, loss = 3.687\n",
      "    [batch 479]: seen 47900 examples at 58.1 eps, loss = 3.685\n",
      "    [batch 486]: seen 48600 examples at 58.3 eps, loss = 3.679\n",
      "    [batch 493]: seen 49300 examples at 58.4 eps, loss = 3.681\n",
      "    [batch 500]: seen 50000 examples at 58.5 eps, loss = 3.683\n",
      "    [batch 507]: seen 50700 examples at 58.6 eps, loss = 3.684\n",
      "    [batch 514]: seen 51400 examples at 58.8 eps, loss = 3.680\n",
      "    [batch 521]: seen 52100 examples at 58.9 eps, loss = 3.684\n",
      "    [batch 528]: seen 52800 examples at 59.0 eps, loss = 3.686\n",
      "    [batch 535]: seen 53500 examples at 59.1 eps, loss = 3.683\n",
      "    [batch 542]: seen 54200 examples at 59.2 eps, loss = 3.680\n",
      "    [batch 549]: seen 54900 examples at 59.3 eps, loss = 3.678\n",
      "    [batch 556]: seen 55600 examples at 59.4 eps, loss = 3.675\n",
      "    [batch 563]: seen 56300 examples at 59.6 eps, loss = 3.675\n",
      "    [batch 570]: seen 57000 examples at 59.7 eps, loss = 3.675\n",
      "    [batch 577]: seen 57700 examples at 59.8 eps, loss = 3.670\n",
      "    [batch 584]: seen 58400 examples at 59.9 eps, loss = 3.668\n",
      "    [batch 591]: seen 59100 examples at 60.0 eps, loss = 3.671\n",
      "    [batch 598]: seen 59800 examples at 60.0 eps, loss = 3.670\n",
      "    [batch 605]: seen 60500 examples at 60.1 eps, loss = 3.671\n",
      "    [batch 612]: seen 61200 examples at 60.2 eps, loss = 3.671\n",
      "    [batch 619]: seen 61900 examples at 60.3 eps, loss = 3.674\n",
      "    [batch 626]: seen 62600 examples at 60.4 eps, loss = 3.678\n",
      "    [batch 633]: seen 63300 examples at 60.5 eps, loss = 3.681\n",
      "    [batch 640]: seen 64000 examples at 60.6 eps, loss = 3.678\n",
      "    [batch 647]: seen 64700 examples at 60.7 eps, loss = 3.681\n",
      "    [batch 654]: seen 65400 examples at 60.7 eps, loss = 3.683\n",
      "    [batch 661]: seen 66100 examples at 60.8 eps, loss = 3.686\n",
      "    [batch 668]: seen 66800 examples at 60.9 eps, loss = 3.682\n",
      "    [batch 675]: seen 67500 examples at 61.0 eps, loss = 3.680\n",
      "    [batch 682]: seen 68200 examples at 61.1 eps, loss = 3.676\n",
      "    [batch 689]: seen 68900 examples at 61.1 eps, loss = 3.675\n",
      "    [batch 696]: seen 69600 examples at 61.2 eps, loss = 3.673\n",
      "    [batch 703]: seen 70300 examples at 61.3 eps, loss = 3.674\n",
      "    [batch 710]: seen 71000 examples at 61.3 eps, loss = 3.676\n",
      "    [batch 717]: seen 71700 examples at 61.4 eps, loss = 3.672\n",
      "    [batch 724]: seen 72400 examples at 61.5 eps, loss = 3.670\n",
      "    [batch 731]: seen 73100 examples at 61.5 eps, loss = 3.673\n",
      "    [batch 738]: seen 73800 examples at 61.6 eps, loss = 3.673\n",
      "    [batch 745]: seen 74500 examples at 61.7 eps, loss = 3.670\n",
      "    [batch 752]: seen 75200 examples at 61.7 eps, loss = 3.668\n",
      "    [batch 757]: seen 75700 examples at 61.6 eps, loss = 3.667\n",
      "    [batch 764]: seen 76400 examples at 61.7 eps, loss = 3.669\n",
      "    [batch 771]: seen 77100 examples at 61.7 eps, loss = 3.672\n",
      "    [batch 778]: seen 77800 examples at 61.8 eps, loss = 3.672\n",
      "    [batch 785]: seen 78500 examples at 61.9 eps, loss = 3.669\n",
      "    [batch 792]: seen 79200 examples at 61.9 eps, loss = 3.669\n",
      "    [batch 799]: seen 79900 examples at 62.0 eps, loss = 3.679\n",
      "    [batch 806]: seen 80600 examples at 62.0 eps, loss = 3.681\n",
      "    [batch 813]: seen 81300 examples at 62.1 eps, loss = 3.683\n",
      "    [batch 820]: seen 82000 examples at 62.1 eps, loss = 3.678\n",
      "    [batch 827]: seen 82700 examples at 62.2 eps, loss = 3.675\n",
      "    [batch 834]: seen 83400 examples at 62.3 eps, loss = 3.675\n",
      "    [batch 841]: seen 84100 examples at 62.3 eps, loss = 3.672\n",
      "    [batch 848]: seen 84800 examples at 62.4 eps, loss = 3.666\n",
      "    [batch 855]: seen 85500 examples at 62.4 eps, loss = 3.668\n",
      "    [batch 862]: seen 86200 examples at 62.5 eps, loss = 3.667\n",
      "    [batch 865]: seen 86500 examples at 62.2 eps, loss = 3.664\n",
      "    [batch 869]: seen 86900 examples at 61.9 eps, loss = 3.663\n",
      "    [batch 876]: seen 87600 examples at 61.9 eps, loss = 3.663\n",
      "    [batch 883]: seen 88300 examples at 62.0 eps, loss = 3.668\n",
      "    [batch 890]: seen 89000 examples at 62.1 eps, loss = 3.668\n",
      "    [batch 897]: seen 89700 examples at 62.1 eps, loss = 3.664\n",
      "    [batch 904]: seen 90400 examples at 62.2 eps, loss = 3.665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 911]: seen 91100 examples at 62.2 eps, loss = 3.668\n",
      "    [batch 918]: seen 91800 examples at 62.3 eps, loss = 3.666\n",
      "    [batch 925]: seen 92500 examples at 62.3 eps, loss = 3.666\n",
      "    [batch 932]: seen 93200 examples at 62.3 eps, loss = 3.668\n",
      "    [batch 939]: seen 93900 examples at 62.4 eps, loss = 3.672\n",
      "    [batch 946]: seen 94600 examples at 62.4 eps, loss = 3.674\n",
      "    [batch 953]: seen 95300 examples at 62.5 eps, loss = 3.674\n",
      "    [batch 960]: seen 96000 examples at 62.5 eps, loss = 3.676\n",
      "    [batch 967]: seen 96700 examples at 62.6 eps, loss = 3.674\n",
      "    [batch 974]: seen 97400 examples at 62.6 eps, loss = 3.676\n",
      "    [batch 981]: seen 98100 examples at 62.7 eps, loss = 3.675\n",
      "    [batch 988]: seen 98800 examples at 62.7 eps, loss = 3.677\n",
      "    [batch 995]: seen 99500 examples at 62.8 eps, loss = 3.679\n",
      "    [batch 1002]: seen 100200 examples at 62.8 eps, loss = 3.679\n",
      "    [batch 1009]: seen 100900 examples at 62.8 eps, loss = 3.676\n",
      "    [batch 1016]: seen 101600 examples at 62.9 eps, loss = 3.672\n",
      "    [batch 1023]: seen 102300 examples at 62.9 eps, loss = 3.677\n",
      "    [batch 1030]: seen 103000 examples at 63.0 eps, loss = 3.676\n",
      "    [batch 1037]: seen 103700 examples at 63.0 eps, loss = 3.679\n",
      "    [batch 1044]: seen 104400 examples at 63.0 eps, loss = 3.677\n",
      "    [batch 1051]: seen 105100 examples at 63.1 eps, loss = 3.676\n",
      "    [batch 1058]: seen 105800 examples at 63.1 eps, loss = 3.675\n",
      "    [batch 1065]: seen 106500 examples at 63.1 eps, loss = 3.679\n",
      "    [batch 1072]: seen 107200 examples at 63.2 eps, loss = 3.675\n",
      "    [batch 1079]: seen 107900 examples at 63.2 eps, loss = 3.673\n",
      "    [batch 1086]: seen 108600 examples at 63.3 eps, loss = 3.676\n",
      "    [batch 1093]: seen 109300 examples at 63.3 eps, loss = 3.678\n",
      "    [batch 1100]: seen 110000 examples at 63.3 eps, loss = 3.671\n",
      "    [batch 1107]: seen 110700 examples at 63.4 eps, loss = 3.671\n",
      "    [batch 1114]: seen 111400 examples at 63.4 eps, loss = 3.674\n",
      "    [batch 1121]: seen 112100 examples at 63.4 eps, loss = 3.671\n",
      "    [batch 1128]: seen 112800 examples at 63.5 eps, loss = 3.673\n",
      "    [batch 1135]: seen 113500 examples at 63.5 eps, loss = 3.677\n",
      "    [batch 1142]: seen 114200 examples at 63.5 eps, loss = 3.676\n",
      "    [batch 1149]: seen 114900 examples at 63.6 eps, loss = 3.677\n",
      "    [batch 1156]: seen 115600 examples at 63.6 eps, loss = 3.677\n",
      "    [batch 1163]: seen 116300 examples at 63.6 eps, loss = 3.674\n",
      "    [batch 1170]: seen 117000 examples at 63.7 eps, loss = 3.676\n",
      "    [batch 1177]: seen 117700 examples at 63.7 eps, loss = 3.679\n",
      "    [batch 1184]: seen 118400 examples at 63.7 eps, loss = 3.684\n",
      "    [batch 1191]: seen 119100 examples at 63.8 eps, loss = 3.682\n",
      "    [batch 1198]: seen 119800 examples at 63.8 eps, loss = 3.683\n",
      "    [batch 1205]: seen 120500 examples at 63.8 eps, loss = 3.685\n",
      "    [batch 1212]: seen 121200 examples at 63.9 eps, loss = 3.689\n",
      "    [batch 1219]: seen 121900 examples at 63.9 eps, loss = 3.689\n",
      "    [batch 1226]: seen 122600 examples at 63.9 eps, loss = 3.689\n",
      "    [batch 1233]: seen 123300 examples at 63.9 eps, loss = 3.689\n",
      "    [batch 1240]: seen 124000 examples at 64.0 eps, loss = 3.693\n",
      "    [batch 1247]: seen 124700 examples at 64.0 eps, loss = 3.696\n",
      "    [batch 1254]: seen 125400 examples at 64.0 eps, loss = 3.699\n",
      "    [batch 1261]: seen 126100 examples at 64.1 eps, loss = 3.695\n",
      "    [batch 1268]: seen 126800 examples at 64.1 eps, loss = 3.698\n",
      "    [batch 1275]: seen 127500 examples at 64.1 eps, loss = 3.694\n",
      "    [batch 1282]: seen 128200 examples at 64.1 eps, loss = 3.693\n",
      "    [batch 1289]: seen 128900 examples at 64.2 eps, loss = 3.691\n",
      "    [batch 1296]: seen 129600 examples at 64.2 eps, loss = 3.688\n",
      "    [batch 1303]: seen 130300 examples at 64.2 eps, loss = 3.688\n",
      "    [batch 1310]: seen 131000 examples at 64.2 eps, loss = 3.691\n",
      "    [batch 1317]: seen 131700 examples at 64.3 eps, loss = 3.687\n",
      "    [batch 1324]: seen 132400 examples at 64.3 eps, loss = 3.687\n",
      "    [batch 1331]: seen 133100 examples at 64.3 eps, loss = 3.687\n",
      "    [batch 1338]: seen 133800 examples at 64.3 eps, loss = 3.688\n",
      "    [batch 1345]: seen 134500 examples at 64.4 eps, loss = 3.687\n",
      "    [batch 1352]: seen 135200 examples at 64.4 eps, loss = 3.684\n",
      "    [batch 1359]: seen 135900 examples at 64.4 eps, loss = 3.681\n",
      "    [batch 1366]: seen 136600 examples at 64.4 eps, loss = 3.684\n",
      "    [batch 1373]: seen 137300 examples at 64.5 eps, loss = 3.680\n",
      "    [batch 1380]: seen 138000 examples at 64.5 eps, loss = 3.680\n",
      "    [batch 1387]: seen 138700 examples at 64.5 eps, loss = 3.682\n",
      "    [batch 1394]: seen 139400 examples at 64.5 eps, loss = 3.681\n",
      "    [batch 1401]: seen 140100 examples at 64.5 eps, loss = 3.679\n",
      "    [batch 1408]: seen 140800 examples at 64.6 eps, loss = 3.675\n",
      "    [batch 1415]: seen 141500 examples at 64.6 eps, loss = 3.677\n",
      "    [batch 1422]: seen 142200 examples at 64.6 eps, loss = 3.673\n",
      "    [batch 1429]: seen 142900 examples at 64.6 eps, loss = 3.673\n",
      "    [batch 1436]: seen 143600 examples at 64.7 eps, loss = 3.676\n",
      "    [batch 1443]: seen 144300 examples at 64.7 eps, loss = 3.675\n",
      "    [batch 1450]: seen 145000 examples at 64.7 eps, loss = 3.673\n",
      "    [batch 1457]: seen 145700 examples at 64.7 eps, loss = 3.669\n",
      "    [batch 1464]: seen 146400 examples at 64.7 eps, loss = 3.670\n",
      "    [batch 1470]: seen 147000 examples at 64.7 eps, loss = 3.663\n",
      "    [batch 1475]: seen 147500 examples at 64.5 eps, loss = 3.660\n",
      "    [batch 1480]: seen 148000 examples at 64.4 eps, loss = 3.665\n",
      "    [batch 1487]: seen 148700 examples at 64.4 eps, loss = 3.665\n",
      "    [batch 1494]: seen 149400 examples at 64.5 eps, loss = 3.661\n",
      "    [batch 1501]: seen 150100 examples at 64.5 eps, loss = 3.662\n",
      "    [batch 1508]: seen 150800 examples at 64.5 eps, loss = 3.663\n",
      "    [batch 1514]: seen 151400 examples at 64.4 eps, loss = 3.659\n",
      "    [batch 1518]: seen 151800 examples at 64.3 eps, loss = 3.658\n",
      "    [batch 1523]: seen 152300 examples at 64.2 eps, loss = 3.657\n",
      "    [batch 1526]: seen 152600 examples at 63.9 eps, loss = 3.651\n",
      "    [batch 1533]: seen 153300 examples at 64.0 eps, loss = 3.654\n",
      "    [batch 1538]: seen 153800 examples at 63.8 eps, loss = 3.651\n",
      "    [batch 1541]: seen 154100 examples at 63.5 eps, loss = 3.649\n",
      "    [batch 1548]: seen 154800 examples at 63.5 eps, loss = 3.650\n",
      "    [batch 1553]: seen 155300 examples at 63.5 eps, loss = 3.649\n",
      "    [batch 1556]: seen 155600 examples at 63.3 eps, loss = 3.647\n",
      "    [batch 1561]: seen 156100 examples at 63.2 eps, loss = 3.651\n",
      "    [batch 1568]: seen 156800 examples at 63.3 eps, loss = 3.655\n",
      "    [batch 1575]: seen 157500 examples at 63.3 eps, loss = 3.658\n",
      "    [batch 1582]: seen 158200 examples at 63.3 eps, loss = 3.655\n",
      "    [batch 1589]: seen 158900 examples at 63.4 eps, loss = 3.657\n",
      "    [batch 1596]: seen 159600 examples at 63.4 eps, loss = 3.658\n",
      "    [batch 1603]: seen 160300 examples at 63.4 eps, loss = 3.658\n",
      "    [batch 1610]: seen 161000 examples at 63.4 eps, loss = 3.663\n",
      "    [batch 1617]: seen 161700 examples at 63.5 eps, loss = 3.658\n",
      "    [batch 1624]: seen 162400 examples at 63.5 eps, loss = 3.661\n",
      "    [batch 1631]: seen 163100 examples at 63.5 eps, loss = 3.656\n",
      "    [batch 1638]: seen 163800 examples at 63.5 eps, loss = 3.661\n",
      "    [batch 1645]: seen 164500 examples at 63.6 eps, loss = 3.659\n",
      "    [batch 1652]: seen 165200 examples at 63.6 eps, loss = 3.658\n",
      "    [batch 1659]: seen 165900 examples at 63.6 eps, loss = 3.652\n",
      "    [batch 1666]: seen 166600 examples at 63.6 eps, loss = 3.653\n",
      "    [batch 1673]: seen 167300 examples at 63.7 eps, loss = 3.651\n",
      "    [batch 1681]: seen 168100 examples at 63.7 eps, loss = 3.653\n",
      "    [batch 1688]: seen 168800 examples at 63.7 eps, loss = 3.652\n",
      "    [batch 1695]: seen 169500 examples at 63.7 eps, loss = 3.655\n",
      "    [batch 1702]: seen 170200 examples at 63.7 eps, loss = 3.655\n",
      "    [batch 1710]: seen 171000 examples at 63.8 eps, loss = 3.654\n",
      "    [batch 1717]: seen 171700 examples at 63.8 eps, loss = 3.654\n",
      "    [batch 1724]: seen 172400 examples at 63.8 eps, loss = 3.650\n",
      "    [batch 1731]: seen 173100 examples at 63.8 eps, loss = 3.654\n",
      "    [batch 1739]: seen 173900 examples at 63.9 eps, loss = 3.654\n",
      "    [batch 1746]: seen 174600 examples at 63.9 eps, loss = 3.656\n",
      "    [batch 1753]: seen 175300 examples at 63.9 eps, loss = 3.658\n",
      "    [batch 1760]: seen 176000 examples at 63.9 eps, loss = 3.656\n",
      "    [batch 1767]: seen 176700 examples at 64.0 eps, loss = 3.658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1774]: seen 177400 examples at 64.0 eps, loss = 3.656\n",
      "    [batch 1782]: seen 178200 examples at 64.0 eps, loss = 3.662\n",
      "    [batch 1789]: seen 178900 examples at 64.0 eps, loss = 3.669\n",
      "    [batch 1796]: seen 179600 examples at 64.0 eps, loss = 3.668\n",
      "    [batch 1803]: seen 180300 examples at 64.1 eps, loss = 3.667\n",
      "    [batch 1810]: seen 181000 examples at 64.1 eps, loss = 3.667\n",
      "    [batch 1817]: seen 181700 examples at 64.1 eps, loss = 3.664\n",
      "    [batch 1824]: seen 182400 examples at 64.1 eps, loss = 3.668\n",
      "    [batch 1831]: seen 183100 examples at 64.1 eps, loss = 3.663\n",
      "    [batch 1838]: seen 183800 examples at 64.2 eps, loss = 3.658\n",
      "    [batch 1845]: seen 184500 examples at 64.2 eps, loss = 3.657\n",
      "    [batch 1853]: seen 185300 examples at 64.2 eps, loss = 3.653\n",
      "    [batch 1860]: seen 186000 examples at 64.2 eps, loss = 3.652\n",
      "    [batch 1867]: seen 186700 examples at 64.2 eps, loss = 3.651\n",
      "    [END] Training complete: Total examples : 187100; Total time: 0:48:31\n",
      "[EPOCH 12] Complete. Avg Loss: 3.6477597145924343; Best Loss: 3.6469753903592252\n",
      "[EPOCH 13] Starting training..\n",
      "    [batch 7]: seen 700 examples at 69.4 eps, loss = 3.648\n",
      "    [batch 11]: seen 1100 examples at 48.9 eps, loss = 3.646\n",
      "    [batch 16]: seen 1600 examples at 48.5 eps, loss = 3.648\n",
      "    [batch 23]: seen 2300 examples at 53.5 eps, loss = 3.645\n",
      "    [batch 30]: seen 3000 examples at 56.6 eps, loss = 3.647\n",
      "    [batch 33]: seen 3300 examples at 51.6 eps, loss = 3.644\n",
      "    [batch 38]: seen 3800 examples at 51.0 eps, loss = 3.647\n",
      "    [batch 43]: seen 4300 examples at 50.6 eps, loss = 3.644\n",
      "    [batch 50]: seen 5000 examples at 52.7 eps, loss = 3.647\n",
      "    [batch 57]: seen 5700 examples at 54.3 eps, loss = 3.648\n",
      "    [batch 64]: seen 6400 examples at 55.6 eps, loss = 3.653\n",
      "    [batch 71]: seen 7100 examples at 56.8 eps, loss = 3.651\n",
      "    [batch 78]: seen 7800 examples at 57.8 eps, loss = 3.646\n",
      "    [batch 85]: seen 8500 examples at 58.6 eps, loss = 3.644\n",
      "    [batch 90]: seen 9000 examples at 57.9 eps, loss = 3.643\n",
      "    [batch 95]: seen 9500 examples at 56.1 eps, loss = 3.643\n",
      "    [batch 102]: seen 10200 examples at 56.9 eps, loss = 3.645\n",
      "    [batch 109]: seen 10900 examples at 57.6 eps, loss = 3.645\n",
      "    [batch 113]: seen 11300 examples at 56.0 eps, loss = 3.641\n",
      "    [batch 116]: seen 11600 examples at 54.4 eps, loss = 3.637\n",
      "    [batch 119]: seen 11900 examples at 53.0 eps, loss = 3.635\n",
      "    [batch 123]: seen 12300 examples at 52.0 eps, loss = 3.633\n",
      "    [batch 130]: seen 13000 examples at 52.0 eps, loss = 3.631\n",
      "    [batch 137]: seen 13700 examples at 52.7 eps, loss = 3.636\n",
      "    [batch 144]: seen 14400 examples at 53.3 eps, loss = 3.639\n",
      "    [batch 151]: seen 15100 examples at 53.9 eps, loss = 3.645\n",
      "    [batch 158]: seen 15800 examples at 54.4 eps, loss = 3.648\n",
      "    [batch 165]: seen 16500 examples at 55.0 eps, loss = 3.649\n",
      "    [batch 173]: seen 17300 examples at 55.5 eps, loss = 3.647\n",
      "    [batch 180]: seen 18000 examples at 56.0 eps, loss = 3.642\n",
      "    [batch 188]: seen 18800 examples at 56.4 eps, loss = 3.641\n",
      "    [batch 195]: seen 19500 examples at 56.8 eps, loss = 3.646\n",
      "    [batch 202]: seen 20200 examples at 57.2 eps, loss = 3.648\n",
      "    [batch 209]: seen 20900 examples at 57.6 eps, loss = 3.643\n",
      "    [batch 216]: seen 21600 examples at 57.9 eps, loss = 3.637\n",
      "    [batch 223]: seen 22300 examples at 58.2 eps, loss = 3.639\n",
      "    [batch 231]: seen 23100 examples at 58.5 eps, loss = 3.638\n",
      "    [batch 239]: seen 23900 examples at 58.9 eps, loss = 3.641\n",
      "    [batch 246]: seen 24600 examples at 59.1 eps, loss = 3.636\n",
      "    [batch 251]: seen 25100 examples at 58.9 eps, loss = 3.632\n",
      "    [batch 258]: seen 25800 examples at 59.1 eps, loss = 3.634\n",
      "    [batch 265]: seen 26500 examples at 59.3 eps, loss = 3.635\n",
      "    [batch 272]: seen 27200 examples at 59.6 eps, loss = 3.631\n",
      "    [batch 275]: seen 27500 examples at 58.8 eps, loss = 3.628\n",
      "    [batch 280]: seen 28000 examples at 58.6 eps, loss = 3.631\n",
      "    [batch 287]: seen 28700 examples at 58.8 eps, loss = 3.629\n",
      "    [batch 294]: seen 29400 examples at 59.0 eps, loss = 3.630\n",
      "    [batch 301]: seen 30100 examples at 59.2 eps, loss = 3.630\n",
      "    [batch 306]: seen 30600 examples at 59.0 eps, loss = 3.626\n",
      "    [batch 314]: seen 31400 examples at 59.2 eps, loss = 3.631\n",
      "    [batch 321]: seen 32100 examples at 59.4 eps, loss = 3.634\n",
      "    [batch 328]: seen 32800 examples at 59.6 eps, loss = 3.638\n",
      "    [batch 335]: seen 33500 examples at 59.8 eps, loss = 3.641\n",
      "    [batch 342]: seen 34200 examples at 60.0 eps, loss = 3.636\n",
      "    [batch 350]: seen 35000 examples at 60.2 eps, loss = 3.637\n",
      "    [batch 357]: seen 35700 examples at 60.3 eps, loss = 3.634\n",
      "    [batch 365]: seen 36500 examples at 60.5 eps, loss = 3.634\n",
      "    [batch 372]: seen 37200 examples at 60.7 eps, loss = 3.634\n",
      "    [batch 379]: seen 37900 examples at 60.8 eps, loss = 3.637\n",
      "    [batch 386]: seen 38600 examples at 61.0 eps, loss = 3.638\n",
      "    [batch 393]: seen 39300 examples at 61.1 eps, loss = 3.638\n",
      "    [batch 400]: seen 40000 examples at 61.2 eps, loss = 3.636\n",
      "    [batch 407]: seen 40700 examples at 61.4 eps, loss = 3.637\n",
      "    [batch 415]: seen 41500 examples at 61.5 eps, loss = 3.633\n",
      "    [batch 422]: seen 42200 examples at 61.6 eps, loss = 3.635\n",
      "    [batch 429]: seen 42900 examples at 61.8 eps, loss = 3.635\n",
      "    [batch 436]: seen 43600 examples at 61.9 eps, loss = 3.629\n",
      "    [batch 443]: seen 44300 examples at 62.0 eps, loss = 3.630\n",
      "    [batch 450]: seen 45000 examples at 61.8 eps, loss = 3.624\n",
      "    [batch 453]: seen 45300 examples at 61.3 eps, loss = 3.623\n",
      "    [batch 460]: seen 46000 examples at 61.4 eps, loss = 3.628\n",
      "    [batch 468]: seen 46800 examples at 61.6 eps, loss = 3.629\n",
      "    [batch 475]: seen 47500 examples at 61.7 eps, loss = 3.630\n",
      "    [batch 482]: seen 48200 examples at 61.8 eps, loss = 3.635\n",
      "    [batch 489]: seen 48900 examples at 61.9 eps, loss = 3.634\n",
      "    [batch 496]: seen 49600 examples at 62.0 eps, loss = 3.635\n",
      "    [batch 503]: seen 50300 examples at 62.1 eps, loss = 3.639\n",
      "    [batch 510]: seen 51000 examples at 62.2 eps, loss = 3.641\n",
      "    [batch 517]: seen 51700 examples at 62.3 eps, loss = 3.644\n",
      "    [batch 524]: seen 52400 examples at 62.3 eps, loss = 3.638\n",
      "    [batch 532]: seen 53200 examples at 62.5 eps, loss = 3.633\n",
      "    [batch 540]: seen 54000 examples at 62.6 eps, loss = 3.634\n",
      "    [batch 547]: seen 54700 examples at 62.6 eps, loss = 3.636\n",
      "    [batch 554]: seen 55400 examples at 62.7 eps, loss = 3.640\n",
      "    [batch 562]: seen 56200 examples at 62.8 eps, loss = 3.637\n",
      "    [batch 569]: seen 56900 examples at 62.9 eps, loss = 3.638\n",
      "    [batch 576]: seen 57600 examples at 63.0 eps, loss = 3.640\n",
      "    [batch 583]: seen 58300 examples at 63.0 eps, loss = 3.643\n",
      "    [batch 590]: seen 59000 examples at 63.1 eps, loss = 3.648\n",
      "    [batch 597]: seen 59700 examples at 63.2 eps, loss = 3.644\n",
      "    [batch 604]: seen 60400 examples at 63.3 eps, loss = 3.642\n",
      "    [batch 611]: seen 61100 examples at 63.3 eps, loss = 3.645\n",
      "    [batch 618]: seen 61800 examples at 63.4 eps, loss = 3.643\n",
      "    [batch 626]: seen 62600 examples at 63.5 eps, loss = 3.638\n",
      "    [batch 633]: seen 63300 examples at 63.5 eps, loss = 3.637\n",
      "    [batch 640]: seen 64000 examples at 63.6 eps, loss = 3.638\n",
      "    [batch 647]: seen 64700 examples at 63.7 eps, loss = 3.638\n",
      "    [batch 654]: seen 65400 examples at 63.7 eps, loss = 3.641\n",
      "    [batch 662]: seen 66200 examples at 63.8 eps, loss = 3.633\n",
      "    [batch 670]: seen 67000 examples at 63.9 eps, loss = 3.633\n",
      "    [batch 677]: seen 67700 examples at 63.9 eps, loss = 3.635\n",
      "    [batch 684]: seen 68400 examples at 64.0 eps, loss = 3.636\n",
      "    [batch 691]: seen 69100 examples at 64.0 eps, loss = 3.635\n",
      "    [batch 698]: seen 69800 examples at 64.1 eps, loss = 3.635\n",
      "    [batch 705]: seen 70500 examples at 64.1 eps, loss = 3.639\n",
      "    [batch 712]: seen 71200 examples at 64.2 eps, loss = 3.638\n",
      "    [batch 719]: seen 71900 examples at 64.2 eps, loss = 3.635\n",
      "    [batch 727]: seen 72700 examples at 64.3 eps, loss = 3.637\n",
      "    [batch 734]: seen 73400 examples at 64.3 eps, loss = 3.639\n",
      "    [batch 741]: seen 74100 examples at 64.4 eps, loss = 3.634\n",
      "    [batch 748]: seen 74800 examples at 64.4 eps, loss = 3.642\n",
      "    [batch 755]: seen 75500 examples at 64.5 eps, loss = 3.643\n",
      "    [batch 762]: seen 76200 examples at 64.5 eps, loss = 3.643\n",
      "    [batch 770]: seen 77000 examples at 64.6 eps, loss = 3.638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 777]: seen 77700 examples at 64.6 eps, loss = 3.636\n",
      "    [batch 784]: seen 78400 examples at 64.7 eps, loss = 3.639\n",
      "    [batch 791]: seen 79100 examples at 64.7 eps, loss = 3.642\n",
      "    [batch 798]: seen 79800 examples at 64.8 eps, loss = 3.643\n",
      "    [batch 805]: seen 80500 examples at 64.8 eps, loss = 3.640\n",
      "    [batch 812]: seen 81200 examples at 64.8 eps, loss = 3.640\n",
      "    [batch 820]: seen 82000 examples at 64.9 eps, loss = 3.640\n",
      "    [batch 827]: seen 82700 examples at 64.9 eps, loss = 3.638\n",
      "    [batch 834]: seen 83400 examples at 65.0 eps, loss = 3.638\n",
      "    [batch 841]: seen 84100 examples at 65.0 eps, loss = 3.639\n",
      "    [batch 848]: seen 84800 examples at 65.0 eps, loss = 3.642\n",
      "    [batch 855]: seen 85500 examples at 65.1 eps, loss = 3.641\n",
      "    [batch 862]: seen 86200 examples at 65.1 eps, loss = 3.644\n",
      "    [batch 869]: seen 86900 examples at 65.1 eps, loss = 3.643\n",
      "    [batch 876]: seen 87600 examples at 65.2 eps, loss = 3.632\n",
      "    [batch 884]: seen 88400 examples at 65.2 eps, loss = 3.631\n",
      "    [batch 891]: seen 89100 examples at 65.3 eps, loss = 3.631\n",
      "    [batch 898]: seen 89800 examples at 65.3 eps, loss = 3.629\n",
      "    [batch 905]: seen 90500 examples at 65.3 eps, loss = 3.631\n",
      "    [batch 912]: seen 91200 examples at 65.4 eps, loss = 3.631\n",
      "    [batch 919]: seen 91900 examples at 65.4 eps, loss = 3.630\n",
      "    [batch 926]: seen 92600 examples at 65.4 eps, loss = 3.627\n",
      "    [batch 933]: seen 93300 examples at 65.4 eps, loss = 3.627\n",
      "    [batch 940]: seen 94000 examples at 65.5 eps, loss = 3.627\n",
      "    [batch 947]: seen 94700 examples at 65.5 eps, loss = 3.624\n",
      "    [batch 954]: seen 95400 examples at 65.5 eps, loss = 3.630\n",
      "    [batch 961]: seen 96100 examples at 65.6 eps, loss = 3.631\n",
      "    [batch 968]: seen 96800 examples at 65.6 eps, loss = 3.630\n",
      "    [batch 975]: seen 97500 examples at 65.6 eps, loss = 3.624\n",
      "    [batch 982]: seen 98200 examples at 65.7 eps, loss = 3.623\n",
      "    [batch 986]: seen 98600 examples at 65.4 eps, loss = 3.622\n",
      "    [batch 993]: seen 99300 examples at 65.4 eps, loss = 3.627\n",
      "    [batch 1000]: seen 100000 examples at 65.4 eps, loss = 3.628\n",
      "    [batch 1008]: seen 100800 examples at 65.5 eps, loss = 3.627\n",
      "    [batch 1015]: seen 101500 examples at 65.5 eps, loss = 3.623\n",
      "    [batch 1018]: seen 101800 examples at 65.2 eps, loss = 3.619\n",
      "    [batch 1025]: seen 102500 examples at 65.2 eps, loss = 3.620\n",
      "    [batch 1032]: seen 103200 examples at 65.3 eps, loss = 3.623\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-20902\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-20902\n",
      "    [batch 1038]: seen 103800 examples at 65.2 eps, loss = 3.626\n",
      "    [batch 1045]: seen 104500 examples at 65.3 eps, loss = 3.631\n",
      "    [batch 1052]: seen 105200 examples at 65.3 eps, loss = 3.637\n",
      "    [batch 1059]: seen 105900 examples at 65.3 eps, loss = 3.638\n",
      "    [batch 1066]: seen 106600 examples at 65.3 eps, loss = 3.638\n",
      "    [batch 1073]: seen 107300 examples at 65.4 eps, loss = 3.635\n",
      "    [batch 1080]: seen 108000 examples at 65.4 eps, loss = 3.635\n",
      "    [batch 1087]: seen 108700 examples at 65.4 eps, loss = 3.638\n",
      "    [batch 1095]: seen 109500 examples at 65.5 eps, loss = 3.635\n",
      "    [batch 1102]: seen 110200 examples at 65.5 eps, loss = 3.631\n",
      "    [batch 1109]: seen 110900 examples at 65.5 eps, loss = 3.630\n",
      "    [batch 1116]: seen 111600 examples at 65.5 eps, loss = 3.629\n",
      "    [batch 1123]: seen 112300 examples at 65.6 eps, loss = 3.623\n",
      "    [batch 1130]: seen 113000 examples at 65.6 eps, loss = 3.622\n",
      "    [batch 1137]: seen 113700 examples at 65.6 eps, loss = 3.624\n",
      "    [batch 1144]: seen 114400 examples at 65.6 eps, loss = 3.629\n",
      "    [batch 1152]: seen 115200 examples at 65.7 eps, loss = 3.622\n",
      "    [batch 1159]: seen 115900 examples at 65.7 eps, loss = 3.623\n",
      "    [batch 1166]: seen 116600 examples at 65.7 eps, loss = 3.624\n",
      "    [batch 1174]: seen 117400 examples at 65.7 eps, loss = 3.623\n",
      "    [batch 1181]: seen 118100 examples at 65.8 eps, loss = 3.627\n",
      "    [batch 1188]: seen 118800 examples at 65.8 eps, loss = 3.631\n",
      "    [batch 1195]: seen 119500 examples at 65.8 eps, loss = 3.632\n",
      "    [batch 1203]: seen 120300 examples at 65.8 eps, loss = 3.631\n",
      "    [batch 1210]: seen 121000 examples at 65.8 eps, loss = 3.626\n",
      "    [batch 1217]: seen 121700 examples at 65.9 eps, loss = 3.626\n",
      "    [batch 1224]: seen 122400 examples at 65.9 eps, loss = 3.630\n",
      "    [batch 1231]: seen 123100 examples at 65.9 eps, loss = 3.629\n",
      "    [batch 1239]: seen 123900 examples at 65.9 eps, loss = 3.631\n",
      "    [batch 1246]: seen 124600 examples at 66.0 eps, loss = 3.628\n",
      "    [batch 1253]: seen 125300 examples at 66.0 eps, loss = 3.627\n",
      "    [batch 1260]: seen 126000 examples at 66.0 eps, loss = 3.623\n",
      "    [batch 1268]: seen 126800 examples at 66.0 eps, loss = 3.622\n",
      "    [batch 1275]: seen 127500 examples at 66.0 eps, loss = 3.619\n",
      "    [batch 1282]: seen 128200 examples at 66.0 eps, loss = 3.618\n",
      "    [batch 1289]: seen 128900 examples at 65.9 eps, loss = 3.617\n",
      "    [batch 1294]: seen 129400 examples at 65.8 eps, loss = 3.617\n",
      "    [batch 1301]: seen 130100 examples at 65.8 eps, loss = 3.620\n",
      "    [batch 1308]: seen 130800 examples at 65.8 eps, loss = 3.622\n",
      "    [batch 1315]: seen 131500 examples at 65.8 eps, loss = 3.627\n",
      "    [batch 1322]: seen 132200 examples at 65.8 eps, loss = 3.624\n",
      "    [batch 1329]: seen 132900 examples at 65.9 eps, loss = 3.621\n",
      "    [batch 1336]: seen 133600 examples at 65.9 eps, loss = 3.624\n",
      "    [batch 1343]: seen 134300 examples at 65.9 eps, loss = 3.620\n",
      "    [batch 1350]: seen 135000 examples at 65.9 eps, loss = 3.618\n",
      "    [batch 1355]: seen 135500 examples at 65.8 eps, loss = 3.615\n",
      "    [batch 1362]: seen 136200 examples at 65.8 eps, loss = 3.616\n",
      "    [batch 1366]: seen 136600 examples at 65.6 eps, loss = 3.614\n",
      "    [batch 1373]: seen 137300 examples at 65.7 eps, loss = 3.617\n",
      "    [batch 1380]: seen 138000 examples at 65.7 eps, loss = 3.617\n",
      "    [batch 1387]: seen 138700 examples at 65.7 eps, loss = 3.626\n",
      "    [batch 1394]: seen 139400 examples at 65.7 eps, loss = 3.630\n",
      "    [batch 1401]: seen 140100 examples at 65.7 eps, loss = 3.627\n",
      "    [batch 1408]: seen 140800 examples at 65.8 eps, loss = 3.627\n",
      "    [batch 1415]: seen 141500 examples at 65.8 eps, loss = 3.625\n",
      "    [batch 1422]: seen 142200 examples at 65.8 eps, loss = 3.629\n",
      "    [batch 1429]: seen 142900 examples at 65.8 eps, loss = 3.631\n",
      "    [batch 1436]: seen 143600 examples at 65.8 eps, loss = 3.632\n",
      "    [batch 1443]: seen 144300 examples at 65.9 eps, loss = 3.633\n",
      "    [batch 1450]: seen 145000 examples at 65.9 eps, loss = 3.631\n",
      "    [batch 1457]: seen 145700 examples at 65.9 eps, loss = 3.629\n",
      "    [batch 1465]: seen 146500 examples at 65.9 eps, loss = 3.626\n",
      "    [batch 1473]: seen 147300 examples at 65.9 eps, loss = 3.624\n",
      "    [batch 1480]: seen 148000 examples at 66.0 eps, loss = 3.625\n",
      "    [batch 1488]: seen 148800 examples at 66.0 eps, loss = 3.630\n",
      "    [batch 1495]: seen 149500 examples at 66.0 eps, loss = 3.631\n",
      "    [batch 1502]: seen 150200 examples at 66.0 eps, loss = 3.630\n",
      "    [batch 1509]: seen 150900 examples at 66.0 eps, loss = 3.633\n",
      "    [batch 1516]: seen 151600 examples at 66.0 eps, loss = 3.629\n",
      "    [batch 1523]: seen 152300 examples at 66.1 eps, loss = 3.627\n",
      "    [batch 1530]: seen 153000 examples at 66.1 eps, loss = 3.624\n",
      "    [batch 1537]: seen 153700 examples at 66.1 eps, loss = 3.626\n",
      "    [batch 1544]: seen 154400 examples at 66.1 eps, loss = 3.628\n",
      "    [batch 1552]: seen 155200 examples at 66.1 eps, loss = 3.622\n",
      "    [batch 1559]: seen 155900 examples at 66.1 eps, loss = 3.619\n",
      "    [batch 1566]: seen 156600 examples at 66.2 eps, loss = 3.626\n",
      "    [batch 1573]: seen 157300 examples at 66.2 eps, loss = 3.627\n",
      "    [batch 1580]: seen 158000 examples at 66.2 eps, loss = 3.628\n",
      "    [batch 1588]: seen 158800 examples at 66.2 eps, loss = 3.623\n",
      "    [batch 1595]: seen 159500 examples at 66.2 eps, loss = 3.628\n",
      "    [batch 1602]: seen 160200 examples at 66.2 eps, loss = 3.630\n",
      "    [batch 1609]: seen 160900 examples at 66.2 eps, loss = 3.627\n",
      "    [batch 1616]: seen 161600 examples at 66.3 eps, loss = 3.629\n",
      "    [batch 1624]: seen 162400 examples at 66.3 eps, loss = 3.625\n",
      "    [batch 1632]: seen 163200 examples at 66.3 eps, loss = 3.627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1639]: seen 163900 examples at 66.3 eps, loss = 3.622\n",
      "    [batch 1646]: seen 164600 examples at 66.3 eps, loss = 3.622\n",
      "    [batch 1653]: seen 165300 examples at 66.3 eps, loss = 3.626\n",
      "    [batch 1660]: seen 166000 examples at 66.4 eps, loss = 3.622\n",
      "    [batch 1668]: seen 166800 examples at 66.4 eps, loss = 3.615\n",
      "    [batch 1673]: seen 167300 examples at 66.2 eps, loss = 3.613\n",
      "    [batch 1677]: seen 167700 examples at 66.0 eps, loss = 3.611\n",
      "    [batch 1682]: seen 168200 examples at 66.0 eps, loss = 3.610\n",
      "    [batch 1687]: seen 168700 examples at 65.8 eps, loss = 3.609\n",
      "    [batch 1691]: seen 169100 examples at 65.6 eps, loss = 3.606\n",
      "    [batch 1696]: seen 169600 examples at 65.6 eps, loss = 3.606\n",
      "    [batch 1703]: seen 170300 examples at 65.6 eps, loss = 3.610\n",
      "    [batch 1710]: seen 171000 examples at 65.5 eps, loss = 3.605\n",
      "    [batch 1714]: seen 171400 examples at 65.4 eps, loss = 3.604\n",
      "    [batch 1721]: seen 172100 examples at 65.4 eps, loss = 3.606\n",
      "    [batch 1728]: seen 172800 examples at 65.4 eps, loss = 3.605\n",
      "    [batch 1731]: seen 173100 examples at 65.1 eps, loss = 3.602\n",
      "    [batch 1734]: seen 173400 examples at 65.0 eps, loss = 3.600\n",
      "    [batch 1737]: seen 173700 examples at 64.8 eps, loss = 3.596\n",
      "    [batch 1740]: seen 174000 examples at 64.6 eps, loss = 3.594\n",
      "    [batch 1745]: seen 174500 examples at 64.5 eps, loss = 3.596\n",
      "    [batch 1752]: seen 175200 examples at 64.6 eps, loss = 3.598\n",
      "    [batch 1759]: seen 175900 examples at 64.6 eps, loss = 3.602\n",
      "    [batch 1766]: seen 176600 examples at 64.6 eps, loss = 3.599\n",
      "    [batch 1773]: seen 177300 examples at 64.6 eps, loss = 3.600\n",
      "    [batch 1780]: seen 178000 examples at 64.6 eps, loss = 3.601\n",
      "    [batch 1787]: seen 178700 examples at 64.7 eps, loss = 3.603\n",
      "    [batch 1794]: seen 179400 examples at 64.7 eps, loss = 3.602\n",
      "    [batch 1801]: seen 180100 examples at 64.7 eps, loss = 3.600\n",
      "    [batch 1808]: seen 180800 examples at 64.7 eps, loss = 3.597\n",
      "    [batch 1815]: seen 181500 examples at 64.7 eps, loss = 3.600\n",
      "    [batch 1822]: seen 182200 examples at 64.7 eps, loss = 3.606\n",
      "    [batch 1829]: seen 182900 examples at 64.8 eps, loss = 3.610\n",
      "    [batch 1836]: seen 183600 examples at 64.8 eps, loss = 3.607\n",
      "    [batch 1843]: seen 184300 examples at 64.8 eps, loss = 3.611\n",
      "    [batch 1850]: seen 185000 examples at 64.8 eps, loss = 3.614\n",
      "    [batch 1857]: seen 185700 examples at 64.8 eps, loss = 3.609\n",
      "    [batch 1864]: seen 186400 examples at 64.9 eps, loss = 3.609\n",
      "    [END] Training complete: Total examples : 187100; Total time: 0:48:04\n",
      "[EPOCH 13] Complete. Avg Loss: 3.6114820040880597; Best Loss: 3.5941309787332787\n",
      "[EPOCH 14] Starting training..\n",
      "    [batch 7]: seen 700 examples at 69.6 eps, loss = 3.612\n",
      "    [batch 14]: seen 1400 examples at 69.7 eps, loss = 3.609\n",
      "    [batch 22]: seen 2200 examples at 69.8 eps, loss = 3.607\n",
      "    [batch 29]: seen 2900 examples at 69.8 eps, loss = 3.609\n",
      "    [batch 36]: seen 3600 examples at 69.8 eps, loss = 3.605\n",
      "    [batch 43]: seen 4300 examples at 69.8 eps, loss = 3.606\n",
      "    [batch 50]: seen 5000 examples at 69.8 eps, loss = 3.606\n",
      "    [batch 57]: seen 5700 examples at 69.8 eps, loss = 3.605\n",
      "    [batch 64]: seen 6400 examples at 69.8 eps, loss = 3.611\n",
      "    [batch 71]: seen 7100 examples at 69.9 eps, loss = 3.611\n",
      "    [batch 78]: seen 7800 examples at 69.9 eps, loss = 3.608\n",
      "    [batch 85]: seen 8500 examples at 69.9 eps, loss = 3.605\n",
      "    [batch 92]: seen 9200 examples at 69.9 eps, loss = 3.600\n",
      "    [batch 99]: seen 9900 examples at 69.9 eps, loss = 3.598\n",
      "    [batch 106]: seen 10600 examples at 69.9 eps, loss = 3.598\n",
      "    [batch 114]: seen 11400 examples at 69.9 eps, loss = 3.596\n",
      "    [batch 118]: seen 11800 examples at 67.0 eps, loss = 3.592\n",
      "    [batch 125]: seen 12500 examples at 67.1 eps, loss = 3.592\n",
      "    [batch 128]: seen 12800 examples at 63.8 eps, loss = 3.590\n",
      "    [batch 131]: seen 13100 examples at 61.9 eps, loss = 3.589\n",
      "    [batch 134]: seen 13400 examples at 60.2 eps, loss = 3.588\n",
      "    [batch 141]: seen 14100 examples at 60.6 eps, loss = 3.590\n",
      "    [batch 148]: seen 14800 examples at 60.2 eps, loss = 3.587\n",
      "    [batch 153]: seen 15300 examples at 58.9 eps, loss = 3.586\n",
      "    [batch 156]: seen 15600 examples at 56.9 eps, loss = 3.581\n",
      "    [batch 163]: seen 16300 examples at 57.4 eps, loss = 3.584\n",
      "    [batch 171]: seen 17100 examples at 57.9 eps, loss = 3.589\n",
      "    [batch 178]: seen 17800 examples at 58.3 eps, loss = 3.586\n",
      "    [batch 185]: seen 18500 examples at 58.6 eps, loss = 3.589\n",
      "    [batch 192]: seen 19200 examples at 59.0 eps, loss = 3.590\n",
      "    [batch 199]: seen 19900 examples at 59.3 eps, loss = 3.591\n",
      "    [batch 207]: seen 20700 examples at 59.7 eps, loss = 3.595\n",
      "    [batch 214]: seen 21400 examples at 59.9 eps, loss = 3.596\n",
      "    [batch 221]: seen 22100 examples at 60.2 eps, loss = 3.598\n",
      "    [batch 229]: seen 22900 examples at 60.5 eps, loss = 3.603\n",
      "    [batch 237]: seen 23700 examples at 60.8 eps, loss = 3.598\n",
      "    [batch 244]: seen 24400 examples at 61.0 eps, loss = 3.596\n",
      "    [batch 251]: seen 25100 examples at 61.2 eps, loss = 3.596\n",
      "    [batch 258]: seen 25800 examples at 61.4 eps, loss = 3.593\n",
      "    [batch 265]: seen 26500 examples at 61.6 eps, loss = 3.590\n",
      "    [batch 273]: seen 27300 examples at 61.8 eps, loss = 3.592\n",
      "    [batch 280]: seen 28000 examples at 62.0 eps, loss = 3.589\n",
      "    [batch 287]: seen 28700 examples at 62.2 eps, loss = 3.586\n",
      "    [batch 294]: seen 29400 examples at 62.4 eps, loss = 3.591\n",
      "    [batch 301]: seen 30100 examples at 62.5 eps, loss = 3.593\n",
      "    [batch 308]: seen 30800 examples at 62.7 eps, loss = 3.601\n",
      "    [batch 315]: seen 31500 examples at 62.8 eps, loss = 3.602\n",
      "    [batch 322]: seen 32200 examples at 62.9 eps, loss = 3.604\n",
      "    [batch 329]: seen 32900 examples at 63.1 eps, loss = 3.596\n",
      "    [batch 336]: seen 33600 examples at 63.2 eps, loss = 3.591\n",
      "    [batch 343]: seen 34300 examples at 63.3 eps, loss = 3.594\n",
      "    [batch 350]: seen 35000 examples at 63.4 eps, loss = 3.595\n",
      "    [batch 358]: seen 35800 examples at 63.6 eps, loss = 3.593\n",
      "    [batch 365]: seen 36500 examples at 63.7 eps, loss = 3.592\n",
      "    [batch 372]: seen 37200 examples at 63.8 eps, loss = 3.595\n",
      "    [batch 380]: seen 38000 examples at 63.9 eps, loss = 3.600\n",
      "    [batch 387]: seen 38700 examples at 64.0 eps, loss = 3.596\n",
      "    [batch 394]: seen 39400 examples at 64.1 eps, loss = 3.597\n",
      "    [batch 401]: seen 40100 examples at 64.2 eps, loss = 3.596\n",
      "    [batch 408]: seen 40800 examples at 64.3 eps, loss = 3.601\n",
      "    [batch 415]: seen 41500 examples at 64.4 eps, loss = 3.596\n",
      "    [batch 423]: seen 42300 examples at 64.5 eps, loss = 3.595\n",
      "    [batch 430]: seen 43000 examples at 64.6 eps, loss = 3.597\n",
      "    [batch 438]: seen 43800 examples at 64.7 eps, loss = 3.596\n",
      "    [batch 445]: seen 44500 examples at 64.7 eps, loss = 3.597\n",
      "    [batch 452]: seen 45200 examples at 64.8 eps, loss = 3.599\n",
      "    [batch 460]: seen 46000 examples at 64.9 eps, loss = 3.593\n",
      "    [batch 467]: seen 46700 examples at 65.0 eps, loss = 3.595\n",
      "    [batch 474]: seen 47400 examples at 65.0 eps, loss = 3.595\n",
      "    [batch 481]: seen 48100 examples at 65.1 eps, loss = 3.597\n",
      "    [batch 488]: seen 48800 examples at 65.2 eps, loss = 3.597\n",
      "    [batch 495]: seen 49500 examples at 65.2 eps, loss = 3.592\n",
      "    [batch 503]: seen 50300 examples at 65.3 eps, loss = 3.589\n",
      "    [batch 510]: seen 51000 examples at 65.3 eps, loss = 3.588\n",
      "    [batch 517]: seen 51700 examples at 65.4 eps, loss = 3.589\n",
      "    [batch 525]: seen 52500 examples at 65.5 eps, loss = 3.590\n",
      "    [batch 532]: seen 53200 examples at 65.5 eps, loss = 3.591\n",
      "    [batch 539]: seen 53900 examples at 65.6 eps, loss = 3.593\n",
      "    [batch 546]: seen 54600 examples at 65.6 eps, loss = 3.595\n",
      "    [batch 553]: seen 55300 examples at 65.7 eps, loss = 3.592\n",
      "    [batch 561]: seen 56100 examples at 65.7 eps, loss = 3.588\n",
      "    [batch 568]: seen 56800 examples at 65.8 eps, loss = 3.582\n",
      "    [batch 575]: seen 57500 examples at 65.8 eps, loss = 3.584\n",
      "    [batch 582]: seen 58200 examples at 65.9 eps, loss = 3.584\n",
      "    [batch 589]: seen 58900 examples at 65.9 eps, loss = 3.589\n",
      "    [batch 597]: seen 59700 examples at 66.0 eps, loss = 3.594\n",
      "    [batch 604]: seen 60400 examples at 66.0 eps, loss = 3.592\n",
      "    [batch 611]: seen 61100 examples at 66.1 eps, loss = 3.588\n",
      "    [batch 619]: seen 61900 examples at 66.1 eps, loss = 3.590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 627]: seen 62700 examples at 66.1 eps, loss = 3.587\n",
      "    [batch 634]: seen 63400 examples at 66.2 eps, loss = 3.588\n",
      "    [batch 642]: seen 64200 examples at 66.2 eps, loss = 3.589\n",
      "    [batch 649]: seen 64900 examples at 66.3 eps, loss = 3.588\n",
      "    [batch 656]: seen 65600 examples at 66.3 eps, loss = 3.593\n",
      "    [batch 663]: seen 66300 examples at 66.3 eps, loss = 3.595\n",
      "    [batch 670]: seen 67000 examples at 66.4 eps, loss = 3.593\n",
      "    [batch 677]: seen 67700 examples at 66.4 eps, loss = 3.590\n",
      "    [batch 684]: seen 68400 examples at 66.4 eps, loss = 3.589\n",
      "    [batch 692]: seen 69200 examples at 66.5 eps, loss = 3.588\n",
      "    [batch 699]: seen 69900 examples at 66.5 eps, loss = 3.587\n",
      "    [batch 706]: seen 70600 examples at 66.5 eps, loss = 3.583\n",
      "    [batch 711]: seen 71100 examples at 66.4 eps, loss = 3.582\n",
      "    [batch 718]: seen 71800 examples at 66.4 eps, loss = 3.580\n",
      "    [batch 725]: seen 72500 examples at 66.4 eps, loss = 3.582\n",
      "    [batch 731]: seen 73100 examples at 66.3 eps, loss = 3.578\n",
      "    [batch 736]: seen 73600 examples at 66.1 eps, loss = 3.582\n",
      "    [batch 743]: seen 74300 examples at 66.1 eps, loss = 3.582\n",
      "    [batch 750]: seen 75000 examples at 66.1 eps, loss = 3.584\n",
      "    [batch 757]: seen 75700 examples at 66.2 eps, loss = 3.580\n",
      "    [batch 764]: seen 76400 examples at 66.2 eps, loss = 3.579\n",
      "    [batch 769]: seen 76900 examples at 66.0 eps, loss = 3.579\n",
      "    [batch 774]: seen 77400 examples at 65.8 eps, loss = 3.577\n",
      "    [batch 782]: seen 78200 examples at 65.9 eps, loss = 3.578\n",
      "    [batch 789]: seen 78900 examples at 65.9 eps, loss = 3.578\n",
      "    [batch 796]: seen 79600 examples at 65.9 eps, loss = 3.582\n",
      "    [batch 803]: seen 80300 examples at 66.0 eps, loss = 3.583\n",
      "    [batch 811]: seen 81100 examples at 66.0 eps, loss = 3.583\n",
      "    [batch 818]: seen 81800 examples at 66.0 eps, loss = 3.583\n",
      "    [batch 825]: seen 82500 examples at 66.1 eps, loss = 3.587\n",
      "    [batch 833]: seen 83300 examples at 66.1 eps, loss = 3.589\n",
      "    [batch 840]: seen 84000 examples at 66.1 eps, loss = 3.592\n",
      "    [batch 847]: seen 84700 examples at 66.2 eps, loss = 3.596\n",
      "    [batch 855]: seen 85500 examples at 66.2 eps, loss = 3.593\n",
      "    [batch 862]: seen 86200 examples at 66.2 eps, loss = 3.591\n",
      "    [batch 869]: seen 86900 examples at 66.3 eps, loss = 3.589\n",
      "    [batch 876]: seen 87600 examples at 66.3 eps, loss = 3.595\n",
      "    [batch 883]: seen 88300 examples at 66.3 eps, loss = 3.591\n",
      "    [batch 890]: seen 89000 examples at 66.3 eps, loss = 3.588\n",
      "    [batch 898]: seen 89800 examples at 66.4 eps, loss = 3.591\n",
      "    [batch 906]: seen 90600 examples at 66.4 eps, loss = 3.597\n",
      "    [batch 913]: seen 91300 examples at 66.4 eps, loss = 3.595\n",
      "    [batch 920]: seen 92000 examples at 66.5 eps, loss = 3.598\n",
      "    [batch 927]: seen 92700 examples at 66.5 eps, loss = 3.599\n",
      "    [batch 934]: seen 93400 examples at 66.5 eps, loss = 3.603\n",
      "    [batch 941]: seen 94100 examples at 66.5 eps, loss = 3.600\n",
      "    [batch 948]: seen 94800 examples at 66.5 eps, loss = 3.598\n",
      "    [batch 955]: seen 95500 examples at 66.6 eps, loss = 3.598\n",
      "    [batch 962]: seen 96200 examples at 66.6 eps, loss = 3.596\n",
      "    [batch 969]: seen 96900 examples at 66.6 eps, loss = 3.594\n",
      "    [batch 977]: seen 97700 examples at 66.6 eps, loss = 3.598\n",
      "    [batch 985]: seen 98500 examples at 66.7 eps, loss = 3.601\n",
      "    [batch 993]: seen 99300 examples at 66.7 eps, loss = 3.604\n",
      "    [batch 1000]: seen 100000 examples at 66.7 eps, loss = 3.604\n",
      "    [batch 1007]: seen 100700 examples at 66.7 eps, loss = 3.601\n",
      "    [batch 1015]: seen 101500 examples at 66.8 eps, loss = 3.599\n",
      "    [batch 1022]: seen 102200 examples at 66.8 eps, loss = 3.601\n",
      "    [batch 1030]: seen 103000 examples at 66.8 eps, loss = 3.599\n",
      "    [batch 1038]: seen 103800 examples at 66.8 eps, loss = 3.591\n",
      "    [batch 1046]: seen 104600 examples at 66.9 eps, loss = 3.590\n",
      "    [batch 1053]: seen 105300 examples at 66.9 eps, loss = 3.593\n",
      "    [batch 1061]: seen 106100 examples at 66.9 eps, loss = 3.598\n",
      "    [batch 1068]: seen 106800 examples at 66.9 eps, loss = 3.594\n",
      "    [batch 1075]: seen 107500 examples at 66.9 eps, loss = 3.595\n",
      "    [batch 1082]: seen 108200 examples at 67.0 eps, loss = 3.595\n",
      "    [batch 1089]: seen 108900 examples at 67.0 eps, loss = 3.592\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-22512\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-22512\n",
      "    [batch 1095]: seen 109500 examples at 66.9 eps, loss = 3.590\n",
      "    [batch 1102]: seen 110200 examples at 66.9 eps, loss = 3.587\n",
      "    [batch 1109]: seen 110900 examples at 67.0 eps, loss = 3.586\n",
      "    [batch 1117]: seen 111700 examples at 67.0 eps, loss = 3.586\n",
      "    [batch 1124]: seen 112400 examples at 67.0 eps, loss = 3.583\n",
      "    [batch 1131]: seen 113100 examples at 67.0 eps, loss = 3.584\n",
      "    [batch 1138]: seen 113800 examples at 67.0 eps, loss = 3.588\n",
      "    [batch 1145]: seen 114500 examples at 67.0 eps, loss = 3.587\n",
      "    [batch 1153]: seen 115300 examples at 67.1 eps, loss = 3.588\n",
      "    [batch 1160]: seen 116000 examples at 67.1 eps, loss = 3.595\n",
      "    [batch 1167]: seen 116700 examples at 67.1 eps, loss = 3.599\n",
      "    [batch 1174]: seen 117400 examples at 67.1 eps, loss = 3.597\n",
      "    [batch 1181]: seen 118100 examples at 67.1 eps, loss = 3.593\n",
      "    [batch 1188]: seen 118800 examples at 67.1 eps, loss = 3.587\n",
      "    [batch 1195]: seen 119500 examples at 67.2 eps, loss = 3.591\n",
      "    [batch 1202]: seen 120200 examples at 67.2 eps, loss = 3.592\n",
      "    [batch 1209]: seen 120900 examples at 67.2 eps, loss = 3.596\n",
      "    [batch 1216]: seen 121600 examples at 67.2 eps, loss = 3.594\n",
      "    [batch 1224]: seen 122400 examples at 67.2 eps, loss = 3.589\n",
      "    [batch 1231]: seen 123100 examples at 67.2 eps, loss = 3.588\n",
      "    [batch 1238]: seen 123800 examples at 67.3 eps, loss = 3.586\n",
      "    [batch 1245]: seen 124500 examples at 67.3 eps, loss = 3.581\n",
      "    [batch 1253]: seen 125300 examples at 67.3 eps, loss = 3.580\n",
      "    [batch 1257]: seen 125700 examples at 67.1 eps, loss = 3.573\n",
      "    [batch 1260]: seen 126000 examples at 66.8 eps, loss = 3.572\n",
      "    [batch 1263]: seen 126300 examples at 66.6 eps, loss = 3.571\n",
      "    [batch 1270]: seen 127000 examples at 66.6 eps, loss = 3.574\n",
      "    [batch 1277]: seen 127700 examples at 66.6 eps, loss = 3.578\n",
      "    [batch 1284]: seen 128400 examples at 66.6 eps, loss = 3.574\n",
      "    [batch 1291]: seen 129100 examples at 66.7 eps, loss = 3.574\n",
      "    [batch 1298]: seen 129800 examples at 66.7 eps, loss = 3.575\n",
      "    [batch 1305]: seen 130500 examples at 66.7 eps, loss = 3.576\n",
      "    [batch 1312]: seen 131200 examples at 66.7 eps, loss = 3.575\n",
      "    [batch 1320]: seen 132000 examples at 66.7 eps, loss = 3.576\n",
      "    [batch 1327]: seen 132700 examples at 66.7 eps, loss = 3.577\n",
      "    [batch 1335]: seen 133500 examples at 66.8 eps, loss = 3.578\n",
      "    [batch 1343]: seen 134300 examples at 66.8 eps, loss = 3.582\n",
      "    [batch 1350]: seen 135000 examples at 66.8 eps, loss = 3.583\n",
      "    [batch 1357]: seen 135700 examples at 66.8 eps, loss = 3.587\n",
      "    [batch 1365]: seen 136500 examples at 66.8 eps, loss = 3.586\n",
      "    [batch 1373]: seen 137300 examples at 66.9 eps, loss = 3.586\n",
      "    [batch 1380]: seen 138000 examples at 66.9 eps, loss = 3.589\n",
      "    [batch 1387]: seen 138700 examples at 66.9 eps, loss = 3.596\n",
      "    [batch 1394]: seen 139400 examples at 66.9 eps, loss = 3.599\n",
      "    [batch 1402]: seen 140200 examples at 66.9 eps, loss = 3.594\n",
      "    [batch 1410]: seen 141000 examples at 66.9 eps, loss = 3.596\n",
      "    [batch 1417]: seen 141700 examples at 66.9 eps, loss = 3.591\n",
      "    [batch 1425]: seen 142500 examples at 67.0 eps, loss = 3.595\n",
      "    [batch 1433]: seen 143300 examples at 67.0 eps, loss = 3.599\n",
      "    [batch 1441]: seen 144100 examples at 67.0 eps, loss = 3.605\n",
      "    [batch 1449]: seen 144900 examples at 67.0 eps, loss = 3.600\n",
      "    [batch 1456]: seen 145600 examples at 67.0 eps, loss = 3.596\n",
      "    [batch 1464]: seen 146400 examples at 67.0 eps, loss = 3.588\n",
      "    [batch 1471]: seen 147100 examples at 67.1 eps, loss = 3.585\n",
      "    [batch 1478]: seen 147800 examples at 67.1 eps, loss = 3.584\n",
      "    [batch 1485]: seen 148500 examples at 67.1 eps, loss = 3.582\n",
      "    [batch 1492]: seen 149200 examples at 67.1 eps, loss = 3.576\n",
      "    [batch 1499]: seen 149900 examples at 67.1 eps, loss = 3.574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1507]: seen 150700 examples at 67.1 eps, loss = 3.576\n",
      "    [batch 1514]: seen 151400 examples at 67.1 eps, loss = 3.574\n",
      "    [batch 1522]: seen 152200 examples at 67.1 eps, loss = 3.575\n",
      "    [batch 1529]: seen 152900 examples at 67.2 eps, loss = 3.579\n",
      "    [batch 1537]: seen 153700 examples at 67.2 eps, loss = 3.581\n",
      "    [batch 1544]: seen 154400 examples at 67.2 eps, loss = 3.588\n",
      "    [batch 1552]: seen 155200 examples at 67.2 eps, loss = 3.591\n",
      "    [batch 1560]: seen 156000 examples at 67.2 eps, loss = 3.592\n",
      "    [batch 1567]: seen 156700 examples at 67.2 eps, loss = 3.591\n",
      "    [batch 1574]: seen 157400 examples at 67.2 eps, loss = 3.592\n",
      "    [batch 1581]: seen 158100 examples at 67.2 eps, loss = 3.592\n",
      "    [batch 1589]: seen 158900 examples at 67.3 eps, loss = 3.589\n",
      "    [batch 1596]: seen 159600 examples at 67.3 eps, loss = 3.587\n",
      "    [batch 1603]: seen 160300 examples at 67.3 eps, loss = 3.586\n",
      "    [batch 1611]: seen 161100 examples at 67.3 eps, loss = 3.592\n",
      "    [batch 1618]: seen 161800 examples at 67.3 eps, loss = 3.587\n",
      "    [batch 1625]: seen 162500 examples at 67.3 eps, loss = 3.587\n",
      "    [batch 1632]: seen 163200 examples at 67.3 eps, loss = 3.589\n",
      "    [batch 1640]: seen 164000 examples at 67.3 eps, loss = 3.585\n",
      "    [batch 1647]: seen 164700 examples at 67.3 eps, loss = 3.583\n",
      "    [batch 1655]: seen 165500 examples at 67.4 eps, loss = 3.586\n",
      "    [batch 1662]: seen 166200 examples at 67.4 eps, loss = 3.586\n",
      "    [batch 1669]: seen 166900 examples at 67.4 eps, loss = 3.588\n",
      "    [batch 1676]: seen 167600 examples at 67.4 eps, loss = 3.588\n",
      "    [batch 1684]: seen 168400 examples at 67.4 eps, loss = 3.592\n",
      "    [batch 1691]: seen 169100 examples at 67.4 eps, loss = 3.589\n",
      "    [batch 1698]: seen 169800 examples at 67.4 eps, loss = 3.586\n",
      "    [batch 1705]: seen 170500 examples at 67.4 eps, loss = 3.587\n",
      "    [batch 1712]: seen 171200 examples at 67.4 eps, loss = 3.582\n",
      "    [batch 1719]: seen 171900 examples at 67.5 eps, loss = 3.587\n",
      "    [batch 1726]: seen 172600 examples at 67.5 eps, loss = 3.589\n",
      "    [batch 1733]: seen 173300 examples at 67.5 eps, loss = 3.583\n",
      "    [batch 1740]: seen 174000 examples at 67.5 eps, loss = 3.584\n",
      "    [batch 1747]: seen 174700 examples at 67.5 eps, loss = 3.590\n",
      "    [batch 1754]: seen 175400 examples at 67.5 eps, loss = 3.587\n",
      "    [batch 1761]: seen 176100 examples at 67.5 eps, loss = 3.588\n",
      "    [batch 1769]: seen 176900 examples at 67.5 eps, loss = 3.590\n",
      "    [batch 1776]: seen 177600 examples at 67.5 eps, loss = 3.590\n",
      "    [batch 1783]: seen 178300 examples at 67.5 eps, loss = 3.591\n",
      "    [batch 1791]: seen 179100 examples at 67.5 eps, loss = 3.593\n",
      "    [batch 1798]: seen 179800 examples at 67.6 eps, loss = 3.596\n",
      "    [batch 1805]: seen 180500 examples at 67.6 eps, loss = 3.594\n",
      "    [batch 1812]: seen 181200 examples at 67.6 eps, loss = 3.593\n",
      "    [batch 1819]: seen 181900 examples at 67.6 eps, loss = 3.593\n",
      "    [batch 1826]: seen 182600 examples at 67.6 eps, loss = 3.592\n",
      "    [batch 1833]: seen 183300 examples at 67.6 eps, loss = 3.596\n",
      "    [batch 1840]: seen 184000 examples at 67.6 eps, loss = 3.597\n",
      "    [batch 1848]: seen 184800 examples at 67.6 eps, loss = 3.597\n",
      "    [batch 1855]: seen 185500 examples at 67.6 eps, loss = 3.597\n",
      "    [batch 1863]: seen 186300 examples at 67.6 eps, loss = 3.595\n",
      "    [batch 1871]: seen 187100 examples at 67.6 eps, loss = 3.594\n",
      "    [END] Training complete: Total examples : 187100; Total time: 0:46:05\n",
      "[EPOCH 14] Complete. Avg Loss: 3.5941254269907383; Best Loss: 3.570534191168686\n",
      "[EPOCH 15] Starting training..\n",
      "    [batch 7]: seen 700 examples at 70.0 eps, loss = 3.593\n",
      "    [batch 14]: seen 1400 examples at 70.0 eps, loss = 3.595\n",
      "    [batch 22]: seen 2200 examples at 70.0 eps, loss = 3.592\n",
      "    [batch 29]: seen 2900 examples at 70.0 eps, loss = 3.589\n",
      "    [batch 36]: seen 3600 examples at 70.0 eps, loss = 3.587\n",
      "    [batch 43]: seen 4300 examples at 69.9 eps, loss = 3.581\n",
      "    [batch 50]: seen 5000 examples at 69.9 eps, loss = 3.579\n",
      "    [batch 57]: seen 5700 examples at 69.9 eps, loss = 3.577\n",
      "    [batch 64]: seen 6400 examples at 69.9 eps, loss = 3.572\n",
      "    [batch 69]: seen 6900 examples at 67.6 eps, loss = 3.572\n",
      "    [batch 74]: seen 7400 examples at 63.9 eps, loss = 3.569\n",
      "    [batch 77]: seen 7700 examples at 60.7 eps, loss = 3.571\n",
      "    [batch 84]: seen 8400 examples at 61.4 eps, loss = 3.569\n",
      "    [batch 89]: seen 8900 examples at 60.5 eps, loss = 3.571\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-23380\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-23380\n",
      "    [batch 96]: seen 9600 examples at 60.5 eps, loss = 3.576\n",
      "    [batch 103]: seen 10300 examples at 61.0 eps, loss = 3.571\n",
      "    [batch 109]: seen 10900 examples at 60.3 eps, loss = 3.566\n",
      "    [batch 112]: seen 11200 examples at 58.4 eps, loss = 3.563\n",
      "    [batch 114]: seen 11400 examples at 56.5 eps, loss = 3.561\n",
      "    [batch 122]: seen 12200 examples at 57.2 eps, loss = 3.563\n",
      "    [batch 128]: seen 12800 examples at 56.8 eps, loss = 3.561\n",
      "    [batch 135]: seen 13500 examples at 56.6 eps, loss = 3.559\n",
      "    [batch 142]: seen 14200 examples at 57.1 eps, loss = 3.562\n",
      "    [batch 148]: seen 14800 examples at 56.8 eps, loss = 3.559\n",
      "    [batch 152]: seen 15200 examples at 55.7 eps, loss = 3.559\n",
      "    [batch 159]: seen 15900 examples at 56.2 eps, loss = 3.561\n",
      "    [batch 166]: seen 16600 examples at 56.7 eps, loss = 3.562\n",
      "    [batch 173]: seen 17300 examples at 57.1 eps, loss = 3.567\n",
      "    [batch 181]: seen 18100 examples at 57.6 eps, loss = 3.570\n",
      "    [batch 189]: seen 18900 examples at 58.0 eps, loss = 3.570\n",
      "    [batch 196]: seen 19600 examples at 58.4 eps, loss = 3.575\n",
      "    [batch 203]: seen 20300 examples at 58.7 eps, loss = 3.576\n",
      "    [batch 211]: seen 21100 examples at 59.1 eps, loss = 3.571\n",
      "    [batch 218]: seen 21800 examples at 59.4 eps, loss = 3.569\n",
      "    [batch 225]: seen 22500 examples at 59.6 eps, loss = 3.571\n",
      "    [batch 233]: seen 23300 examples at 59.9 eps, loss = 3.569\n",
      "    [batch 240]: seen 24000 examples at 60.2 eps, loss = 3.567\n",
      "    [batch 248]: seen 24800 examples at 60.5 eps, loss = 3.567\n",
      "    [batch 255]: seen 25500 examples at 60.7 eps, loss = 3.567\n",
      "    [batch 262]: seen 26200 examples at 60.9 eps, loss = 3.566\n",
      "    [batch 270]: seen 27000 examples at 61.1 eps, loss = 3.567\n",
      "    [batch 277]: seen 27700 examples at 61.3 eps, loss = 3.564\n",
      "    [batch 284]: seen 28400 examples at 61.5 eps, loss = 3.568\n",
      "    [batch 291]: seen 29100 examples at 61.7 eps, loss = 3.567\n",
      "    [batch 298]: seen 29800 examples at 61.9 eps, loss = 3.565\n",
      "    [batch 306]: seen 30600 examples at 62.1 eps, loss = 3.568\n",
      "    [batch 313]: seen 31300 examples at 62.2 eps, loss = 3.568\n",
      "    [batch 321]: seen 32100 examples at 62.4 eps, loss = 3.573\n",
      "    [batch 328]: seen 32800 examples at 62.5 eps, loss = 3.572\n",
      "    [batch 336]: seen 33600 examples at 62.7 eps, loss = 3.579\n",
      "    [batch 343]: seen 34300 examples at 62.8 eps, loss = 3.574\n",
      "    [batch 350]: seen 35000 examples at 62.9 eps, loss = 3.574\n",
      "    [batch 357]: seen 35700 examples at 63.1 eps, loss = 3.574\n",
      "    [batch 364]: seen 36400 examples at 63.2 eps, loss = 3.573\n",
      "    [batch 371]: seen 37100 examples at 63.3 eps, loss = 3.575\n",
      "    [batch 378]: seen 37800 examples at 63.4 eps, loss = 3.574\n",
      "    [batch 385]: seen 38500 examples at 63.5 eps, loss = 3.572\n",
      "    [batch 393]: seen 39300 examples at 63.6 eps, loss = 3.573\n",
      "    [batch 400]: seen 40000 examples at 63.7 eps, loss = 3.572\n",
      "    [batch 407]: seen 40700 examples at 63.8 eps, loss = 3.571\n",
      "    [batch 414]: seen 41400 examples at 63.9 eps, loss = 3.569\n",
      "    [batch 421]: seen 42100 examples at 64.0 eps, loss = 3.570\n",
      "    [batch 428]: seen 42800 examples at 64.1 eps, loss = 3.575\n",
      "    [batch 435]: seen 43500 examples at 64.2 eps, loss = 3.578\n",
      "    [batch 442]: seen 44200 examples at 64.3 eps, loss = 3.573\n",
      "    [batch 450]: seen 45000 examples at 64.4 eps, loss = 3.568\n",
      "    [batch 458]: seen 45800 examples at 64.5 eps, loss = 3.567\n",
      "    [batch 466]: seen 46600 examples at 64.6 eps, loss = 3.568\n",
      "    [batch 473]: seen 47300 examples at 64.6 eps, loss = 3.561\n",
      "    [batch 481]: seen 48100 examples at 64.7 eps, loss = 3.565\n",
      "    [batch 487]: seen 48700 examples at 64.5 eps, loss = 3.558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 492]: seen 49200 examples at 64.3 eps, loss = 3.560\n",
      "    [batch 499]: seen 49900 examples at 64.3 eps, loss = 3.563\n",
      "    [batch 507]: seen 50700 examples at 64.4 eps, loss = 3.566\n",
      "    [batch 514]: seen 51400 examples at 64.5 eps, loss = 3.566\n",
      "    [batch 521]: seen 52100 examples at 64.5 eps, loss = 3.564\n",
      "    [batch 528]: seen 52800 examples at 64.6 eps, loss = 3.569\n",
      "    [batch 535]: seen 53500 examples at 64.7 eps, loss = 3.570\n",
      "    [batch 542]: seen 54200 examples at 64.7 eps, loss = 3.572\n",
      "    [batch 549]: seen 54900 examples at 64.8 eps, loss = 3.573\n",
      "    [batch 557]: seen 55700 examples at 64.9 eps, loss = 3.572\n",
      "    [batch 564]: seen 56400 examples at 64.9 eps, loss = 3.570\n",
      "    [batch 571]: seen 57100 examples at 65.0 eps, loss = 3.574\n",
      "    [batch 579]: seen 57900 examples at 65.0 eps, loss = 3.572\n",
      "    [batch 586]: seen 58600 examples at 65.1 eps, loss = 3.569\n",
      "    [batch 593]: seen 59300 examples at 65.1 eps, loss = 3.570\n",
      "    [batch 600]: seen 60000 examples at 65.2 eps, loss = 3.575\n",
      "    [batch 607]: seen 60700 examples at 65.3 eps, loss = 3.570\n",
      "    [batch 614]: seen 61400 examples at 65.3 eps, loss = 3.572\n",
      "    [batch 621]: seen 62100 examples at 65.3 eps, loss = 3.568\n",
      "    [batch 629]: seen 62900 examples at 65.4 eps, loss = 3.566\n",
      "    [batch 637]: seen 63700 examples at 65.5 eps, loss = 3.569\n",
      "    [batch 645]: seen 64500 examples at 65.5 eps, loss = 3.565\n",
      "    [batch 652]: seen 65200 examples at 65.6 eps, loss = 3.564\n",
      "    [batch 660]: seen 66000 examples at 65.6 eps, loss = 3.562\n",
      "    [batch 667]: seen 66700 examples at 65.7 eps, loss = 3.563\n",
      "    [batch 674]: seen 67400 examples at 65.7 eps, loss = 3.564\n",
      "    [batch 682]: seen 68200 examples at 65.7 eps, loss = 3.565\n",
      "    [batch 689]: seen 68900 examples at 65.8 eps, loss = 3.565\n",
      "    [batch 696]: seen 69600 examples at 65.8 eps, loss = 3.569\n",
      "    [batch 704]: seen 70400 examples at 65.9 eps, loss = 3.569\n",
      "    [batch 711]: seen 71100 examples at 65.9 eps, loss = 3.569\n",
      "    [batch 718]: seen 71800 examples at 65.9 eps, loss = 3.567\n",
      "    [batch 725]: seen 72500 examples at 66.0 eps, loss = 3.564\n",
      "    [batch 732]: seen 73200 examples at 66.0 eps, loss = 3.561\n",
      "    [batch 740]: seen 74000 examples at 66.0 eps, loss = 3.563\n",
      "    [batch 747]: seen 74700 examples at 66.1 eps, loss = 3.563\n",
      "    [batch 754]: seen 75400 examples at 66.1 eps, loss = 3.562\n",
      "    [batch 761]: seen 76100 examples at 66.2 eps, loss = 3.566\n",
      "    [batch 768]: seen 76800 examples at 66.2 eps, loss = 3.567\n",
      "    [batch 775]: seen 77500 examples at 66.2 eps, loss = 3.565\n",
      "    [batch 783]: seen 78300 examples at 66.3 eps, loss = 3.560\n",
      "    [batch 790]: seen 79000 examples at 66.3 eps, loss = 3.561\n",
      "    [batch 797]: seen 79700 examples at 66.3 eps, loss = 3.563\n",
      "    [batch 805]: seen 80500 examples at 66.3 eps, loss = 3.570\n",
      "    [batch 812]: seen 81200 examples at 66.4 eps, loss = 3.564\n",
      "    [batch 820]: seen 82000 examples at 66.4 eps, loss = 3.568\n",
      "    [batch 828]: seen 82800 examples at 66.4 eps, loss = 3.569\n",
      "    [batch 836]: seen 83600 examples at 66.5 eps, loss = 3.565\n",
      "    [batch 843]: seen 84300 examples at 66.5 eps, loss = 3.568\n",
      "    [batch 850]: seen 85000 examples at 66.5 eps, loss = 3.571\n",
      "    [batch 857]: seen 85700 examples at 66.6 eps, loss = 3.564\n",
      "    [batch 864]: seen 86400 examples at 66.6 eps, loss = 3.564\n",
      "    [batch 871]: seen 87100 examples at 66.6 eps, loss = 3.569\n",
      "    [batch 878]: seen 87800 examples at 66.6 eps, loss = 3.569\n",
      "    [batch 886]: seen 88600 examples at 66.7 eps, loss = 3.571\n",
      "    [batch 893]: seen 89300 examples at 66.7 eps, loss = 3.569\n",
      "    [batch 900]: seen 90000 examples at 66.7 eps, loss = 3.567\n",
      "    [batch 907]: seen 90700 examples at 66.7 eps, loss = 3.568\n",
      "    [batch 914]: seen 91400 examples at 66.8 eps, loss = 3.569\n",
      "    [batch 921]: seen 92100 examples at 66.8 eps, loss = 3.562\n",
      "    [batch 928]: seen 92800 examples at 66.8 eps, loss = 3.564\n",
      "    [batch 936]: seen 93600 examples at 66.8 eps, loss = 3.564\n",
      "    [batch 943]: seen 94300 examples at 66.8 eps, loss = 3.562\n",
      "    [batch 950]: seen 95000 examples at 66.9 eps, loss = 3.565\n",
      "    [batch 957]: seen 95700 examples at 66.9 eps, loss = 3.563\n",
      "    [batch 964]: seen 96400 examples at 66.9 eps, loss = 3.561\n",
      "    [batch 971]: seen 97100 examples at 66.9 eps, loss = 3.559\n",
      "    [batch 975]: seen 97500 examples at 66.6 eps, loss = 3.557\n",
      "    [batch 978]: seen 97800 examples at 66.2 eps, loss = 3.554\n",
      "    [batch 983]: seen 98300 examples at 66.1 eps, loss = 3.559\n",
      "    [batch 990]: seen 99000 examples at 66.1 eps, loss = 3.557\n",
      "    [batch 997]: seen 99700 examples at 66.1 eps, loss = 3.561\n",
      "    [batch 1004]: seen 100400 examples at 66.1 eps, loss = 3.561\n",
      "    [batch 1011]: seen 101100 examples at 66.2 eps, loss = 3.560\n",
      "    [batch 1018]: seen 101800 examples at 66.2 eps, loss = 3.560\n",
      "    [batch 1025]: seen 102500 examples at 66.2 eps, loss = 3.559\n",
      "    [batch 1032]: seen 103200 examples at 66.2 eps, loss = 3.560\n",
      "    [batch 1039]: seen 103900 examples at 66.3 eps, loss = 3.563\n",
      "    [batch 1046]: seen 104600 examples at 66.3 eps, loss = 3.565\n",
      "    [batch 1053]: seen 105300 examples at 66.3 eps, loss = 3.568\n",
      "    [batch 1060]: seen 106000 examples at 66.3 eps, loss = 3.567\n",
      "    [batch 1067]: seen 106700 examples at 66.4 eps, loss = 3.563\n",
      "    [batch 1074]: seen 107400 examples at 66.4 eps, loss = 3.566\n",
      "    [batch 1081]: seen 108100 examples at 66.4 eps, loss = 3.564\n",
      "    [batch 1088]: seen 108800 examples at 66.4 eps, loss = 3.563\n",
      "    [batch 1095]: seen 109500 examples at 66.4 eps, loss = 3.558\n",
      "    [batch 1102]: seen 110200 examples at 66.5 eps, loss = 3.557\n",
      "    [batch 1109]: seen 110900 examples at 66.5 eps, loss = 3.559\n",
      "    [batch 1116]: seen 111600 examples at 66.5 eps, loss = 3.555\n",
      "    [batch 1119]: seen 111900 examples at 66.2 eps, loss = 3.551\n",
      "    [batch 1124]: seen 112400 examples at 66.1 eps, loss = 3.553\n",
      "    [batch 1131]: seen 113100 examples at 66.2 eps, loss = 3.556\n",
      "    [batch 1138]: seen 113800 examples at 66.2 eps, loss = 3.558\n",
      "    [batch 1145]: seen 114500 examples at 66.2 eps, loss = 3.558\n",
      "    [batch 1152]: seen 115200 examples at 66.2 eps, loss = 3.560\n",
      "    [batch 1159]: seen 115900 examples at 66.2 eps, loss = 3.557\n",
      "    [batch 1166]: seen 116600 examples at 66.3 eps, loss = 3.560\n",
      "    [batch 1173]: seen 117300 examples at 66.3 eps, loss = 3.558\n",
      "    [batch 1180]: seen 118000 examples at 66.3 eps, loss = 3.558\n",
      "    [batch 1187]: seen 118700 examples at 66.3 eps, loss = 3.557\n",
      "    [batch 1194]: seen 119400 examples at 66.3 eps, loss = 3.552\n",
      "    [batch 1199]: seen 119900 examples at 66.2 eps, loss = 3.550\n",
      "    [batch 1204]: seen 120400 examples at 66.1 eps, loss = 3.550\n",
      "    [batch 1211]: seen 121100 examples at 66.1 eps, loss = 3.552\n",
      "    [batch 1218]: seen 121800 examples at 66.1 eps, loss = 3.549\n",
      "    [batch 1221]: seen 122100 examples at 65.9 eps, loss = 3.548\n",
      "    [batch 1224]: seen 122400 examples at 65.7 eps, loss = 3.546\n",
      "    [batch 1231]: seen 123100 examples at 65.6 eps, loss = 3.546\n",
      "    [batch 1236]: seen 123600 examples at 65.5 eps, loss = 3.545\n",
      "    [batch 1239]: seen 123900 examples at 65.3 eps, loss = 3.544\n",
      "    [batch 1242]: seen 124200 examples at 65.1 eps, loss = 3.543\n",
      "    [batch 1245]: seen 124500 examples at 64.8 eps, loss = 3.542\n",
      "    [batch 1248]: seen 124800 examples at 64.5 eps, loss = 3.539\n",
      "    [batch 1253]: seen 125300 examples at 64.4 eps, loss = 3.543\n",
      "    [batch 1260]: seen 126000 examples at 64.4 eps, loss = 3.546\n",
      "    [batch 1267]: seen 126700 examples at 64.5 eps, loss = 3.546\n",
      "    [batch 1274]: seen 127400 examples at 64.5 eps, loss = 3.551\n",
      "    [batch 1281]: seen 128100 examples at 64.5 eps, loss = 3.560\n",
      "    [batch 1288]: seen 128800 examples at 64.5 eps, loss = 3.556\n",
      "    [batch 1296]: seen 129600 examples at 64.6 eps, loss = 3.558\n",
      "    [batch 1303]: seen 130300 examples at 64.6 eps, loss = 3.554\n",
      "    [batch 1311]: seen 131100 examples at 64.6 eps, loss = 3.549\n",
      "    [batch 1318]: seen 131800 examples at 64.7 eps, loss = 3.555\n",
      "    [batch 1325]: seen 132500 examples at 64.7 eps, loss = 3.557\n",
      "    [batch 1332]: seen 133200 examples at 64.7 eps, loss = 3.555\n",
      "    [batch 1339]: seen 133900 examples at 64.7 eps, loss = 3.558\n",
      "    [batch 1346]: seen 134600 examples at 64.8 eps, loss = 3.561\n",
      "    [batch 1353]: seen 135300 examples at 64.8 eps, loss = 3.557\n",
      "    [batch 1360]: seen 136000 examples at 64.8 eps, loss = 3.560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1367]: seen 136700 examples at 64.8 eps, loss = 3.561\n",
      "    [batch 1374]: seen 137400 examples at 64.9 eps, loss = 3.557\n",
      "    [batch 1382]: seen 138200 examples at 64.9 eps, loss = 3.555\n",
      "    [batch 1389]: seen 138900 examples at 64.9 eps, loss = 3.556\n",
      "    [batch 1396]: seen 139600 examples at 64.9 eps, loss = 3.555\n",
      "    [batch 1403]: seen 140300 examples at 64.9 eps, loss = 3.555\n",
      "    [batch 1410]: seen 141000 examples at 65.0 eps, loss = 3.559\n",
      "    [batch 1417]: seen 141700 examples at 65.0 eps, loss = 3.555\n",
      "    [batch 1424]: seen 142400 examples at 65.0 eps, loss = 3.554\n",
      "    [batch 1431]: seen 143100 examples at 65.0 eps, loss = 3.553\n",
      "    [batch 1438]: seen 143800 examples at 65.1 eps, loss = 3.550\n",
      "    [batch 1445]: seen 144500 examples at 65.1 eps, loss = 3.552\n",
      "    [batch 1452]: seen 145200 examples at 65.1 eps, loss = 3.551\n",
      "    [batch 1459]: seen 145900 examples at 65.1 eps, loss = 3.550\n",
      "    [batch 1466]: seen 146600 examples at 65.1 eps, loss = 3.547\n",
      "    [batch 1474]: seen 147400 examples at 65.2 eps, loss = 3.542\n",
      "    [batch 1482]: seen 148200 examples at 65.2 eps, loss = 3.542\n",
      "    [batch 1489]: seen 148900 examples at 65.2 eps, loss = 3.543\n",
      "    [batch 1497]: seen 149700 examples at 65.2 eps, loss = 3.542\n",
      "    [batch 1502]: seen 150200 examples at 65.1 eps, loss = 3.537\n",
      "    [batch 1507]: seen 150700 examples at 65.0 eps, loss = 3.537\n",
      "    [batch 1510]: seen 151000 examples at 64.8 eps, loss = 3.535\n",
      "    [batch 1515]: seen 151500 examples at 64.7 eps, loss = 3.533\n",
      "    [batch 1522]: seen 152200 examples at 64.8 eps, loss = 3.537\n",
      "    [batch 1529]: seen 152900 examples at 64.8 eps, loss = 3.537\n",
      "    [batch 1536]: seen 153600 examples at 64.8 eps, loss = 3.535\n",
      "    [batch 1544]: seen 154400 examples at 64.8 eps, loss = 3.536\n",
      "    [batch 1551]: seen 155100 examples at 64.8 eps, loss = 3.536\n",
      "    [batch 1556]: seen 155600 examples at 64.8 eps, loss = 3.534\n",
      "    [batch 1563]: seen 156300 examples at 64.8 eps, loss = 3.537\n",
      "    [batch 1571]: seen 157100 examples at 64.8 eps, loss = 3.538\n",
      "    [batch 1578]: seen 157800 examples at 64.8 eps, loss = 3.537\n",
      "    [batch 1585]: seen 158500 examples at 64.9 eps, loss = 3.539\n",
      "    [batch 1592]: seen 159200 examples at 64.9 eps, loss = 3.541\n",
      "    [batch 1599]: seen 159900 examples at 64.9 eps, loss = 3.545\n",
      "    [batch 1606]: seen 160600 examples at 64.9 eps, loss = 3.542\n",
      "    [batch 1613]: seen 161300 examples at 64.9 eps, loss = 3.544\n",
      "    [batch 1621]: seen 162100 examples at 65.0 eps, loss = 3.548\n",
      "    [batch 1628]: seen 162800 examples at 65.0 eps, loss = 3.544\n",
      "    [batch 1635]: seen 163500 examples at 65.0 eps, loss = 3.544\n",
      "    [batch 1642]: seen 164200 examples at 65.0 eps, loss = 3.543\n",
      "    [batch 1649]: seen 164900 examples at 65.0 eps, loss = 3.544\n",
      "    [batch 1656]: seen 165600 examples at 65.1 eps, loss = 3.547\n",
      "    [batch 1663]: seen 166300 examples at 65.1 eps, loss = 3.550\n",
      "    [batch 1670]: seen 167000 examples at 65.1 eps, loss = 3.551\n",
      "    [batch 1677]: seen 167700 examples at 65.1 eps, loss = 3.554\n",
      "    [batch 1684]: seen 168400 examples at 65.1 eps, loss = 3.558\n",
      "    [batch 1691]: seen 169100 examples at 65.1 eps, loss = 3.560\n",
      "    [batch 1698]: seen 169800 examples at 65.2 eps, loss = 3.557\n",
      "    [batch 1705]: seen 170500 examples at 65.2 eps, loss = 3.557\n",
      "    [batch 1712]: seen 171200 examples at 65.2 eps, loss = 3.559\n",
      "    [batch 1719]: seen 171900 examples at 65.2 eps, loss = 3.564\n",
      "    [batch 1726]: seen 172600 examples at 65.2 eps, loss = 3.560\n",
      "    [batch 1733]: seen 173300 examples at 65.3 eps, loss = 3.561\n",
      "    [batch 1740]: seen 174000 examples at 65.3 eps, loss = 3.564\n",
      "    [batch 1747]: seen 174700 examples at 65.3 eps, loss = 3.563\n",
      "    [batch 1754]: seen 175400 examples at 65.3 eps, loss = 3.562\n",
      "    [batch 1761]: seen 176100 examples at 65.3 eps, loss = 3.561\n",
      "    [batch 1769]: seen 176900 examples at 65.3 eps, loss = 3.554\n",
      "    [batch 1776]: seen 177600 examples at 65.4 eps, loss = 3.553\n",
      "    [batch 1783]: seen 178300 examples at 65.4 eps, loss = 3.550\n",
      "    [batch 1790]: seen 179000 examples at 65.4 eps, loss = 3.555\n",
      "    [batch 1797]: seen 179700 examples at 65.4 eps, loss = 3.556\n",
      "    [batch 1804]: seen 180400 examples at 65.4 eps, loss = 3.553\n",
      "    [batch 1811]: seen 181100 examples at 65.4 eps, loss = 3.548\n",
      "    [batch 1818]: seen 181800 examples at 65.5 eps, loss = 3.549\n",
      "    [batch 1825]: seen 182500 examples at 65.5 eps, loss = 3.550\n",
      "    [batch 1832]: seen 183200 examples at 65.5 eps, loss = 3.549\n",
      "    [batch 1839]: seen 183900 examples at 65.5 eps, loss = 3.543\n",
      "    [batch 1846]: seen 184600 examples at 65.5 eps, loss = 3.539\n",
      "    [batch 1853]: seen 185300 examples at 65.5 eps, loss = 3.541\n",
      "    [batch 1860]: seen 186000 examples at 65.6 eps, loss = 3.542\n",
      "    [batch 1867]: seen 186700 examples at 65.6 eps, loss = 3.542\n",
      "    [END] Training complete: Total examples : 187100; Total time: 0:47:33\n",
      "[EPOCH 15] Complete. Avg Loss: 3.5450947573841636; Best Loss: 3.5316903163915288\n",
      "[EPOCH 16] Starting training..\n",
      "    [batch 8]: seen 800 examples at 70.1 eps, loss = 3.541\n",
      "    [batch 16]: seen 1600 examples at 70.0 eps, loss = 3.539\n",
      "    [batch 23]: seen 2300 examples at 70.0 eps, loss = 3.538\n",
      "    [batch 30]: seen 3000 examples at 70.0 eps, loss = 3.534\n",
      "    [batch 38]: seen 3800 examples at 70.0 eps, loss = 3.535\n",
      "    [batch 46]: seen 4600 examples at 70.0 eps, loss = 3.537\n",
      "    [batch 54]: seen 5400 examples at 70.0 eps, loss = 3.541\n",
      "    [batch 61]: seen 6100 examples at 70.0 eps, loss = 3.539\n",
      "    [batch 68]: seen 6800 examples at 70.0 eps, loss = 3.543\n",
      "    [batch 76]: seen 7600 examples at 70.0 eps, loss = 3.544\n",
      "    [batch 84]: seen 8400 examples at 70.0 eps, loss = 3.540\n",
      "    [batch 91]: seen 9100 examples at 70.0 eps, loss = 3.540\n",
      "    [batch 98]: seen 9800 examples at 70.0 eps, loss = 3.537\n",
      "    [batch 105]: seen 10500 examples at 70.0 eps, loss = 3.538\n",
      "    [batch 112]: seen 11200 examples at 70.0 eps, loss = 3.534\n",
      "    [batch 119]: seen 11900 examples at 70.0 eps, loss = 3.536\n",
      "    [batch 126]: seen 12600 examples at 70.0 eps, loss = 3.536\n",
      "    [batch 133]: seen 13300 examples at 69.9 eps, loss = 3.535\n",
      "    [batch 141]: seen 14100 examples at 69.9 eps, loss = 3.534\n",
      "    [batch 148]: seen 14800 examples at 69.9 eps, loss = 3.532\n",
      "    [batch 155]: seen 15500 examples at 69.9 eps, loss = 3.534\n",
      "    [batch 162]: seen 16200 examples at 69.9 eps, loss = 3.535\n",
      "    [batch 170]: seen 17000 examples at 69.9 eps, loss = 3.536\n",
      "    [batch 178]: seen 17800 examples at 69.0 eps, loss = 3.531\n",
      "    [batch 185]: seen 18500 examples at 69.1 eps, loss = 3.535\n",
      "    [batch 192]: seen 19200 examples at 69.1 eps, loss = 3.531\n",
      "    [batch 195]: seen 19500 examples at 67.5 eps, loss = 3.529\n",
      "    [batch 202]: seen 20200 examples at 67.6 eps, loss = 3.534\n",
      "    [batch 209]: seen 20900 examples at 67.7 eps, loss = 3.533\n",
      "    [batch 216]: seen 21600 examples at 67.7 eps, loss = 3.535\n",
      "    [batch 223]: seen 22300 examples at 67.8 eps, loss = 3.537\n",
      "    [batch 231]: seen 23100 examples at 67.9 eps, loss = 3.537\n",
      "    [batch 238]: seen 23800 examples at 67.9 eps, loss = 3.536\n",
      "    [batch 245]: seen 24500 examples at 68.0 eps, loss = 3.536\n",
      "    [batch 252]: seen 25200 examples at 68.0 eps, loss = 3.536\n",
      "    [batch 259]: seen 25900 examples at 68.1 eps, loss = 3.530\n",
      "    [batch 264]: seen 26400 examples at 67.5 eps, loss = 3.530\n",
      "    [batch 270]: seen 27000 examples at 67.0 eps, loss = 3.528\n",
      "    [batch 275]: seen 27500 examples at 66.5 eps, loss = 3.531\n",
      "    [batch 282]: seen 28200 examples at 66.6 eps, loss = 3.530\n",
      "    [batch 287]: seen 28700 examples at 66.2 eps, loss = 3.526\n",
      "    [batch 294]: seen 29400 examples at 66.3 eps, loss = 3.532\n",
      "    [batch 301]: seen 30100 examples at 66.3 eps, loss = 3.533\n",
      "    [batch 308]: seen 30800 examples at 66.4 eps, loss = 3.538\n",
      "    [batch 315]: seen 31500 examples at 66.5 eps, loss = 3.542\n",
      "    [batch 322]: seen 32200 examples at 66.6 eps, loss = 3.541\n",
      "    [batch 329]: seen 32900 examples at 66.6 eps, loss = 3.539\n",
      "    [batch 336]: seen 33600 examples at 66.7 eps, loss = 3.541\n",
      "    [batch 343]: seen 34300 examples at 66.7 eps, loss = 3.539\n",
      "    [batch 350]: seen 35000 examples at 66.8 eps, loss = 3.541\n",
      "    [batch 357]: seen 35700 examples at 66.8 eps, loss = 3.537\n",
      "    [batch 364]: seen 36400 examples at 66.9 eps, loss = 3.536\n",
      "    [batch 371]: seen 37100 examples at 67.0 eps, loss = 3.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 378]: seen 37800 examples at 67.0 eps, loss = 3.536\n",
      "    [batch 385]: seen 38500 examples at 67.1 eps, loss = 3.536\n",
      "    [batch 392]: seen 39200 examples at 67.1 eps, loss = 3.534\n",
      "    [batch 399]: seen 39900 examples at 67.2 eps, loss = 3.534\n",
      "    [batch 406]: seen 40600 examples at 67.2 eps, loss = 3.535\n",
      "    [batch 413]: seen 41300 examples at 67.2 eps, loss = 3.533\n",
      "    [batch 421]: seen 42100 examples at 67.3 eps, loss = 3.533\n",
      "    [batch 428]: seen 42800 examples at 67.3 eps, loss = 3.530\n",
      "    [batch 435]: seen 43500 examples at 67.4 eps, loss = 3.530\n",
      "    [batch 443]: seen 44300 examples at 67.4 eps, loss = 3.529\n",
      "    [batch 450]: seen 45000 examples at 67.5 eps, loss = 3.530\n",
      "    [batch 455]: seen 45500 examples at 67.2 eps, loss = 3.524\n",
      "    [batch 462]: seen 46200 examples at 67.2 eps, loss = 3.526\n",
      "    [batch 464]: seen 46400 examples at 66.5 eps, loss = 3.522\n",
      "    [batch 467]: seen 46700 examples at 65.6 eps, loss = 3.520\n",
      "    [batch 472]: seen 47200 examples at 65.0 eps, loss = 3.518\n",
      "    [batch 477]: seen 47700 examples at 64.8 eps, loss = 3.519\n",
      "    [batch 484]: seen 48400 examples at 64.8 eps, loss = 3.523\n",
      "    [batch 491]: seen 49100 examples at 64.9 eps, loss = 3.527\n",
      "    [batch 498]: seen 49800 examples at 65.0 eps, loss = 3.525\n",
      "    [batch 505]: seen 50500 examples at 65.0 eps, loss = 3.525\n",
      "    [batch 513]: seen 51300 examples at 65.1 eps, loss = 3.521\n",
      "    [batch 521]: seen 52100 examples at 65.2 eps, loss = 3.522\n",
      "    [batch 528]: seen 52800 examples at 65.2 eps, loss = 3.521\n",
      "    [batch 535]: seen 53500 examples at 65.3 eps, loss = 3.522\n",
      "    [batch 543]: seen 54300 examples at 65.3 eps, loss = 3.524\n",
      "    [batch 550]: seen 55000 examples at 65.4 eps, loss = 3.524\n",
      "    [batch 557]: seen 55700 examples at 65.4 eps, loss = 3.518\n",
      "    [batch 565]: seen 56500 examples at 65.5 eps, loss = 3.522\n",
      "    [batch 572]: seen 57200 examples at 65.6 eps, loss = 3.526\n",
      "    [batch 580]: seen 58000 examples at 65.6 eps, loss = 3.528\n",
      "    [batch 587]: seen 58700 examples at 65.7 eps, loss = 3.533\n",
      "    [batch 594]: seen 59400 examples at 65.7 eps, loss = 3.536\n",
      "    [batch 601]: seen 60100 examples at 65.8 eps, loss = 3.534\n",
      "    [batch 608]: seen 60800 examples at 65.8 eps, loss = 3.534\n",
      "    [batch 615]: seen 61500 examples at 65.8 eps, loss = 3.531\n",
      "    [batch 622]: seen 62200 examples at 65.9 eps, loss = 3.531\n",
      "    [batch 629]: seen 62900 examples at 65.9 eps, loss = 3.527\n",
      "    [batch 636]: seen 63600 examples at 66.0 eps, loss = 3.525\n",
      "    [batch 643]: seen 64300 examples at 66.0 eps, loss = 3.525\n",
      "    [batch 650]: seen 65000 examples at 66.0 eps, loss = 3.526\n",
      "    [batch 657]: seen 65700 examples at 66.1 eps, loss = 3.527\n",
      "    [batch 665]: seen 66500 examples at 66.1 eps, loss = 3.521\n",
      "    [batch 672]: seen 67200 examples at 66.2 eps, loss = 3.517\n",
      "    [batch 676]: seen 67600 examples at 65.8 eps, loss = 3.515\n",
      "    [batch 679]: seen 67900 examples at 65.4 eps, loss = 3.514\n",
      "    [batch 684]: seen 68400 examples at 65.2 eps, loss = 3.514\n",
      "    [batch 691]: seen 69100 examples at 65.2 eps, loss = 3.513\n",
      "    [batch 696]: seen 69600 examples at 64.8 eps, loss = 3.512\n",
      "    [batch 703]: seen 70300 examples at 64.9 eps, loss = 3.513\n",
      "    [batch 708]: seen 70800 examples at 64.7 eps, loss = 3.513\n",
      "    [batch 713]: seen 71300 examples at 64.6 eps, loss = 3.509\n",
      "    [batch 716]: seen 71600 examples at 64.2 eps, loss = 3.507\n",
      "    [batch 723]: seen 72300 examples at 64.2 eps, loss = 3.509\n",
      "    [batch 730]: seen 73000 examples at 64.3 eps, loss = 3.508\n",
      "    [batch 737]: seen 73700 examples at 64.3 eps, loss = 3.512\n",
      "    [batch 744]: seen 74400 examples at 64.4 eps, loss = 3.518\n",
      "    [batch 751]: seen 75100 examples at 64.4 eps, loss = 3.519\n",
      "    [batch 758]: seen 75800 examples at 64.5 eps, loss = 3.517\n",
      "    [batch 765]: seen 76500 examples at 64.5 eps, loss = 3.514\n",
      "    [batch 772]: seen 77200 examples at 64.6 eps, loss = 3.514\n",
      "    [batch 779]: seen 77900 examples at 64.6 eps, loss = 3.510\n",
      "    [batch 786]: seen 78600 examples at 64.6 eps, loss = 3.510\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-25872\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-25872\n",
      "    [batch 792]: seen 79200 examples at 64.6 eps, loss = 3.510\n",
      "    [batch 799]: seen 79900 examples at 64.6 eps, loss = 3.516\n",
      "    [batch 806]: seen 80600 examples at 64.7 eps, loss = 3.518\n",
      "    [batch 813]: seen 81300 examples at 64.7 eps, loss = 3.523\n",
      "    [batch 820]: seen 82000 examples at 64.8 eps, loss = 3.523\n",
      "    [batch 827]: seen 82700 examples at 64.8 eps, loss = 3.523\n",
      "    [batch 834]: seen 83400 examples at 64.8 eps, loss = 3.528\n",
      "    [batch 841]: seen 84100 examples at 64.9 eps, loss = 3.531\n",
      "    [batch 849]: seen 84900 examples at 64.9 eps, loss = 3.535\n",
      "    [batch 856]: seen 85600 examples at 65.0 eps, loss = 3.534\n",
      "    [batch 863]: seen 86300 examples at 65.0 eps, loss = 3.535\n",
      "    [batch 871]: seen 87100 examples at 65.0 eps, loss = 3.534\n",
      "    [batch 878]: seen 87800 examples at 65.1 eps, loss = 3.535\n",
      "    [batch 885]: seen 88500 examples at 65.1 eps, loss = 3.538\n",
      "    [batch 892]: seen 89200 examples at 65.1 eps, loss = 3.540\n",
      "    [batch 899]: seen 89900 examples at 65.2 eps, loss = 3.543\n",
      "    [batch 906]: seen 90600 examples at 65.2 eps, loss = 3.542\n",
      "    [batch 914]: seen 91400 examples at 65.3 eps, loss = 3.547\n",
      "    [batch 921]: seen 92100 examples at 65.3 eps, loss = 3.550\n",
      "    [batch 928]: seen 92800 examples at 65.3 eps, loss = 3.547\n",
      "    [batch 935]: seen 93500 examples at 65.3 eps, loss = 3.541\n",
      "    [batch 942]: seen 94200 examples at 65.4 eps, loss = 3.543\n",
      "    [batch 949]: seen 94900 examples at 65.4 eps, loss = 3.540\n",
      "    [batch 956]: seen 95600 examples at 65.4 eps, loss = 3.537\n",
      "    [batch 963]: seen 96300 examples at 65.5 eps, loss = 3.540\n",
      "    [batch 970]: seen 97000 examples at 65.5 eps, loss = 3.544\n",
      "    [batch 977]: seen 97700 examples at 65.5 eps, loss = 3.541\n",
      "    [batch 984]: seen 98400 examples at 65.6 eps, loss = 3.543\n",
      "    [batch 992]: seen 99200 examples at 65.6 eps, loss = 3.540\n",
      "    [batch 999]: seen 99900 examples at 65.6 eps, loss = 3.539\n",
      "    [batch 1006]: seen 100600 examples at 65.7 eps, loss = 3.541\n",
      "    [batch 1014]: seen 101400 examples at 65.7 eps, loss = 3.542\n",
      "    [batch 1021]: seen 102100 examples at 65.7 eps, loss = 3.547\n",
      "    [batch 1028]: seen 102800 examples at 65.7 eps, loss = 3.541\n",
      "    [batch 1035]: seen 103500 examples at 65.8 eps, loss = 3.543\n",
      "    [batch 1042]: seen 104200 examples at 65.8 eps, loss = 3.544\n",
      "    [batch 1049]: seen 104900 examples at 65.8 eps, loss = 3.539\n",
      "    [batch 1056]: seen 105600 examples at 65.8 eps, loss = 3.536\n",
      "    [batch 1063]: seen 106300 examples at 65.9 eps, loss = 3.534\n",
      "    [batch 1070]: seen 107000 examples at 65.9 eps, loss = 3.529\n",
      "    [batch 1077]: seen 107700 examples at 65.9 eps, loss = 3.529\n",
      "    [batch 1085]: seen 108500 examples at 65.9 eps, loss = 3.525\n",
      "    [batch 1092]: seen 109200 examples at 66.0 eps, loss = 3.526\n",
      "    [batch 1100]: seen 110000 examples at 66.0 eps, loss = 3.530\n",
      "    [batch 1107]: seen 110700 examples at 66.0 eps, loss = 3.528\n",
      "    [batch 1114]: seen 111400 examples at 66.0 eps, loss = 3.525\n",
      "    [batch 1121]: seen 112100 examples at 66.1 eps, loss = 3.525\n",
      "    [batch 1128]: seen 112800 examples at 66.1 eps, loss = 3.534\n",
      "    [batch 1135]: seen 113500 examples at 66.1 eps, loss = 3.531\n",
      "    [batch 1142]: seen 114200 examples at 66.1 eps, loss = 3.535\n",
      "    [batch 1149]: seen 114900 examples at 66.2 eps, loss = 3.534\n",
      "    [batch 1156]: seen 115600 examples at 66.2 eps, loss = 3.533\n",
      "    [batch 1164]: seen 116400 examples at 66.2 eps, loss = 3.535\n",
      "    [batch 1171]: seen 117100 examples at 66.2 eps, loss = 3.539\n",
      "    [batch 1178]: seen 117800 examples at 66.2 eps, loss = 3.537\n",
      "    [batch 1186]: seen 118600 examples at 66.3 eps, loss = 3.532\n",
      "    [batch 1193]: seen 119300 examples at 66.3 eps, loss = 3.529\n",
      "    [batch 1201]: seen 120100 examples at 66.3 eps, loss = 3.529\n",
      "    [batch 1208]: seen 120800 examples at 66.3 eps, loss = 3.518\n",
      "    [batch 1215]: seen 121500 examples at 66.3 eps, loss = 3.522\n",
      "    [batch 1222]: seen 122200 examples at 66.4 eps, loss = 3.524\n",
      "    [batch 1229]: seen 122900 examples at 66.4 eps, loss = 3.520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1236]: seen 123600 examples at 66.4 eps, loss = 3.520\n",
      "    [batch 1243]: seen 124300 examples at 66.4 eps, loss = 3.525\n",
      "    [batch 1250]: seen 125000 examples at 66.4 eps, loss = 3.529\n",
      "    [batch 1257]: seen 125700 examples at 66.5 eps, loss = 3.527\n",
      "    [batch 1264]: seen 126400 examples at 66.5 eps, loss = 3.533\n",
      "    [batch 1271]: seen 127100 examples at 66.5 eps, loss = 3.534\n",
      "    [batch 1278]: seen 127800 examples at 66.5 eps, loss = 3.536\n",
      "    [batch 1285]: seen 128500 examples at 66.5 eps, loss = 3.530\n",
      "    [batch 1293]: seen 129300 examples at 66.6 eps, loss = 3.533\n",
      "    [batch 1300]: seen 130000 examples at 66.6 eps, loss = 3.536\n",
      "    [batch 1307]: seen 130700 examples at 66.6 eps, loss = 3.537\n",
      "    [batch 1314]: seen 131400 examples at 66.6 eps, loss = 3.531\n",
      "    [batch 1321]: seen 132100 examples at 66.6 eps, loss = 3.532\n",
      "    [batch 1329]: seen 132900 examples at 66.6 eps, loss = 3.530\n",
      "    [batch 1336]: seen 133600 examples at 66.7 eps, loss = 3.528\n",
      "    [batch 1343]: seen 134300 examples at 66.7 eps, loss = 3.528\n",
      "    [batch 1350]: seen 135000 examples at 66.7 eps, loss = 3.527\n",
      "    [batch 1357]: seen 135700 examples at 66.7 eps, loss = 3.526\n",
      "    [batch 1364]: seen 136400 examples at 66.7 eps, loss = 3.524\n",
      "    [batch 1371]: seen 137100 examples at 66.7 eps, loss = 3.525\n",
      "    [batch 1379]: seen 137900 examples at 66.7 eps, loss = 3.528\n",
      "    [batch 1387]: seen 138700 examples at 66.8 eps, loss = 3.528\n",
      "    [batch 1394]: seen 139400 examples at 66.8 eps, loss = 3.524\n",
      "    [batch 1401]: seen 140100 examples at 66.8 eps, loss = 3.520\n",
      "    [batch 1408]: seen 140800 examples at 66.8 eps, loss = 3.522\n",
      "    [batch 1415]: seen 141500 examples at 66.8 eps, loss = 3.527\n",
      "    [batch 1422]: seen 142200 examples at 66.8 eps, loss = 3.521\n",
      "    [batch 1429]: seen 142900 examples at 66.9 eps, loss = 3.521\n",
      "    [batch 1436]: seen 143600 examples at 66.9 eps, loss = 3.522\n",
      "    [batch 1443]: seen 144300 examples at 66.9 eps, loss = 3.522\n",
      "    [batch 1450]: seen 145000 examples at 66.9 eps, loss = 3.523\n",
      "    [batch 1457]: seen 145700 examples at 66.9 eps, loss = 3.524\n",
      "    [batch 1464]: seen 146400 examples at 66.9 eps, loss = 3.520\n",
      "    [batch 1471]: seen 147100 examples at 66.9 eps, loss = 3.522\n",
      "    [batch 1478]: seen 147800 examples at 66.9 eps, loss = 3.524\n",
      "    [batch 1485]: seen 148500 examples at 67.0 eps, loss = 3.526\n",
      "    [batch 1492]: seen 149200 examples at 67.0 eps, loss = 3.520\n",
      "    [batch 1500]: seen 150000 examples at 67.0 eps, loss = 3.524\n",
      "    [batch 1508]: seen 150800 examples at 67.0 eps, loss = 3.528\n",
      "    [batch 1515]: seen 151500 examples at 67.0 eps, loss = 3.529\n",
      "    [batch 1522]: seen 152200 examples at 67.0 eps, loss = 3.526\n",
      "    [batch 1530]: seen 153000 examples at 67.0 eps, loss = 3.528\n",
      "    [batch 1537]: seen 153700 examples at 67.1 eps, loss = 3.528\n",
      "    [batch 1544]: seen 154400 examples at 67.1 eps, loss = 3.525\n",
      "    [batch 1551]: seen 155100 examples at 67.1 eps, loss = 3.527\n",
      "    [batch 1558]: seen 155800 examples at 67.1 eps, loss = 3.534\n",
      "    [batch 1565]: seen 156500 examples at 67.1 eps, loss = 3.535\n",
      "    [batch 1572]: seen 157200 examples at 67.1 eps, loss = 3.531\n",
      "    [batch 1579]: seen 157900 examples at 67.1 eps, loss = 3.526\n",
      "    [batch 1586]: seen 158600 examples at 67.1 eps, loss = 3.526\n",
      "    [batch 1593]: seen 159300 examples at 67.2 eps, loss = 3.522\n",
      "    [batch 1600]: seen 160000 examples at 67.2 eps, loss = 3.521\n",
      "    [batch 1608]: seen 160800 examples at 67.2 eps, loss = 3.514\n",
      "    [batch 1615]: seen 161500 examples at 67.2 eps, loss = 3.516\n",
      "    [batch 1622]: seen 162200 examples at 67.2 eps, loss = 3.517\n",
      "    [batch 1629]: seen 162900 examples at 67.2 eps, loss = 3.516\n",
      "    [batch 1636]: seen 163600 examples at 67.2 eps, loss = 3.516\n",
      "    [batch 1643]: seen 164300 examples at 67.2 eps, loss = 3.513\n",
      "    [batch 1650]: seen 165000 examples at 67.2 eps, loss = 3.511\n",
      "    [batch 1657]: seen 165700 examples at 67.3 eps, loss = 3.511\n",
      "    [batch 1664]: seen 166400 examples at 67.3 eps, loss = 3.511\n",
      "    [batch 1672]: seen 167200 examples at 67.3 eps, loss = 3.512\n",
      "    [batch 1679]: seen 167900 examples at 67.3 eps, loss = 3.514\n",
      "    [batch 1686]: seen 168600 examples at 67.3 eps, loss = 3.515\n",
      "    [batch 1693]: seen 169300 examples at 67.3 eps, loss = 3.512\n",
      "    [batch 1700]: seen 170000 examples at 67.3 eps, loss = 3.510\n",
      "    [batch 1707]: seen 170700 examples at 67.3 eps, loss = 3.509\n",
      "    [batch 1714]: seen 171400 examples at 67.3 eps, loss = 3.511\n",
      "    [batch 1721]: seen 172100 examples at 67.4 eps, loss = 3.512\n",
      "    [batch 1728]: seen 172800 examples at 67.4 eps, loss = 3.516\n",
      "    [batch 1735]: seen 173500 examples at 67.4 eps, loss = 3.517\n",
      "    [batch 1742]: seen 174200 examples at 67.4 eps, loss = 3.514\n",
      "    [batch 1749]: seen 174900 examples at 67.4 eps, loss = 3.509\n",
      "    [batch 1754]: seen 175400 examples at 67.3 eps, loss = 3.507\n",
      "    [batch 1757]: seen 175700 examples at 67.1 eps, loss = 3.506\n",
      "    [batch 1764]: seen 176400 examples at 67.2 eps, loss = 3.508\n",
      "    [batch 1771]: seen 177100 examples at 67.2 eps, loss = 3.508\n",
      "    [batch 1776]: seen 177600 examples at 67.1 eps, loss = 3.505\n",
      "    [batch 1781]: seen 178100 examples at 67.0 eps, loss = 3.505\n",
      "    [batch 1784]: seen 178400 examples at 66.8 eps, loss = 3.502\n",
      "    [batch 1789]: seen 178900 examples at 66.7 eps, loss = 3.501\n",
      "    [batch 1794]: seen 179400 examples at 66.6 eps, loss = 3.500\n",
      "    [batch 1801]: seen 180100 examples at 66.6 eps, loss = 3.502\n",
      "    [batch 1808]: seen 180800 examples at 66.6 eps, loss = 3.508\n",
      "    [batch 1815]: seen 181500 examples at 66.7 eps, loss = 3.508\n",
      "    [batch 1822]: seen 182200 examples at 66.7 eps, loss = 3.509\n",
      "    [batch 1829]: seen 182900 examples at 66.7 eps, loss = 3.509\n",
      "    [batch 1836]: seen 183600 examples at 66.7 eps, loss = 3.506\n",
      "    [batch 1843]: seen 184300 examples at 66.7 eps, loss = 3.504\n",
      "    [batch 1850]: seen 185000 examples at 66.7 eps, loss = 3.504\n",
      "    [batch 1857]: seen 185700 examples at 66.7 eps, loss = 3.509\n",
      "    [batch 1864]: seen 186400 examples at 66.7 eps, loss = 3.507\n",
      "    [batch 1871]: seen 187100 examples at 66.7 eps, loss = 3.511\n",
      "    [END] Training complete: Total examples : 187100; Total time: 0:46:43\n",
      "[EPOCH 16] Complete. Avg Loss: 3.510656529213595; Best Loss: 3.500422455630612\n",
      "[EPOCH 17] Starting training..\n",
      "    [batch 7]: seen 700 examples at 69.9 eps, loss = 3.512\n",
      "    [batch 14]: seen 1400 examples at 69.9 eps, loss = 3.511\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b68c7ecd3e99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurr_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-17e3ed376236>\u001b[0m in \u001b[0;36mtrain_continue\u001b[0;34m(hps, epochs, train_step, curr_best, best_loss, avg_loss, restore, epoch_start)\u001b[0m\n\u001b[1;32m     36\u001b[0m                                                             \u001b[0mhps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                                             \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                                                             avg_loss)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcurr_best\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266/final_0/W266_Final/model_4/training_util.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(lm, session, batches, summary_writer, train_dir, train_step, saver, hps, best_loss, avg_loss)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunTrainStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266/final_0/W266_Final/model_4/training_util.py\u001b[0m in \u001b[0;36mrunTrainStep\u001b[0;34m(lm, session, batch)\u001b[0m\n\u001b[1;32m     87\u001b[0m     }\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_loss = 3.694\n",
    "best_loss = 3.694\n",
    "curr_best = best_loss\n",
    "train_step = 18059\n",
    "epochs = 15\n",
    "restore = True\n",
    "epoch_start = 11\n",
    "\n",
    "train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train session 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 252\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 110\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 2 1/2 76\n",
      "\n",
      "\n",
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: 16:03\n",
      "INFO:tensorflow:Fetching data..\n",
      "INFO:tensorflow:Creating batches..\n",
      "INFO:tensorflow:[TOTAL Batches]  : 2808\n",
      "INFO:tensorflow:[TOTAL Examples] : 280778\n",
      "INFO:tensorflow:Creating batches..COMPLETE\n",
      "INFO:tensorflow:Building core graph...\n",
      "INFO:tensorflow:Adding attention_decoder timestep 0 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 1 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 2 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 3 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 4 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 5 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 6 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 7 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 8 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 9 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 10 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 11 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 12 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 13 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 14 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 15 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 16 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 17 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 18 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 19 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 20 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 21 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 22 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 23 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 24 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 25 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 26 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 27 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 28 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 29 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 30 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 31 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 32 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 33 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 34 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 35 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 36 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 37 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 38 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 39 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 40 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 41 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 42 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 43 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 44 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 45 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 46 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 47 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 48 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 49 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 50 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 51 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 52 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 53 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 54 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 55 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 56 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 57 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 58 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 59 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 60 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 61 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 62 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 63 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 64 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 65 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 66 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 67 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 68 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 69 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 70 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 71 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 72 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 73 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 74 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 75 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 76 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 77 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 78 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 79 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 80 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 81 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 82 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 83 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 84 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 85 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 86 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 87 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 88 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 89 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 90 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 91 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 92 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 93 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 94 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 95 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 96 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 97 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 98 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 99 of 100\n",
      "INFO:tensorflow:Building projection graph...\n",
      "INFO:tensorflow:Building projection graph...COMPLETE\n",
      "INFO:tensorflow:Building Loss graph...\n",
      "INFO:tensorflow:Building Loss graph...COMPLETE\n",
      "INFO:tensorflow:Building core graph...COMPLETE\n",
      "INFO:tensorflow:Building train graph...\n",
      "INFO:tensorflow:Building train graph...COMPLETE\n",
      "INFO:tensorflow:Building summary graph...\n",
      "INFO:tensorflow:Building summary graph...COMPLETE\n",
      "WARNING:tensorflow:From <ipython-input-5-17e3ed376236>:18: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-26879\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-26879\n",
      "[EPOCH 17] Starting training..\n",
      "    [batch 1]: seen 100 examples : 3.4 eps, Loss: 3.659, Avg loss: 3.601, Best loss: 3.600, cov loss: 0.176\n",
      "    [batch 4]: seen 400 examples : 9.5 eps, Loss: 3.557, Avg loss: 3.599, Best loss: 3.599, cov loss: 0.176\n",
      "    [batch 6]: seen 600 examples : 11.5 eps, Loss: 3.401, Avg loss: 3.596, Best loss: 3.596, cov loss: 0.164\n",
      "    [batch 8]: seen 800 examples : 12.8 eps, Loss: 3.244, Avg loss: 3.591, Best loss: 3.591, cov loss: 0.165\n",
      "    [batch 10]: seen 1000 examples : 13.8 eps, Loss: 3.493, Avg loss: 3.588, Best loss: 3.588, cov loss: 0.154\n",
      "    [batch 12]: seen 1200 examples : 14.5 eps, Loss: 3.424, Avg loss: 3.586, Best loss: 3.586, cov loss: 0.177\n",
      "    [batch 14]: seen 1400 examples : 15.1 eps, Loss: 3.464, Avg loss: 3.583, Best loss: 3.583, cov loss: 0.187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 17]: seen 1700 examples : 16.2 eps, Loss: 3.487, Avg loss: 3.581, Best loss: 3.581, cov loss: 0.189\n",
      "    [batch 20]: seen 2000 examples : 17.2 eps, Loss: 3.428, Avg loss: 3.577, Best loss: 3.577, cov loss: 0.170\n",
      "    [batch 24]: seen 2400 examples : 18.4 eps, Loss: 3.412, Avg loss: 3.575, Best loss: 3.575, cov loss: 0.200\n",
      "    [batch 26]: seen 2600 examples : 18.5 eps, Loss: 3.556, Avg loss: 3.574, Best loss: 3.574, cov loss: 0.188\n",
      "    [batch 33]: seen 3300 examples : 21.9 eps, Loss: 3.484, Avg loss: 3.575, Best loss: 3.574, cov loss: 0.178\n",
      "    [batch 40]: seen 4000 examples : 24.3 eps, Loss: 3.362, Avg loss: 3.572, Best loss: 3.572, cov loss: 0.173\n",
      "    [batch 47]: seen 4700 examples : 26.9 eps, Loss: 3.615, Avg loss: 3.574, Best loss: 3.572, cov loss: 0.187\n",
      "    [batch 52]: seen 5200 examples : 28.0 eps, Loss: 3.793, Avg loss: 3.574, Best loss: 3.571, cov loss: 0.198\n",
      "    [batch 59]: seen 5900 examples : 30.1 eps, Loss: 3.614, Avg loss: 3.577, Best loss: 3.571, cov loss: 0.190\n",
      "    [batch 66]: seen 6600 examples : 32.0 eps, Loss: 3.614, Avg loss: 3.573, Best loss: 3.571, cov loss: 0.171\n",
      "    [batch 70]: seen 7000 examples : 31.9 eps, Loss: 3.433, Avg loss: 3.567, Best loss: 3.567, cov loss: 0.169\n",
      "    [batch 75]: seen 7500 examples : 32.1 eps, Loss: 3.327, Avg loss: 3.564, Best loss: 3.564, cov loss: 0.169\n",
      "    [batch 80]: seen 8000 examples : 32.2 eps, Loss: 3.491, Avg loss: 3.563, Best loss: 3.563, cov loss: 0.189\n",
      "    [batch 87]: seen 8700 examples : 33.1 eps, Loss: 3.423, Avg loss: 3.562, Best loss: 3.562, cov loss: 0.175\n",
      "    [batch 90]: seen 9000 examples : 32.8 eps, Loss: 3.385, Avg loss: 3.560, Best loss: 3.560, cov loss: 0.172\n",
      "    [batch 94]: seen 9400 examples : 32.7 eps, Loss: 3.524, Avg loss: 3.559, Best loss: 3.559, cov loss: 0.197\n",
      "    [batch 97]: seen 9700 examples : 32.5 eps, Loss: 3.150, Avg loss: 3.553, Best loss: 3.553, cov loss: 0.149\n",
      "    [batch 101]: seen 10100 examples : 32.3 eps, Loss: 3.551, Avg loss: 3.552, Best loss: 3.552, cov loss: 0.161\n",
      "    [batch 106]: seen 10600 examples : 32.8 eps, Loss: 3.732, Avg loss: 3.554, Best loss: 3.552, cov loss: 0.204\n",
      "    [batch 110]: seen 11000 examples : 32.7 eps, Loss: 3.430, Avg loss: 3.550, Best loss: 3.550, cov loss: 0.169\n",
      "    [batch 114]: seen 11400 examples : 32.6 eps, Loss: 3.344, Avg loss: 3.548, Best loss: 3.548, cov loss: 0.174\n",
      "    [batch 119]: seen 11900 examples : 33.0 eps, Loss: 3.698, Avg loss: 3.551, Best loss: 3.548, cov loss: 0.183\n",
      "    [batch 126]: seen 12600 examples : 34.0 eps, Loss: 3.557, Avg loss: 3.553, Best loss: 3.548, cov loss: 0.176\n",
      "    [batch 130]: seen 13000 examples : 34.1 eps, Loss: 3.664, Avg loss: 3.549, Best loss: 3.547, cov loss: 0.175\n",
      "    [batch 137]: seen 13700 examples : 34.7 eps, Loss: 3.498, Avg loss: 3.547, Best loss: 3.547, cov loss: 0.178\n",
      "    [batch 144]: seen 14400 examples : 35.5 eps, Loss: 3.520, Avg loss: 3.549, Best loss: 3.547, cov loss: 0.174\n",
      "    [batch 149]: seen 14900 examples : 35.5 eps, Loss: 3.460, Avg loss: 3.543, Best loss: 3.543, cov loss: 0.183\n",
      "    [batch 154]: seen 15400 examples : 35.8 eps, Loss: 3.533, Avg loss: 3.546, Best loss: 3.542, cov loss: 0.192\n",
      "    [batch 161]: seen 16100 examples : 36.5 eps, Loss: 3.292, Avg loss: 3.550, Best loss: 3.542, cov loss: 0.170\n",
      "    [batch 168]: seen 16800 examples : 37.2 eps, Loss: 3.584, Avg loss: 3.553, Best loss: 3.542, cov loss: 0.181\n",
      "    [batch 175]: seen 17500 examples : 37.6 eps, Loss: 3.329, Avg loss: 3.542, Best loss: 3.542, cov loss: 0.159\n",
      "    [batch 179]: seen 17900 examples : 37.4 eps, Loss: 3.387, Avg loss: 3.540, Best loss: 3.540, cov loss: 0.166\n",
      "    [batch 181]: seen 18100 examples : 37.1 eps, Loss: 3.360, Avg loss: 3.537, Best loss: 3.537, cov loss: 0.178\n",
      "    [batch 186]: seen 18600 examples : 37.2 eps, Loss: 3.514, Avg loss: 3.537, Best loss: 3.537, cov loss: 0.168\n",
      "    [batch 190]: seen 19000 examples : 37.0 eps, Loss: 3.380, Avg loss: 3.536, Best loss: 3.536, cov loss: 0.153\n",
      "    [batch 195]: seen 19500 examples : 37.2 eps, Loss: 3.570, Avg loss: 3.537, Best loss: 3.535, cov loss: 0.195\n",
      "    [batch 202]: seen 20200 examples : 37.8 eps, Loss: 3.549, Avg loss: 3.536, Best loss: 3.535, cov loss: 0.162\n",
      "    [batch 209]: seen 20900 examples : 38.4 eps, Loss: 3.545, Avg loss: 3.538, Best loss: 3.535, cov loss: 0.195\n",
      "    [batch 216]: seen 21600 examples : 38.9 eps, Loss: 3.390, Avg loss: 3.535, Best loss: 3.535, cov loss: 0.179\n",
      "    [batch 219]: seen 21900 examples : 38.7 eps, Loss: 3.534, Avg loss: 3.535, Best loss: 3.535, cov loss: 0.174\n",
      "    [batch 224]: seen 22400 examples : 38.5 eps, Loss: 3.293, Avg loss: 3.533, Best loss: 3.533, cov loss: 0.168\n",
      "    [batch 228]: seen 22800 examples : 38.4 eps, Loss: 3.395, Avg loss: 3.532, Best loss: 3.532, cov loss: 0.160\n",
      "    [batch 232]: seen 23200 examples : 38.2 eps, Loss: 3.493, Avg loss: 3.530, Best loss: 3.530, cov loss: 0.169\n",
      "    [batch 237]: seen 23700 examples : 38.1 eps, Loss: 3.281, Avg loss: 3.529, Best loss: 3.529, cov loss: 0.165\n",
      "    [batch 239]: seen 23900 examples : 37.8 eps, Loss: 3.451, Avg loss: 3.528, Best loss: 3.528, cov loss: 0.186\n",
      "    [batch 246]: seen 24600 examples : 38.3 eps, Loss: 3.618, Avg loss: 3.532, Best loss: 3.528, cov loss: 0.193\n",
      "    [batch 253]: seen 25300 examples : 38.8 eps, Loss: 3.516, Avg loss: 3.534, Best loss: 3.528, cov loss: 0.170\n",
      "    [batch 260]: seen 26000 examples : 39.2 eps, Loss: 3.765, Avg loss: 3.536, Best loss: 3.528, cov loss: 0.219\n",
      "    [batch 265]: seen 26500 examples : 39.3 eps, Loss: 3.555, Avg loss: 3.528, Best loss: 3.528, cov loss: 0.169\n",
      "    [batch 268]: seen 26800 examples : 39.1 eps, Loss: 3.510, Avg loss: 3.527, Best loss: 3.527, cov loss: 0.173\n",
      "    [batch 271]: seen 27100 examples : 38.9 eps, Loss: 3.386, Avg loss: 3.523, Best loss: 3.523, cov loss: 0.173\n",
      "    [batch 276]: seen 27600 examples : 39.0 eps, Loss: 3.527, Avg loss: 3.522, Best loss: 3.521, cov loss: 0.189\n",
      "    [batch 281]: seen 28100 examples : 38.9 eps, Loss: 3.381, Avg loss: 3.520, Best loss: 3.520, cov loss: 0.162\n",
      "    [batch 286]: seen 28600 examples : 39.0 eps, Loss: 3.447, Avg loss: 3.520, Best loss: 3.520, cov loss: 0.171\n",
      "    [batch 289]: seen 28900 examples : 38.8 eps, Loss: 3.442, Avg loss: 3.518, Best loss: 3.518, cov loss: 0.181\n",
      "    [batch 294]: seen 29400 examples : 38.9 eps, Loss: 3.517, Avg loss: 3.521, Best loss: 3.516, cov loss: 0.172\n",
      "    [batch 301]: seen 30100 examples : 39.3 eps, Loss: 3.623, Avg loss: 3.523, Best loss: 3.516, cov loss: 0.164\n",
      "    [batch 308]: seen 30800 examples : 39.7 eps, Loss: 3.618, Avg loss: 3.523, Best loss: 3.516, cov loss: 0.183\n",
      "    [batch 315]: seen 31500 examples : 40.0 eps, Loss: 3.496, Avg loss: 3.519, Best loss: 3.516, cov loss: 0.183\n",
      "    [batch 322]: seen 32200 examples : 40.4 eps, Loss: 3.397, Avg loss: 3.518, Best loss: 3.516, cov loss: 0.177\n",
      "    [batch 327]: seen 32700 examples : 40.5 eps, Loss: 3.270, Avg loss: 3.515, Best loss: 3.515, cov loss: 0.169\n",
      "    [batch 332]: seen 33200 examples : 40.5 eps, Loss: 3.588, Avg loss: 3.516, Best loss: 3.515, cov loss: 0.190\n",
      "    [batch 337]: seen 33700 examples : 40.6 eps, Loss: 3.504, Avg loss: 3.519, Best loss: 3.514, cov loss: 0.179\n",
      "    [batch 344]: seen 34400 examples : 40.9 eps, Loss: 3.308, Avg loss: 3.517, Best loss: 3.514, cov loss: 0.160\n",
      "    [batch 351]: seen 35100 examples : 41.3 eps, Loss: 3.523, Avg loss: 3.515, Best loss: 3.514, cov loss: 0.187\n",
      "    [batch 358]: seen 35800 examples : 41.6 eps, Loss: 3.570, Avg loss: 3.518, Best loss: 3.514, cov loss: 0.160\n",
      "    [batch 365]: seen 36500 examples : 41.9 eps, Loss: 3.610, Avg loss: 3.518, Best loss: 3.514, cov loss: 0.179\n",
      "    [batch 372]: seen 37200 examples : 42.2 eps, Loss: 3.433, Avg loss: 3.517, Best loss: 3.514, cov loss: 0.201\n",
      "    [batch 379]: seen 37900 examples : 42.5 eps, Loss: 3.572, Avg loss: 3.519, Best loss: 3.514, cov loss: 0.176\n",
      "    [batch 386]: seen 38600 examples : 42.8 eps, Loss: 3.454, Avg loss: 3.521, Best loss: 3.514, cov loss: 0.185\n",
      "    [batch 393]: seen 39300 examples : 43.1 eps, Loss: 3.235, Avg loss: 3.522, Best loss: 3.514, cov loss: 0.166\n",
      "    [batch 400]: seen 40000 examples : 43.4 eps, Loss: 3.698, Avg loss: 3.523, Best loss: 3.514, cov loss: 0.182\n",
      "    [batch 407]: seen 40700 examples : 43.6 eps, Loss: 3.294, Avg loss: 3.523, Best loss: 3.514, cov loss: 0.186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 414]: seen 41400 examples : 43.9 eps, Loss: 3.332, Avg loss: 3.519, Best loss: 3.514, cov loss: 0.165\n",
      "    [batch 421]: seen 42100 examples : 44.2 eps, Loss: 3.475, Avg loss: 3.519, Best loss: 3.514, cov loss: 0.178\n",
      "    [batch 428]: seen 42800 examples : 44.4 eps, Loss: 3.366, Avg loss: 3.516, Best loss: 3.514, cov loss: 0.193\n",
      "    [batch 435]: seen 43500 examples : 44.7 eps, Loss: 3.223, Avg loss: 3.516, Best loss: 3.514, cov loss: 0.168\n",
      "    [batch 440]: seen 44000 examples : 44.7 eps, Loss: 3.651, Avg loss: 3.515, Best loss: 3.513, cov loss: 0.190\n",
      "    [batch 443]: seen 44300 examples : 44.5 eps, Loss: 3.455, Avg loss: 3.512, Best loss: 3.512, cov loss: 0.185\n",
      "    [batch 445]: seen 44500 examples : 44.2 eps, Loss: 3.280, Avg loss: 3.508, Best loss: 3.508, cov loss: 0.178\n",
      "    [batch 449]: seen 44900 examples : 44.2 eps, Loss: 3.725, Avg loss: 3.512, Best loss: 3.508, cov loss: 0.179\n",
      "    [batch 456]: seen 45600 examples : 44.4 eps, Loss: 3.587, Avg loss: 3.515, Best loss: 3.508, cov loss: 0.191\n",
      "    [batch 463]: seen 46300 examples : 44.7 eps, Loss: 3.531, Avg loss: 3.519, Best loss: 3.508, cov loss: 0.170\n",
      "    [batch 470]: seen 47000 examples : 44.9 eps, Loss: 3.368, Avg loss: 3.515, Best loss: 3.508, cov loss: 0.161\n",
      "    [batch 477]: seen 47700 examples : 45.1 eps, Loss: 3.632, Avg loss: 3.521, Best loss: 3.508, cov loss: 0.180\n",
      "    [batch 484]: seen 48400 examples : 45.3 eps, Loss: 3.360, Avg loss: 3.515, Best loss: 3.508, cov loss: 0.163\n",
      "    [batch 491]: seen 49100 examples : 45.6 eps, Loss: 3.514, Avg loss: 3.514, Best loss: 3.508, cov loss: 0.170\n",
      "    [batch 498]: seen 49800 examples : 45.8 eps, Loss: 3.541, Avg loss: 3.516, Best loss: 3.508, cov loss: 0.186\n",
      "    [batch 505]: seen 50500 examples : 46.0 eps, Loss: 3.547, Avg loss: 3.516, Best loss: 3.508, cov loss: 0.177\n",
      "    [batch 512]: seen 51200 examples : 46.2 eps, Loss: 3.429, Avg loss: 3.515, Best loss: 3.508, cov loss: 0.192\n",
      "    [batch 519]: seen 51900 examples : 46.4 eps, Loss: 3.762, Avg loss: 3.519, Best loss: 3.508, cov loss: 0.175\n",
      "    [batch 526]: seen 52600 examples : 46.6 eps, Loss: 3.453, Avg loss: 3.518, Best loss: 3.508, cov loss: 0.170\n",
      "    [batch 533]: seen 53300 examples : 46.8 eps, Loss: 3.485, Avg loss: 3.519, Best loss: 3.508, cov loss: 0.164\n",
      "    [batch 540]: seen 54000 examples : 47.0 eps, Loss: 3.509, Avg loss: 3.515, Best loss: 3.508, cov loss: 0.198\n",
      "    [batch 547]: seen 54700 examples : 47.2 eps, Loss: 3.512, Avg loss: 3.513, Best loss: 3.508, cov loss: 0.166\n",
      "    [batch 554]: seen 55400 examples : 47.4 eps, Loss: 3.421, Avg loss: 3.512, Best loss: 3.508, cov loss: 0.181\n",
      "    [batch 561]: seen 56100 examples : 47.5 eps, Loss: 3.562, Avg loss: 3.512, Best loss: 3.508, cov loss: 0.182\n",
      "    [batch 568]: seen 56800 examples : 47.7 eps, Loss: 3.584, Avg loss: 3.511, Best loss: 3.508, cov loss: 0.172\n",
      "    [batch 575]: seen 57500 examples : 47.9 eps, Loss: 3.434, Avg loss: 3.512, Best loss: 3.508, cov loss: 0.183\n",
      "    [batch 582]: seen 58200 examples : 48.1 eps, Loss: 3.447, Avg loss: 3.513, Best loss: 3.508, cov loss: 0.186\n",
      "    [batch 589]: seen 58900 examples : 48.2 eps, Loss: 3.540, Avg loss: 3.511, Best loss: 3.508, cov loss: 0.202\n",
      "    [batch 596]: seen 59600 examples : 48.4 eps, Loss: 3.811, Avg loss: 3.515, Best loss: 3.508, cov loss: 0.207\n",
      "    [batch 603]: seen 60300 examples : 48.6 eps, Loss: 3.616, Avg loss: 3.517, Best loss: 3.508, cov loss: 0.179\n",
      "    [batch 610]: seen 61000 examples : 48.7 eps, Loss: 3.470, Avg loss: 3.521, Best loss: 3.508, cov loss: 0.161\n",
      "    [batch 617]: seen 61700 examples : 48.9 eps, Loss: 3.445, Avg loss: 3.524, Best loss: 3.508, cov loss: 0.169\n",
      "    [batch 624]: seen 62400 examples : 49.0 eps, Loss: 3.571, Avg loss: 3.525, Best loss: 3.508, cov loss: 0.162\n",
      "    [batch 631]: seen 63100 examples : 49.2 eps, Loss: 3.410, Avg loss: 3.518, Best loss: 3.508, cov loss: 0.174\n",
      "    [batch 638]: seen 63800 examples : 49.4 eps, Loss: 3.529, Avg loss: 3.516, Best loss: 3.508, cov loss: 0.185\n",
      "    [batch 645]: seen 64500 examples : 49.5 eps, Loss: 3.499, Avg loss: 3.519, Best loss: 3.508, cov loss: 0.191\n",
      "    [batch 652]: seen 65200 examples : 49.7 eps, Loss: 3.483, Avg loss: 3.515, Best loss: 3.508, cov loss: 0.158\n",
      "    [batch 659]: seen 65900 examples : 49.8 eps, Loss: 3.609, Avg loss: 3.512, Best loss: 3.508, cov loss: 0.198\n",
      "    [batch 666]: seen 66600 examples : 49.9 eps, Loss: 3.581, Avg loss: 3.515, Best loss: 3.508, cov loss: 0.179\n",
      "    [batch 673]: seen 67300 examples : 50.1 eps, Loss: 3.439, Avg loss: 3.516, Best loss: 3.508, cov loss: 0.171\n",
      "    [batch 680]: seen 68000 examples : 50.2 eps, Loss: 3.635, Avg loss: 3.521, Best loss: 3.508, cov loss: 0.185\n",
      "    [batch 687]: seen 68700 examples : 50.4 eps, Loss: 3.500, Avg loss: 3.520, Best loss: 3.508, cov loss: 0.187\n",
      "    [batch 694]: seen 69400 examples : 50.5 eps, Loss: 3.525, Avg loss: 3.521, Best loss: 3.508, cov loss: 0.163\n",
      "    [batch 701]: seen 70100 examples : 50.6 eps, Loss: 3.686, Avg loss: 3.527, Best loss: 3.508, cov loss: 0.199\n",
      "    [batch 708]: seen 70800 examples : 50.8 eps, Loss: 3.650, Avg loss: 3.532, Best loss: 3.508, cov loss: 0.169\n",
      "    [batch 715]: seen 71500 examples : 50.9 eps, Loss: 3.405, Avg loss: 3.528, Best loss: 3.508, cov loss: 0.154\n",
      "    [batch 722]: seen 72200 examples : 51.0 eps, Loss: 3.590, Avg loss: 3.530, Best loss: 3.508, cov loss: 0.181\n",
      "    [batch 729]: seen 72900 examples : 51.1 eps, Loss: 3.705, Avg loss: 3.530, Best loss: 3.508, cov loss: 0.186\n",
      "    [batch 736]: seen 73600 examples : 51.3 eps, Loss: 3.290, Avg loss: 3.524, Best loss: 3.508, cov loss: 0.182\n",
      "    [batch 743]: seen 74300 examples : 51.4 eps, Loss: 3.533, Avg loss: 3.521, Best loss: 3.508, cov loss: 0.178\n",
      "    [batch 750]: seen 75000 examples : 51.5 eps, Loss: 3.547, Avg loss: 3.521, Best loss: 3.508, cov loss: 0.167\n",
      "    [batch 757]: seen 75700 examples : 51.6 eps, Loss: 3.321, Avg loss: 3.516, Best loss: 3.508, cov loss: 0.152\n",
      "    [batch 764]: seen 76400 examples : 51.7 eps, Loss: 3.482, Avg loss: 3.514, Best loss: 3.508, cov loss: 0.179\n",
      "    [batch 771]: seen 77100 examples : 51.8 eps, Loss: 3.584, Avg loss: 3.514, Best loss: 3.508, cov loss: 0.173\n",
      "    [batch 778]: seen 77800 examples : 52.0 eps, Loss: 3.538, Avg loss: 3.517, Best loss: 3.508, cov loss: 0.195\n",
      "    [batch 785]: seen 78500 examples : 52.1 eps, Loss: 3.429, Avg loss: 3.515, Best loss: 3.508, cov loss: 0.178\n",
      "    [batch 792]: seen 79200 examples : 52.2 eps, Loss: 3.709, Avg loss: 3.519, Best loss: 3.508, cov loss: 0.180\n",
      "    [batch 799]: seen 79900 examples : 52.3 eps, Loss: 3.553, Avg loss: 3.514, Best loss: 3.508, cov loss: 0.175\n",
      "    [batch 806]: seen 80600 examples : 52.4 eps, Loss: 3.606, Avg loss: 3.516, Best loss: 3.508, cov loss: 0.177\n",
      "    [batch 813]: seen 81300 examples : 52.5 eps, Loss: 3.532, Avg loss: 3.516, Best loss: 3.508, cov loss: 0.188\n",
      "    [batch 820]: seen 82000 examples : 52.6 eps, Loss: 3.657, Avg loss: 3.511, Best loss: 3.508, cov loss: 0.186\n",
      "    [batch 827]: seen 82700 examples : 52.7 eps, Loss: 3.436, Avg loss: 3.509, Best loss: 3.508, cov loss: 0.183\n",
      "    [batch 830]: seen 83000 examples : 52.5 eps, Loss: 3.256, Avg loss: 3.505, Best loss: 3.505, cov loss: 0.182\n",
      "    [batch 833]: seen 83300 examples : 52.2 eps, Loss: 3.440, Avg loss: 3.503, Best loss: 3.503, cov loss: 0.167\n",
      "    [batch 840]: seen 84000 examples : 52.3 eps, Loss: 3.541, Avg loss: 3.510, Best loss: 3.503, cov loss: 0.182\n",
      "    [batch 847]: seen 84700 examples : 52.4 eps, Loss: 3.654, Avg loss: 3.509, Best loss: 3.503, cov loss: 0.200\n",
      "    [batch 854]: seen 85400 examples : 52.5 eps, Loss: 3.600, Avg loss: 3.514, Best loss: 3.503, cov loss: 0.187\n",
      "    [batch 861]: seen 86100 examples : 52.6 eps, Loss: 3.520, Avg loss: 3.513, Best loss: 3.503, cov loss: 0.187\n",
      "    [batch 868]: seen 86800 examples : 52.7 eps, Loss: 3.522, Avg loss: 3.519, Best loss: 3.503, cov loss: 0.176\n",
      "    [batch 875]: seen 87500 examples : 52.8 eps, Loss: 3.674, Avg loss: 3.518, Best loss: 3.503, cov loss: 0.183\n",
      "    [batch 882]: seen 88200 examples : 52.9 eps, Loss: 3.785, Avg loss: 3.523, Best loss: 3.503, cov loss: 0.213\n",
      "    [batch 889]: seen 88900 examples : 53.0 eps, Loss: 3.383, Avg loss: 3.521, Best loss: 3.503, cov loss: 0.187\n",
      "    [batch 896]: seen 89600 examples : 53.1 eps, Loss: 3.362, Avg loss: 3.521, Best loss: 3.503, cov loss: 0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 903]: seen 90300 examples : 53.2 eps, Loss: 3.558, Avg loss: 3.520, Best loss: 3.503, cov loss: 0.185\n",
      "    [batch 910]: seen 91000 examples : 53.3 eps, Loss: 3.514, Avg loss: 3.521, Best loss: 3.503, cov loss: 0.166\n",
      "    [batch 917]: seen 91700 examples : 53.4 eps, Loss: 3.786, Avg loss: 3.520, Best loss: 3.503, cov loss: 0.189\n",
      "    [batch 924]: seen 92400 examples : 53.5 eps, Loss: 3.417, Avg loss: 3.522, Best loss: 3.503, cov loss: 0.191\n",
      "    [batch 931]: seen 93100 examples : 53.6 eps, Loss: 3.574, Avg loss: 3.524, Best loss: 3.503, cov loss: 0.189\n",
      "    [batch 938]: seen 93800 examples : 53.6 eps, Loss: 3.578, Avg loss: 3.525, Best loss: 3.503, cov loss: 0.176\n",
      "    [batch 945]: seen 94500 examples : 53.7 eps, Loss: 3.574, Avg loss: 3.520, Best loss: 3.503, cov loss: 0.190\n",
      "    [batch 952]: seen 95200 examples : 53.8 eps, Loss: 3.553, Avg loss: 3.518, Best loss: 3.503, cov loss: 0.177\n",
      "    [batch 959]: seen 95900 examples : 53.9 eps, Loss: 3.761, Avg loss: 3.514, Best loss: 3.503, cov loss: 0.194\n",
      "    [batch 966]: seen 96600 examples : 54.0 eps, Loss: 3.540, Avg loss: 3.513, Best loss: 3.503, cov loss: 0.191\n",
      "    [batch 973]: seen 97300 examples : 54.1 eps, Loss: 3.315, Avg loss: 3.514, Best loss: 3.503, cov loss: 0.177\n",
      "    [batch 980]: seen 98000 examples : 54.1 eps, Loss: 3.602, Avg loss: 3.517, Best loss: 3.503, cov loss: 0.185\n",
      "    [batch 987]: seen 98700 examples : 54.2 eps, Loss: 3.488, Avg loss: 3.516, Best loss: 3.503, cov loss: 0.158\n",
      "    [batch 994]: seen 99400 examples : 54.3 eps, Loss: 3.457, Avg loss: 3.514, Best loss: 3.503, cov loss: 0.187\n",
      "    [batch 1001]: seen 100100 examples : 54.4 eps, Loss: 3.442, Avg loss: 3.513, Best loss: 3.503, cov loss: 0.168\n",
      "    [batch 1008]: seen 100800 examples : 54.5 eps, Loss: 3.534, Avg loss: 3.516, Best loss: 3.503, cov loss: 0.198\n",
      "    [batch 1015]: seen 101500 examples : 54.5 eps, Loss: 3.507, Avg loss: 3.516, Best loss: 3.503, cov loss: 0.168\n",
      "    [batch 1022]: seen 102200 examples : 54.6 eps, Loss: 3.682, Avg loss: 3.518, Best loss: 3.503, cov loss: 0.189\n",
      "    [batch 1029]: seen 102900 examples : 54.7 eps, Loss: 3.287, Avg loss: 3.516, Best loss: 3.503, cov loss: 0.161\n",
      "    [batch 1036]: seen 103600 examples : 54.8 eps, Loss: 3.612, Avg loss: 3.521, Best loss: 3.503, cov loss: 0.179\n",
      "    [batch 1043]: seen 104300 examples : 54.8 eps, Loss: 3.547, Avg loss: 3.526, Best loss: 3.503, cov loss: 0.184\n",
      "    [batch 1050]: seen 105000 examples : 54.9 eps, Loss: 3.425, Avg loss: 3.523, Best loss: 3.503, cov loss: 0.175\n",
      "    [batch 1057]: seen 105700 examples : 55.0 eps, Loss: 3.419, Avg loss: 3.520, Best loss: 3.503, cov loss: 0.165\n",
      "    [batch 1064]: seen 106400 examples : 55.1 eps, Loss: 3.582, Avg loss: 3.517, Best loss: 3.503, cov loss: 0.180\n",
      "    [batch 1071]: seen 107100 examples : 55.1 eps, Loss: 3.577, Avg loss: 3.519, Best loss: 3.503, cov loss: 0.171\n",
      "    [batch 1078]: seen 107800 examples : 55.2 eps, Loss: 3.407, Avg loss: 3.522, Best loss: 3.503, cov loss: 0.175\n",
      "    [batch 1085]: seen 108500 examples : 55.3 eps, Loss: 3.349, Avg loss: 3.518, Best loss: 3.503, cov loss: 0.185\n",
      "    [batch 1092]: seen 109200 examples : 55.3 eps, Loss: 3.323, Avg loss: 3.514, Best loss: 3.503, cov loss: 0.173\n",
      "    [batch 1099]: seen 109900 examples : 55.4 eps, Loss: 3.622, Avg loss: 3.517, Best loss: 3.503, cov loss: 0.169\n",
      "    [batch 1106]: seen 110600 examples : 55.5 eps, Loss: 3.542, Avg loss: 3.516, Best loss: 3.503, cov loss: 0.178\n",
      "    [batch 1113]: seen 111300 examples : 55.5 eps, Loss: 3.638, Avg loss: 3.514, Best loss: 3.503, cov loss: 0.193\n",
      "    [batch 1120]: seen 112000 examples : 55.6 eps, Loss: 3.702, Avg loss: 3.516, Best loss: 3.503, cov loss: 0.197\n",
      "    [batch 1127]: seen 112700 examples : 55.7 eps, Loss: 3.315, Avg loss: 3.518, Best loss: 3.503, cov loss: 0.165\n",
      "    [batch 1134]: seen 113400 examples : 55.7 eps, Loss: 3.242, Avg loss: 3.511, Best loss: 3.503, cov loss: 0.168\n",
      "    [batch 1141]: seen 114100 examples : 55.8 eps, Loss: 3.476, Avg loss: 3.512, Best loss: 3.503, cov loss: 0.180\n",
      "    [batch 1148]: seen 114800 examples : 55.9 eps, Loss: 3.648, Avg loss: 3.515, Best loss: 3.503, cov loss: 0.188\n",
      "    [batch 1155]: seen 115500 examples : 55.9 eps, Loss: 3.558, Avg loss: 3.516, Best loss: 3.503, cov loss: 0.178\n",
      "    [batch 1162]: seen 116200 examples : 56.0 eps, Loss: 3.496, Avg loss: 3.514, Best loss: 3.503, cov loss: 0.162\n",
      "    [batch 1169]: seen 116900 examples : 56.0 eps, Loss: 3.469, Avg loss: 3.509, Best loss: 3.503, cov loss: 0.183\n",
      "    [batch 1176]: seen 117600 examples : 56.1 eps, Loss: 3.602, Avg loss: 3.515, Best loss: 3.503, cov loss: 0.182\n",
      "    [batch 1183]: seen 118300 examples : 56.2 eps, Loss: 3.610, Avg loss: 3.514, Best loss: 3.503, cov loss: 0.189\n",
      "    [batch 1190]: seen 119000 examples : 56.2 eps, Loss: 3.480, Avg loss: 3.516, Best loss: 3.503, cov loss: 0.179\n",
      "    [batch 1197]: seen 119700 examples : 56.3 eps, Loss: 3.352, Avg loss: 3.507, Best loss: 3.503, cov loss: 0.188\n",
      "    [batch 1202]: seen 120200 examples : 56.2 eps, Loss: 3.508, Avg loss: 3.503, Best loss: 3.503, cov loss: 0.187\n",
      "    [batch 1208]: seen 120800 examples : 56.2 eps, Loss: 3.443, Avg loss: 3.502, Best loss: 3.502, cov loss: 0.173\n",
      "    [batch 1212]: seen 121200 examples : 56.1 eps, Loss: 3.458, Avg loss: 3.502, Best loss: 3.502, cov loss: 0.171\n",
      "    [batch 1217]: seen 121700 examples : 56.0 eps, Loss: 3.539, Avg loss: 3.505, Best loss: 3.501, cov loss: 0.168\n",
      "    [batch 1224]: seen 122400 examples : 56.1 eps, Loss: 3.384, Avg loss: 3.503, Best loss: 3.501, cov loss: 0.172\n",
      "    [batch 1231]: seen 123100 examples : 56.1 eps, Loss: 3.740, Avg loss: 3.504, Best loss: 3.501, cov loss: 0.193\n",
      "    [batch 1238]: seen 123800 examples : 56.2 eps, Loss: 3.703, Avg loss: 3.509, Best loss: 3.501, cov loss: 0.170\n",
      "    [batch 1245]: seen 124500 examples : 56.2 eps, Loss: 3.602, Avg loss: 3.511, Best loss: 3.501, cov loss: 0.179\n",
      "    [batch 1252]: seen 125200 examples : 56.3 eps, Loss: 3.537, Avg loss: 3.514, Best loss: 3.501, cov loss: 0.184\n",
      "    [batch 1259]: seen 125900 examples : 56.3 eps, Loss: 3.510, Avg loss: 3.513, Best loss: 3.501, cov loss: 0.173\n",
      "    [batch 1266]: seen 126600 examples : 56.4 eps, Loss: 3.513, Avg loss: 3.512, Best loss: 3.501, cov loss: 0.181\n",
      "    [batch 1273]: seen 127300 examples : 56.5 eps, Loss: 3.461, Avg loss: 3.512, Best loss: 3.501, cov loss: 0.163\n",
      "    [batch 1280]: seen 128000 examples : 56.5 eps, Loss: 3.499, Avg loss: 3.512, Best loss: 3.501, cov loss: 0.185\n",
      "    [batch 1287]: seen 128700 examples : 56.6 eps, Loss: 3.639, Avg loss: 3.510, Best loss: 3.501, cov loss: 0.181\n",
      "    [batch 1294]: seen 129400 examples : 56.6 eps, Loss: 3.407, Avg loss: 3.512, Best loss: 3.501, cov loss: 0.172\n",
      "    [batch 1301]: seen 130100 examples : 56.7 eps, Loss: 3.401, Avg loss: 3.511, Best loss: 3.501, cov loss: 0.186\n",
      "    [batch 1308]: seen 130800 examples : 56.7 eps, Loss: 3.445, Avg loss: 3.516, Best loss: 3.501, cov loss: 0.163\n",
      "    [batch 1315]: seen 131500 examples : 56.8 eps, Loss: 3.605, Avg loss: 3.513, Best loss: 3.501, cov loss: 0.179\n",
      "    [batch 1322]: seen 132200 examples : 56.8 eps, Loss: 3.465, Avg loss: 3.512, Best loss: 3.501, cov loss: 0.165\n",
      "    [batch 1329]: seen 132900 examples : 56.9 eps, Loss: 3.490, Avg loss: 3.513, Best loss: 3.501, cov loss: 0.183\n",
      "    [batch 1336]: seen 133600 examples : 56.9 eps, Loss: 3.428, Avg loss: 3.513, Best loss: 3.501, cov loss: 0.180\n",
      "    [batch 1343]: seen 134300 examples : 57.0 eps, Loss: 3.515, Avg loss: 3.508, Best loss: 3.501, cov loss: 0.184\n",
      "    [batch 1350]: seen 135000 examples : 57.0 eps, Loss: 3.517, Avg loss: 3.512, Best loss: 3.501, cov loss: 0.167\n",
      "    [batch 1357]: seen 135700 examples : 57.1 eps, Loss: 3.705, Avg loss: 3.522, Best loss: 3.501, cov loss: 0.183\n",
      "    [batch 1364]: seen 136400 examples : 57.1 eps, Loss: 3.440, Avg loss: 3.522, Best loss: 3.501, cov loss: 0.180\n",
      "    [batch 1371]: seen 137100 examples : 57.2 eps, Loss: 3.329, Avg loss: 3.519, Best loss: 3.501, cov loss: 0.163\n",
      "    [batch 1378]: seen 137800 examples : 57.2 eps, Loss: 3.531, Avg loss: 3.517, Best loss: 3.501, cov loss: 0.158\n",
      "    [batch 1385]: seen 138500 examples : 57.3 eps, Loss: 3.550, Avg loss: 3.515, Best loss: 3.501, cov loss: 0.190\n",
      "    [batch 1392]: seen 139200 examples : 57.3 eps, Loss: 3.493, Avg loss: 3.511, Best loss: 3.501, cov loss: 0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1399]: seen 139900 examples : 57.4 eps, Loss: 3.533, Avg loss: 3.512, Best loss: 3.501, cov loss: 0.175\n",
      "    [batch 1406]: seen 140600 examples : 57.4 eps, Loss: 3.446, Avg loss: 3.513, Best loss: 3.501, cov loss: 0.193\n",
      "    [batch 1413]: seen 141300 examples : 57.5 eps, Loss: 3.791, Avg loss: 3.512, Best loss: 3.501, cov loss: 0.189\n",
      "    [batch 1420]: seen 142000 examples : 57.5 eps, Loss: 3.465, Avg loss: 3.511, Best loss: 3.501, cov loss: 0.163\n",
      "    [batch 1427]: seen 142700 examples : 57.6 eps, Loss: 3.645, Avg loss: 3.516, Best loss: 3.501, cov loss: 0.185\n",
      "    [batch 1434]: seen 143400 examples : 57.6 eps, Loss: 3.363, Avg loss: 3.511, Best loss: 3.501, cov loss: 0.175\n",
      "    [batch 1441]: seen 144100 examples : 57.6 eps, Loss: 3.532, Avg loss: 3.515, Best loss: 3.501, cov loss: 0.184\n",
      "    [batch 1448]: seen 144800 examples : 57.7 eps, Loss: 3.479, Avg loss: 3.510, Best loss: 3.501, cov loss: 0.188\n",
      "    [batch 1455]: seen 145500 examples : 57.7 eps, Loss: 3.556, Avg loss: 3.511, Best loss: 3.501, cov loss: 0.184\n",
      "    [batch 1462]: seen 146200 examples : 57.8 eps, Loss: 3.633, Avg loss: 3.512, Best loss: 3.501, cov loss: 0.176\n",
      "    [batch 1469]: seen 146900 examples : 57.8 eps, Loss: 3.586, Avg loss: 3.510, Best loss: 3.501, cov loss: 0.210\n",
      "    [batch 1476]: seen 147600 examples : 57.9 eps, Loss: 3.433, Avg loss: 3.516, Best loss: 3.501, cov loss: 0.179\n",
      "    [batch 1483]: seen 148300 examples : 57.9 eps, Loss: 3.455, Avg loss: 3.508, Best loss: 3.501, cov loss: 0.170\n",
      "    [batch 1490]: seen 149000 examples : 57.9 eps, Loss: 3.565, Avg loss: 3.508, Best loss: 3.501, cov loss: 0.173\n",
      "    [batch 1497]: seen 149700 examples : 58.0 eps, Loss: 3.546, Avg loss: 3.507, Best loss: 3.501, cov loss: 0.164\n",
      "    [batch 1504]: seen 150400 examples : 58.0 eps, Loss: 3.624, Avg loss: 3.506, Best loss: 3.501, cov loss: 0.206\n",
      "    [batch 1511]: seen 151100 examples : 58.1 eps, Loss: 3.466, Avg loss: 3.505, Best loss: 3.501, cov loss: 0.172\n",
      "    [batch 1518]: seen 151800 examples : 58.1 eps, Loss: 3.363, Avg loss: 3.504, Best loss: 3.501, cov loss: 0.169\n",
      "    [batch 1524]: seen 152400 examples : 58.1 eps, Loss: 3.371, Avg loss: 3.500, Best loss: 3.500, cov loss: 0.179\n",
      "    [batch 1529]: seen 152900 examples : 58.0 eps, Loss: 3.477, Avg loss: 3.499, Best loss: 3.499, cov loss: 0.173\n",
      "    [batch 1534]: seen 153400 examples : 57.8 eps, Loss: 3.440, Avg loss: 3.497, Best loss: 3.497, cov loss: 0.158\n",
      "    [batch 1541]: seen 154100 examples : 57.9 eps, Loss: 3.464, Avg loss: 3.499, Best loss: 3.497, cov loss: 0.179\n",
      "    [batch 1548]: seen 154800 examples : 57.9 eps, Loss: 3.310, Avg loss: 3.499, Best loss: 3.497, cov loss: 0.177\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-28413\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-28413\n",
      "    [batch 1554]: seen 155400 examples : 57.9 eps, Loss: 3.656, Avg loss: 3.499, Best loss: 3.497, cov loss: 0.197\n",
      "    [batch 1559]: seen 155900 examples : 57.9 eps, Loss: 3.220, Avg loss: 3.495, Best loss: 3.495, cov loss: 0.151\n",
      "    [batch 1564]: seen 156400 examples : 57.8 eps, Loss: 3.528, Avg loss: 3.497, Best loss: 3.493, cov loss: 0.176\n",
      "    [batch 1571]: seen 157100 examples : 57.9 eps, Loss: 3.484, Avg loss: 3.497, Best loss: 3.493, cov loss: 0.168\n",
      "    [batch 1575]: seen 157500 examples : 57.8 eps, Loss: 3.746, Avg loss: 3.496, Best loss: 3.493, cov loss: 0.198\n",
      "    [batch 1580]: seen 158000 examples : 57.7 eps, Loss: 3.299, Avg loss: 3.489, Best loss: 3.489, cov loss: 0.180\n",
      "    [batch 1585]: seen 158500 examples : 57.6 eps, Loss: 3.461, Avg loss: 3.492, Best loss: 3.489, cov loss: 0.182\n",
      "    [batch 1590]: seen 159000 examples : 57.6 eps, Loss: 3.482, Avg loss: 3.489, Best loss: 3.489, cov loss: 0.170\n",
      "    [batch 1593]: seen 159300 examples : 57.5 eps, Loss: 3.741, Avg loss: 3.491, Best loss: 3.489, cov loss: 0.174\n",
      "    [batch 1600]: seen 160000 examples : 57.5 eps, Loss: 3.273, Avg loss: 3.491, Best loss: 3.489, cov loss: 0.181\n",
      "    [batch 1607]: seen 160700 examples : 57.5 eps, Loss: 3.417, Avg loss: 3.489, Best loss: 3.489, cov loss: 0.174\n",
      "    [batch 1610]: seen 161000 examples : 57.4 eps, Loss: 3.429, Avg loss: 3.488, Best loss: 3.488, cov loss: 0.177\n",
      "    [batch 1615]: seen 161500 examples : 57.4 eps, Loss: 3.469, Avg loss: 3.489, Best loss: 3.487, cov loss: 0.193\n",
      "    [batch 1622]: seen 162200 examples : 57.4 eps, Loss: 3.640, Avg loss: 3.495, Best loss: 3.487, cov loss: 0.188\n",
      "    [batch 1629]: seen 162900 examples : 57.5 eps, Loss: 3.519, Avg loss: 3.497, Best loss: 3.487, cov loss: 0.177\n",
      "    [batch 1636]: seen 163600 examples : 57.5 eps, Loss: 3.573, Avg loss: 3.502, Best loss: 3.487, cov loss: 0.192\n",
      "    [batch 1643]: seen 164300 examples : 57.5 eps, Loss: 3.530, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.181\n",
      "    [batch 1650]: seen 165000 examples : 57.6 eps, Loss: 3.418, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.163\n",
      "    [batch 1657]: seen 165700 examples : 57.6 eps, Loss: 3.549, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.181\n",
      "    [batch 1664]: seen 166400 examples : 57.7 eps, Loss: 3.535, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.176\n",
      "    [batch 1671]: seen 167100 examples : 57.7 eps, Loss: 3.581, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.182\n",
      "    [batch 1678]: seen 167800 examples : 57.7 eps, Loss: 3.448, Avg loss: 3.502, Best loss: 3.487, cov loss: 0.177\n",
      "    [batch 1685]: seen 168500 examples : 57.8 eps, Loss: 3.429, Avg loss: 3.500, Best loss: 3.487, cov loss: 0.173\n",
      "    [batch 1692]: seen 169200 examples : 57.8 eps, Loss: 3.414, Avg loss: 3.496, Best loss: 3.487, cov loss: 0.182\n",
      "    [batch 1699]: seen 169900 examples : 57.8 eps, Loss: 3.572, Avg loss: 3.496, Best loss: 3.487, cov loss: 0.177\n",
      "    [batch 1706]: seen 170600 examples : 57.9 eps, Loss: 3.643, Avg loss: 3.501, Best loss: 3.487, cov loss: 0.186\n",
      "    [batch 1713]: seen 171300 examples : 57.9 eps, Loss: 3.593, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.195\n",
      "    [batch 1720]: seen 172000 examples : 58.0 eps, Loss: 3.649, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.187\n",
      "    [batch 1727]: seen 172700 examples : 58.0 eps, Loss: 3.583, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.191\n",
      "    [batch 1734]: seen 173400 examples : 58.0 eps, Loss: 3.435, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.180\n",
      "    [batch 1741]: seen 174100 examples : 58.1 eps, Loss: 3.419, Avg loss: 3.512, Best loss: 3.487, cov loss: 0.177\n",
      "    [batch 1748]: seen 174800 examples : 58.1 eps, Loss: 3.544, Avg loss: 3.513, Best loss: 3.487, cov loss: 0.197\n",
      "    [batch 1755]: seen 175500 examples : 58.1 eps, Loss: 3.286, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.187\n",
      "    [batch 1762]: seen 176200 examples : 58.2 eps, Loss: 3.737, Avg loss: 3.514, Best loss: 3.487, cov loss: 0.196\n",
      "    [batch 1769]: seen 176900 examples : 58.2 eps, Loss: 3.166, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.158\n",
      "    [batch 1776]: seen 177600 examples : 58.2 eps, Loss: 3.647, Avg loss: 3.511, Best loss: 3.487, cov loss: 0.204\n",
      "    [batch 1783]: seen 178300 examples : 58.3 eps, Loss: 3.398, Avg loss: 3.512, Best loss: 3.487, cov loss: 0.183\n",
      "    [batch 1790]: seen 179000 examples : 58.3 eps, Loss: 3.647, Avg loss: 3.511, Best loss: 3.487, cov loss: 0.181\n",
      "    [batch 1797]: seen 179700 examples : 58.3 eps, Loss: 3.662, Avg loss: 3.516, Best loss: 3.487, cov loss: 0.165\n",
      "    [batch 1804]: seen 180400 examples : 58.4 eps, Loss: 3.497, Avg loss: 3.518, Best loss: 3.487, cov loss: 0.168\n",
      "    [batch 1811]: seen 181100 examples : 58.4 eps, Loss: 3.553, Avg loss: 3.519, Best loss: 3.487, cov loss: 0.198\n",
      "    [batch 1818]: seen 181800 examples : 58.4 eps, Loss: 3.281, Avg loss: 3.515, Best loss: 3.487, cov loss: 0.175\n",
      "    [batch 1825]: seen 182500 examples : 58.5 eps, Loss: 3.599, Avg loss: 3.513, Best loss: 3.487, cov loss: 0.170\n",
      "    [batch 1832]: seen 183200 examples : 58.5 eps, Loss: 3.543, Avg loss: 3.514, Best loss: 3.487, cov loss: 0.182\n",
      "    [batch 1839]: seen 183900 examples : 58.5 eps, Loss: 3.470, Avg loss: 3.510, Best loss: 3.487, cov loss: 0.174\n",
      "    [batch 1846]: seen 184600 examples : 58.6 eps, Loss: 3.433, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1853]: seen 185300 examples : 58.6 eps, Loss: 3.488, Avg loss: 3.511, Best loss: 3.487, cov loss: 0.159\n",
      "    [batch 1860]: seen 186000 examples : 58.6 eps, Loss: 3.338, Avg loss: 3.511, Best loss: 3.487, cov loss: 0.162\n",
      "    [batch 1867]: seen 186700 examples : 58.7 eps, Loss: 3.291, Avg loss: 3.510, Best loss: 3.487, cov loss: 0.199\n",
      "    [batch 1874]: seen 187400 examples : 58.7 eps, Loss: 3.619, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.168\n",
      "    [batch 1881]: seen 188100 examples : 58.7 eps, Loss: 3.504, Avg loss: 3.508, Best loss: 3.487, cov loss: 0.172\n",
      "    [batch 1888]: seen 188800 examples : 58.8 eps, Loss: 3.751, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.200\n",
      "    [batch 1895]: seen 189500 examples : 58.8 eps, Loss: 3.183, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.161\n",
      "    [batch 1902]: seen 190200 examples : 58.8 eps, Loss: 3.351, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.185\n",
      "    [batch 1909]: seen 190900 examples : 58.9 eps, Loss: 3.470, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.173\n",
      "    [batch 1916]: seen 191600 examples : 58.9 eps, Loss: 3.536, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.176\n",
      "    [batch 1923]: seen 192300 examples : 58.9 eps, Loss: 3.361, Avg loss: 3.508, Best loss: 3.487, cov loss: 0.164\n",
      "    [batch 1930]: seen 193000 examples : 58.9 eps, Loss: 3.479, Avg loss: 3.510, Best loss: 3.487, cov loss: 0.181\n",
      "    [batch 1937]: seen 193700 examples : 59.0 eps, Loss: 3.403, Avg loss: 3.508, Best loss: 3.487, cov loss: 0.177\n",
      "    [batch 1944]: seen 194400 examples : 59.0 eps, Loss: 3.565, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.175\n",
      "    [batch 1951]: seen 195100 examples : 59.0 eps, Loss: 3.605, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.189\n",
      "    [batch 1958]: seen 195800 examples : 59.1 eps, Loss: 3.441, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.164\n",
      "    [batch 1965]: seen 196500 examples : 59.1 eps, Loss: 3.507, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.193\n",
      "    [batch 1972]: seen 197200 examples : 59.1 eps, Loss: 3.482, Avg loss: 3.504, Best loss: 3.487, cov loss: 0.173\n",
      "    [batch 1979]: seen 197900 examples : 59.2 eps, Loss: 3.514, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.202\n",
      "    [batch 1986]: seen 198600 examples : 59.2 eps, Loss: 3.641, Avg loss: 3.512, Best loss: 3.487, cov loss: 0.182\n",
      "    [batch 1993]: seen 199300 examples : 59.2 eps, Loss: 3.517, Avg loss: 3.511, Best loss: 3.487, cov loss: 0.188\n",
      "    [batch 2000]: seen 200000 examples : 59.2 eps, Loss: 3.609, Avg loss: 3.512, Best loss: 3.487, cov loss: 0.178\n",
      "    [batch 2007]: seen 200700 examples : 59.3 eps, Loss: 3.379, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.181\n",
      "    [batch 2014]: seen 201400 examples : 59.3 eps, Loss: 3.602, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.188\n",
      "    [batch 2021]: seen 202100 examples : 59.3 eps, Loss: 3.581, Avg loss: 3.508, Best loss: 3.487, cov loss: 0.171\n",
      "    [batch 2028]: seen 202800 examples : 59.4 eps, Loss: 3.534, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.168\n",
      "    [batch 2035]: seen 203500 examples : 59.4 eps, Loss: 3.419, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.178\n",
      "    [batch 2042]: seen 204200 examples : 59.4 eps, Loss: 3.401, Avg loss: 3.504, Best loss: 3.487, cov loss: 0.190\n",
      "    [batch 2049]: seen 204900 examples : 59.4 eps, Loss: 3.483, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.180\n",
      "    [batch 2056]: seen 205600 examples : 59.5 eps, Loss: 3.466, Avg loss: 3.508, Best loss: 3.487, cov loss: 0.196\n",
      "    [batch 2063]: seen 206300 examples : 59.5 eps, Loss: 3.432, Avg loss: 3.510, Best loss: 3.487, cov loss: 0.185\n",
      "    [batch 2070]: seen 207000 examples : 59.5 eps, Loss: 3.361, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.189\n",
      "    [batch 2077]: seen 207700 examples : 59.5 eps, Loss: 3.364, Avg loss: 3.501, Best loss: 3.487, cov loss: 0.167\n",
      "    [batch 2084]: seen 208400 examples : 59.6 eps, Loss: 3.222, Avg loss: 3.497, Best loss: 3.487, cov loss: 0.184\n",
      "    [batch 2091]: seen 209100 examples : 59.6 eps, Loss: 3.390, Avg loss: 3.496, Best loss: 3.487, cov loss: 0.177\n",
      "    [batch 2098]: seen 209800 examples : 59.6 eps, Loss: 3.748, Avg loss: 3.502, Best loss: 3.487, cov loss: 0.171\n",
      "    [batch 2105]: seen 210500 examples : 59.7 eps, Loss: 3.505, Avg loss: 3.500, Best loss: 3.487, cov loss: 0.174\n",
      "    [batch 2112]: seen 211200 examples : 59.7 eps, Loss: 3.564, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.179\n",
      "    [batch 2119]: seen 211900 examples : 59.7 eps, Loss: 3.530, Avg loss: 3.508, Best loss: 3.487, cov loss: 0.173\n",
      "    [batch 2126]: seen 212600 examples : 59.7 eps, Loss: 3.696, Avg loss: 3.513, Best loss: 3.487, cov loss: 0.171\n",
      "    [batch 2133]: seen 213300 examples : 59.8 eps, Loss: 3.403, Avg loss: 3.512, Best loss: 3.487, cov loss: 0.170\n",
      "    [batch 2140]: seen 214000 examples : 59.8 eps, Loss: 3.599, Avg loss: 3.511, Best loss: 3.487, cov loss: 0.183\n",
      "    [batch 2147]: seen 214700 examples : 59.8 eps, Loss: 3.575, Avg loss: 3.510, Best loss: 3.487, cov loss: 0.193\n",
      "    [batch 2154]: seen 215400 examples : 59.8 eps, Loss: 3.535, Avg loss: 3.513, Best loss: 3.487, cov loss: 0.172\n",
      "    [batch 2161]: seen 216100 examples : 59.9 eps, Loss: 3.496, Avg loss: 3.510, Best loss: 3.487, cov loss: 0.188\n",
      "    [batch 2168]: seen 216800 examples : 59.9 eps, Loss: 3.446, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.161\n",
      "    [batch 2175]: seen 217500 examples : 59.9 eps, Loss: 3.563, Avg loss: 3.508, Best loss: 3.487, cov loss: 0.179\n",
      "    [batch 2182]: seen 218200 examples : 59.9 eps, Loss: 3.655, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.199\n",
      "    [batch 2189]: seen 218900 examples : 60.0 eps, Loss: 3.431, Avg loss: 3.508, Best loss: 3.487, cov loss: 0.173\n",
      "    [batch 2196]: seen 219600 examples : 60.0 eps, Loss: 3.304, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.155\n",
      "    [batch 2203]: seen 220300 examples : 60.0 eps, Loss: 3.469, Avg loss: 3.510, Best loss: 3.487, cov loss: 0.177\n",
      "    [batch 2210]: seen 221000 examples : 60.0 eps, Loss: 3.534, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.173\n",
      "    [batch 2217]: seen 221700 examples : 60.1 eps, Loss: 3.472, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.190\n",
      "    [batch 2224]: seen 222400 examples : 60.1 eps, Loss: 3.411, Avg loss: 3.502, Best loss: 3.487, cov loss: 0.164\n",
      "    [batch 2231]: seen 223100 examples : 60.1 eps, Loss: 3.526, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.172\n",
      "    [batch 2238]: seen 223800 examples : 60.1 eps, Loss: 3.574, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.189\n",
      "    [batch 2245]: seen 224500 examples : 60.2 eps, Loss: 3.360, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.167\n",
      "    [batch 2252]: seen 225200 examples : 60.2 eps, Loss: 3.482, Avg loss: 3.504, Best loss: 3.487, cov loss: 0.189\n",
      "    [batch 2259]: seen 225900 examples : 60.2 eps, Loss: 3.519, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.187\n",
      "    [batch 2266]: seen 226600 examples : 60.2 eps, Loss: 3.262, Avg loss: 3.500, Best loss: 3.487, cov loss: 0.150\n",
      "    [batch 2273]: seen 227300 examples : 60.3 eps, Loss: 3.524, Avg loss: 3.499, Best loss: 3.487, cov loss: 0.167\n",
      "    [batch 2280]: seen 228000 examples : 60.3 eps, Loss: 3.530, Avg loss: 3.501, Best loss: 3.487, cov loss: 0.183\n",
      "    [batch 2287]: seen 228700 examples : 60.3 eps, Loss: 3.540, Avg loss: 3.501, Best loss: 3.487, cov loss: 0.175\n",
      "    [batch 2294]: seen 229400 examples : 60.3 eps, Loss: 3.517, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.177\n",
      "    [batch 2301]: seen 230100 examples : 60.4 eps, Loss: 3.531, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.187\n",
      "    [batch 2308]: seen 230800 examples : 60.4 eps, Loss: 3.353, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.161\n",
      "    [batch 2315]: seen 231500 examples : 60.4 eps, Loss: 3.769, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.192\n",
      "    [batch 2322]: seen 232200 examples : 60.4 eps, Loss: 3.703, Avg loss: 3.511, Best loss: 3.487, cov loss: 0.193\n",
      "    [batch 2329]: seen 232900 examples : 60.4 eps, Loss: 3.443, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.162\n",
      "    [batch 2336]: seen 233600 examples : 60.5 eps, Loss: 3.625, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.179\n",
      "    [batch 2343]: seen 234300 examples : 60.5 eps, Loss: 3.486, Avg loss: 3.501, Best loss: 3.487, cov loss: 0.185\n",
      "    [batch 2350]: seen 235000 examples : 60.5 eps, Loss: 3.584, Avg loss: 3.502, Best loss: 3.487, cov loss: 0.162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2357]: seen 235700 examples : 60.5 eps, Loss: 3.539, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.176\n",
      "    [batch 2364]: seen 236400 examples : 60.6 eps, Loss: 3.770, Avg loss: 3.511, Best loss: 3.487, cov loss: 0.188\n",
      "    [batch 2371]: seen 237100 examples : 60.6 eps, Loss: 3.497, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.161\n",
      "    [batch 2378]: seen 237800 examples : 60.6 eps, Loss: 3.616, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.201\n",
      "    [batch 2385]: seen 238500 examples : 60.6 eps, Loss: 3.472, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.196\n",
      "    [batch 2392]: seen 239200 examples : 60.6 eps, Loss: 3.411, Avg loss: 3.497, Best loss: 3.487, cov loss: 0.161\n",
      "    [batch 2399]: seen 239900 examples : 60.7 eps, Loss: 3.675, Avg loss: 3.495, Best loss: 3.487, cov loss: 0.199\n",
      "    [batch 2406]: seen 240600 examples : 60.7 eps, Loss: 3.603, Avg loss: 3.497, Best loss: 3.487, cov loss: 0.171\n",
      "    [batch 2413]: seen 241300 examples : 60.7 eps, Loss: 3.422, Avg loss: 3.494, Best loss: 3.487, cov loss: 0.181\n",
      "    [batch 2420]: seen 242000 examples : 60.7 eps, Loss: 3.495, Avg loss: 3.492, Best loss: 3.487, cov loss: 0.190\n",
      "    [batch 2427]: seen 242700 examples : 60.7 eps, Loss: 3.419, Avg loss: 3.496, Best loss: 3.487, cov loss: 0.171\n",
      "    [batch 2434]: seen 243400 examples : 60.8 eps, Loss: 3.496, Avg loss: 3.498, Best loss: 3.487, cov loss: 0.185\n",
      "    [batch 2441]: seen 244100 examples : 60.8 eps, Loss: 3.472, Avg loss: 3.498, Best loss: 3.487, cov loss: 0.193\n",
      "    [batch 2448]: seen 244800 examples : 60.8 eps, Loss: 3.386, Avg loss: 3.496, Best loss: 3.487, cov loss: 0.169\n",
      "    [batch 2455]: seen 245500 examples : 60.8 eps, Loss: 3.402, Avg loss: 3.500, Best loss: 3.487, cov loss: 0.172\n",
      "    [batch 2462]: seen 246200 examples : 60.9 eps, Loss: 3.384, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.166\n",
      "    [batch 2469]: seen 246900 examples : 60.9 eps, Loss: 3.558, Avg loss: 3.508, Best loss: 3.487, cov loss: 0.182\n",
      "    [batch 2476]: seen 247600 examples : 60.9 eps, Loss: 3.340, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.168\n",
      "    [batch 2483]: seen 248300 examples : 60.9 eps, Loss: 3.343, Avg loss: 3.504, Best loss: 3.487, cov loss: 0.189\n",
      "    [batch 2490]: seen 249000 examples : 60.9 eps, Loss: 3.486, Avg loss: 3.502, Best loss: 3.487, cov loss: 0.177\n",
      "    [batch 2497]: seen 249700 examples : 61.0 eps, Loss: 3.335, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.165\n",
      "    [batch 2504]: seen 250400 examples : 61.0 eps, Loss: 3.456, Avg loss: 3.500, Best loss: 3.487, cov loss: 0.158\n",
      "    [batch 2511]: seen 251100 examples : 61.0 eps, Loss: 3.623, Avg loss: 3.499, Best loss: 3.487, cov loss: 0.187\n",
      "    [batch 2518]: seen 251800 examples : 61.0 eps, Loss: 3.662, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.188\n",
      "    [batch 2525]: seen 252500 examples : 61.0 eps, Loss: 3.380, Avg loss: 3.504, Best loss: 3.487, cov loss: 0.174\n",
      "    [batch 2532]: seen 253200 examples : 61.1 eps, Loss: 3.567, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.194\n",
      "    [batch 2539]: seen 253900 examples : 61.1 eps, Loss: 3.562, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.199\n",
      "    [batch 2546]: seen 254600 examples : 61.1 eps, Loss: 3.454, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.172\n",
      "    [batch 2553]: seen 255300 examples : 61.1 eps, Loss: 3.561, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.175\n",
      "    [batch 2560]: seen 256000 examples : 61.1 eps, Loss: 3.486, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.182\n",
      "    [batch 2567]: seen 256700 examples : 61.1 eps, Loss: 3.450, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.180\n",
      "    [batch 2574]: seen 257400 examples : 61.2 eps, Loss: 3.450, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.180\n",
      "    [batch 2581]: seen 258100 examples : 61.2 eps, Loss: 3.510, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.173\n",
      "    [batch 2588]: seen 258800 examples : 61.2 eps, Loss: 3.480, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.168\n",
      "    [batch 2595]: seen 259500 examples : 61.2 eps, Loss: 3.516, Avg loss: 3.510, Best loss: 3.487, cov loss: 0.184\n",
      "    [batch 2602]: seen 260200 examples : 61.2 eps, Loss: 3.664, Avg loss: 3.510, Best loss: 3.487, cov loss: 0.163\n",
      "    [batch 2609]: seen 260900 examples : 61.3 eps, Loss: 3.459, Avg loss: 3.512, Best loss: 3.487, cov loss: 0.176\n",
      "    [batch 2616]: seen 261600 examples : 61.3 eps, Loss: 3.399, Avg loss: 3.508, Best loss: 3.487, cov loss: 0.161\n",
      "    [batch 2623]: seen 262300 examples : 61.3 eps, Loss: 3.381, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.169\n",
      "    [batch 2630]: seen 263000 examples : 61.3 eps, Loss: 3.498, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.193\n",
      "    [batch 2637]: seen 263700 examples : 61.3 eps, Loss: 3.326, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.180\n",
      "    [batch 2644]: seen 264400 examples : 61.4 eps, Loss: 3.514, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.180\n",
      "    [batch 2651]: seen 265100 examples : 61.4 eps, Loss: 3.704, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.194\n",
      "    [batch 2658]: seen 265800 examples : 61.4 eps, Loss: 3.283, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.172\n",
      "    [batch 2665]: seen 266500 examples : 61.4 eps, Loss: 3.413, Avg loss: 3.504, Best loss: 3.487, cov loss: 0.182\n",
      "    [batch 2672]: seen 267200 examples : 61.4 eps, Loss: 3.243, Avg loss: 3.498, Best loss: 3.487, cov loss: 0.169\n",
      "    [batch 2679]: seen 267900 examples : 61.4 eps, Loss: 3.310, Avg loss: 3.496, Best loss: 3.487, cov loss: 0.184\n",
      "    [batch 2686]: seen 268600 examples : 61.5 eps, Loss: 3.280, Avg loss: 3.497, Best loss: 3.487, cov loss: 0.162\n",
      "    [batch 2693]: seen 269300 examples : 61.5 eps, Loss: 3.488, Avg loss: 3.498, Best loss: 3.487, cov loss: 0.174\n",
      "    [batch 2700]: seen 270000 examples : 61.5 eps, Loss: 3.588, Avg loss: 3.495, Best loss: 3.487, cov loss: 0.156\n",
      "    [batch 2707]: seen 270700 examples : 61.5 eps, Loss: 3.506, Avg loss: 3.494, Best loss: 3.487, cov loss: 0.165\n",
      "    [batch 2714]: seen 271400 examples : 61.5 eps, Loss: 3.487, Avg loss: 3.492, Best loss: 3.487, cov loss: 0.185\n",
      "    [batch 2721]: seen 272100 examples : 61.5 eps, Loss: 3.491, Avg loss: 3.493, Best loss: 3.487, cov loss: 0.184\n",
      "    [batch 2728]: seen 272800 examples : 61.6 eps, Loss: 3.291, Avg loss: 3.490, Best loss: 3.487, cov loss: 0.153\n",
      "    [batch 2735]: seen 273500 examples : 61.6 eps, Loss: 3.577, Avg loss: 3.491, Best loss: 3.487, cov loss: 0.185\n",
      "    [batch 2742]: seen 274200 examples : 61.6 eps, Loss: 3.546, Avg loss: 3.493, Best loss: 3.487, cov loss: 0.187\n",
      "    [batch 2749]: seen 274900 examples : 61.6 eps, Loss: 3.473, Avg loss: 3.496, Best loss: 3.487, cov loss: 0.179\n",
      "    [batch 2756]: seen 275600 examples : 61.6 eps, Loss: 3.513, Avg loss: 3.500, Best loss: 3.487, cov loss: 0.172\n",
      "    [batch 2763]: seen 276300 examples : 61.6 eps, Loss: 3.583, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.178\n",
      "    [batch 2770]: seen 277000 examples : 61.7 eps, Loss: 3.649, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.188\n",
      "    [batch 2777]: seen 277700 examples : 61.7 eps, Loss: 3.587, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.198\n",
      "    [batch 2784]: seen 278400 examples : 61.7 eps, Loss: 3.565, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.185\n",
      "    [batch 2791]: seen 279100 examples : 61.7 eps, Loss: 3.417, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.179\n",
      "    [batch 2798]: seen 279800 examples : 61.7 eps, Loss: 3.589, Avg loss: 3.508, Best loss: 3.487, cov loss: 0.172\n",
      "    [batch 2805]: seen 280500 examples : 61.7 eps, Loss: 3.461, Avg loss: 3.508, Best loss: 3.487, cov loss: 0.183\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:15:45\n",
      "[EPOCH 17] Complete. Avg Loss: 3.5063743162337735; Best Loss: 3.486833858990144\n",
      "[EPOCH 18] Starting training..\n",
      "    [batch 7]: seen 700 examples : 69.0 eps, Loss: 3.576, Avg loss: 3.509, Best loss: 3.487, cov loss: 0.172\n",
      "    [batch 14]: seen 1400 examples : 69.0 eps, Loss: 3.587, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.183\n",
      "    [batch 21]: seen 2100 examples : 69.0 eps, Loss: 3.615, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.177\n",
      "    [batch 28]: seen 2800 examples : 69.0 eps, Loss: 3.434, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.167\n",
      "    [batch 35]: seen 3500 examples : 69.0 eps, Loss: 3.294, Avg loss: 3.503, Best loss: 3.487, cov loss: 0.176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 42]: seen 4200 examples : 69.0 eps, Loss: 3.485, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.167\n",
      "    [batch 49]: seen 4900 examples : 69.0 eps, Loss: 3.516, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.174\n",
      "    [batch 56]: seen 5600 examples : 69.0 eps, Loss: 3.418, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.178\n",
      "    [batch 63]: seen 6300 examples : 69.0 eps, Loss: 3.403, Avg loss: 3.504, Best loss: 3.487, cov loss: 0.168\n",
      "    [batch 70]: seen 7000 examples : 69.0 eps, Loss: 3.549, Avg loss: 3.507, Best loss: 3.487, cov loss: 0.197\n",
      "    [batch 77]: seen 7700 examples : 69.0 eps, Loss: 3.406, Avg loss: 3.506, Best loss: 3.487, cov loss: 0.172\n",
      "    [batch 84]: seen 8400 examples : 69.0 eps, Loss: 3.324, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.176\n",
      "    [batch 91]: seen 9100 examples : 69.0 eps, Loss: 3.396, Avg loss: 3.505, Best loss: 3.487, cov loss: 0.166\n",
      "    [batch 98]: seen 9800 examples : 69.0 eps, Loss: 3.301, Avg loss: 3.502, Best loss: 3.487, cov loss: 0.179\n",
      "    [batch 105]: seen 10500 examples : 69.0 eps, Loss: 3.510, Avg loss: 3.495, Best loss: 3.487, cov loss: 0.151\n",
      "    [batch 112]: seen 11200 examples : 69.0 eps, Loss: 3.499, Avg loss: 3.496, Best loss: 3.487, cov loss: 0.186\n",
      "    [batch 119]: seen 11900 examples : 69.0 eps, Loss: 3.377, Avg loss: 3.493, Best loss: 3.487, cov loss: 0.177\n",
      "    [batch 126]: seen 12600 examples : 69.0 eps, Loss: 3.657, Avg loss: 3.490, Best loss: 3.487, cov loss: 0.184\n",
      "    [batch 133]: seen 13300 examples : 69.0 eps, Loss: 3.533, Avg loss: 3.489, Best loss: 3.487, cov loss: 0.184\n",
      "    [batch 140]: seen 14000 examples : 67.9 eps, Loss: 3.392, Avg loss: 3.486, Best loss: 3.486, cov loss: 0.168\n",
      "    [batch 147]: seen 14700 examples : 67.9 eps, Loss: 3.478, Avg loss: 3.491, Best loss: 3.486, cov loss: 0.180\n",
      "    [batch 154]: seen 15400 examples : 68.0 eps, Loss: 3.705, Avg loss: 3.496, Best loss: 3.486, cov loss: 0.184\n",
      "    [batch 161]: seen 16100 examples : 68.0 eps, Loss: 3.598, Avg loss: 3.499, Best loss: 3.486, cov loss: 0.185\n",
      "    [batch 168]: seen 16800 examples : 68.1 eps, Loss: 3.717, Avg loss: 3.500, Best loss: 3.486, cov loss: 0.177\n",
      "    [batch 175]: seen 17500 examples : 68.1 eps, Loss: 3.349, Avg loss: 3.497, Best loss: 3.486, cov loss: 0.159\n",
      "    [batch 182]: seen 18200 examples : 68.1 eps, Loss: 3.564, Avg loss: 3.498, Best loss: 3.486, cov loss: 0.184\n",
      "    [batch 189]: seen 18900 examples : 68.2 eps, Loss: 3.582, Avg loss: 3.497, Best loss: 3.486, cov loss: 0.189\n",
      "    [batch 196]: seen 19600 examples : 68.2 eps, Loss: 3.275, Avg loss: 3.492, Best loss: 3.486, cov loss: 0.157\n",
      "    [batch 202]: seen 20200 examples : 67.4 eps, Loss: 3.236, Avg loss: 3.486, Best loss: 3.486, cov loss: 0.175\n",
      "    [batch 205]: seen 20500 examples : 65.2 eps, Loss: 3.358, Avg loss: 3.483, Best loss: 3.483, cov loss: 0.166\n",
      "    [batch 209]: seen 20900 examples : 63.9 eps, Loss: 3.402, Avg loss: 3.482, Best loss: 3.482, cov loss: 0.157\n",
      "    [batch 212]: seen 21200 examples : 62.5 eps, Loss: 3.274, Avg loss: 3.479, Best loss: 3.479, cov loss: 0.170\n",
      "    [batch 215]: seen 21500 examples : 61.4 eps, Loss: 3.454, Avg loss: 3.478, Best loss: 3.478, cov loss: 0.158\n",
      "    [batch 222]: seen 22200 examples : 61.0 eps, Loss: 3.366, Avg loss: 3.478, Best loss: 3.478, cov loss: 0.180\n",
      "    [batch 226]: seen 22600 examples : 60.0 eps, Loss: 3.412, Avg loss: 3.478, Best loss: 3.478, cov loss: 0.175\n",
      "    [batch 231]: seen 23100 examples : 59.6 eps, Loss: 3.361, Avg loss: 3.477, Best loss: 3.477, cov loss: 0.172\n",
      "    [batch 236]: seen 23600 examples : 59.3 eps, Loss: 3.513, Avg loss: 3.478, Best loss: 3.476, cov loss: 0.154\n",
      "    [batch 241]: seen 24100 examples : 59.0 eps, Loss: 3.527, Avg loss: 3.477, Best loss: 3.476, cov loss: 0.182\n",
      "    [batch 248]: seen 24800 examples : 59.2 eps, Loss: 3.537, Avg loss: 3.480, Best loss: 3.476, cov loss: 0.181\n",
      "    [batch 255]: seen 25500 examples : 59.4 eps, Loss: 3.425, Avg loss: 3.480, Best loss: 3.476, cov loss: 0.169\n",
      "    [batch 262]: seen 26200 examples : 59.7 eps, Loss: 3.545, Avg loss: 3.482, Best loss: 3.476, cov loss: 0.180\n",
      "    [batch 269]: seen 26900 examples : 59.9 eps, Loss: 3.576, Avg loss: 3.484, Best loss: 3.476, cov loss: 0.189\n",
      "    [batch 276]: seen 27600 examples : 60.1 eps, Loss: 3.429, Avg loss: 3.478, Best loss: 3.476, cov loss: 0.169\n",
      "    [batch 283]: seen 28300 examples : 60.3 eps, Loss: 3.505, Avg loss: 3.476, Best loss: 3.476, cov loss: 0.176\n",
      "    [batch 287]: seen 28700 examples : 59.5 eps, Loss: 3.358, Avg loss: 3.474, Best loss: 3.474, cov loss: 0.188\n",
      "    [batch 291]: seen 29100 examples : 58.8 eps, Loss: 3.297, Avg loss: 3.472, Best loss: 3.472, cov loss: 0.171\n",
      "    [batch 295]: seen 29500 examples : 58.1 eps, Loss: 3.286, Avg loss: 3.471, Best loss: 3.471, cov loss: 0.169\n",
      "    [batch 298]: seen 29800 examples : 57.5 eps, Loss: 3.510, Avg loss: 3.470, Best loss: 3.469, cov loss: 0.197\n",
      "    [batch 301]: seen 30100 examples : 56.7 eps, Loss: 3.434, Avg loss: 3.469, Best loss: 3.469, cov loss: 0.180\n",
      "    [batch 308]: seen 30800 examples : 57.0 eps, Loss: 3.701, Avg loss: 3.472, Best loss: 3.469, cov loss: 0.184\n",
      "    [batch 315]: seen 31500 examples : 57.2 eps, Loss: 3.654, Avg loss: 3.477, Best loss: 3.469, cov loss: 0.199\n",
      "    [batch 322]: seen 32200 examples : 57.4 eps, Loss: 3.615, Avg loss: 3.483, Best loss: 3.469, cov loss: 0.177\n",
      "    [batch 329]: seen 32900 examples : 57.6 eps, Loss: 3.440, Avg loss: 3.482, Best loss: 3.469, cov loss: 0.183\n",
      "    [batch 336]: seen 33600 examples : 57.8 eps, Loss: 3.407, Avg loss: 3.481, Best loss: 3.469, cov loss: 0.161\n",
      "    [batch 343]: seen 34300 examples : 58.0 eps, Loss: 3.267, Avg loss: 3.480, Best loss: 3.469, cov loss: 0.159\n",
      "    [batch 350]: seen 35000 examples : 58.2 eps, Loss: 3.355, Avg loss: 3.479, Best loss: 3.469, cov loss: 0.180\n",
      "    [batch 357]: seen 35700 examples : 58.4 eps, Loss: 3.492, Avg loss: 3.478, Best loss: 3.469, cov loss: 0.176\n",
      "    [batch 364]: seen 36400 examples : 58.5 eps, Loss: 3.427, Avg loss: 3.475, Best loss: 3.469, cov loss: 0.178\n",
      "    [batch 371]: seen 37100 examples : 58.7 eps, Loss: 3.401, Avg loss: 3.473, Best loss: 3.469, cov loss: 0.171\n",
      "    [batch 378]: seen 37800 examples : 58.9 eps, Loss: 3.646, Avg loss: 3.476, Best loss: 3.469, cov loss: 0.178\n",
      "    [batch 385]: seen 38500 examples : 59.0 eps, Loss: 3.347, Avg loss: 3.478, Best loss: 3.469, cov loss: 0.169\n",
      "    [batch 392]: seen 39200 examples : 59.2 eps, Loss: 3.557, Avg loss: 3.482, Best loss: 3.469, cov loss: 0.165\n",
      "    [batch 399]: seen 39900 examples : 59.3 eps, Loss: 3.539, Avg loss: 3.487, Best loss: 3.469, cov loss: 0.180\n",
      "    [batch 406]: seen 40600 examples : 59.5 eps, Loss: 3.490, Avg loss: 3.487, Best loss: 3.469, cov loss: 0.181\n",
      "    [batch 413]: seen 41300 examples : 59.6 eps, Loss: 3.202, Avg loss: 3.481, Best loss: 3.469, cov loss: 0.159\n",
      "    [batch 420]: seen 42000 examples : 59.7 eps, Loss: 3.644, Avg loss: 3.483, Best loss: 3.469, cov loss: 0.177\n",
      "    [batch 427]: seen 42700 examples : 59.9 eps, Loss: 3.187, Avg loss: 3.479, Best loss: 3.469, cov loss: 0.162\n",
      "    [batch 434]: seen 43400 examples : 60.0 eps, Loss: 3.405, Avg loss: 3.484, Best loss: 3.469, cov loss: 0.197\n",
      "    [batch 441]: seen 44100 examples : 60.1 eps, Loss: 3.547, Avg loss: 3.486, Best loss: 3.469, cov loss: 0.175\n",
      "    [batch 448]: seen 44800 examples : 60.2 eps, Loss: 3.276, Avg loss: 3.487, Best loss: 3.469, cov loss: 0.149\n",
      "    [batch 455]: seen 45500 examples : 60.4 eps, Loss: 3.579, Avg loss: 3.483, Best loss: 3.469, cov loss: 0.178\n",
      "    [batch 462]: seen 46200 examples : 60.5 eps, Loss: 3.480, Avg loss: 3.484, Best loss: 3.469, cov loss: 0.161\n",
      "    [batch 469]: seen 46900 examples : 60.6 eps, Loss: 3.339, Avg loss: 3.478, Best loss: 3.469, cov loss: 0.166\n",
      "    [batch 476]: seen 47600 examples : 60.7 eps, Loss: 3.350, Avg loss: 3.473, Best loss: 3.469, cov loss: 0.168\n",
      "    [batch 483]: seen 48300 examples : 60.8 eps, Loss: 3.482, Avg loss: 3.474, Best loss: 3.469, cov loss: 0.175\n",
      "    [batch 489]: seen 48900 examples : 60.6 eps, Loss: 3.288, Avg loss: 3.469, Best loss: 3.469, cov loss: 0.162\n",
      "    [batch 492]: seen 49200 examples : 60.2 eps, Loss: 3.580, Avg loss: 3.468, Best loss: 3.466, cov loss: 0.201\n",
      "    [batch 497]: seen 49700 examples : 60.0 eps, Loss: 3.497, Avg loss: 3.468, Best loss: 3.466, cov loss: 0.158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 504]: seen 50400 examples : 60.1 eps, Loss: 3.252, Avg loss: 3.467, Best loss: 3.466, cov loss: 0.165\n",
      "    [batch 511]: seen 51100 examples : 60.2 eps, Loss: 3.372, Avg loss: 3.472, Best loss: 3.466, cov loss: 0.157\n",
      "    [batch 518]: seen 51800 examples : 60.3 eps, Loss: 3.534, Avg loss: 3.472, Best loss: 3.466, cov loss: 0.187\n",
      "    [batch 525]: seen 52500 examples : 60.4 eps, Loss: 3.218, Avg loss: 3.473, Best loss: 3.466, cov loss: 0.159\n",
      "    [batch 532]: seen 53200 examples : 60.5 eps, Loss: 3.495, Avg loss: 3.475, Best loss: 3.466, cov loss: 0.172\n",
      "    [batch 539]: seen 53900 examples : 60.6 eps, Loss: 3.726, Avg loss: 3.483, Best loss: 3.466, cov loss: 0.195\n",
      "    [batch 546]: seen 54600 examples : 60.7 eps, Loss: 3.544, Avg loss: 3.482, Best loss: 3.466, cov loss: 0.197\n",
      "    [batch 553]: seen 55300 examples : 60.8 eps, Loss: 3.376, Avg loss: 3.481, Best loss: 3.466, cov loss: 0.160\n",
      "    [batch 560]: seen 56000 examples : 60.9 eps, Loss: 3.348, Avg loss: 3.480, Best loss: 3.466, cov loss: 0.169\n",
      "    [batch 567]: seen 56700 examples : 61.0 eps, Loss: 3.479, Avg loss: 3.482, Best loss: 3.466, cov loss: 0.167\n",
      "    [batch 574]: seen 57400 examples : 61.1 eps, Loss: 3.495, Avg loss: 3.481, Best loss: 3.466, cov loss: 0.170\n",
      "    [batch 581]: seen 58100 examples : 61.1 eps, Loss: 3.661, Avg loss: 3.480, Best loss: 3.466, cov loss: 0.188\n",
      "    [batch 588]: seen 58800 examples : 61.2 eps, Loss: 3.526, Avg loss: 3.486, Best loss: 3.466, cov loss: 0.168\n",
      "    [batch 595]: seen 59500 examples : 61.3 eps, Loss: 3.602, Avg loss: 3.488, Best loss: 3.466, cov loss: 0.217\n",
      "    [batch 602]: seen 60200 examples : 61.4 eps, Loss: 3.547, Avg loss: 3.488, Best loss: 3.466, cov loss: 0.166\n",
      "    [batch 609]: seen 60900 examples : 61.5 eps, Loss: 3.487, Avg loss: 3.489, Best loss: 3.466, cov loss: 0.179\n",
      "    [batch 616]: seen 61600 examples : 61.5 eps, Loss: 3.413, Avg loss: 3.491, Best loss: 3.466, cov loss: 0.173\n",
      "    [batch 623]: seen 62300 examples : 61.6 eps, Loss: 3.422, Avg loss: 3.487, Best loss: 3.466, cov loss: 0.174\n",
      "    [batch 630]: seen 63000 examples : 61.7 eps, Loss: 3.482, Avg loss: 3.480, Best loss: 3.466, cov loss: 0.193\n",
      "    [batch 637]: seen 63700 examples : 61.8 eps, Loss: 3.432, Avg loss: 3.482, Best loss: 3.466, cov loss: 0.166\n",
      "    [batch 644]: seen 64400 examples : 61.8 eps, Loss: 3.603, Avg loss: 3.480, Best loss: 3.466, cov loss: 0.185\n",
      "    [batch 651]: seen 65100 examples : 61.9 eps, Loss: 3.510, Avg loss: 3.485, Best loss: 3.466, cov loss: 0.200\n",
      "    [batch 658]: seen 65800 examples : 62.0 eps, Loss: 3.606, Avg loss: 3.489, Best loss: 3.466, cov loss: 0.186\n",
      "    [batch 665]: seen 66500 examples : 62.0 eps, Loss: 3.505, Avg loss: 3.487, Best loss: 3.466, cov loss: 0.189\n",
      "    [batch 672]: seen 67200 examples : 62.1 eps, Loss: 3.486, Avg loss: 3.483, Best loss: 3.466, cov loss: 0.167\n",
      "    [batch 679]: seen 67900 examples : 62.2 eps, Loss: 3.514, Avg loss: 3.477, Best loss: 3.466, cov loss: 0.188\n",
      "    [batch 686]: seen 68600 examples : 62.2 eps, Loss: 3.361, Avg loss: 3.477, Best loss: 3.466, cov loss: 0.185\n",
      "    [batch 693]: seen 69300 examples : 62.3 eps, Loss: 3.489, Avg loss: 3.476, Best loss: 3.466, cov loss: 0.178\n",
      "    [batch 700]: seen 70000 examples : 62.4 eps, Loss: 3.456, Avg loss: 3.473, Best loss: 3.466, cov loss: 0.181\n",
      "    [batch 707]: seen 70700 examples : 62.4 eps, Loss: 3.577, Avg loss: 3.475, Best loss: 3.466, cov loss: 0.176\n",
      "    [batch 714]: seen 71400 examples : 62.5 eps, Loss: 3.575, Avg loss: 3.477, Best loss: 3.466, cov loss: 0.181\n",
      "    [batch 721]: seen 72100 examples : 62.5 eps, Loss: 3.285, Avg loss: 3.474, Best loss: 3.466, cov loss: 0.152\n",
      "    [batch 728]: seen 72800 examples : 62.6 eps, Loss: 3.410, Avg loss: 3.472, Best loss: 3.466, cov loss: 0.175\n",
      "    [batch 735]: seen 73500 examples : 62.6 eps, Loss: 3.646, Avg loss: 3.472, Best loss: 3.466, cov loss: 0.185\n",
      "    [batch 742]: seen 74200 examples : 62.7 eps, Loss: 3.515, Avg loss: 3.470, Best loss: 3.466, cov loss: 0.178\n",
      "    [batch 747]: seen 74700 examples : 62.6 eps, Loss: 3.267, Avg loss: 3.465, Best loss: 3.465, cov loss: 0.175\n",
      "    [batch 754]: seen 75400 examples : 62.6 eps, Loss: 3.470, Avg loss: 3.467, Best loss: 3.465, cov loss: 0.170\n",
      "    [batch 761]: seen 76100 examples : 62.7 eps, Loss: 3.367, Avg loss: 3.468, Best loss: 3.465, cov loss: 0.173\n",
      "    [batch 768]: seen 76800 examples : 62.7 eps, Loss: 3.323, Avg loss: 3.466, Best loss: 3.465, cov loss: 0.166\n",
      "    [batch 775]: seen 77500 examples : 62.8 eps, Loss: 3.258, Avg loss: 3.466, Best loss: 3.465, cov loss: 0.168\n",
      "    [batch 780]: seen 78000 examples : 62.5 eps, Loss: 3.289, Avg loss: 3.464, Best loss: 3.464, cov loss: 0.179\n",
      "    [batch 787]: seen 78700 examples : 62.5 eps, Loss: 3.455, Avg loss: 3.466, Best loss: 3.464, cov loss: 0.172\n",
      "    [batch 794]: seen 79400 examples : 62.6 eps, Loss: 3.495, Avg loss: 3.469, Best loss: 3.464, cov loss: 0.161\n",
      "    [batch 801]: seen 80100 examples : 62.6 eps, Loss: 3.396, Avg loss: 3.469, Best loss: 3.464, cov loss: 0.173\n",
      "    [batch 808]: seen 80800 examples : 62.7 eps, Loss: 3.632, Avg loss: 3.472, Best loss: 3.464, cov loss: 0.208\n",
      "    [batch 815]: seen 81500 examples : 62.7 eps, Loss: 3.318, Avg loss: 3.468, Best loss: 3.464, cov loss: 0.155\n",
      "    [batch 822]: seen 82200 examples : 62.8 eps, Loss: 3.560, Avg loss: 3.474, Best loss: 3.464, cov loss: 0.183\n",
      "    [batch 829]: seen 82900 examples : 62.8 eps, Loss: 3.431, Avg loss: 3.472, Best loss: 3.464, cov loss: 0.160\n",
      "    [batch 836]: seen 83600 examples : 62.9 eps, Loss: 3.523, Avg loss: 3.471, Best loss: 3.464, cov loss: 0.180\n",
      "    [batch 843]: seen 84300 examples : 62.9 eps, Loss: 3.266, Avg loss: 3.468, Best loss: 3.464, cov loss: 0.183\n",
      "    [batch 848]: seen 84800 examples : 62.8 eps, Loss: 3.245, Avg loss: 3.464, Best loss: 3.464, cov loss: 0.162\n",
      "    [batch 853]: seen 85300 examples : 62.7 eps, Loss: 3.454, Avg loss: 3.467, Best loss: 3.464, cov loss: 0.191\n",
      "    [batch 860]: seen 86000 examples : 62.7 eps, Loss: 3.659, Avg loss: 3.467, Best loss: 3.464, cov loss: 0.183\n",
      "    [batch 865]: seen 86500 examples : 62.6 eps, Loss: 3.475, Avg loss: 3.464, Best loss: 3.463, cov loss: 0.175\n",
      "    [batch 872]: seen 87200 examples : 62.6 eps, Loss: 3.628, Avg loss: 3.466, Best loss: 3.463, cov loss: 0.174\n",
      "    [batch 879]: seen 87900 examples : 62.7 eps, Loss: 3.571, Avg loss: 3.470, Best loss: 3.463, cov loss: 0.184\n",
      "    [batch 886]: seen 88600 examples : 62.7 eps, Loss: 3.452, Avg loss: 3.468, Best loss: 3.463, cov loss: 0.197\n",
      "    [batch 893]: seen 89300 examples : 62.8 eps, Loss: 3.647, Avg loss: 3.469, Best loss: 3.463, cov loss: 0.171\n",
      "    [batch 900]: seen 90000 examples : 62.8 eps, Loss: 3.522, Avg loss: 3.466, Best loss: 3.463, cov loss: 0.171\n",
      "    [batch 907]: seen 90700 examples : 62.9 eps, Loss: 3.540, Avg loss: 3.473, Best loss: 3.463, cov loss: 0.181\n",
      "    [batch 914]: seen 91400 examples : 62.9 eps, Loss: 3.268, Avg loss: 3.471, Best loss: 3.463, cov loss: 0.180\n",
      "    [batch 921]: seen 92100 examples : 62.9 eps, Loss: 3.573, Avg loss: 3.468, Best loss: 3.463, cov loss: 0.165\n",
      "    [batch 928]: seen 92800 examples : 63.0 eps, Loss: 3.247, Avg loss: 3.463, Best loss: 3.463, cov loss: 0.175\n",
      "    [batch 933]: seen 93300 examples : 62.7 eps, Loss: 3.372, Avg loss: 3.462, Best loss: 3.462, cov loss: 0.166\n",
      "    [batch 938]: seen 93800 examples : 62.6 eps, Loss: 3.392, Avg loss: 3.461, Best loss: 3.461, cov loss: 0.187\n",
      "    [batch 941]: seen 94100 examples : 62.2 eps, Loss: 3.102, Avg loss: 3.455, Best loss: 3.455, cov loss: 0.145\n",
      "    [batch 948]: seen 94800 examples : 62.2 eps, Loss: 3.484, Avg loss: 3.457, Best loss: 3.455, cov loss: 0.164\n",
      "    [batch 955]: seen 95500 examples : 62.3 eps, Loss: 3.570, Avg loss: 3.463, Best loss: 3.455, cov loss: 0.181\n",
      "    [batch 962]: seen 96200 examples : 62.3 eps, Loss: 3.444, Avg loss: 3.465, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 969]: seen 96900 examples : 62.4 eps, Loss: 3.503, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.164\n",
      "    [batch 976]: seen 97600 examples : 62.4 eps, Loss: 3.465, Avg loss: 3.465, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 983]: seen 98300 examples : 62.5 eps, Loss: 3.354, Avg loss: 3.462, Best loss: 3.455, cov loss: 0.171\n",
      "    [batch 990]: seen 99000 examples : 62.5 eps, Loss: 3.400, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 997]: seen 99700 examples : 62.5 eps, Loss: 3.522, Avg loss: 3.463, Best loss: 3.455, cov loss: 0.178\n",
      "    [batch 1004]: seen 100400 examples : 62.6 eps, Loss: 3.435, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.179\n",
      "    [batch 1011]: seen 101100 examples : 62.6 eps, Loss: 3.491, Avg loss: 3.465, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 1018]: seen 101800 examples : 62.7 eps, Loss: 3.681, Avg loss: 3.470, Best loss: 3.455, cov loss: 0.202\n",
      "    [batch 1025]: seen 102500 examples : 62.7 eps, Loss: 3.660, Avg loss: 3.474, Best loss: 3.455, cov loss: 0.171\n",
      "    [batch 1032]: seen 103200 examples : 62.7 eps, Loss: 3.256, Avg loss: 3.469, Best loss: 3.455, cov loss: 0.171\n",
      "    [batch 1039]: seen 103900 examples : 62.8 eps, Loss: 3.616, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.163\n",
      "    [batch 1046]: seen 104600 examples : 62.8 eps, Loss: 3.352, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.175\n",
      "    [batch 1053]: seen 105300 examples : 62.8 eps, Loss: 3.182, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.171\n",
      "    [batch 1060]: seen 106000 examples : 62.9 eps, Loss: 3.432, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.184\n",
      "    [batch 1067]: seen 106700 examples : 62.9 eps, Loss: 3.223, Avg loss: 3.465, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 1074]: seen 107400 examples : 63.0 eps, Loss: 3.544, Avg loss: 3.470, Best loss: 3.455, cov loss: 0.164\n",
      "    [batch 1081]: seen 108100 examples : 63.0 eps, Loss: 3.470, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 1088]: seen 108800 examples : 63.0 eps, Loss: 3.529, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.190\n",
      "    [batch 1095]: seen 109500 examples : 63.1 eps, Loss: 3.435, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.169\n",
      "    [batch 1102]: seen 110200 examples : 63.1 eps, Loss: 3.489, Avg loss: 3.470, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 1109]: seen 110900 examples : 63.1 eps, Loss: 3.333, Avg loss: 3.472, Best loss: 3.455, cov loss: 0.176\n",
      "    [batch 1116]: seen 111600 examples : 63.2 eps, Loss: 3.493, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 1123]: seen 112300 examples : 63.2 eps, Loss: 3.479, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.161\n",
      "    [batch 1130]: seen 113000 examples : 63.2 eps, Loss: 3.498, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 1137]: seen 113700 examples : 63.3 eps, Loss: 3.380, Avg loss: 3.462, Best loss: 3.455, cov loss: 0.192\n",
      "    [batch 1144]: seen 114400 examples : 63.3 eps, Loss: 3.317, Avg loss: 3.465, Best loss: 3.455, cov loss: 0.194\n",
      "    [batch 1151]: seen 115100 examples : 63.3 eps, Loss: 3.295, Avg loss: 3.465, Best loss: 3.455, cov loss: 0.163\n",
      "    [batch 1158]: seen 115800 examples : 63.4 eps, Loss: 3.355, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.187\n",
      "    [batch 1165]: seen 116500 examples : 63.4 eps, Loss: 3.506, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.181\n",
      "    [batch 1172]: seen 117200 examples : 63.4 eps, Loss: 3.543, Avg loss: 3.463, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 1179]: seen 117900 examples : 63.5 eps, Loss: 3.601, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.183\n",
      "    [batch 1186]: seen 118600 examples : 63.5 eps, Loss: 3.618, Avg loss: 3.469, Best loss: 3.455, cov loss: 0.185\n",
      "    [batch 1193]: seen 119300 examples : 63.5 eps, Loss: 3.613, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.170\n",
      "    [batch 1200]: seen 120000 examples : 63.5 eps, Loss: 3.408, Avg loss: 3.476, Best loss: 3.455, cov loss: 0.184\n",
      "    [batch 1207]: seen 120700 examples : 63.6 eps, Loss: 3.519, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.179\n",
      "    [batch 1214]: seen 121400 examples : 63.6 eps, Loss: 3.376, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 1221]: seen 122100 examples : 63.6 eps, Loss: 3.454, Avg loss: 3.479, Best loss: 3.455, cov loss: 0.182\n",
      "    [batch 1228]: seen 122800 examples : 63.7 eps, Loss: 3.401, Avg loss: 3.474, Best loss: 3.455, cov loss: 0.166\n",
      "    [batch 1235]: seen 123500 examples : 63.7 eps, Loss: 3.496, Avg loss: 3.472, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 1242]: seen 124200 examples : 63.7 eps, Loss: 3.514, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.182\n",
      "    [batch 1249]: seen 124900 examples : 63.7 eps, Loss: 3.656, Avg loss: 3.481, Best loss: 3.455, cov loss: 0.194\n",
      "    [batch 1256]: seen 125600 examples : 63.8 eps, Loss: 3.356, Avg loss: 3.482, Best loss: 3.455, cov loss: 0.171\n",
      "    [batch 1263]: seen 126300 examples : 63.8 eps, Loss: 3.670, Avg loss: 3.484, Best loss: 3.455, cov loss: 0.183\n",
      "    [batch 1270]: seen 127000 examples : 63.8 eps, Loss: 3.489, Avg loss: 3.486, Best loss: 3.455, cov loss: 0.176\n",
      "    [batch 1277]: seen 127700 examples : 63.8 eps, Loss: 3.499, Avg loss: 3.488, Best loss: 3.455, cov loss: 0.175\n",
      "    [batch 1284]: seen 128400 examples : 63.9 eps, Loss: 3.531, Avg loss: 3.488, Best loss: 3.455, cov loss: 0.171\n",
      "    [batch 1291]: seen 129100 examples : 63.9 eps, Loss: 3.519, Avg loss: 3.484, Best loss: 3.455, cov loss: 0.192\n",
      "    [batch 1298]: seen 129800 examples : 63.9 eps, Loss: 3.473, Avg loss: 3.481, Best loss: 3.455, cov loss: 0.167\n",
      "    [batch 1305]: seen 130500 examples : 64.0 eps, Loss: 3.525, Avg loss: 3.480, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 1312]: seen 131200 examples : 64.0 eps, Loss: 3.660, Avg loss: 3.478, Best loss: 3.455, cov loss: 0.172\n",
      "    [batch 1319]: seen 131900 examples : 64.0 eps, Loss: 3.466, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 1326]: seen 132600 examples : 64.0 eps, Loss: 3.422, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.184\n",
      "    [batch 1333]: seen 133300 examples : 64.1 eps, Loss: 3.089, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.165\n",
      "    [batch 1340]: seen 134000 examples : 64.1 eps, Loss: 3.559, Avg loss: 3.469, Best loss: 3.455, cov loss: 0.185\n",
      "    [batch 1347]: seen 134700 examples : 64.1 eps, Loss: 3.444, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.183\n",
      "    [batch 1354]: seen 135400 examples : 64.1 eps, Loss: 3.457, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.181\n",
      "    [batch 1361]: seen 136100 examples : 64.1 eps, Loss: 3.474, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.179\n",
      "    [batch 1368]: seen 136800 examples : 64.2 eps, Loss: 3.497, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.182\n",
      "    [batch 1375]: seen 137500 examples : 64.2 eps, Loss: 3.442, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.179\n",
      "    [batch 1382]: seen 138200 examples : 64.2 eps, Loss: 3.373, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.170\n",
      "    [batch 1389]: seen 138900 examples : 64.2 eps, Loss: 3.447, Avg loss: 3.465, Best loss: 3.455, cov loss: 0.181\n",
      "    [batch 1396]: seen 139600 examples : 64.3 eps, Loss: 3.344, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.181\n",
      "    [batch 1403]: seen 140300 examples : 64.3 eps, Loss: 3.324, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.181\n",
      "    [batch 1410]: seen 141000 examples : 64.3 eps, Loss: 3.432, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.169\n",
      "    [batch 1417]: seen 141700 examples : 64.3 eps, Loss: 3.807, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.193\n",
      "    [batch 1424]: seen 142400 examples : 64.3 eps, Loss: 3.480, Avg loss: 3.477, Best loss: 3.455, cov loss: 0.178\n",
      "    [batch 1431]: seen 143100 examples : 64.4 eps, Loss: 3.430, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.168\n",
      "    [batch 1438]: seen 143800 examples : 64.4 eps, Loss: 3.573, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 1445]: seen 144500 examples : 64.4 eps, Loss: 3.417, Avg loss: 3.477, Best loss: 3.455, cov loss: 0.179\n",
      "    [batch 1452]: seen 145200 examples : 64.4 eps, Loss: 3.509, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.199\n",
      "    [batch 1459]: seen 145900 examples : 64.5 eps, Loss: 3.550, Avg loss: 3.474, Best loss: 3.455, cov loss: 0.201\n",
      "    [batch 1466]: seen 146600 examples : 64.5 eps, Loss: 3.428, Avg loss: 3.474, Best loss: 3.455, cov loss: 0.168\n",
      "    [batch 1473]: seen 147300 examples : 64.5 eps, Loss: 3.364, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.172\n",
      "    [batch 1480]: seen 148000 examples : 64.5 eps, Loss: 3.513, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.190\n",
      "    [batch 1487]: seen 148700 examples : 64.5 eps, Loss: 3.613, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 1494]: seen 149400 examples : 64.6 eps, Loss: 3.512, Avg loss: 3.477, Best loss: 3.455, cov loss: 0.150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1501]: seen 150100 examples : 64.6 eps, Loss: 3.406, Avg loss: 3.478, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 1508]: seen 150800 examples : 64.6 eps, Loss: 3.651, Avg loss: 3.479, Best loss: 3.455, cov loss: 0.165\n",
      "    [batch 1515]: seen 151500 examples : 64.6 eps, Loss: 3.532, Avg loss: 3.478, Best loss: 3.455, cov loss: 0.188\n",
      "    [batch 1522]: seen 152200 examples : 64.6 eps, Loss: 3.451, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.181\n",
      "    [batch 1529]: seen 152900 examples : 64.6 eps, Loss: 3.359, Avg loss: 3.477, Best loss: 3.455, cov loss: 0.169\n",
      "    [batch 1536]: seen 153600 examples : 64.7 eps, Loss: 3.736, Avg loss: 3.474, Best loss: 3.455, cov loss: 0.193\n",
      "    [batch 1543]: seen 154300 examples : 64.7 eps, Loss: 3.448, Avg loss: 3.469, Best loss: 3.455, cov loss: 0.168\n",
      "    [batch 1550]: seen 155000 examples : 64.7 eps, Loss: 3.585, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.191\n",
      "    [batch 1557]: seen 155700 examples : 64.7 eps, Loss: 3.394, Avg loss: 3.470, Best loss: 3.455, cov loss: 0.160\n",
      "    [batch 1564]: seen 156400 examples : 64.7 eps, Loss: 3.411, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.168\n",
      "    [batch 1571]: seen 157100 examples : 64.8 eps, Loss: 3.407, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.173\n",
      "    [batch 1578]: seen 157800 examples : 64.8 eps, Loss: 3.466, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.193\n",
      "    [batch 1585]: seen 158500 examples : 64.8 eps, Loss: 3.415, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.164\n",
      "    [batch 1592]: seen 159200 examples : 64.8 eps, Loss: 3.520, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.178\n",
      "    [batch 1599]: seen 159900 examples : 64.8 eps, Loss: 3.447, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.166\n",
      "    [batch 1606]: seen 160600 examples : 64.8 eps, Loss: 3.256, Avg loss: 3.470, Best loss: 3.455, cov loss: 0.164\n",
      "    [batch 1613]: seen 161300 examples : 64.9 eps, Loss: 3.388, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.183\n",
      "    [batch 1620]: seen 162000 examples : 64.9 eps, Loss: 3.624, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.166\n",
      "    [batch 1627]: seen 162700 examples : 64.9 eps, Loss: 3.472, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.189\n",
      "    [batch 1634]: seen 163400 examples : 64.9 eps, Loss: 3.485, Avg loss: 3.461, Best loss: 3.455, cov loss: 0.184\n",
      "    [batch 1641]: seen 164100 examples : 64.9 eps, Loss: 3.345, Avg loss: 3.460, Best loss: 3.455, cov loss: 0.183\n",
      "    [batch 1648]: seen 164800 examples : 64.9 eps, Loss: 3.624, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.192\n",
      "    [batch 1655]: seen 165500 examples : 65.0 eps, Loss: 3.397, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.171\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-30608\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-30608\n",
      "    [batch 1661]: seen 166100 examples : 64.9 eps, Loss: 3.555, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.181\n",
      "    [batch 1668]: seen 166800 examples : 64.9 eps, Loss: 3.413, Avg loss: 3.474, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 1675]: seen 167500 examples : 65.0 eps, Loss: 3.610, Avg loss: 3.480, Best loss: 3.455, cov loss: 0.178\n",
      "    [batch 1682]: seen 168200 examples : 65.0 eps, Loss: 3.460, Avg loss: 3.477, Best loss: 3.455, cov loss: 0.159\n",
      "    [batch 1689]: seen 168900 examples : 65.0 eps, Loss: 3.413, Avg loss: 3.474, Best loss: 3.455, cov loss: 0.169\n",
      "    [batch 1696]: seen 169600 examples : 65.0 eps, Loss: 3.498, Avg loss: 3.476, Best loss: 3.455, cov loss: 0.184\n",
      "    [batch 1703]: seen 170300 examples : 65.0 eps, Loss: 3.540, Avg loss: 3.474, Best loss: 3.455, cov loss: 0.172\n",
      "    [batch 1710]: seen 171000 examples : 65.0 eps, Loss: 3.532, Avg loss: 3.477, Best loss: 3.455, cov loss: 0.172\n",
      "    [batch 1717]: seen 171700 examples : 65.1 eps, Loss: 3.462, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.181\n",
      "    [batch 1724]: seen 172400 examples : 65.1 eps, Loss: 3.292, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.180\n",
      "    [batch 1731]: seen 173100 examples : 65.1 eps, Loss: 3.581, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 1738]: seen 173800 examples : 65.1 eps, Loss: 3.503, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 1745]: seen 174500 examples : 65.1 eps, Loss: 3.339, Avg loss: 3.476, Best loss: 3.455, cov loss: 0.189\n",
      "    [batch 1752]: seen 175200 examples : 65.1 eps, Loss: 3.613, Avg loss: 3.477, Best loss: 3.455, cov loss: 0.203\n",
      "    [batch 1759]: seen 175900 examples : 65.1 eps, Loss: 3.562, Avg loss: 3.476, Best loss: 3.455, cov loss: 0.184\n",
      "    [batch 1766]: seen 176600 examples : 65.2 eps, Loss: 3.672, Avg loss: 3.477, Best loss: 3.455, cov loss: 0.190\n",
      "    [batch 1773]: seen 177300 examples : 65.2 eps, Loss: 3.562, Avg loss: 3.477, Best loss: 3.455, cov loss: 0.166\n",
      "    [batch 1780]: seen 178000 examples : 65.2 eps, Loss: 3.607, Avg loss: 3.482, Best loss: 3.455, cov loss: 0.188\n",
      "    [batch 1787]: seen 178700 examples : 65.2 eps, Loss: 3.633, Avg loss: 3.491, Best loss: 3.455, cov loss: 0.216\n",
      "    [batch 1794]: seen 179400 examples : 65.2 eps, Loss: 3.574, Avg loss: 3.492, Best loss: 3.455, cov loss: 0.190\n",
      "    [batch 1801]: seen 180100 examples : 65.2 eps, Loss: 3.463, Avg loss: 3.491, Best loss: 3.455, cov loss: 0.181\n",
      "    [batch 1808]: seen 180800 examples : 65.2 eps, Loss: 3.462, Avg loss: 3.491, Best loss: 3.455, cov loss: 0.187\n",
      "    [batch 1815]: seen 181500 examples : 65.3 eps, Loss: 3.452, Avg loss: 3.491, Best loss: 3.455, cov loss: 0.173\n",
      "    [batch 1822]: seen 182200 examples : 65.3 eps, Loss: 3.496, Avg loss: 3.488, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 1829]: seen 182900 examples : 65.3 eps, Loss: 3.528, Avg loss: 3.489, Best loss: 3.455, cov loss: 0.181\n",
      "    [batch 1836]: seen 183600 examples : 65.3 eps, Loss: 3.592, Avg loss: 3.492, Best loss: 3.455, cov loss: 0.182\n",
      "    [batch 1843]: seen 184300 examples : 65.3 eps, Loss: 3.294, Avg loss: 3.490, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 1850]: seen 185000 examples : 65.3 eps, Loss: 3.453, Avg loss: 3.486, Best loss: 3.455, cov loss: 0.203\n",
      "    [batch 1857]: seen 185700 examples : 65.3 eps, Loss: 3.393, Avg loss: 3.486, Best loss: 3.455, cov loss: 0.179\n",
      "    [batch 1864]: seen 186400 examples : 65.4 eps, Loss: 3.511, Avg loss: 3.484, Best loss: 3.455, cov loss: 0.179\n",
      "    [batch 1871]: seen 187100 examples : 65.4 eps, Loss: 3.366, Avg loss: 3.477, Best loss: 3.455, cov loss: 0.171\n",
      "    [batch 1878]: seen 187800 examples : 65.4 eps, Loss: 3.481, Avg loss: 3.479, Best loss: 3.455, cov loss: 0.174\n",
      "    [batch 1885]: seen 188500 examples : 65.4 eps, Loss: 3.389, Avg loss: 3.481, Best loss: 3.455, cov loss: 0.171\n",
      "    [batch 1892]: seen 189200 examples : 65.4 eps, Loss: 3.374, Avg loss: 3.477, Best loss: 3.455, cov loss: 0.183\n",
      "    [batch 1899]: seen 189900 examples : 65.4 eps, Loss: 3.557, Avg loss: 3.477, Best loss: 3.455, cov loss: 0.153\n",
      "    [batch 1906]: seen 190600 examples : 65.4 eps, Loss: 3.360, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.162\n",
      "    [batch 1913]: seen 191300 examples : 65.4 eps, Loss: 3.482, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.160\n",
      "    [batch 1920]: seen 192000 examples : 65.5 eps, Loss: 3.440, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.180\n",
      "    [batch 1927]: seen 192700 examples : 65.5 eps, Loss: 3.414, Avg loss: 3.465, Best loss: 3.455, cov loss: 0.191\n",
      "    [batch 1934]: seen 193400 examples : 65.5 eps, Loss: 3.490, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.173\n",
      "    [batch 1941]: seen 194100 examples : 65.5 eps, Loss: 3.418, Avg loss: 3.457, Best loss: 3.455, cov loss: 0.179\n",
      "    [batch 1948]: seen 194800 examples : 65.5 eps, Loss: 3.544, Avg loss: 3.460, Best loss: 3.455, cov loss: 0.200\n",
      "    [batch 1955]: seen 195500 examples : 65.5 eps, Loss: 3.245, Avg loss: 3.457, Best loss: 3.455, cov loss: 0.154\n",
      "    [batch 1962]: seen 196200 examples : 65.5 eps, Loss: 3.407, Avg loss: 3.458, Best loss: 3.455, cov loss: 0.176\n",
      "    [batch 1969]: seen 196900 examples : 65.5 eps, Loss: 3.390, Avg loss: 3.459, Best loss: 3.455, cov loss: 0.182\n",
      "    [batch 1976]: seen 197600 examples : 65.6 eps, Loss: 3.400, Avg loss: 3.463, Best loss: 3.455, cov loss: 0.166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1983]: seen 198300 examples : 65.6 eps, Loss: 3.419, Avg loss: 3.462, Best loss: 3.455, cov loss: 0.170\n",
      "    [batch 1990]: seen 199000 examples : 65.6 eps, Loss: 3.415, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.169\n",
      "    [batch 1997]: seen 199700 examples : 65.6 eps, Loss: 3.783, Avg loss: 3.469, Best loss: 3.455, cov loss: 0.172\n",
      "    [batch 2004]: seen 200400 examples : 65.6 eps, Loss: 3.508, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.171\n",
      "    [batch 2011]: seen 201100 examples : 65.6 eps, Loss: 3.435, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.178\n",
      "    [batch 2018]: seen 201800 examples : 65.6 eps, Loss: 3.484, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 2025]: seen 202500 examples : 65.6 eps, Loss: 3.379, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.188\n",
      "    [batch 2032]: seen 203200 examples : 65.6 eps, Loss: 3.461, Avg loss: 3.465, Best loss: 3.455, cov loss: 0.175\n",
      "    [batch 2039]: seen 203900 examples : 65.7 eps, Loss: 3.452, Avg loss: 3.461, Best loss: 3.455, cov loss: 0.172\n",
      "    [batch 2046]: seen 204600 examples : 65.7 eps, Loss: 3.642, Avg loss: 3.462, Best loss: 3.455, cov loss: 0.201\n",
      "    [batch 2053]: seen 205300 examples : 65.7 eps, Loss: 3.403, Avg loss: 3.460, Best loss: 3.455, cov loss: 0.169\n",
      "    [batch 2060]: seen 206000 examples : 65.7 eps, Loss: 3.553, Avg loss: 3.461, Best loss: 3.455, cov loss: 0.175\n",
      "    [batch 2067]: seen 206700 examples : 65.7 eps, Loss: 3.498, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.165\n",
      "    [batch 2074]: seen 207400 examples : 65.7 eps, Loss: 3.356, Avg loss: 3.470, Best loss: 3.455, cov loss: 0.171\n",
      "    [batch 2081]: seen 208100 examples : 65.7 eps, Loss: 3.558, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 2088]: seen 208800 examples : 65.7 eps, Loss: 3.249, Avg loss: 3.469, Best loss: 3.455, cov loss: 0.178\n",
      "    [batch 2095]: seen 209500 examples : 65.7 eps, Loss: 3.613, Avg loss: 3.469, Best loss: 3.455, cov loss: 0.176\n",
      "    [batch 2102]: seen 210200 examples : 65.8 eps, Loss: 3.688, Avg loss: 3.472, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 2109]: seen 210900 examples : 65.8 eps, Loss: 3.340, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.174\n",
      "    [batch 2116]: seen 211600 examples : 65.8 eps, Loss: 3.401, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.166\n",
      "    [batch 2123]: seen 212300 examples : 65.8 eps, Loss: 3.425, Avg loss: 3.465, Best loss: 3.455, cov loss: 0.174\n",
      "    [batch 2130]: seen 213000 examples : 65.8 eps, Loss: 3.558, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.185\n",
      "    [batch 2137]: seen 213700 examples : 65.8 eps, Loss: 3.573, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.173\n",
      "    [batch 2144]: seen 214400 examples : 65.8 eps, Loss: 3.485, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.179\n",
      "    [batch 2151]: seen 215100 examples : 65.8 eps, Loss: 3.396, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.178\n",
      "    [batch 2158]: seen 215800 examples : 65.8 eps, Loss: 3.548, Avg loss: 3.474, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 2165]: seen 216500 examples : 65.8 eps, Loss: 3.356, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.168\n",
      "    [batch 2172]: seen 217200 examples : 65.9 eps, Loss: 3.468, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.175\n",
      "    [batch 2179]: seen 217900 examples : 65.9 eps, Loss: 3.234, Avg loss: 3.472, Best loss: 3.455, cov loss: 0.155\n",
      "    [batch 2186]: seen 218600 examples : 65.9 eps, Loss: 3.434, Avg loss: 3.472, Best loss: 3.455, cov loss: 0.204\n",
      "    [batch 2193]: seen 219300 examples : 65.9 eps, Loss: 3.207, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.176\n",
      "    [batch 2200]: seen 220000 examples : 65.9 eps, Loss: 3.431, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.163\n",
      "    [batch 2207]: seen 220700 examples : 65.9 eps, Loss: 3.297, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 2214]: seen 221400 examples : 65.9 eps, Loss: 3.532, Avg loss: 3.463, Best loss: 3.455, cov loss: 0.192\n",
      "    [batch 2221]: seen 222100 examples : 65.9 eps, Loss: 3.448, Avg loss: 3.461, Best loss: 3.455, cov loss: 0.175\n",
      "    [batch 2228]: seen 222800 examples : 65.9 eps, Loss: 3.424, Avg loss: 3.462, Best loss: 3.455, cov loss: 0.188\n",
      "    [batch 2235]: seen 223500 examples : 65.9 eps, Loss: 3.257, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.190\n",
      "    [batch 2242]: seen 224200 examples : 65.9 eps, Loss: 3.497, Avg loss: 3.463, Best loss: 3.455, cov loss: 0.168\n",
      "    [batch 2249]: seen 224900 examples : 66.0 eps, Loss: 3.525, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.166\n",
      "    [batch 2256]: seen 225600 examples : 66.0 eps, Loss: 3.354, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.180\n",
      "    [batch 2263]: seen 226300 examples : 66.0 eps, Loss: 3.437, Avg loss: 3.469, Best loss: 3.455, cov loss: 0.160\n",
      "    [batch 2270]: seen 227000 examples : 66.0 eps, Loss: 3.522, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.173\n",
      "    [batch 2277]: seen 227700 examples : 66.0 eps, Loss: 3.314, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.176\n",
      "    [batch 2284]: seen 228400 examples : 66.0 eps, Loss: 3.422, Avg loss: 3.463, Best loss: 3.455, cov loss: 0.181\n",
      "    [batch 2291]: seen 229100 examples : 66.0 eps, Loss: 3.321, Avg loss: 3.461, Best loss: 3.455, cov loss: 0.191\n",
      "    [batch 2298]: seen 229800 examples : 66.0 eps, Loss: 3.506, Avg loss: 3.461, Best loss: 3.455, cov loss: 0.183\n",
      "    [batch 2305]: seen 230500 examples : 66.0 eps, Loss: 3.632, Avg loss: 3.462, Best loss: 3.455, cov loss: 0.178\n",
      "    [batch 2312]: seen 231200 examples : 66.0 eps, Loss: 3.704, Avg loss: 3.462, Best loss: 3.455, cov loss: 0.201\n",
      "    [batch 2319]: seen 231900 examples : 66.0 eps, Loss: 3.438, Avg loss: 3.462, Best loss: 3.455, cov loss: 0.172\n",
      "    [batch 2326]: seen 232600 examples : 66.1 eps, Loss: 3.418, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.195\n",
      "    [batch 2333]: seen 233300 examples : 66.1 eps, Loss: 3.565, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.184\n",
      "    [batch 2340]: seen 234000 examples : 66.1 eps, Loss: 3.583, Avg loss: 3.470, Best loss: 3.455, cov loss: 0.168\n",
      "    [batch 2347]: seen 234700 examples : 66.1 eps, Loss: 3.542, Avg loss: 3.474, Best loss: 3.455, cov loss: 0.180\n",
      "    [batch 2354]: seen 235400 examples : 66.1 eps, Loss: 3.368, Avg loss: 3.476, Best loss: 3.455, cov loss: 0.174\n",
      "    [batch 2361]: seen 236100 examples : 66.1 eps, Loss: 3.650, Avg loss: 3.476, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 2368]: seen 236800 examples : 66.1 eps, Loss: 3.473, Avg loss: 3.476, Best loss: 3.455, cov loss: 0.174\n",
      "    [batch 2375]: seen 237500 examples : 66.1 eps, Loss: 3.401, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.184\n",
      "    [batch 2382]: seen 238200 examples : 66.1 eps, Loss: 3.376, Avg loss: 3.472, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 2389]: seen 238900 examples : 66.1 eps, Loss: 3.607, Avg loss: 3.476, Best loss: 3.455, cov loss: 0.207\n",
      "    [batch 2396]: seen 239600 examples : 66.1 eps, Loss: 3.511, Avg loss: 3.475, Best loss: 3.455, cov loss: 0.178\n",
      "    [batch 2403]: seen 240300 examples : 66.1 eps, Loss: 3.394, Avg loss: 3.469, Best loss: 3.455, cov loss: 0.172\n",
      "    [batch 2410]: seen 241000 examples : 66.2 eps, Loss: 3.560, Avg loss: 3.470, Best loss: 3.455, cov loss: 0.188\n",
      "    [batch 2417]: seen 241700 examples : 66.2 eps, Loss: 3.503, Avg loss: 3.470, Best loss: 3.455, cov loss: 0.175\n",
      "    [batch 2424]: seen 242400 examples : 66.2 eps, Loss: 3.526, Avg loss: 3.470, Best loss: 3.455, cov loss: 0.186\n",
      "    [batch 2431]: seen 243100 examples : 66.2 eps, Loss: 3.209, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.147\n",
      "    [batch 2438]: seen 243800 examples : 66.2 eps, Loss: 3.414, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.178\n",
      "    [batch 2445]: seen 244500 examples : 66.2 eps, Loss: 3.223, Avg loss: 3.461, Best loss: 3.455, cov loss: 0.170\n",
      "    [batch 2452]: seen 245200 examples : 66.2 eps, Loss: 3.412, Avg loss: 3.461, Best loss: 3.455, cov loss: 0.176\n",
      "    [batch 2459]: seen 245900 examples : 66.2 eps, Loss: 3.222, Avg loss: 3.461, Best loss: 3.455, cov loss: 0.166\n",
      "    [batch 2466]: seen 246600 examples : 66.2 eps, Loss: 3.474, Avg loss: 3.458, Best loss: 3.455, cov loss: 0.163\n",
      "    [batch 2473]: seen 247300 examples : 66.2 eps, Loss: 3.492, Avg loss: 3.460, Best loss: 3.455, cov loss: 0.177\n",
      "    [batch 2480]: seen 248000 examples : 66.2 eps, Loss: 3.417, Avg loss: 3.458, Best loss: 3.455, cov loss: 0.176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2487]: seen 248700 examples : 66.2 eps, Loss: 3.531, Avg loss: 3.457, Best loss: 3.455, cov loss: 0.173\n",
      "    [batch 2494]: seen 249400 examples : 66.2 eps, Loss: 3.426, Avg loss: 3.456, Best loss: 3.455, cov loss: 0.157\n",
      "    [batch 2501]: seen 250100 examples : 66.2 eps, Loss: 3.414, Avg loss: 3.460, Best loss: 3.455, cov loss: 0.161\n",
      "    [batch 2508]: seen 250800 examples : 66.3 eps, Loss: 3.330, Avg loss: 3.458, Best loss: 3.455, cov loss: 0.191\n",
      "    [batch 2515]: seen 251500 examples : 66.3 eps, Loss: 3.674, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.179\n",
      "    [batch 2522]: seen 252200 examples : 66.3 eps, Loss: 3.429, Avg loss: 3.464, Best loss: 3.455, cov loss: 0.184\n",
      "    [batch 2529]: seen 252900 examples : 66.3 eps, Loss: 3.331, Avg loss: 3.462, Best loss: 3.455, cov loss: 0.195\n",
      "    [batch 2536]: seen 253600 examples : 66.3 eps, Loss: 3.489, Avg loss: 3.472, Best loss: 3.455, cov loss: 0.191\n",
      "    [batch 2543]: seen 254300 examples : 66.3 eps, Loss: 3.452, Avg loss: 3.472, Best loss: 3.455, cov loss: 0.170\n",
      "    [batch 2550]: seen 255000 examples : 66.3 eps, Loss: 3.349, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.176\n",
      "    [batch 2557]: seen 255700 examples : 66.3 eps, Loss: 3.551, Avg loss: 3.471, Best loss: 3.455, cov loss: 0.179\n",
      "    [batch 2564]: seen 256400 examples : 66.3 eps, Loss: 3.426, Avg loss: 3.472, Best loss: 3.455, cov loss: 0.182\n",
      "    [batch 2571]: seen 257100 examples : 66.3 eps, Loss: 3.409, Avg loss: 3.472, Best loss: 3.455, cov loss: 0.179\n",
      "    [batch 2578]: seen 257800 examples : 66.3 eps, Loss: 3.475, Avg loss: 3.473, Best loss: 3.455, cov loss: 0.184\n",
      "    [batch 2585]: seen 258500 examples : 66.3 eps, Loss: 3.322, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.158\n",
      "    [batch 2592]: seen 259200 examples : 66.3 eps, Loss: 3.419, Avg loss: 3.463, Best loss: 3.455, cov loss: 0.185\n",
      "    [batch 2599]: seen 259900 examples : 66.4 eps, Loss: 3.409, Avg loss: 3.466, Best loss: 3.455, cov loss: 0.180\n",
      "    [batch 2606]: seen 260600 examples : 66.4 eps, Loss: 3.463, Avg loss: 3.468, Best loss: 3.455, cov loss: 0.180\n",
      "    [batch 2613]: seen 261300 examples : 66.4 eps, Loss: 3.727, Avg loss: 3.467, Best loss: 3.455, cov loss: 0.194\n",
      "    [batch 2620]: seen 262000 examples : 66.4 eps, Loss: 3.706, Avg loss: 3.463, Best loss: 3.455, cov loss: 0.169\n",
      "    [batch 2627]: seen 262700 examples : 66.4 eps, Loss: 3.319, Avg loss: 3.461, Best loss: 3.455, cov loss: 0.163\n",
      "    [batch 2634]: seen 263400 examples : 66.4 eps, Loss: 3.372, Avg loss: 3.459, Best loss: 3.455, cov loss: 0.180\n",
      "    [batch 2641]: seen 264100 examples : 66.4 eps, Loss: 3.274, Avg loss: 3.462, Best loss: 3.455, cov loss: 0.176\n",
      "    [batch 2648]: seen 264800 examples : 66.4 eps, Loss: 3.326, Avg loss: 3.461, Best loss: 3.455, cov loss: 0.192\n",
      "    [batch 2655]: seen 265500 examples : 66.4 eps, Loss: 3.550, Avg loss: 3.463, Best loss: 3.455, cov loss: 0.171\n",
      "    [batch 2662]: seen 266200 examples : 66.4 eps, Loss: 3.310, Avg loss: 3.460, Best loss: 3.455, cov loss: 0.174\n",
      "    [batch 2667]: seen 266700 examples : 66.4 eps, Loss: 3.330, Avg loss: 3.455, Best loss: 3.454, cov loss: 0.182\n",
      "    [batch 2674]: seen 267400 examples : 66.4 eps, Loss: 3.531, Avg loss: 3.457, Best loss: 3.454, cov loss: 0.172\n",
      "    [batch 2681]: seen 268100 examples : 66.4 eps, Loss: 3.494, Avg loss: 3.457, Best loss: 3.454, cov loss: 0.175\n",
      "    [batch 2688]: seen 268800 examples : 66.4 eps, Loss: 3.464, Avg loss: 3.461, Best loss: 3.454, cov loss: 0.169\n",
      "    [batch 2695]: seen 269500 examples : 66.4 eps, Loss: 3.513, Avg loss: 3.460, Best loss: 3.454, cov loss: 0.189\n",
      "    [batch 2702]: seen 270200 examples : 66.4 eps, Loss: 3.220, Avg loss: 3.456, Best loss: 3.454, cov loss: 0.181\n",
      "    [batch 2709]: seen 270900 examples : 66.4 eps, Loss: 3.507, Avg loss: 3.461, Best loss: 3.454, cov loss: 0.165\n",
      "    [batch 2716]: seen 271600 examples : 66.4 eps, Loss: 3.315, Avg loss: 3.459, Best loss: 3.454, cov loss: 0.189\n",
      "    [batch 2723]: seen 272300 examples : 66.4 eps, Loss: 3.243, Avg loss: 3.460, Best loss: 3.454, cov loss: 0.182\n",
      "    [batch 2730]: seen 273000 examples : 66.4 eps, Loss: 3.392, Avg loss: 3.462, Best loss: 3.454, cov loss: 0.184\n",
      "    [batch 2737]: seen 273700 examples : 66.4 eps, Loss: 3.384, Avg loss: 3.461, Best loss: 3.454, cov loss: 0.169\n",
      "    [batch 2744]: seen 274400 examples : 66.4 eps, Loss: 3.290, Avg loss: 3.458, Best loss: 3.454, cov loss: 0.178\n",
      "    [batch 2751]: seen 275100 examples : 66.4 eps, Loss: 3.374, Avg loss: 3.456, Best loss: 3.454, cov loss: 0.162\n",
      "    [batch 2754]: seen 275400 examples : 66.3 eps, Loss: 3.345, Avg loss: 3.453, Best loss: 3.453, cov loss: 0.187\n",
      "    [batch 2757]: seen 275700 examples : 66.2 eps, Loss: 3.521, Avg loss: 3.451, Best loss: 3.451, cov loss: 0.181\n",
      "    [batch 2763]: seen 276300 examples : 66.2 eps, Loss: 3.250, Avg loss: 3.449, Best loss: 3.449, cov loss: 0.189\n",
      "    [batch 2768]: seen 276800 examples : 66.1 eps, Loss: 3.121, Avg loss: 3.446, Best loss: 3.446, cov loss: 0.176\n",
      "    [batch 2773]: seen 277300 examples : 66.0 eps, Loss: 3.434, Avg loss: 3.447, Best loss: 3.445, cov loss: 0.172\n",
      "    [batch 2780]: seen 278000 examples : 66.0 eps, Loss: 3.465, Avg loss: 3.447, Best loss: 3.445, cov loss: 0.176\n",
      "    [batch 2787]: seen 278700 examples : 66.0 eps, Loss: 3.329, Avg loss: 3.447, Best loss: 3.445, cov loss: 0.188\n",
      "    [batch 2794]: seen 279400 examples : 66.0 eps, Loss: 3.606, Avg loss: 3.449, Best loss: 3.445, cov loss: 0.172\n",
      "    [batch 2801]: seen 280100 examples : 66.0 eps, Loss: 3.425, Avg loss: 3.448, Best loss: 3.445, cov loss: 0.186\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:10:50\n",
      "[EPOCH 18] Complete. Avg Loss: 3.446132824255838; Best Loss: 3.4447559276463466\n",
      "[EPOCH 19] Starting training..\n",
      "    [batch 3]: seen 300 examples : 26.7 eps, Loss: 3.434, Avg loss: 3.444, Best loss: 3.444, cov loss: 0.188\n",
      "    [batch 6]: seen 600 examples : 26.7 eps, Loss: 3.548, Avg loss: 3.442, Best loss: 3.441, cov loss: 0.183\n",
      "    [batch 11]: seen 1100 examples : 30.0 eps, Loss: 3.150, Avg loss: 3.437, Best loss: 3.437, cov loss: 0.155\n",
      "    [batch 14]: seen 1400 examples : 27.3 eps, Loss: 3.311, Avg loss: 3.433, Best loss: 3.433, cov loss: 0.195\n",
      "    [batch 16]: seen 1600 examples : 25.8 eps, Loss: 3.385, Avg loss: 3.432, Best loss: 3.432, cov loss: 0.167\n",
      "    [batch 19]: seen 1900 examples : 26.0 eps, Loss: 3.182, Avg loss: 3.430, Best loss: 3.430, cov loss: 0.159\n",
      "    [batch 22]: seen 2200 examples : 26.1 eps, Loss: 3.200, Avg loss: 3.427, Best loss: 3.427, cov loss: 0.178\n",
      "    [batch 25]: seen 2500 examples : 26.1 eps, Loss: 3.332, Avg loss: 3.426, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 32]: seen 3200 examples : 30.2 eps, Loss: 3.403, Avg loss: 3.428, Best loss: 3.426, cov loss: 0.175\n",
      "    [batch 39]: seen 3900 examples : 33.6 eps, Loss: 3.380, Avg loss: 3.434, Best loss: 3.426, cov loss: 0.189\n",
      "    [batch 46]: seen 4600 examples : 36.5 eps, Loss: 3.515, Avg loss: 3.435, Best loss: 3.426, cov loss: 0.178\n",
      "    [batch 53]: seen 5300 examples : 38.9 eps, Loss: 3.653, Avg loss: 3.436, Best loss: 3.426, cov loss: 0.168\n",
      "    [batch 60]: seen 6000 examples : 41.0 eps, Loss: 3.359, Avg loss: 3.435, Best loss: 3.426, cov loss: 0.179\n",
      "    [batch 67]: seen 6700 examples : 42.8 eps, Loss: 3.593, Avg loss: 3.435, Best loss: 3.426, cov loss: 0.178\n",
      "    [batch 74]: seen 7400 examples : 44.4 eps, Loss: 3.464, Avg loss: 3.435, Best loss: 3.426, cov loss: 0.159\n",
      "    [batch 81]: seen 8100 examples : 45.8 eps, Loss: 3.525, Avg loss: 3.436, Best loss: 3.426, cov loss: 0.179\n",
      "    [batch 88]: seen 8800 examples : 47.1 eps, Loss: 3.437, Avg loss: 3.434, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 95]: seen 9500 examples : 48.2 eps, Loss: 3.284, Avg loss: 3.430, Best loss: 3.426, cov loss: 0.186\n",
      "    [batch 102]: seen 10200 examples : 49.3 eps, Loss: 3.394, Avg loss: 3.428, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 109]: seen 10900 examples : 50.2 eps, Loss: 3.598, Avg loss: 3.430, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 116]: seen 11600 examples : 51.0 eps, Loss: 3.578, Avg loss: 3.433, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 123]: seen 12300 examples : 51.8 eps, Loss: 3.491, Avg loss: 3.439, Best loss: 3.426, cov loss: 0.179\n",
      "    [batch 130]: seen 13000 examples : 52.5 eps, Loss: 3.393, Avg loss: 3.438, Best loss: 3.426, cov loss: 0.172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 137]: seen 13700 examples : 53.1 eps, Loss: 3.487, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.177\n",
      "    [batch 144]: seen 14400 examples : 53.7 eps, Loss: 3.337, Avg loss: 3.439, Best loss: 3.426, cov loss: 0.173\n",
      "    [batch 151]: seen 15100 examples : 54.3 eps, Loss: 3.376, Avg loss: 3.439, Best loss: 3.426, cov loss: 0.168\n",
      "    [batch 158]: seen 15800 examples : 54.8 eps, Loss: 3.380, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.179\n",
      "    [batch 165]: seen 16500 examples : 55.3 eps, Loss: 3.305, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.156\n",
      "    [batch 172]: seen 17200 examples : 55.8 eps, Loss: 3.434, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.161\n",
      "    [batch 179]: seen 17900 examples : 56.2 eps, Loss: 3.343, Avg loss: 3.440, Best loss: 3.426, cov loss: 0.175\n",
      "    [batch 186]: seen 18600 examples : 56.6 eps, Loss: 3.440, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.173\n",
      "    [batch 193]: seen 19300 examples : 56.9 eps, Loss: 3.425, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.176\n",
      "    [batch 200]: seen 20000 examples : 57.3 eps, Loss: 3.387, Avg loss: 3.442, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 207]: seen 20700 examples : 57.6 eps, Loss: 3.501, Avg loss: 3.439, Best loss: 3.426, cov loss: 0.198\n",
      "    [batch 214]: seen 21400 examples : 57.9 eps, Loss: 3.630, Avg loss: 3.438, Best loss: 3.426, cov loss: 0.195\n",
      "    [batch 221]: seen 22100 examples : 58.2 eps, Loss: 3.338, Avg loss: 3.440, Best loss: 3.426, cov loss: 0.163\n",
      "    [batch 228]: seen 22800 examples : 58.5 eps, Loss: 3.364, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.170\n",
      "    [batch 235]: seen 23500 examples : 58.8 eps, Loss: 3.269, Avg loss: 3.438, Best loss: 3.426, cov loss: 0.176\n",
      "    [batch 242]: seen 24200 examples : 59.0 eps, Loss: 3.442, Avg loss: 3.436, Best loss: 3.426, cov loss: 0.177\n",
      "    [batch 249]: seen 24900 examples : 59.3 eps, Loss: 3.234, Avg loss: 3.434, Best loss: 3.426, cov loss: 0.188\n",
      "    [batch 256]: seen 25600 examples : 59.5 eps, Loss: 3.260, Avg loss: 3.435, Best loss: 3.426, cov loss: 0.159\n",
      "    [batch 263]: seen 26300 examples : 59.7 eps, Loss: 3.522, Avg loss: 3.437, Best loss: 3.426, cov loss: 0.183\n",
      "    [batch 270]: seen 27000 examples : 59.9 eps, Loss: 3.542, Avg loss: 3.439, Best loss: 3.426, cov loss: 0.193\n",
      "    [batch 277]: seen 27700 examples : 60.1 eps, Loss: 3.422, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.158\n",
      "    [batch 284]: seen 28400 examples : 60.3 eps, Loss: 3.618, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.199\n",
      "    [batch 291]: seen 29100 examples : 60.5 eps, Loss: 3.392, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.175\n",
      "    [batch 298]: seen 29800 examples : 60.7 eps, Loss: 3.385, Avg loss: 3.440, Best loss: 3.426, cov loss: 0.160\n",
      "    [batch 305]: seen 30500 examples : 60.9 eps, Loss: 3.407, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.165\n",
      "    [batch 312]: seen 31200 examples : 61.0 eps, Loss: 3.456, Avg loss: 3.445, Best loss: 3.426, cov loss: 0.182\n",
      "    [batch 319]: seen 31900 examples : 61.2 eps, Loss: 3.420, Avg loss: 3.442, Best loss: 3.426, cov loss: 0.175\n",
      "    [batch 326]: seen 32600 examples : 61.3 eps, Loss: 3.442, Avg loss: 3.438, Best loss: 3.426, cov loss: 0.173\n",
      "    [batch 333]: seen 33300 examples : 61.5 eps, Loss: 3.312, Avg loss: 3.434, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 340]: seen 34000 examples : 61.6 eps, Loss: 3.489, Avg loss: 3.434, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 347]: seen 34700 examples : 61.8 eps, Loss: 3.471, Avg loss: 3.435, Best loss: 3.426, cov loss: 0.177\n",
      "    [batch 354]: seen 35400 examples : 61.9 eps, Loss: 3.688, Avg loss: 3.440, Best loss: 3.426, cov loss: 0.206\n",
      "    [batch 361]: seen 36100 examples : 62.0 eps, Loss: 3.397, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.170\n",
      "    [batch 368]: seen 36800 examples : 62.1 eps, Loss: 3.364, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 375]: seen 37500 examples : 62.2 eps, Loss: 3.166, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.167\n",
      "    [batch 382]: seen 38200 examples : 62.4 eps, Loss: 3.586, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 389]: seen 38900 examples : 62.5 eps, Loss: 3.265, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 396]: seen 39600 examples : 62.6 eps, Loss: 3.490, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.187\n",
      "    [batch 403]: seen 40300 examples : 62.7 eps, Loss: 3.718, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.197\n",
      "    [batch 410]: seen 41000 examples : 62.8 eps, Loss: 3.381, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.168\n",
      "    [batch 417]: seen 41700 examples : 62.9 eps, Loss: 3.506, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.193\n",
      "    [batch 424]: seen 42400 examples : 63.0 eps, Loss: 3.594, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 431]: seen 43100 examples : 63.1 eps, Loss: 3.451, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 438]: seen 43800 examples : 63.1 eps, Loss: 3.212, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.156\n",
      "    [batch 445]: seen 44500 examples : 63.2 eps, Loss: 3.458, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.181\n",
      "    [batch 452]: seen 45200 examples : 63.3 eps, Loss: 3.573, Avg loss: 3.445, Best loss: 3.426, cov loss: 0.165\n",
      "    [batch 459]: seen 45900 examples : 63.4 eps, Loss: 3.548, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.182\n",
      "    [batch 466]: seen 46600 examples : 63.5 eps, Loss: 3.434, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.185\n",
      "    [batch 473]: seen 47300 examples : 63.5 eps, Loss: 3.328, Avg loss: 3.445, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 480]: seen 48000 examples : 63.6 eps, Loss: 3.376, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.161\n",
      "    [batch 487]: seen 48700 examples : 63.7 eps, Loss: 3.217, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 494]: seen 49400 examples : 63.8 eps, Loss: 3.332, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 501]: seen 50100 examples : 63.8 eps, Loss: 3.645, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.186\n",
      "    [batch 508]: seen 50800 examples : 63.9 eps, Loss: 3.488, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.176\n",
      "    [batch 515]: seen 51500 examples : 64.0 eps, Loss: 3.119, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.156\n",
      "    [batch 522]: seen 52200 examples : 64.0 eps, Loss: 3.468, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 529]: seen 52900 examples : 64.1 eps, Loss: 3.576, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.191\n",
      "    [batch 536]: seen 53600 examples : 64.1 eps, Loss: 3.584, Avg loss: 3.456, Best loss: 3.426, cov loss: 0.184\n",
      "    [batch 543]: seen 54300 examples : 64.2 eps, Loss: 3.432, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.179\n",
      "    [batch 550]: seen 55000 examples : 64.3 eps, Loss: 3.399, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.155\n",
      "    [batch 557]: seen 55700 examples : 64.3 eps, Loss: 3.517, Avg loss: 3.455, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 564]: seen 56400 examples : 64.4 eps, Loss: 3.343, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.168\n",
      "    [batch 571]: seen 57100 examples : 64.4 eps, Loss: 3.373, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.190\n",
      "    [batch 578]: seen 57800 examples : 64.5 eps, Loss: 3.320, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 585]: seen 58500 examples : 64.5 eps, Loss: 3.360, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 592]: seen 59200 examples : 64.6 eps, Loss: 3.424, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 599]: seen 59900 examples : 64.6 eps, Loss: 3.474, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 606]: seen 60600 examples : 64.7 eps, Loss: 3.376, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.196\n",
      "    [batch 613]: seen 61300 examples : 64.7 eps, Loss: 3.495, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.177\n",
      "    [batch 620]: seen 62000 examples : 64.8 eps, Loss: 3.403, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.175\n",
      "    [batch 627]: seen 62700 examples : 64.8 eps, Loss: 3.523, Avg loss: 3.438, Best loss: 3.426, cov loss: 0.189\n",
      "    [batch 634]: seen 63400 examples : 64.9 eps, Loss: 3.722, Avg loss: 3.442, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 641]: seen 64100 examples : 64.9 eps, Loss: 3.425, Avg loss: 3.439, Best loss: 3.426, cov loss: 0.160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 648]: seen 64800 examples : 64.9 eps, Loss: 3.592, Avg loss: 3.439, Best loss: 3.426, cov loss: 0.177\n",
      "    [batch 655]: seen 65500 examples : 65.0 eps, Loss: 3.424, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 662]: seen 66200 examples : 65.0 eps, Loss: 3.502, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.186\n",
      "    [batch 669]: seen 66900 examples : 65.1 eps, Loss: 3.489, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.175\n",
      "    [batch 676]: seen 67600 examples : 65.1 eps, Loss: 3.355, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.184\n",
      "    [batch 683]: seen 68300 examples : 65.1 eps, Loss: 3.369, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 690]: seen 69000 examples : 65.2 eps, Loss: 3.419, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 697]: seen 69700 examples : 65.2 eps, Loss: 3.427, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.186\n",
      "    [batch 704]: seen 70400 examples : 65.2 eps, Loss: 3.633, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.159\n",
      "    [batch 711]: seen 71100 examples : 65.3 eps, Loss: 3.431, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.182\n",
      "    [batch 718]: seen 71800 examples : 65.3 eps, Loss: 3.337, Avg loss: 3.456, Best loss: 3.426, cov loss: 0.167\n",
      "    [batch 725]: seen 72500 examples : 65.4 eps, Loss: 3.582, Avg loss: 3.456, Best loss: 3.426, cov loss: 0.162\n",
      "    [batch 732]: seen 73200 examples : 65.4 eps, Loss: 3.502, Avg loss: 3.454, Best loss: 3.426, cov loss: 0.179\n",
      "    [batch 739]: seen 73900 examples : 65.4 eps, Loss: 3.309, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 746]: seen 74600 examples : 65.5 eps, Loss: 3.589, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.164\n",
      "    [batch 753]: seen 75300 examples : 65.5 eps, Loss: 3.485, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.162\n",
      "    [batch 760]: seen 76000 examples : 65.5 eps, Loss: 3.360, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.165\n",
      "    [batch 767]: seen 76700 examples : 65.5 eps, Loss: 3.359, Avg loss: 3.454, Best loss: 3.426, cov loss: 0.181\n",
      "    [batch 774]: seen 77400 examples : 65.6 eps, Loss: 3.642, Avg loss: 3.455, Best loss: 3.426, cov loss: 0.182\n",
      "    [batch 781]: seen 78100 examples : 65.6 eps, Loss: 3.387, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.173\n",
      "    [batch 788]: seen 78800 examples : 65.6 eps, Loss: 3.450, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.175\n",
      "    [batch 795]: seen 79500 examples : 65.7 eps, Loss: 3.637, Avg loss: 3.455, Best loss: 3.426, cov loss: 0.187\n",
      "    [batch 802]: seen 80200 examples : 65.7 eps, Loss: 3.362, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.170\n",
      "    [batch 809]: seen 80900 examples : 65.7 eps, Loss: 3.693, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.168\n",
      "    [batch 816]: seen 81600 examples : 65.7 eps, Loss: 3.346, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 823]: seen 82300 examples : 65.8 eps, Loss: 3.373, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.183\n",
      "    [batch 830]: seen 83000 examples : 65.8 eps, Loss: 3.410, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.163\n",
      "    [batch 837]: seen 83700 examples : 65.8 eps, Loss: 3.447, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.182\n",
      "    [batch 844]: seen 84400 examples : 65.9 eps, Loss: 3.548, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.165\n",
      "    [batch 851]: seen 85100 examples : 65.9 eps, Loss: 3.423, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.185\n",
      "    [batch 858]: seen 85800 examples : 65.9 eps, Loss: 3.248, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.184\n",
      "    [batch 865]: seen 86500 examples : 65.9 eps, Loss: 3.390, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.186\n",
      "    [batch 872]: seen 87200 examples : 65.9 eps, Loss: 3.594, Avg loss: 3.455, Best loss: 3.426, cov loss: 0.171\n",
      "    [batch 879]: seen 87900 examples : 66.0 eps, Loss: 3.553, Avg loss: 3.458, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 886]: seen 88600 examples : 66.0 eps, Loss: 3.272, Avg loss: 3.454, Best loss: 3.426, cov loss: 0.173\n",
      "    [batch 893]: seen 89300 examples : 66.0 eps, Loss: 3.522, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.183\n",
      "    [batch 900]: seen 90000 examples : 66.0 eps, Loss: 3.629, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 907]: seen 90700 examples : 66.1 eps, Loss: 3.491, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.177\n",
      "    [batch 914]: seen 91400 examples : 66.1 eps, Loss: 3.442, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.184\n",
      "    [batch 921]: seen 92100 examples : 66.1 eps, Loss: 3.513, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.171\n",
      "    [batch 928]: seen 92800 examples : 66.1 eps, Loss: 3.512, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.180\n",
      "    [batch 935]: seen 93500 examples : 66.1 eps, Loss: 3.245, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.183\n",
      "    [batch 942]: seen 94200 examples : 66.2 eps, Loss: 3.360, Avg loss: 3.440, Best loss: 3.426, cov loss: 0.170\n",
      "    [batch 949]: seen 94900 examples : 66.2 eps, Loss: 3.520, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.189\n",
      "    [batch 956]: seen 95600 examples : 66.2 eps, Loss: 3.380, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.171\n",
      "    [batch 963]: seen 96300 examples : 66.2 eps, Loss: 3.354, Avg loss: 3.440, Best loss: 3.426, cov loss: 0.182\n",
      "    [batch 970]: seen 97000 examples : 66.2 eps, Loss: 3.419, Avg loss: 3.438, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 977]: seen 97700 examples : 66.3 eps, Loss: 3.385, Avg loss: 3.436, Best loss: 3.426, cov loss: 0.164\n",
      "    [batch 984]: seen 98400 examples : 66.3 eps, Loss: 3.527, Avg loss: 3.434, Best loss: 3.426, cov loss: 0.168\n",
      "    [batch 991]: seen 99100 examples : 66.3 eps, Loss: 3.346, Avg loss: 3.432, Best loss: 3.426, cov loss: 0.157\n",
      "    [batch 998]: seen 99800 examples : 66.3 eps, Loss: 3.637, Avg loss: 3.435, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 1005]: seen 100500 examples : 66.3 eps, Loss: 3.523, Avg loss: 3.436, Best loss: 3.426, cov loss: 0.197\n",
      "    [batch 1012]: seen 101200 examples : 66.3 eps, Loss: 3.518, Avg loss: 3.439, Best loss: 3.426, cov loss: 0.175\n",
      "    [batch 1019]: seen 101900 examples : 66.4 eps, Loss: 3.699, Avg loss: 3.442, Best loss: 3.426, cov loss: 0.170\n",
      "    [batch 1026]: seen 102600 examples : 66.4 eps, Loss: 3.435, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.192\n",
      "    [batch 1033]: seen 103300 examples : 66.4 eps, Loss: 3.336, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.178\n",
      "    [batch 1040]: seen 104000 examples : 66.4 eps, Loss: 3.401, Avg loss: 3.445, Best loss: 3.426, cov loss: 0.179\n",
      "    [batch 1047]: seen 104700 examples : 66.4 eps, Loss: 3.494, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.194\n",
      "    [batch 1054]: seen 105400 examples : 66.4 eps, Loss: 3.437, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 1061]: seen 106100 examples : 66.5 eps, Loss: 3.643, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.178\n",
      "    [batch 1068]: seen 106800 examples : 66.5 eps, Loss: 3.489, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.188\n",
      "    [batch 1075]: seen 107500 examples : 66.5 eps, Loss: 3.308, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.164\n",
      "    [batch 1082]: seen 108200 examples : 66.5 eps, Loss: 3.611, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.173\n",
      "    [batch 1089]: seen 108900 examples : 66.5 eps, Loss: 3.431, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.177\n",
      "    [batch 1096]: seen 109600 examples : 66.5 eps, Loss: 3.509, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 1103]: seen 110300 examples : 66.6 eps, Loss: 3.414, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.183\n",
      "    [batch 1110]: seen 111000 examples : 66.6 eps, Loss: 3.478, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.177\n",
      "    [batch 1117]: seen 111700 examples : 66.6 eps, Loss: 3.444, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.161\n",
      "    [batch 1124]: seen 112400 examples : 66.6 eps, Loss: 3.296, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 1131]: seen 113100 examples : 66.6 eps, Loss: 3.463, Avg loss: 3.442, Best loss: 3.426, cov loss: 0.176\n",
      "    [batch 1138]: seen 113800 examples : 66.6 eps, Loss: 3.553, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.176\n",
      "    [batch 1145]: seen 114500 examples : 66.6 eps, Loss: 3.623, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.191\n",
      "    [batch 1152]: seen 115200 examples : 66.7 eps, Loss: 3.616, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1159]: seen 115900 examples : 66.7 eps, Loss: 3.507, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 1166]: seen 116600 examples : 66.7 eps, Loss: 3.506, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.164\n",
      "    [batch 1173]: seen 117300 examples : 66.7 eps, Loss: 3.409, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 1180]: seen 118000 examples : 66.7 eps, Loss: 3.475, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.177\n",
      "    [batch 1187]: seen 118700 examples : 66.7 eps, Loss: 3.448, Avg loss: 3.454, Best loss: 3.426, cov loss: 0.180\n",
      "    [batch 1194]: seen 119400 examples : 66.7 eps, Loss: 3.402, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.180\n",
      "    [batch 1201]: seen 120100 examples : 66.8 eps, Loss: 3.549, Avg loss: 3.456, Best loss: 3.426, cov loss: 0.193\n",
      "    [batch 1208]: seen 120800 examples : 66.8 eps, Loss: 3.451, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.151\n",
      "    [batch 1215]: seen 121500 examples : 66.8 eps, Loss: 3.421, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 1222]: seen 122200 examples : 66.8 eps, Loss: 3.495, Avg loss: 3.445, Best loss: 3.426, cov loss: 0.194\n",
      "    [batch 1229]: seen 122900 examples : 66.8 eps, Loss: 3.429, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.170\n",
      "    [batch 1236]: seen 123600 examples : 66.8 eps, Loss: 3.537, Avg loss: 3.455, Best loss: 3.426, cov loss: 0.169\n",
      "    [batch 1243]: seen 124300 examples : 66.8 eps, Loss: 3.401, Avg loss: 3.455, Best loss: 3.426, cov loss: 0.191\n",
      "    [batch 1250]: seen 125000 examples : 66.8 eps, Loss: 3.698, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.176\n",
      "    [batch 1257]: seen 125700 examples : 66.8 eps, Loss: 3.528, Avg loss: 3.454, Best loss: 3.426, cov loss: 0.167\n",
      "    [batch 1264]: seen 126400 examples : 66.9 eps, Loss: 3.404, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.173\n",
      "    [batch 1271]: seen 127100 examples : 66.9 eps, Loss: 3.515, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.186\n",
      "    [batch 1278]: seen 127800 examples : 66.9 eps, Loss: 3.380, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.157\n",
      "    [batch 1285]: seen 128500 examples : 66.9 eps, Loss: 3.344, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.170\n",
      "    [batch 1292]: seen 129200 examples : 66.9 eps, Loss: 3.433, Avg loss: 3.454, Best loss: 3.426, cov loss: 0.194\n",
      "    [batch 1299]: seen 129900 examples : 66.9 eps, Loss: 3.491, Avg loss: 3.457, Best loss: 3.426, cov loss: 0.195\n",
      "    [batch 1306]: seen 130600 examples : 66.9 eps, Loss: 3.355, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.178\n",
      "    [batch 1313]: seen 131300 examples : 66.9 eps, Loss: 3.695, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.194\n",
      "    [batch 1320]: seen 132000 examples : 66.9 eps, Loss: 3.319, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 1327]: seen 132700 examples : 67.0 eps, Loss: 3.381, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.181\n",
      "    [batch 1334]: seen 133400 examples : 67.0 eps, Loss: 3.291, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 1341]: seen 134100 examples : 67.0 eps, Loss: 3.450, Avg loss: 3.442, Best loss: 3.426, cov loss: 0.152\n",
      "    [batch 1348]: seen 134800 examples : 67.0 eps, Loss: 3.435, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.195\n",
      "    [batch 1355]: seen 135500 examples : 67.0 eps, Loss: 3.504, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.181\n",
      "    [batch 1362]: seen 136200 examples : 67.0 eps, Loss: 3.548, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.183\n",
      "    [batch 1369]: seen 136900 examples : 67.0 eps, Loss: 3.507, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 1376]: seen 137600 examples : 67.0 eps, Loss: 3.545, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.163\n",
      "    [batch 1383]: seen 138300 examples : 67.0 eps, Loss: 3.426, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 1390]: seen 139000 examples : 67.0 eps, Loss: 3.290, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.177\n",
      "    [batch 1397]: seen 139700 examples : 67.1 eps, Loss: 3.437, Avg loss: 3.442, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 1404]: seen 140400 examples : 67.1 eps, Loss: 3.539, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.197\n",
      "    [batch 1411]: seen 141100 examples : 67.1 eps, Loss: 3.519, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.182\n",
      "    [batch 1418]: seen 141800 examples : 67.1 eps, Loss: 3.357, Avg loss: 3.445, Best loss: 3.426, cov loss: 0.177\n",
      "    [batch 1425]: seen 142500 examples : 67.1 eps, Loss: 3.404, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.168\n",
      "    [batch 1432]: seen 143200 examples : 67.1 eps, Loss: 3.424, Avg loss: 3.445, Best loss: 3.426, cov loss: 0.197\n",
      "    [batch 1439]: seen 143900 examples : 67.1 eps, Loss: 3.450, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.183\n",
      "    [batch 1446]: seen 144600 examples : 67.1 eps, Loss: 3.247, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.159\n",
      "    [batch 1453]: seen 145300 examples : 67.1 eps, Loss: 3.490, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.181\n",
      "    [batch 1460]: seen 146000 examples : 67.1 eps, Loss: 3.599, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 1467]: seen 146700 examples : 67.1 eps, Loss: 3.441, Avg loss: 3.454, Best loss: 3.426, cov loss: 0.188\n",
      "    [batch 1474]: seen 147400 examples : 67.2 eps, Loss: 3.430, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 1481]: seen 148100 examples : 67.2 eps, Loss: 3.260, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 1488]: seen 148800 examples : 67.2 eps, Loss: 3.584, Avg loss: 3.457, Best loss: 3.426, cov loss: 0.202\n",
      "    [batch 1495]: seen 149500 examples : 67.2 eps, Loss: 3.441, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.184\n",
      "    [batch 1502]: seen 150200 examples : 67.2 eps, Loss: 3.567, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.192\n",
      "    [batch 1509]: seen 150900 examples : 67.2 eps, Loss: 3.578, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.165\n",
      "    [batch 1516]: seen 151600 examples : 67.2 eps, Loss: 3.513, Avg loss: 3.445, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 1523]: seen 152300 examples : 67.2 eps, Loss: 3.548, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.167\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-31785\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-31785\n",
      "    [batch 1529]: seen 152900 examples : 67.2 eps, Loss: 3.343, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 1536]: seen 153600 examples : 67.2 eps, Loss: 3.326, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.183\n",
      "    [batch 1543]: seen 154300 examples : 67.2 eps, Loss: 3.279, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 1550]: seen 155000 examples : 67.2 eps, Loss: 3.471, Avg loss: 3.456, Best loss: 3.426, cov loss: 0.187\n",
      "    [batch 1557]: seen 155700 examples : 67.2 eps, Loss: 3.421, Avg loss: 3.455, Best loss: 3.426, cov loss: 0.173\n",
      "    [batch 1564]: seen 156400 examples : 67.2 eps, Loss: 3.406, Avg loss: 3.455, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 1571]: seen 157100 examples : 67.2 eps, Loss: 3.386, Avg loss: 3.458, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 1578]: seen 157800 examples : 67.2 eps, Loss: 3.529, Avg loss: 3.459, Best loss: 3.426, cov loss: 0.171\n",
      "    [batch 1585]: seen 158500 examples : 67.2 eps, Loss: 3.393, Avg loss: 3.457, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 1592]: seen 159200 examples : 67.2 eps, Loss: 3.415, Avg loss: 3.457, Best loss: 3.426, cov loss: 0.179\n",
      "    [batch 1599]: seen 159900 examples : 67.2 eps, Loss: 3.351, Avg loss: 3.455, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 1606]: seen 160600 examples : 67.3 eps, Loss: 3.355, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 1613]: seen 161300 examples : 67.3 eps, Loss: 3.481, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.158\n",
      "    [batch 1620]: seen 162000 examples : 67.3 eps, Loss: 3.550, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.178\n",
      "    [batch 1627]: seen 162700 examples : 67.3 eps, Loss: 3.359, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.161\n",
      "    [batch 1634]: seen 163400 examples : 67.3 eps, Loss: 3.454, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1641]: seen 164100 examples : 67.3 eps, Loss: 3.523, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.181\n",
      "    [batch 1648]: seen 164800 examples : 67.3 eps, Loss: 3.158, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.167\n",
      "    [batch 1655]: seen 165500 examples : 67.3 eps, Loss: 3.225, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 1662]: seen 166200 examples : 67.3 eps, Loss: 3.456, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.184\n",
      "    [batch 1669]: seen 166900 examples : 67.3 eps, Loss: 3.459, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.200\n",
      "    [batch 1676]: seen 167600 examples : 67.3 eps, Loss: 3.416, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.161\n",
      "    [batch 1683]: seen 168300 examples : 67.3 eps, Loss: 3.393, Avg loss: 3.442, Best loss: 3.426, cov loss: 0.190\n",
      "    [batch 1690]: seen 169000 examples : 67.3 eps, Loss: 3.280, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.168\n",
      "    [batch 1697]: seen 169700 examples : 67.3 eps, Loss: 3.518, Avg loss: 3.445, Best loss: 3.426, cov loss: 0.181\n",
      "    [batch 1704]: seen 170400 examples : 67.4 eps, Loss: 3.392, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.183\n",
      "    [batch 1711]: seen 171100 examples : 67.4 eps, Loss: 3.600, Avg loss: 3.440, Best loss: 3.426, cov loss: 0.193\n",
      "    [batch 1718]: seen 171800 examples : 67.4 eps, Loss: 3.580, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.184\n",
      "    [batch 1725]: seen 172500 examples : 67.4 eps, Loss: 3.489, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.179\n",
      "    [batch 1732]: seen 173200 examples : 67.4 eps, Loss: 3.329, Avg loss: 3.440, Best loss: 3.426, cov loss: 0.188\n",
      "    [batch 1739]: seen 173900 examples : 67.4 eps, Loss: 3.542, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.176\n",
      "    [batch 1746]: seen 174600 examples : 67.4 eps, Loss: 3.393, Avg loss: 3.445, Best loss: 3.426, cov loss: 0.176\n",
      "    [batch 1753]: seen 175300 examples : 67.4 eps, Loss: 3.350, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.163\n",
      "    [batch 1760]: seen 176000 examples : 67.4 eps, Loss: 3.449, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.173\n",
      "    [batch 1767]: seen 176700 examples : 67.4 eps, Loss: 3.362, Avg loss: 3.442, Best loss: 3.426, cov loss: 0.178\n",
      "    [batch 1774]: seen 177400 examples : 67.4 eps, Loss: 3.368, Avg loss: 3.437, Best loss: 3.426, cov loss: 0.171\n",
      "    [batch 1781]: seen 178100 examples : 67.4 eps, Loss: 3.427, Avg loss: 3.436, Best loss: 3.426, cov loss: 0.189\n",
      "    [batch 1788]: seen 178800 examples : 67.4 eps, Loss: 3.528, Avg loss: 3.436, Best loss: 3.426, cov loss: 0.177\n",
      "    [batch 1795]: seen 179500 examples : 67.4 eps, Loss: 3.309, Avg loss: 3.436, Best loss: 3.426, cov loss: 0.150\n",
      "    [batch 1802]: seen 180200 examples : 67.4 eps, Loss: 3.275, Avg loss: 3.436, Best loss: 3.426, cov loss: 0.174\n",
      "    [batch 1809]: seen 180900 examples : 67.4 eps, Loss: 3.608, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.186\n",
      "    [batch 1816]: seen 181600 examples : 67.5 eps, Loss: 3.425, Avg loss: 3.442, Best loss: 3.426, cov loss: 0.180\n",
      "    [batch 1823]: seen 182300 examples : 67.5 eps, Loss: 3.669, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.178\n",
      "    [batch 1830]: seen 183000 examples : 67.5 eps, Loss: 3.352, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.179\n",
      "    [batch 1837]: seen 183700 examples : 67.5 eps, Loss: 3.364, Avg loss: 3.443, Best loss: 3.426, cov loss: 0.167\n",
      "    [batch 1844]: seen 184400 examples : 67.5 eps, Loss: 3.434, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.179\n",
      "    [batch 1851]: seen 185100 examples : 67.5 eps, Loss: 3.490, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.161\n",
      "    [batch 1858]: seen 185800 examples : 67.5 eps, Loss: 3.373, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.176\n",
      "    [batch 1865]: seen 186500 examples : 67.5 eps, Loss: 3.508, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.168\n",
      "    [batch 1872]: seen 187200 examples : 67.5 eps, Loss: 3.398, Avg loss: 3.446, Best loss: 3.426, cov loss: 0.176\n",
      "    [batch 1879]: seen 187900 examples : 67.5 eps, Loss: 3.459, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.187\n",
      "    [batch 1886]: seen 188600 examples : 67.5 eps, Loss: 3.314, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.155\n",
      "    [batch 1893]: seen 189300 examples : 67.5 eps, Loss: 3.462, Avg loss: 3.449, Best loss: 3.426, cov loss: 0.180\n",
      "    [batch 1900]: seen 190000 examples : 67.5 eps, Loss: 3.500, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 1907]: seen 190700 examples : 67.5 eps, Loss: 3.515, Avg loss: 3.451, Best loss: 3.426, cov loss: 0.153\n",
      "    [batch 1914]: seen 191400 examples : 67.5 eps, Loss: 3.398, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 1921]: seen 192100 examples : 67.5 eps, Loss: 3.523, Avg loss: 3.455, Best loss: 3.426, cov loss: 0.183\n",
      "    [batch 1928]: seen 192800 examples : 67.5 eps, Loss: 3.570, Avg loss: 3.458, Best loss: 3.426, cov loss: 0.201\n",
      "    [batch 1935]: seen 193500 examples : 67.5 eps, Loss: 3.590, Avg loss: 3.460, Best loss: 3.426, cov loss: 0.192\n",
      "    [batch 1942]: seen 194200 examples : 67.6 eps, Loss: 3.373, Avg loss: 3.456, Best loss: 3.426, cov loss: 0.161\n",
      "    [batch 1949]: seen 194900 examples : 67.6 eps, Loss: 3.408, Avg loss: 3.454, Best loss: 3.426, cov loss: 0.189\n",
      "    [batch 1956]: seen 195600 examples : 67.6 eps, Loss: 3.536, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.180\n",
      "    [batch 1963]: seen 196300 examples : 67.6 eps, Loss: 3.570, Avg loss: 3.455, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 1970]: seen 197000 examples : 67.6 eps, Loss: 3.400, Avg loss: 3.453, Best loss: 3.426, cov loss: 0.158\n",
      "    [batch 1977]: seen 197700 examples : 67.6 eps, Loss: 3.432, Avg loss: 3.454, Best loss: 3.426, cov loss: 0.163\n",
      "    [batch 1984]: seen 198400 examples : 67.6 eps, Loss: 3.396, Avg loss: 3.452, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 1991]: seen 199100 examples : 67.6 eps, Loss: 3.319, Avg loss: 3.450, Best loss: 3.426, cov loss: 0.163\n",
      "    [batch 1998]: seen 199800 examples : 67.6 eps, Loss: 3.587, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.161\n",
      "    [batch 2005]: seen 200500 examples : 67.6 eps, Loss: 3.532, Avg loss: 3.448, Best loss: 3.426, cov loss: 0.165\n",
      "    [batch 2012]: seen 201200 examples : 67.6 eps, Loss: 3.422, Avg loss: 3.447, Best loss: 3.426, cov loss: 0.189\n",
      "    [batch 2019]: seen 201900 examples : 67.6 eps, Loss: 3.423, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.167\n",
      "    [batch 2026]: seen 202600 examples : 67.6 eps, Loss: 3.371, Avg loss: 3.444, Best loss: 3.426, cov loss: 0.172\n",
      "    [batch 2033]: seen 203300 examples : 67.6 eps, Loss: 3.485, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.175\n",
      "    [batch 2040]: seen 204000 examples : 67.6 eps, Loss: 3.378, Avg loss: 3.441, Best loss: 3.426, cov loss: 0.166\n",
      "    [batch 2047]: seen 204700 examples : 67.6 eps, Loss: 3.428, Avg loss: 3.439, Best loss: 3.426, cov loss: 0.179\n",
      "    [batch 2054]: seen 205400 examples : 67.6 eps, Loss: 3.086, Avg loss: 3.430, Best loss: 3.426, cov loss: 0.149\n",
      "    [batch 2061]: seen 206100 examples : 67.6 eps, Loss: 3.475, Avg loss: 3.428, Best loss: 3.426, cov loss: 0.184\n",
      "    [batch 2068]: seen 206800 examples : 67.6 eps, Loss: 3.300, Avg loss: 3.429, Best loss: 3.426, cov loss: 0.164\n",
      "    [batch 2075]: seen 207500 examples : 67.6 eps, Loss: 3.162, Avg loss: 3.426, Best loss: 3.426, cov loss: 0.170\n",
      "    [batch 2078]: seen 207800 examples : 67.4 eps, Loss: 3.371, Avg loss: 3.424, Best loss: 3.424, cov loss: 0.185\n",
      "    [batch 2083]: seen 208300 examples : 67.3 eps, Loss: 3.553, Avg loss: 3.429, Best loss: 3.424, cov loss: 0.177\n",
      "    [batch 2090]: seen 209000 examples : 67.4 eps, Loss: 3.585, Avg loss: 3.433, Best loss: 3.424, cov loss: 0.191\n",
      "    [batch 2097]: seen 209700 examples : 67.4 eps, Loss: 3.498, Avg loss: 3.436, Best loss: 3.424, cov loss: 0.179\n",
      "    [batch 2104]: seen 210400 examples : 67.4 eps, Loss: 3.500, Avg loss: 3.441, Best loss: 3.424, cov loss: 0.180\n",
      "    [batch 2111]: seen 211100 examples : 67.4 eps, Loss: 3.377, Avg loss: 3.442, Best loss: 3.424, cov loss: 0.187\n",
      "    [batch 2118]: seen 211800 examples : 67.4 eps, Loss: 3.491, Avg loss: 3.440, Best loss: 3.424, cov loss: 0.171\n",
      "    [batch 2125]: seen 212500 examples : 67.4 eps, Loss: 3.346, Avg loss: 3.441, Best loss: 3.424, cov loss: 0.175\n",
      "    [batch 2132]: seen 213200 examples : 67.4 eps, Loss: 3.449, Avg loss: 3.445, Best loss: 3.424, cov loss: 0.173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2139]: seen 213900 examples : 67.4 eps, Loss: 3.616, Avg loss: 3.448, Best loss: 3.424, cov loss: 0.160\n",
      "    [batch 2146]: seen 214600 examples : 67.4 eps, Loss: 3.231, Avg loss: 3.446, Best loss: 3.424, cov loss: 0.164\n",
      "    [batch 2153]: seen 215300 examples : 67.4 eps, Loss: 3.624, Avg loss: 3.444, Best loss: 3.424, cov loss: 0.199\n",
      "    [batch 2160]: seen 216000 examples : 67.4 eps, Loss: 3.546, Avg loss: 3.448, Best loss: 3.424, cov loss: 0.180\n",
      "    [batch 2167]: seen 216700 examples : 67.4 eps, Loss: 3.278, Avg loss: 3.438, Best loss: 3.424, cov loss: 0.164\n",
      "    [batch 2174]: seen 217400 examples : 67.4 eps, Loss: 3.422, Avg loss: 3.440, Best loss: 3.424, cov loss: 0.193\n",
      "    [batch 2181]: seen 218100 examples : 67.4 eps, Loss: 3.445, Avg loss: 3.435, Best loss: 3.424, cov loss: 0.169\n",
      "    [batch 2188]: seen 218800 examples : 67.4 eps, Loss: 3.356, Avg loss: 3.431, Best loss: 3.424, cov loss: 0.183\n",
      "    [batch 2195]: seen 219500 examples : 67.4 eps, Loss: 3.407, Avg loss: 3.433, Best loss: 3.424, cov loss: 0.177\n",
      "    [batch 2202]: seen 220200 examples : 67.4 eps, Loss: 3.461, Avg loss: 3.437, Best loss: 3.424, cov loss: 0.170\n",
      "    [batch 2209]: seen 220900 examples : 67.4 eps, Loss: 3.423, Avg loss: 3.436, Best loss: 3.424, cov loss: 0.170\n",
      "    [batch 2216]: seen 221600 examples : 67.4 eps, Loss: 3.344, Avg loss: 3.433, Best loss: 3.424, cov loss: 0.182\n",
      "    [batch 2223]: seen 222300 examples : 67.4 eps, Loss: 3.421, Avg loss: 3.430, Best loss: 3.424, cov loss: 0.185\n",
      "    [batch 2230]: seen 223000 examples : 67.5 eps, Loss: 3.394, Avg loss: 3.431, Best loss: 3.424, cov loss: 0.165\n",
      "    [batch 2237]: seen 223700 examples : 67.5 eps, Loss: 3.478, Avg loss: 3.430, Best loss: 3.424, cov loss: 0.180\n",
      "    [batch 2244]: seen 224400 examples : 67.5 eps, Loss: 3.335, Avg loss: 3.431, Best loss: 3.424, cov loss: 0.190\n",
      "    [batch 2251]: seen 225100 examples : 67.5 eps, Loss: 3.353, Avg loss: 3.437, Best loss: 3.424, cov loss: 0.159\n",
      "    [batch 2258]: seen 225800 examples : 67.5 eps, Loss: 3.504, Avg loss: 3.441, Best loss: 3.424, cov loss: 0.180\n",
      "    [batch 2265]: seen 226500 examples : 67.5 eps, Loss: 3.266, Avg loss: 3.439, Best loss: 3.424, cov loss: 0.171\n",
      "    [batch 2272]: seen 227200 examples : 67.5 eps, Loss: 3.479, Avg loss: 3.437, Best loss: 3.424, cov loss: 0.174\n",
      "    [batch 2279]: seen 227900 examples : 67.5 eps, Loss: 3.574, Avg loss: 3.437, Best loss: 3.424, cov loss: 0.173\n",
      "    [batch 2286]: seen 228600 examples : 67.5 eps, Loss: 3.274, Avg loss: 3.436, Best loss: 3.424, cov loss: 0.164\n",
      "    [batch 2293]: seen 229300 examples : 67.5 eps, Loss: 3.498, Avg loss: 3.436, Best loss: 3.424, cov loss: 0.174\n",
      "    [batch 2300]: seen 230000 examples : 67.5 eps, Loss: 3.565, Avg loss: 3.436, Best loss: 3.424, cov loss: 0.173\n",
      "    [batch 2307]: seen 230700 examples : 67.5 eps, Loss: 3.399, Avg loss: 3.442, Best loss: 3.424, cov loss: 0.177\n",
      "    [batch 2314]: seen 231400 examples : 67.5 eps, Loss: 3.311, Avg loss: 3.443, Best loss: 3.424, cov loss: 0.186\n",
      "    [batch 2321]: seen 232100 examples : 67.5 eps, Loss: 3.236, Avg loss: 3.444, Best loss: 3.424, cov loss: 0.164\n",
      "    [batch 2328]: seen 232800 examples : 67.5 eps, Loss: 3.409, Avg loss: 3.441, Best loss: 3.424, cov loss: 0.166\n",
      "    [batch 2335]: seen 233500 examples : 67.5 eps, Loss: 3.299, Avg loss: 3.442, Best loss: 3.424, cov loss: 0.159\n",
      "    [batch 2342]: seen 234200 examples : 67.5 eps, Loss: 3.442, Avg loss: 3.447, Best loss: 3.424, cov loss: 0.173\n",
      "    [batch 2349]: seen 234900 examples : 67.5 eps, Loss: 3.670, Avg loss: 3.448, Best loss: 3.424, cov loss: 0.184\n",
      "    [batch 2356]: seen 235600 examples : 67.5 eps, Loss: 3.478, Avg loss: 3.452, Best loss: 3.424, cov loss: 0.168\n",
      "    [batch 2363]: seen 236300 examples : 67.5 eps, Loss: 3.535, Avg loss: 3.449, Best loss: 3.424, cov loss: 0.174\n",
      "    [batch 2370]: seen 237000 examples : 67.5 eps, Loss: 3.388, Avg loss: 3.446, Best loss: 3.424, cov loss: 0.173\n",
      "    [batch 2377]: seen 237700 examples : 67.5 eps, Loss: 3.398, Avg loss: 3.444, Best loss: 3.424, cov loss: 0.172\n",
      "    [batch 2384]: seen 238400 examples : 67.6 eps, Loss: 3.653, Avg loss: 3.445, Best loss: 3.424, cov loss: 0.173\n",
      "    [batch 2391]: seen 239100 examples : 67.6 eps, Loss: 3.376, Avg loss: 3.441, Best loss: 3.424, cov loss: 0.167\n",
      "    [batch 2398]: seen 239800 examples : 67.6 eps, Loss: 3.523, Avg loss: 3.447, Best loss: 3.424, cov loss: 0.192\n",
      "    [batch 2405]: seen 240500 examples : 67.6 eps, Loss: 3.463, Avg loss: 3.447, Best loss: 3.424, cov loss: 0.164\n",
      "    [batch 2412]: seen 241200 examples : 67.6 eps, Loss: 3.291, Avg loss: 3.447, Best loss: 3.424, cov loss: 0.180\n",
      "    [batch 2419]: seen 241900 examples : 67.6 eps, Loss: 3.442, Avg loss: 3.449, Best loss: 3.424, cov loss: 0.191\n",
      "    [batch 2426]: seen 242600 examples : 67.6 eps, Loss: 3.329, Avg loss: 3.448, Best loss: 3.424, cov loss: 0.163\n",
      "    [batch 2433]: seen 243300 examples : 67.6 eps, Loss: 3.561, Avg loss: 3.445, Best loss: 3.424, cov loss: 0.175\n",
      "    [batch 2440]: seen 244000 examples : 67.6 eps, Loss: 3.528, Avg loss: 3.448, Best loss: 3.424, cov loss: 0.197\n",
      "    [batch 2447]: seen 244700 examples : 67.6 eps, Loss: 3.357, Avg loss: 3.446, Best loss: 3.424, cov loss: 0.161\n",
      "    [batch 2454]: seen 245400 examples : 67.6 eps, Loss: 3.359, Avg loss: 3.445, Best loss: 3.424, cov loss: 0.187\n",
      "    [batch 2461]: seen 246100 examples : 67.6 eps, Loss: 3.421, Avg loss: 3.443, Best loss: 3.424, cov loss: 0.185\n",
      "    [batch 2468]: seen 246800 examples : 67.6 eps, Loss: 3.478, Avg loss: 3.441, Best loss: 3.424, cov loss: 0.195\n",
      "    [batch 2475]: seen 247500 examples : 67.6 eps, Loss: 3.451, Avg loss: 3.444, Best loss: 3.424, cov loss: 0.176\n",
      "    [batch 2482]: seen 248200 examples : 67.6 eps, Loss: 3.557, Avg loss: 3.448, Best loss: 3.424, cov loss: 0.178\n",
      "    [batch 2489]: seen 248900 examples : 67.6 eps, Loss: 3.472, Avg loss: 3.446, Best loss: 3.424, cov loss: 0.185\n",
      "    [batch 2496]: seen 249600 examples : 67.6 eps, Loss: 3.378, Avg loss: 3.449, Best loss: 3.424, cov loss: 0.173\n",
      "    [batch 2503]: seen 250300 examples : 67.6 eps, Loss: 3.529, Avg loss: 3.451, Best loss: 3.424, cov loss: 0.183\n",
      "    [batch 2510]: seen 251000 examples : 67.6 eps, Loss: 3.406, Avg loss: 3.448, Best loss: 3.424, cov loss: 0.165\n",
      "    [batch 2517]: seen 251700 examples : 67.6 eps, Loss: 3.515, Avg loss: 3.446, Best loss: 3.424, cov loss: 0.182\n",
      "    [batch 2524]: seen 252400 examples : 67.6 eps, Loss: 3.396, Avg loss: 3.446, Best loss: 3.424, cov loss: 0.170\n",
      "    [batch 2531]: seen 253100 examples : 67.6 eps, Loss: 3.457, Avg loss: 3.445, Best loss: 3.424, cov loss: 0.175\n",
      "    [batch 2538]: seen 253800 examples : 67.6 eps, Loss: 3.591, Avg loss: 3.449, Best loss: 3.424, cov loss: 0.174\n",
      "    [batch 2545]: seen 254500 examples : 67.6 eps, Loss: 3.455, Avg loss: 3.451, Best loss: 3.424, cov loss: 0.181\n",
      "    [batch 2552]: seen 255200 examples : 67.6 eps, Loss: 3.449, Avg loss: 3.454, Best loss: 3.424, cov loss: 0.182\n",
      "    [batch 2559]: seen 255900 examples : 67.6 eps, Loss: 3.494, Avg loss: 3.459, Best loss: 3.424, cov loss: 0.164\n",
      "    [batch 2566]: seen 256600 examples : 67.7 eps, Loss: 3.344, Avg loss: 3.456, Best loss: 3.424, cov loss: 0.158\n",
      "    [batch 2573]: seen 257300 examples : 67.7 eps, Loss: 3.420, Avg loss: 3.457, Best loss: 3.424, cov loss: 0.161\n",
      "    [batch 2580]: seen 258000 examples : 67.7 eps, Loss: 3.403, Avg loss: 3.454, Best loss: 3.424, cov loss: 0.174\n",
      "    [batch 2587]: seen 258700 examples : 67.7 eps, Loss: 3.274, Avg loss: 3.449, Best loss: 3.424, cov loss: 0.164\n",
      "    [batch 2594]: seen 259400 examples : 67.7 eps, Loss: 3.360, Avg loss: 3.453, Best loss: 3.424, cov loss: 0.182\n",
      "    [batch 2601]: seen 260100 examples : 67.7 eps, Loss: 3.389, Avg loss: 3.451, Best loss: 3.424, cov loss: 0.169\n",
      "    [batch 2608]: seen 260800 examples : 67.7 eps, Loss: 3.552, Avg loss: 3.450, Best loss: 3.424, cov loss: 0.175\n",
      "    [batch 2615]: seen 261500 examples : 67.7 eps, Loss: 3.477, Avg loss: 3.449, Best loss: 3.424, cov loss: 0.186\n",
      "    [batch 2622]: seen 262200 examples : 67.7 eps, Loss: 3.544, Avg loss: 3.449, Best loss: 3.424, cov loss: 0.170\n",
      "    [batch 2629]: seen 262900 examples : 67.7 eps, Loss: 3.463, Avg loss: 3.449, Best loss: 3.424, cov loss: 0.158\n",
      "    [batch 2636]: seen 263600 examples : 67.7 eps, Loss: 3.609, Avg loss: 3.448, Best loss: 3.424, cov loss: 0.177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2643]: seen 264300 examples : 67.7 eps, Loss: 3.530, Avg loss: 3.450, Best loss: 3.424, cov loss: 0.182\n",
      "    [batch 2650]: seen 265000 examples : 67.7 eps, Loss: 3.615, Avg loss: 3.451, Best loss: 3.424, cov loss: 0.175\n",
      "    [batch 2657]: seen 265700 examples : 67.7 eps, Loss: 3.532, Avg loss: 3.446, Best loss: 3.424, cov loss: 0.203\n",
      "    [batch 2664]: seen 266400 examples : 67.7 eps, Loss: 3.589, Avg loss: 3.449, Best loss: 3.424, cov loss: 0.177\n",
      "    [batch 2671]: seen 267100 examples : 67.7 eps, Loss: 3.435, Avg loss: 3.449, Best loss: 3.424, cov loss: 0.185\n",
      "    [batch 2678]: seen 267800 examples : 67.7 eps, Loss: 3.351, Avg loss: 3.453, Best loss: 3.424, cov loss: 0.169\n",
      "    [batch 2685]: seen 268500 examples : 67.7 eps, Loss: 3.276, Avg loss: 3.450, Best loss: 3.424, cov loss: 0.169\n",
      "    [batch 2692]: seen 269200 examples : 67.7 eps, Loss: 3.570, Avg loss: 3.449, Best loss: 3.424, cov loss: 0.181\n",
      "    [batch 2699]: seen 269900 examples : 67.7 eps, Loss: 3.510, Avg loss: 3.448, Best loss: 3.424, cov loss: 0.194\n",
      "    [batch 2706]: seen 270600 examples : 67.7 eps, Loss: 3.412, Avg loss: 3.443, Best loss: 3.424, cov loss: 0.173\n",
      "    [batch 2713]: seen 271300 examples : 67.7 eps, Loss: 3.336, Avg loss: 3.443, Best loss: 3.424, cov loss: 0.174\n",
      "    [batch 2720]: seen 272000 examples : 67.7 eps, Loss: 3.471, Avg loss: 3.444, Best loss: 3.424, cov loss: 0.168\n",
      "    [batch 2727]: seen 272700 examples : 67.7 eps, Loss: 3.491, Avg loss: 3.444, Best loss: 3.424, cov loss: 0.178\n",
      "    [batch 2734]: seen 273400 examples : 67.7 eps, Loss: 3.343, Avg loss: 3.440, Best loss: 3.424, cov loss: 0.173\n",
      "    [batch 2741]: seen 274100 examples : 67.7 eps, Loss: 3.494, Avg loss: 3.439, Best loss: 3.424, cov loss: 0.186\n",
      "    [batch 2748]: seen 274800 examples : 67.7 eps, Loss: 3.269, Avg loss: 3.436, Best loss: 3.424, cov loss: 0.167\n",
      "    [batch 2755]: seen 275500 examples : 67.7 eps, Loss: 3.364, Avg loss: 3.435, Best loss: 3.424, cov loss: 0.166\n",
      "    [batch 2762]: seen 276200 examples : 67.7 eps, Loss: 3.667, Avg loss: 3.438, Best loss: 3.424, cov loss: 0.173\n",
      "    [batch 2769]: seen 276900 examples : 67.8 eps, Loss: 3.445, Avg loss: 3.439, Best loss: 3.424, cov loss: 0.162\n",
      "    [batch 2776]: seen 277600 examples : 67.8 eps, Loss: 3.250, Avg loss: 3.442, Best loss: 3.424, cov loss: 0.171\n",
      "    [batch 2783]: seen 278300 examples : 67.8 eps, Loss: 3.355, Avg loss: 3.443, Best loss: 3.424, cov loss: 0.175\n",
      "    [batch 2790]: seen 279000 examples : 67.8 eps, Loss: 3.554, Avg loss: 3.445, Best loss: 3.424, cov loss: 0.170\n",
      "    [batch 2797]: seen 279700 examples : 67.8 eps, Loss: 3.635, Avg loss: 3.448, Best loss: 3.424, cov loss: 0.181\n",
      "    [batch 2804]: seen 280400 examples : 67.8 eps, Loss: 3.307, Avg loss: 3.442, Best loss: 3.424, cov loss: 0.171\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:09:02\n",
      "[EPOCH 19] Complete. Avg Loss: 3.4395535758698528; Best Loss: 3.4240924413956453\n",
      "[EPOCH 20] Starting training..\n",
      "    [batch 7]: seen 700 examples : 69.1 eps, Loss: 3.423, Avg loss: 3.440, Best loss: 3.424, cov loss: 0.176\n",
      "    [batch 14]: seen 1400 examples : 69.1 eps, Loss: 3.454, Avg loss: 3.442, Best loss: 3.424, cov loss: 0.196\n",
      "    [batch 21]: seen 2100 examples : 69.1 eps, Loss: 3.391, Avg loss: 3.437, Best loss: 3.424, cov loss: 0.166\n",
      "    [batch 28]: seen 2800 examples : 69.1 eps, Loss: 3.604, Avg loss: 3.443, Best loss: 3.424, cov loss: 0.164\n",
      "    [batch 35]: seen 3500 examples : 69.0 eps, Loss: 3.422, Avg loss: 3.444, Best loss: 3.424, cov loss: 0.188\n",
      "    [batch 42]: seen 4200 examples : 69.0 eps, Loss: 3.453, Avg loss: 3.442, Best loss: 3.424, cov loss: 0.163\n",
      "    [batch 49]: seen 4900 examples : 69.0 eps, Loss: 3.455, Avg loss: 3.445, Best loss: 3.424, cov loss: 0.173\n",
      "    [batch 56]: seen 5600 examples : 69.0 eps, Loss: 3.554, Avg loss: 3.445, Best loss: 3.424, cov loss: 0.180\n",
      "    [batch 63]: seen 6300 examples : 69.0 eps, Loss: 3.380, Avg loss: 3.439, Best loss: 3.424, cov loss: 0.160\n",
      "    [batch 70]: seen 7000 examples : 69.0 eps, Loss: 3.382, Avg loss: 3.437, Best loss: 3.424, cov loss: 0.184\n",
      "    [batch 77]: seen 7700 examples : 69.0 eps, Loss: 3.338, Avg loss: 3.437, Best loss: 3.424, cov loss: 0.161\n",
      "    [batch 84]: seen 8400 examples : 69.0 eps, Loss: 3.381, Avg loss: 3.436, Best loss: 3.424, cov loss: 0.173\n",
      "    [batch 91]: seen 9100 examples : 69.0 eps, Loss: 3.443, Avg loss: 3.435, Best loss: 3.424, cov loss: 0.168\n",
      "    [batch 98]: seen 9800 examples : 69.0 eps, Loss: 3.288, Avg loss: 3.434, Best loss: 3.424, cov loss: 0.169\n",
      "    [batch 105]: seen 10500 examples : 69.0 eps, Loss: 3.397, Avg loss: 3.430, Best loss: 3.424, cov loss: 0.185\n",
      "    [batch 112]: seen 11200 examples : 69.0 eps, Loss: 3.429, Avg loss: 3.432, Best loss: 3.424, cov loss: 0.163\n",
      "    [batch 119]: seen 11900 examples : 69.0 eps, Loss: 3.432, Avg loss: 3.433, Best loss: 3.424, cov loss: 0.157\n",
      "    [batch 126]: seen 12600 examples : 69.0 eps, Loss: 3.701, Avg loss: 3.437, Best loss: 3.424, cov loss: 0.203\n",
      "    [batch 133]: seen 13300 examples : 69.0 eps, Loss: 3.481, Avg loss: 3.437, Best loss: 3.424, cov loss: 0.169\n",
      "    [batch 140]: seen 14000 examples : 69.0 eps, Loss: 3.442, Avg loss: 3.440, Best loss: 3.424, cov loss: 0.182\n",
      "    [batch 147]: seen 14700 examples : 68.9 eps, Loss: 3.394, Avg loss: 3.442, Best loss: 3.424, cov loss: 0.181\n",
      "    [batch 154]: seen 15400 examples : 69.0 eps, Loss: 3.259, Avg loss: 3.434, Best loss: 3.424, cov loss: 0.174\n",
      "    [batch 161]: seen 16100 examples : 68.9 eps, Loss: 3.505, Avg loss: 3.435, Best loss: 3.424, cov loss: 0.197\n",
      "    [batch 168]: seen 16800 examples : 68.9 eps, Loss: 3.373, Avg loss: 3.433, Best loss: 3.424, cov loss: 0.162\n",
      "    [batch 175]: seen 17500 examples : 68.9 eps, Loss: 3.509, Avg loss: 3.432, Best loss: 3.424, cov loss: 0.170\n",
      "    [batch 182]: seen 18200 examples : 68.9 eps, Loss: 3.286, Avg loss: 3.429, Best loss: 3.424, cov loss: 0.180\n",
      "    [batch 189]: seen 18900 examples : 68.9 eps, Loss: 3.334, Avg loss: 3.427, Best loss: 3.424, cov loss: 0.158\n",
      "    [batch 196]: seen 19600 examples : 68.9 eps, Loss: 3.317, Avg loss: 3.424, Best loss: 3.424, cov loss: 0.184\n",
      "    [batch 199]: seen 19900 examples : 66.6 eps, Loss: 3.413, Avg loss: 3.421, Best loss: 3.421, cov loss: 0.166\n",
      "    [batch 202]: seen 20200 examples : 65.1 eps, Loss: 3.393, Avg loss: 3.421, Best loss: 3.421, cov loss: 0.172\n",
      "    [batch 209]: seen 20900 examples : 65.2 eps, Loss: 3.483, Avg loss: 3.424, Best loss: 3.421, cov loss: 0.178\n",
      "    [batch 216]: seen 21600 examples : 65.4 eps, Loss: 3.495, Avg loss: 3.424, Best loss: 3.421, cov loss: 0.196\n",
      "    [batch 223]: seen 22300 examples : 65.5 eps, Loss: 3.481, Avg loss: 3.425, Best loss: 3.421, cov loss: 0.183\n",
      "    [batch 230]: seen 23000 examples : 65.6 eps, Loss: 3.192, Avg loss: 3.423, Best loss: 3.421, cov loss: 0.168\n",
      "    [batch 237]: seen 23700 examples : 65.7 eps, Loss: 3.463, Avg loss: 3.425, Best loss: 3.421, cov loss: 0.165\n",
      "    [batch 244]: seen 24400 examples : 65.7 eps, Loss: 3.576, Avg loss: 3.429, Best loss: 3.421, cov loss: 0.167\n",
      "    [batch 251]: seen 25100 examples : 65.8 eps, Loss: 3.466, Avg loss: 3.430, Best loss: 3.421, cov loss: 0.172\n",
      "    [batch 258]: seen 25800 examples : 65.9 eps, Loss: 3.479, Avg loss: 3.433, Best loss: 3.421, cov loss: 0.177\n",
      "    [batch 265]: seen 26500 examples : 66.0 eps, Loss: 3.239, Avg loss: 3.430, Best loss: 3.421, cov loss: 0.188\n",
      "    [batch 272]: seen 27200 examples : 66.1 eps, Loss: 3.580, Avg loss: 3.430, Best loss: 3.421, cov loss: 0.172\n",
      "    [batch 279]: seen 27900 examples : 66.1 eps, Loss: 3.207, Avg loss: 3.427, Best loss: 3.421, cov loss: 0.164\n",
      "    [batch 286]: seen 28600 examples : 66.2 eps, Loss: 3.322, Avg loss: 3.429, Best loss: 3.421, cov loss: 0.182\n",
      "    [batch 293]: seen 29300 examples : 66.3 eps, Loss: 3.506, Avg loss: 3.430, Best loss: 3.421, cov loss: 0.198\n",
      "    [batch 300]: seen 30000 examples : 66.3 eps, Loss: 3.705, Avg loss: 3.429, Best loss: 3.421, cov loss: 0.184\n",
      "    [batch 307]: seen 30700 examples : 66.4 eps, Loss: 3.400, Avg loss: 3.432, Best loss: 3.421, cov loss: 0.182\n",
      "    [batch 314]: seen 31400 examples : 66.4 eps, Loss: 3.608, Avg loss: 3.436, Best loss: 3.421, cov loss: 0.174\n",
      "    [batch 321]: seen 32100 examples : 66.5 eps, Loss: 3.505, Avg loss: 3.437, Best loss: 3.421, cov loss: 0.193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 328]: seen 32800 examples : 66.6 eps, Loss: 3.350, Avg loss: 3.434, Best loss: 3.421, cov loss: 0.181\n",
      "    [batch 335]: seen 33500 examples : 66.6 eps, Loss: 3.337, Avg loss: 3.431, Best loss: 3.421, cov loss: 0.166\n",
      "    [batch 342]: seen 34200 examples : 66.7 eps, Loss: 3.414, Avg loss: 3.427, Best loss: 3.421, cov loss: 0.169\n",
      "    [batch 349]: seen 34900 examples : 66.7 eps, Loss: 3.585, Avg loss: 3.430, Best loss: 3.421, cov loss: 0.169\n",
      "    [batch 356]: seen 35600 examples : 66.7 eps, Loss: 3.485, Avg loss: 3.431, Best loss: 3.421, cov loss: 0.174\n",
      "    [batch 363]: seen 36300 examples : 66.8 eps, Loss: 3.449, Avg loss: 3.433, Best loss: 3.421, cov loss: 0.169\n",
      "    [batch 370]: seen 37000 examples : 66.8 eps, Loss: 3.368, Avg loss: 3.428, Best loss: 3.421, cov loss: 0.170\n",
      "    [batch 377]: seen 37700 examples : 66.9 eps, Loss: 3.563, Avg loss: 3.432, Best loss: 3.421, cov loss: 0.183\n",
      "    [batch 384]: seen 38400 examples : 66.9 eps, Loss: 3.594, Avg loss: 3.431, Best loss: 3.421, cov loss: 0.184\n",
      "    [batch 391]: seen 39100 examples : 66.9 eps, Loss: 3.132, Avg loss: 3.430, Best loss: 3.421, cov loss: 0.164\n",
      "    [batch 398]: seen 39800 examples : 67.0 eps, Loss: 3.456, Avg loss: 3.433, Best loss: 3.421, cov loss: 0.190\n",
      "    [batch 405]: seen 40500 examples : 67.0 eps, Loss: 3.421, Avg loss: 3.429, Best loss: 3.421, cov loss: 0.168\n",
      "    [batch 412]: seen 41200 examples : 67.0 eps, Loss: 3.520, Avg loss: 3.434, Best loss: 3.421, cov loss: 0.159\n",
      "    [batch 419]: seen 41900 examples : 67.1 eps, Loss: 3.428, Avg loss: 3.434, Best loss: 3.421, cov loss: 0.169\n",
      "    [batch 426]: seen 42600 examples : 67.1 eps, Loss: 3.243, Avg loss: 3.429, Best loss: 3.421, cov loss: 0.166\n",
      "    [batch 433]: seen 43300 examples : 67.1 eps, Loss: 3.338, Avg loss: 3.426, Best loss: 3.421, cov loss: 0.182\n",
      "    [batch 440]: seen 44000 examples : 67.2 eps, Loss: 3.535, Avg loss: 3.428, Best loss: 3.421, cov loss: 0.182\n",
      "    [batch 447]: seen 44700 examples : 67.2 eps, Loss: 3.306, Avg loss: 3.426, Best loss: 3.421, cov loss: 0.168\n",
      "    [batch 454]: seen 45400 examples : 67.2 eps, Loss: 3.517, Avg loss: 3.426, Best loss: 3.421, cov loss: 0.170\n",
      "    [batch 461]: seen 46100 examples : 67.2 eps, Loss: 3.345, Avg loss: 3.426, Best loss: 3.421, cov loss: 0.174\n",
      "    [batch 468]: seen 46800 examples : 67.3 eps, Loss: 3.395, Avg loss: 3.429, Best loss: 3.421, cov loss: 0.161\n",
      "    [batch 475]: seen 47500 examples : 67.3 eps, Loss: 3.428, Avg loss: 3.429, Best loss: 3.421, cov loss: 0.185\n",
      "    [batch 482]: seen 48200 examples : 67.3 eps, Loss: 3.430, Avg loss: 3.430, Best loss: 3.421, cov loss: 0.171\n",
      "    [batch 489]: seen 48900 examples : 67.3 eps, Loss: 3.315, Avg loss: 3.429, Best loss: 3.421, cov loss: 0.170\n",
      "    [batch 496]: seen 49600 examples : 67.4 eps, Loss: 3.339, Avg loss: 3.433, Best loss: 3.421, cov loss: 0.162\n",
      "    [batch 503]: seen 50300 examples : 67.4 eps, Loss: 3.487, Avg loss: 3.435, Best loss: 3.421, cov loss: 0.182\n",
      "    [batch 510]: seen 51000 examples : 67.4 eps, Loss: 3.669, Avg loss: 3.434, Best loss: 3.421, cov loss: 0.162\n",
      "    [batch 517]: seen 51700 examples : 67.4 eps, Loss: 3.420, Avg loss: 3.431, Best loss: 3.421, cov loss: 0.172\n",
      "    [batch 524]: seen 52400 examples : 67.4 eps, Loss: 3.430, Avg loss: 3.435, Best loss: 3.421, cov loss: 0.166\n",
      "    [batch 531]: seen 53100 examples : 67.5 eps, Loss: 3.366, Avg loss: 3.434, Best loss: 3.421, cov loss: 0.178\n",
      "    [batch 538]: seen 53800 examples : 67.5 eps, Loss: 3.633, Avg loss: 3.438, Best loss: 3.421, cov loss: 0.181\n",
      "    [batch 545]: seen 54500 examples : 67.5 eps, Loss: 3.328, Avg loss: 3.439, Best loss: 3.421, cov loss: 0.160\n",
      "    [batch 552]: seen 55200 examples : 67.5 eps, Loss: 3.422, Avg loss: 3.438, Best loss: 3.421, cov loss: 0.176\n",
      "    [batch 559]: seen 55900 examples : 67.5 eps, Loss: 3.582, Avg loss: 3.438, Best loss: 3.421, cov loss: 0.160\n",
      "    [batch 566]: seen 56600 examples : 67.6 eps, Loss: 3.392, Avg loss: 3.437, Best loss: 3.421, cov loss: 0.163\n",
      "    [batch 573]: seen 57300 examples : 67.6 eps, Loss: 3.187, Avg loss: 3.433, Best loss: 3.421, cov loss: 0.156\n",
      "    [batch 580]: seen 58000 examples : 67.6 eps, Loss: 3.454, Avg loss: 3.430, Best loss: 3.421, cov loss: 0.167\n",
      "    [batch 587]: seen 58700 examples : 67.6 eps, Loss: 3.423, Avg loss: 3.428, Best loss: 3.421, cov loss: 0.186\n",
      "    [batch 594]: seen 59400 examples : 67.6 eps, Loss: 3.390, Avg loss: 3.430, Best loss: 3.421, cov loss: 0.165\n",
      "    [batch 601]: seen 60100 examples : 67.6 eps, Loss: 3.359, Avg loss: 3.423, Best loss: 3.421, cov loss: 0.176\n",
      "    [batch 608]: seen 60800 examples : 67.7 eps, Loss: 3.620, Avg loss: 3.426, Best loss: 3.421, cov loss: 0.199\n",
      "    [batch 615]: seen 61500 examples : 67.7 eps, Loss: 3.518, Avg loss: 3.430, Best loss: 3.421, cov loss: 0.193\n",
      "    [batch 622]: seen 62200 examples : 67.7 eps, Loss: 3.403, Avg loss: 3.424, Best loss: 3.421, cov loss: 0.181\n",
      "    [batch 629]: seen 62900 examples : 67.7 eps, Loss: 3.395, Avg loss: 3.423, Best loss: 3.421, cov loss: 0.176\n",
      "    [batch 634]: seen 63400 examples : 67.2 eps, Loss: 3.301, Avg loss: 3.420, Best loss: 3.420, cov loss: 0.154\n",
      "    [batch 636]: seen 63600 examples : 66.7 eps, Loss: 3.399, Avg loss: 3.416, Best loss: 3.416, cov loss: 0.183\n",
      "    [batch 640]: seen 64000 examples : 66.2 eps, Loss: 3.223, Avg loss: 3.413, Best loss: 3.413, cov loss: 0.186\n",
      "    [batch 647]: seen 64700 examples : 66.2 eps, Loss: 3.383, Avg loss: 3.418, Best loss: 3.413, cov loss: 0.159\n",
      "    [batch 654]: seen 65400 examples : 66.3 eps, Loss: 3.325, Avg loss: 3.416, Best loss: 3.413, cov loss: 0.180\n",
      "    [batch 661]: seen 66100 examples : 66.3 eps, Loss: 3.451, Avg loss: 3.418, Best loss: 3.413, cov loss: 0.186\n",
      "    [batch 668]: seen 66800 examples : 66.3 eps, Loss: 3.386, Avg loss: 3.418, Best loss: 3.413, cov loss: 0.167\n",
      "    [batch 675]: seen 67500 examples : 66.4 eps, Loss: 3.605, Avg loss: 3.420, Best loss: 3.413, cov loss: 0.186\n",
      "    [batch 682]: seen 68200 examples : 66.4 eps, Loss: 3.484, Avg loss: 3.420, Best loss: 3.413, cov loss: 0.182\n",
      "    [batch 689]: seen 68900 examples : 66.4 eps, Loss: 3.425, Avg loss: 3.421, Best loss: 3.413, cov loss: 0.185\n",
      "    [batch 696]: seen 69600 examples : 66.4 eps, Loss: 3.399, Avg loss: 3.421, Best loss: 3.413, cov loss: 0.166\n",
      "    [batch 703]: seen 70300 examples : 66.5 eps, Loss: 3.373, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.170\n",
      "    [batch 710]: seen 71000 examples : 66.5 eps, Loss: 3.412, Avg loss: 3.420, Best loss: 3.413, cov loss: 0.166\n",
      "    [batch 717]: seen 71700 examples : 66.5 eps, Loss: 3.425, Avg loss: 3.425, Best loss: 3.413, cov loss: 0.193\n",
      "    [batch 724]: seen 72400 examples : 66.5 eps, Loss: 3.288, Avg loss: 3.423, Best loss: 3.413, cov loss: 0.164\n",
      "    [batch 731]: seen 73100 examples : 66.6 eps, Loss: 3.309, Avg loss: 3.418, Best loss: 3.413, cov loss: 0.159\n",
      "    [batch 738]: seen 73800 examples : 66.6 eps, Loss: 3.491, Avg loss: 3.421, Best loss: 3.413, cov loss: 0.179\n",
      "    [batch 745]: seen 74500 examples : 66.6 eps, Loss: 3.428, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.172\n",
      "    [batch 752]: seen 75200 examples : 66.6 eps, Loss: 3.479, Avg loss: 3.420, Best loss: 3.413, cov loss: 0.174\n",
      "    [batch 759]: seen 75900 examples : 66.7 eps, Loss: 3.510, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.193\n",
      "    [batch 766]: seen 76600 examples : 66.7 eps, Loss: 3.490, Avg loss: 3.419, Best loss: 3.413, cov loss: 0.196\n",
      "    [batch 773]: seen 77300 examples : 66.7 eps, Loss: 3.588, Avg loss: 3.423, Best loss: 3.413, cov loss: 0.171\n",
      "    [batch 780]: seen 78000 examples : 66.7 eps, Loss: 3.426, Avg loss: 3.423, Best loss: 3.413, cov loss: 0.186\n",
      "    [batch 787]: seen 78700 examples : 66.7 eps, Loss: 3.387, Avg loss: 3.425, Best loss: 3.413, cov loss: 0.178\n",
      "    [batch 794]: seen 79400 examples : 66.8 eps, Loss: 3.540, Avg loss: 3.425, Best loss: 3.413, cov loss: 0.176\n",
      "    [batch 801]: seen 80100 examples : 66.8 eps, Loss: 3.346, Avg loss: 3.421, Best loss: 3.413, cov loss: 0.185\n",
      "    [batch 808]: seen 80800 examples : 66.8 eps, Loss: 3.402, Avg loss: 3.426, Best loss: 3.413, cov loss: 0.161\n",
      "    [batch 815]: seen 81500 examples : 66.8 eps, Loss: 3.240, Avg loss: 3.427, Best loss: 3.413, cov loss: 0.152\n",
      "    [batch 822]: seen 82200 examples : 66.8 eps, Loss: 3.251, Avg loss: 3.420, Best loss: 3.413, cov loss: 0.177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 829]: seen 82900 examples : 66.8 eps, Loss: 3.384, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.171\n",
      "    [batch 836]: seen 83600 examples : 66.9 eps, Loss: 3.184, Avg loss: 3.419, Best loss: 3.413, cov loss: 0.174\n",
      "    [batch 843]: seen 84300 examples : 66.9 eps, Loss: 3.161, Avg loss: 3.418, Best loss: 3.413, cov loss: 0.156\n",
      "    [batch 850]: seen 85000 examples : 66.9 eps, Loss: 3.325, Avg loss: 3.423, Best loss: 3.413, cov loss: 0.162\n",
      "    [batch 857]: seen 85700 examples : 66.9 eps, Loss: 3.307, Avg loss: 3.426, Best loss: 3.413, cov loss: 0.165\n",
      "    [batch 864]: seen 86400 examples : 66.9 eps, Loss: 3.489, Avg loss: 3.425, Best loss: 3.413, cov loss: 0.183\n",
      "    [batch 871]: seen 87100 examples : 66.9 eps, Loss: 3.291, Avg loss: 3.426, Best loss: 3.413, cov loss: 0.172\n",
      "    [batch 878]: seen 87800 examples : 67.0 eps, Loss: 3.422, Avg loss: 3.427, Best loss: 3.413, cov loss: 0.172\n",
      "    [batch 885]: seen 88500 examples : 67.0 eps, Loss: 3.626, Avg loss: 3.426, Best loss: 3.413, cov loss: 0.186\n",
      "    [batch 892]: seen 89200 examples : 67.0 eps, Loss: 3.342, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.161\n",
      "    [batch 899]: seen 89900 examples : 67.0 eps, Loss: 3.431, Avg loss: 3.420, Best loss: 3.413, cov loss: 0.176\n",
      "    [batch 906]: seen 90600 examples : 67.0 eps, Loss: 3.568, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.194\n",
      "    [batch 913]: seen 91300 examples : 67.0 eps, Loss: 3.272, Avg loss: 3.421, Best loss: 3.413, cov loss: 0.169\n",
      "    [batch 920]: seen 92000 examples : 67.0 eps, Loss: 3.415, Avg loss: 3.417, Best loss: 3.413, cov loss: 0.176\n",
      "    [batch 927]: seen 92700 examples : 67.1 eps, Loss: 3.630, Avg loss: 3.417, Best loss: 3.413, cov loss: 0.186\n",
      "    [batch 934]: seen 93400 examples : 67.1 eps, Loss: 3.660, Avg loss: 3.417, Best loss: 3.413, cov loss: 0.192\n",
      "    [batch 941]: seen 94100 examples : 67.1 eps, Loss: 3.352, Avg loss: 3.416, Best loss: 3.413, cov loss: 0.167\n",
      "    [batch 948]: seen 94800 examples : 67.1 eps, Loss: 3.381, Avg loss: 3.413, Best loss: 3.413, cov loss: 0.171\n",
      "    [batch 955]: seen 95500 examples : 67.1 eps, Loss: 3.449, Avg loss: 3.420, Best loss: 3.413, cov loss: 0.173\n",
      "    [batch 962]: seen 96200 examples : 67.1 eps, Loss: 3.390, Avg loss: 3.419, Best loss: 3.413, cov loss: 0.163\n",
      "    [batch 969]: seen 96900 examples : 67.1 eps, Loss: 3.429, Avg loss: 3.424, Best loss: 3.413, cov loss: 0.178\n",
      "    [batch 976]: seen 97600 examples : 67.2 eps, Loss: 3.506, Avg loss: 3.424, Best loss: 3.413, cov loss: 0.180\n",
      "    [batch 983]: seen 98300 examples : 67.2 eps, Loss: 3.607, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.184\n",
      "    [batch 990]: seen 99000 examples : 67.2 eps, Loss: 3.368, Avg loss: 3.423, Best loss: 3.413, cov loss: 0.169\n",
      "    [batch 997]: seen 99700 examples : 67.2 eps, Loss: 3.556, Avg loss: 3.427, Best loss: 3.413, cov loss: 0.182\n",
      "    [batch 1004]: seen 100400 examples : 67.2 eps, Loss: 3.677, Avg loss: 3.427, Best loss: 3.413, cov loss: 0.182\n",
      "    [batch 1011]: seen 101100 examples : 67.2 eps, Loss: 3.257, Avg loss: 3.430, Best loss: 3.413, cov loss: 0.166\n",
      "    [batch 1018]: seen 101800 examples : 67.2 eps, Loss: 3.522, Avg loss: 3.434, Best loss: 3.413, cov loss: 0.175\n",
      "    [batch 1025]: seen 102500 examples : 67.2 eps, Loss: 3.292, Avg loss: 3.435, Best loss: 3.413, cov loss: 0.165\n",
      "    [batch 1032]: seen 103200 examples : 67.2 eps, Loss: 3.441, Avg loss: 3.432, Best loss: 3.413, cov loss: 0.165\n",
      "    [batch 1039]: seen 103900 examples : 67.3 eps, Loss: 3.506, Avg loss: 3.437, Best loss: 3.413, cov loss: 0.175\n",
      "    [batch 1046]: seen 104600 examples : 67.3 eps, Loss: 3.526, Avg loss: 3.439, Best loss: 3.413, cov loss: 0.163\n",
      "    [batch 1053]: seen 105300 examples : 67.3 eps, Loss: 3.385, Avg loss: 3.435, Best loss: 3.413, cov loss: 0.167\n",
      "    [batch 1060]: seen 106000 examples : 67.3 eps, Loss: 3.499, Avg loss: 3.434, Best loss: 3.413, cov loss: 0.180\n",
      "    [batch 1067]: seen 106700 examples : 67.3 eps, Loss: 3.407, Avg loss: 3.435, Best loss: 3.413, cov loss: 0.167\n",
      "    [batch 1074]: seen 107400 examples : 67.3 eps, Loss: 3.680, Avg loss: 3.438, Best loss: 3.413, cov loss: 0.162\n",
      "    [batch 1081]: seen 108100 examples : 67.3 eps, Loss: 3.506, Avg loss: 3.436, Best loss: 3.413, cov loss: 0.162\n",
      "    [batch 1088]: seen 108800 examples : 67.3 eps, Loss: 3.200, Avg loss: 3.433, Best loss: 3.413, cov loss: 0.165\n",
      "    [batch 1095]: seen 109500 examples : 67.4 eps, Loss: 3.465, Avg loss: 3.434, Best loss: 3.413, cov loss: 0.177\n",
      "    [batch 1102]: seen 110200 examples : 67.4 eps, Loss: 3.482, Avg loss: 3.438, Best loss: 3.413, cov loss: 0.179\n",
      "    [batch 1109]: seen 110900 examples : 67.4 eps, Loss: 3.331, Avg loss: 3.439, Best loss: 3.413, cov loss: 0.158\n",
      "    [batch 1116]: seen 111600 examples : 67.4 eps, Loss: 3.391, Avg loss: 3.435, Best loss: 3.413, cov loss: 0.204\n",
      "    [batch 1123]: seen 112300 examples : 67.4 eps, Loss: 3.462, Avg loss: 3.432, Best loss: 3.413, cov loss: 0.153\n",
      "    [batch 1130]: seen 113000 examples : 67.4 eps, Loss: 3.519, Avg loss: 3.433, Best loss: 3.413, cov loss: 0.171\n",
      "    [batch 1137]: seen 113700 examples : 67.4 eps, Loss: 3.372, Avg loss: 3.432, Best loss: 3.413, cov loss: 0.163\n",
      "    [batch 1144]: seen 114400 examples : 67.4 eps, Loss: 3.455, Avg loss: 3.433, Best loss: 3.413, cov loss: 0.172\n",
      "    [batch 1151]: seen 115100 examples : 67.4 eps, Loss: 3.574, Avg loss: 3.431, Best loss: 3.413, cov loss: 0.195\n",
      "    [batch 1158]: seen 115800 examples : 67.4 eps, Loss: 3.334, Avg loss: 3.431, Best loss: 3.413, cov loss: 0.173\n",
      "    [batch 1165]: seen 116500 examples : 67.4 eps, Loss: 3.568, Avg loss: 3.431, Best loss: 3.413, cov loss: 0.171\n",
      "    [batch 1172]: seen 117200 examples : 67.5 eps, Loss: 3.603, Avg loss: 3.430, Best loss: 3.413, cov loss: 0.189\n",
      "    [batch 1179]: seen 117900 examples : 67.5 eps, Loss: 3.435, Avg loss: 3.431, Best loss: 3.413, cov loss: 0.168\n",
      "    [batch 1186]: seen 118600 examples : 67.5 eps, Loss: 3.332, Avg loss: 3.435, Best loss: 3.413, cov loss: 0.170\n",
      "    [batch 1193]: seen 119300 examples : 67.5 eps, Loss: 3.257, Avg loss: 3.434, Best loss: 3.413, cov loss: 0.173\n",
      "    [batch 1200]: seen 120000 examples : 67.5 eps, Loss: 3.661, Avg loss: 3.435, Best loss: 3.413, cov loss: 0.181\n",
      "    [batch 1207]: seen 120700 examples : 67.5 eps, Loss: 3.454, Avg loss: 3.431, Best loss: 3.413, cov loss: 0.162\n",
      "    [batch 1214]: seen 121400 examples : 67.5 eps, Loss: 3.619, Avg loss: 3.433, Best loss: 3.413, cov loss: 0.194\n",
      "    [batch 1221]: seen 122100 examples : 67.5 eps, Loss: 3.509, Avg loss: 3.434, Best loss: 3.413, cov loss: 0.166\n",
      "    [batch 1228]: seen 122800 examples : 67.5 eps, Loss: 3.403, Avg loss: 3.438, Best loss: 3.413, cov loss: 0.198\n",
      "    [batch 1235]: seen 123500 examples : 67.5 eps, Loss: 3.481, Avg loss: 3.438, Best loss: 3.413, cov loss: 0.191\n",
      "    [batch 1242]: seen 124200 examples : 67.5 eps, Loss: 3.239, Avg loss: 3.435, Best loss: 3.413, cov loss: 0.184\n",
      "    [batch 1249]: seen 124900 examples : 67.5 eps, Loss: 3.643, Avg loss: 3.439, Best loss: 3.413, cov loss: 0.186\n",
      "    [batch 1256]: seen 125600 examples : 67.6 eps, Loss: 3.281, Avg loss: 3.437, Best loss: 3.413, cov loss: 0.177\n",
      "    [batch 1263]: seen 126300 examples : 67.6 eps, Loss: 3.344, Avg loss: 3.433, Best loss: 3.413, cov loss: 0.192\n",
      "    [batch 1270]: seen 127000 examples : 67.6 eps, Loss: 3.312, Avg loss: 3.436, Best loss: 3.413, cov loss: 0.162\n",
      "    [batch 1277]: seen 127700 examples : 67.6 eps, Loss: 3.349, Avg loss: 3.434, Best loss: 3.413, cov loss: 0.164\n",
      "    [batch 1284]: seen 128400 examples : 67.6 eps, Loss: 3.419, Avg loss: 3.434, Best loss: 3.413, cov loss: 0.171\n",
      "    [batch 1291]: seen 129100 examples : 67.6 eps, Loss: 3.152, Avg loss: 3.427, Best loss: 3.413, cov loss: 0.170\n",
      "    [batch 1298]: seen 129800 examples : 67.6 eps, Loss: 3.244, Avg loss: 3.431, Best loss: 3.413, cov loss: 0.183\n",
      "    [batch 1305]: seen 130500 examples : 67.6 eps, Loss: 3.409, Avg loss: 3.434, Best loss: 3.413, cov loss: 0.160\n",
      "    [batch 1312]: seen 131200 examples : 67.6 eps, Loss: 3.426, Avg loss: 3.436, Best loss: 3.413, cov loss: 0.178\n",
      "    [batch 1319]: seen 131900 examples : 67.6 eps, Loss: 3.631, Avg loss: 3.439, Best loss: 3.413, cov loss: 0.166\n",
      "    [batch 1326]: seen 132600 examples : 67.6 eps, Loss: 3.401, Avg loss: 3.438, Best loss: 3.413, cov loss: 0.173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1333]: seen 133300 examples : 67.6 eps, Loss: 3.522, Avg loss: 3.435, Best loss: 3.413, cov loss: 0.183\n",
      "    [batch 1340]: seen 134000 examples : 67.6 eps, Loss: 3.594, Avg loss: 3.435, Best loss: 3.413, cov loss: 0.197\n",
      "    [batch 1347]: seen 134700 examples : 67.7 eps, Loss: 3.405, Avg loss: 3.434, Best loss: 3.413, cov loss: 0.176\n",
      "    [batch 1354]: seen 135400 examples : 67.7 eps, Loss: 3.486, Avg loss: 3.432, Best loss: 3.413, cov loss: 0.177\n",
      "    [batch 1361]: seen 136100 examples : 67.7 eps, Loss: 3.366, Avg loss: 3.433, Best loss: 3.413, cov loss: 0.161\n",
      "    [batch 1368]: seen 136800 examples : 67.7 eps, Loss: 3.444, Avg loss: 3.432, Best loss: 3.413, cov loss: 0.177\n",
      "    [batch 1375]: seen 137500 examples : 67.7 eps, Loss: 3.536, Avg loss: 3.429, Best loss: 3.413, cov loss: 0.172\n",
      "    [batch 1382]: seen 138200 examples : 67.7 eps, Loss: 3.568, Avg loss: 3.428, Best loss: 3.413, cov loss: 0.181\n",
      "    [batch 1389]: seen 138900 examples : 67.7 eps, Loss: 3.461, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.159\n",
      "    [batch 1396]: seen 139600 examples : 67.7 eps, Loss: 3.333, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.174\n",
      "    [batch 1403]: seen 140300 examples : 67.7 eps, Loss: 3.445, Avg loss: 3.423, Best loss: 3.413, cov loss: 0.163\n",
      "    [batch 1410]: seen 141000 examples : 67.7 eps, Loss: 3.376, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.179\n",
      "    [batch 1417]: seen 141700 examples : 67.7 eps, Loss: 3.486, Avg loss: 3.427, Best loss: 3.413, cov loss: 0.164\n",
      "    [batch 1424]: seen 142400 examples : 67.7 eps, Loss: 3.298, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.169\n",
      "    [batch 1431]: seen 143100 examples : 67.7 eps, Loss: 3.504, Avg loss: 3.424, Best loss: 3.413, cov loss: 0.191\n",
      "    [batch 1438]: seen 143800 examples : 67.7 eps, Loss: 3.506, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.172\n",
      "    [batch 1445]: seen 144500 examples : 67.7 eps, Loss: 3.289, Avg loss: 3.421, Best loss: 3.413, cov loss: 0.141\n",
      "    [batch 1452]: seen 145200 examples : 67.7 eps, Loss: 3.300, Avg loss: 3.425, Best loss: 3.413, cov loss: 0.187\n",
      "    [batch 1459]: seen 145900 examples : 67.8 eps, Loss: 3.502, Avg loss: 3.424, Best loss: 3.413, cov loss: 0.168\n",
      "    [batch 1466]: seen 146600 examples : 67.8 eps, Loss: 3.402, Avg loss: 3.420, Best loss: 3.413, cov loss: 0.171\n",
      "    [batch 1473]: seen 147300 examples : 67.8 eps, Loss: 3.524, Avg loss: 3.428, Best loss: 3.413, cov loss: 0.195\n",
      "    [batch 1480]: seen 148000 examples : 67.8 eps, Loss: 3.514, Avg loss: 3.431, Best loss: 3.413, cov loss: 0.178\n",
      "    [batch 1487]: seen 148700 examples : 67.8 eps, Loss: 3.250, Avg loss: 3.425, Best loss: 3.413, cov loss: 0.148\n",
      "    [batch 1494]: seen 149400 examples : 67.8 eps, Loss: 3.512, Avg loss: 3.423, Best loss: 3.413, cov loss: 0.192\n",
      "    [batch 1501]: seen 150100 examples : 67.8 eps, Loss: 3.407, Avg loss: 3.421, Best loss: 3.413, cov loss: 0.162\n",
      "    [batch 1508]: seen 150800 examples : 67.8 eps, Loss: 3.455, Avg loss: 3.422, Best loss: 3.413, cov loss: 0.179\n",
      "    [batch 1515]: seen 151500 examples : 67.8 eps, Loss: 3.470, Avg loss: 3.421, Best loss: 3.413, cov loss: 0.175\n",
      "    [batch 1522]: seen 152200 examples : 67.8 eps, Loss: 3.372, Avg loss: 3.417, Best loss: 3.413, cov loss: 0.178\n",
      "    [batch 1529]: seen 152900 examples : 67.8 eps, Loss: 3.499, Avg loss: 3.414, Best loss: 3.413, cov loss: 0.164\n",
      "    [batch 1536]: seen 153600 examples : 67.8 eps, Loss: 3.350, Avg loss: 3.413, Best loss: 3.413, cov loss: 0.159\n",
      "    [batch 1541]: seen 154100 examples : 67.7 eps, Loss: 3.595, Avg loss: 3.414, Best loss: 3.411, cov loss: 0.196\n",
      "    [batch 1548]: seen 154800 examples : 67.7 eps, Loss: 3.268, Avg loss: 3.417, Best loss: 3.411, cov loss: 0.153\n",
      "    [batch 1555]: seen 155500 examples : 67.7 eps, Loss: 3.433, Avg loss: 3.418, Best loss: 3.411, cov loss: 0.172\n",
      "    [batch 1562]: seen 156200 examples : 67.7 eps, Loss: 3.321, Avg loss: 3.414, Best loss: 3.411, cov loss: 0.183\n",
      "    [batch 1569]: seen 156900 examples : 67.7 eps, Loss: 3.394, Avg loss: 3.415, Best loss: 3.411, cov loss: 0.165\n",
      "    [batch 1576]: seen 157600 examples : 67.7 eps, Loss: 3.506, Avg loss: 3.420, Best loss: 3.411, cov loss: 0.186\n",
      "    [batch 1583]: seen 158300 examples : 67.8 eps, Loss: 3.338, Avg loss: 3.418, Best loss: 3.411, cov loss: 0.159\n",
      "    [batch 1590]: seen 159000 examples : 67.8 eps, Loss: 3.521, Avg loss: 3.417, Best loss: 3.411, cov loss: 0.180\n",
      "    [batch 1597]: seen 159700 examples : 67.8 eps, Loss: 3.477, Avg loss: 3.421, Best loss: 3.411, cov loss: 0.171\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-34602\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-34602\n",
      "    [batch 1603]: seen 160300 examples : 67.7 eps, Loss: 3.275, Avg loss: 3.422, Best loss: 3.411, cov loss: 0.164\n",
      "    [batch 1610]: seen 161000 examples : 67.7 eps, Loss: 3.593, Avg loss: 3.426, Best loss: 3.411, cov loss: 0.178\n",
      "    [batch 1617]: seen 161700 examples : 67.7 eps, Loss: 3.416, Avg loss: 3.428, Best loss: 3.411, cov loss: 0.194\n",
      "    [batch 1624]: seen 162400 examples : 67.7 eps, Loss: 3.203, Avg loss: 3.423, Best loss: 3.411, cov loss: 0.178\n",
      "    [batch 1631]: seen 163100 examples : 67.7 eps, Loss: 3.423, Avg loss: 3.426, Best loss: 3.411, cov loss: 0.160\n",
      "    [batch 1638]: seen 163800 examples : 67.8 eps, Loss: 3.577, Avg loss: 3.422, Best loss: 3.411, cov loss: 0.196\n",
      "    [batch 1645]: seen 164500 examples : 67.8 eps, Loss: 3.586, Avg loss: 3.425, Best loss: 3.411, cov loss: 0.160\n",
      "    [batch 1652]: seen 165200 examples : 67.8 eps, Loss: 3.292, Avg loss: 3.425, Best loss: 3.411, cov loss: 0.179\n",
      "    [batch 1659]: seen 165900 examples : 67.8 eps, Loss: 3.483, Avg loss: 3.426, Best loss: 3.411, cov loss: 0.176\n",
      "    [batch 1666]: seen 166600 examples : 67.8 eps, Loss: 3.246, Avg loss: 3.427, Best loss: 3.411, cov loss: 0.155\n",
      "    [batch 1673]: seen 167300 examples : 67.8 eps, Loss: 3.576, Avg loss: 3.428, Best loss: 3.411, cov loss: 0.175\n",
      "    [batch 1680]: seen 168000 examples : 67.8 eps, Loss: 3.551, Avg loss: 3.429, Best loss: 3.411, cov loss: 0.154\n",
      "    [batch 1687]: seen 168700 examples : 67.8 eps, Loss: 3.274, Avg loss: 3.427, Best loss: 3.411, cov loss: 0.176\n",
      "    [batch 1694]: seen 169400 examples : 67.8 eps, Loss: 3.351, Avg loss: 3.428, Best loss: 3.411, cov loss: 0.169\n",
      "    [batch 1701]: seen 170100 examples : 67.8 eps, Loss: 3.347, Avg loss: 3.427, Best loss: 3.411, cov loss: 0.174\n",
      "    [batch 1708]: seen 170800 examples : 67.8 eps, Loss: 3.333, Avg loss: 3.427, Best loss: 3.411, cov loss: 0.172\n",
      "    [batch 1715]: seen 171500 examples : 67.8 eps, Loss: 3.493, Avg loss: 3.425, Best loss: 3.411, cov loss: 0.164\n",
      "    [batch 1722]: seen 172200 examples : 67.8 eps, Loss: 3.398, Avg loss: 3.426, Best loss: 3.411, cov loss: 0.163\n",
      "    [batch 1729]: seen 172900 examples : 67.8 eps, Loss: 3.521, Avg loss: 3.427, Best loss: 3.411, cov loss: 0.178\n",
      "    [batch 1736]: seen 173600 examples : 67.8 eps, Loss: 3.561, Avg loss: 3.428, Best loss: 3.411, cov loss: 0.189\n",
      "    [batch 1743]: seen 174300 examples : 67.8 eps, Loss: 3.680, Avg loss: 3.431, Best loss: 3.411, cov loss: 0.184\n",
      "    [batch 1750]: seen 175000 examples : 67.8 eps, Loss: 3.526, Avg loss: 3.435, Best loss: 3.411, cov loss: 0.171\n",
      "    [batch 1757]: seen 175700 examples : 67.8 eps, Loss: 3.301, Avg loss: 3.429, Best loss: 3.411, cov loss: 0.158\n",
      "    [batch 1764]: seen 176400 examples : 67.8 eps, Loss: 3.388, Avg loss: 3.428, Best loss: 3.411, cov loss: 0.174\n",
      "    [batch 1771]: seen 177100 examples : 67.8 eps, Loss: 3.420, Avg loss: 3.429, Best loss: 3.411, cov loss: 0.169\n",
      "    [batch 1778]: seen 177800 examples : 67.8 eps, Loss: 3.429, Avg loss: 3.431, Best loss: 3.411, cov loss: 0.181\n",
      "    [batch 1785]: seen 178500 examples : 67.9 eps, Loss: 3.354, Avg loss: 3.434, Best loss: 3.411, cov loss: 0.154\n",
      "    [batch 1792]: seen 179200 examples : 67.9 eps, Loss: 3.571, Avg loss: 3.435, Best loss: 3.411, cov loss: 0.187\n",
      "    [batch 1799]: seen 179900 examples : 67.9 eps, Loss: 3.474, Avg loss: 3.441, Best loss: 3.411, cov loss: 0.188\n",
      "    [batch 1806]: seen 180600 examples : 67.9 eps, Loss: 3.553, Avg loss: 3.435, Best loss: 3.411, cov loss: 0.172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1813]: seen 181300 examples : 67.9 eps, Loss: 3.378, Avg loss: 3.433, Best loss: 3.411, cov loss: 0.152\n",
      "    [batch 1820]: seen 182000 examples : 67.9 eps, Loss: 3.524, Avg loss: 3.432, Best loss: 3.411, cov loss: 0.164\n",
      "    [batch 1827]: seen 182700 examples : 67.9 eps, Loss: 3.502, Avg loss: 3.430, Best loss: 3.411, cov loss: 0.175\n",
      "    [batch 1834]: seen 183400 examples : 67.9 eps, Loss: 3.298, Avg loss: 3.429, Best loss: 3.411, cov loss: 0.169\n",
      "    [batch 1841]: seen 184100 examples : 67.9 eps, Loss: 3.588, Avg loss: 3.431, Best loss: 3.411, cov loss: 0.185\n",
      "    [batch 1848]: seen 184800 examples : 67.9 eps, Loss: 3.414, Avg loss: 3.433, Best loss: 3.411, cov loss: 0.185\n",
      "    [batch 1855]: seen 185500 examples : 67.9 eps, Loss: 3.465, Avg loss: 3.429, Best loss: 3.411, cov loss: 0.169\n",
      "    [batch 1862]: seen 186200 examples : 67.9 eps, Loss: 3.698, Avg loss: 3.427, Best loss: 3.411, cov loss: 0.174\n",
      "    [batch 1869]: seen 186900 examples : 67.9 eps, Loss: 3.294, Avg loss: 3.423, Best loss: 3.411, cov loss: 0.174\n",
      "    [batch 1876]: seen 187600 examples : 67.9 eps, Loss: 3.413, Avg loss: 3.422, Best loss: 3.411, cov loss: 0.193\n",
      "    [batch 1883]: seen 188300 examples : 67.9 eps, Loss: 3.383, Avg loss: 3.421, Best loss: 3.411, cov loss: 0.160\n",
      "    [batch 1890]: seen 189000 examples : 67.9 eps, Loss: 3.408, Avg loss: 3.421, Best loss: 3.411, cov loss: 0.167\n",
      "    [batch 1897]: seen 189700 examples : 67.9 eps, Loss: 3.384, Avg loss: 3.420, Best loss: 3.411, cov loss: 0.170\n",
      "    [batch 1904]: seen 190400 examples : 67.9 eps, Loss: 3.554, Avg loss: 3.423, Best loss: 3.411, cov loss: 0.177\n",
      "    [batch 1911]: seen 191100 examples : 67.9 eps, Loss: 3.399, Avg loss: 3.426, Best loss: 3.411, cov loss: 0.179\n",
      "    [batch 1918]: seen 191800 examples : 67.9 eps, Loss: 3.264, Avg loss: 3.427, Best loss: 3.411, cov loss: 0.159\n",
      "    [batch 1925]: seen 192500 examples : 67.9 eps, Loss: 3.399, Avg loss: 3.425, Best loss: 3.411, cov loss: 0.156\n",
      "    [batch 1932]: seen 193200 examples : 67.9 eps, Loss: 3.384, Avg loss: 3.425, Best loss: 3.411, cov loss: 0.159\n",
      "    [batch 1939]: seen 193900 examples : 67.9 eps, Loss: 3.698, Avg loss: 3.427, Best loss: 3.411, cov loss: 0.157\n",
      "    [batch 1946]: seen 194600 examples : 67.9 eps, Loss: 3.343, Avg loss: 3.425, Best loss: 3.411, cov loss: 0.173\n",
      "    [batch 1953]: seen 195300 examples : 68.0 eps, Loss: 3.502, Avg loss: 3.430, Best loss: 3.411, cov loss: 0.191\n",
      "    [batch 1960]: seen 196000 examples : 68.0 eps, Loss: 3.397, Avg loss: 3.426, Best loss: 3.411, cov loss: 0.154\n",
      "    [batch 1967]: seen 196700 examples : 68.0 eps, Loss: 3.561, Avg loss: 3.426, Best loss: 3.411, cov loss: 0.174\n",
      "    [batch 1974]: seen 197400 examples : 68.0 eps, Loss: 3.219, Avg loss: 3.424, Best loss: 3.411, cov loss: 0.163\n",
      "    [batch 1981]: seen 198100 examples : 68.0 eps, Loss: 3.208, Avg loss: 3.424, Best loss: 3.411, cov loss: 0.159\n",
      "    [batch 1988]: seen 198800 examples : 68.0 eps, Loss: 3.549, Avg loss: 3.422, Best loss: 3.411, cov loss: 0.167\n",
      "    [batch 1995]: seen 199500 examples : 68.0 eps, Loss: 3.378, Avg loss: 3.426, Best loss: 3.411, cov loss: 0.159\n",
      "    [batch 2002]: seen 200200 examples : 68.0 eps, Loss: 3.484, Avg loss: 3.426, Best loss: 3.411, cov loss: 0.173\n",
      "    [batch 2009]: seen 200900 examples : 68.0 eps, Loss: 3.317, Avg loss: 3.427, Best loss: 3.411, cov loss: 0.190\n",
      "    [batch 2016]: seen 201600 examples : 68.0 eps, Loss: 3.560, Avg loss: 3.430, Best loss: 3.411, cov loss: 0.173\n",
      "    [batch 2023]: seen 202300 examples : 68.0 eps, Loss: 3.488, Avg loss: 3.429, Best loss: 3.411, cov loss: 0.166\n",
      "    [batch 2030]: seen 203000 examples : 68.0 eps, Loss: 3.385, Avg loss: 3.427, Best loss: 3.411, cov loss: 0.159\n",
      "    [batch 2037]: seen 203700 examples : 68.0 eps, Loss: 3.606, Avg loss: 3.427, Best loss: 3.411, cov loss: 0.181\n",
      "    [batch 2044]: seen 204400 examples : 68.0 eps, Loss: 3.565, Avg loss: 3.428, Best loss: 3.411, cov loss: 0.183\n",
      "    [batch 2051]: seen 205100 examples : 68.0 eps, Loss: 3.585, Avg loss: 3.430, Best loss: 3.411, cov loss: 0.201\n",
      "    [batch 2058]: seen 205800 examples : 68.0 eps, Loss: 3.499, Avg loss: 3.430, Best loss: 3.411, cov loss: 0.167\n",
      "    [batch 2065]: seen 206500 examples : 68.0 eps, Loss: 3.345, Avg loss: 3.424, Best loss: 3.411, cov loss: 0.166\n",
      "    [batch 2072]: seen 207200 examples : 68.0 eps, Loss: 3.323, Avg loss: 3.416, Best loss: 3.411, cov loss: 0.168\n",
      "    [batch 2079]: seen 207900 examples : 68.0 eps, Loss: 3.330, Avg loss: 3.417, Best loss: 3.411, cov loss: 0.160\n",
      "    [batch 2086]: seen 208600 examples : 68.0 eps, Loss: 3.327, Avg loss: 3.417, Best loss: 3.411, cov loss: 0.175\n",
      "    [batch 2093]: seen 209300 examples : 68.0 eps, Loss: 3.407, Avg loss: 3.418, Best loss: 3.411, cov loss: 0.168\n",
      "    [batch 2100]: seen 210000 examples : 68.0 eps, Loss: 3.575, Avg loss: 3.419, Best loss: 3.411, cov loss: 0.170\n",
      "    [batch 2107]: seen 210700 examples : 68.0 eps, Loss: 3.425, Avg loss: 3.420, Best loss: 3.411, cov loss: 0.191\n",
      "    [batch 2114]: seen 211400 examples : 68.0 eps, Loss: 3.436, Avg loss: 3.418, Best loss: 3.411, cov loss: 0.195\n",
      "    [batch 2121]: seen 212100 examples : 68.0 eps, Loss: 3.457, Avg loss: 3.416, Best loss: 3.411, cov loss: 0.172\n",
      "    [batch 2128]: seen 212800 examples : 68.0 eps, Loss: 3.458, Avg loss: 3.422, Best loss: 3.411, cov loss: 0.179\n",
      "    [batch 2135]: seen 213500 examples : 68.0 eps, Loss: 3.494, Avg loss: 3.419, Best loss: 3.411, cov loss: 0.162\n",
      "    [batch 2142]: seen 214200 examples : 68.0 eps, Loss: 3.373, Avg loss: 3.412, Best loss: 3.411, cov loss: 0.157\n",
      "    [batch 2145]: seen 214500 examples : 67.9 eps, Loss: 3.298, Avg loss: 3.409, Best loss: 3.409, cov loss: 0.191\n",
      "    [batch 2150]: seen 215000 examples : 67.8 eps, Loss: 3.330, Avg loss: 3.409, Best loss: 3.409, cov loss: 0.159\n",
      "    [batch 2157]: seen 215700 examples : 67.8 eps, Loss: 3.275, Avg loss: 3.410, Best loss: 3.409, cov loss: 0.161\n",
      "    [batch 2162]: seen 216200 examples : 67.8 eps, Loss: 3.506, Avg loss: 3.410, Best loss: 3.409, cov loss: 0.170\n",
      "    [batch 2167]: seen 216700 examples : 67.7 eps, Loss: 3.380, Avg loss: 3.412, Best loss: 3.408, cov loss: 0.186\n",
      "    [batch 2173]: seen 217300 examples : 67.6 eps, Loss: 3.099, Avg loss: 3.406, Best loss: 3.406, cov loss: 0.140\n",
      "    [batch 2177]: seen 217700 examples : 67.5 eps, Loss: 3.341, Avg loss: 3.405, Best loss: 3.405, cov loss: 0.158\n",
      "    [batch 2184]: seen 218400 examples : 67.5 eps, Loss: 3.359, Avg loss: 3.409, Best loss: 3.405, cov loss: 0.156\n",
      "    [batch 2191]: seen 219100 examples : 67.5 eps, Loss: 3.499, Avg loss: 3.414, Best loss: 3.405, cov loss: 0.175\n",
      "    [batch 2198]: seen 219800 examples : 67.5 eps, Loss: 3.371, Avg loss: 3.413, Best loss: 3.405, cov loss: 0.185\n",
      "    [batch 2205]: seen 220500 examples : 67.5 eps, Loss: 3.445, Avg loss: 3.414, Best loss: 3.405, cov loss: 0.184\n",
      "    [batch 2212]: seen 221200 examples : 67.5 eps, Loss: 3.420, Avg loss: 3.412, Best loss: 3.405, cov loss: 0.173\n",
      "    [batch 2219]: seen 221900 examples : 67.5 eps, Loss: 3.283, Avg loss: 3.412, Best loss: 3.405, cov loss: 0.164\n",
      "    [batch 2226]: seen 222600 examples : 67.5 eps, Loss: 3.488, Avg loss: 3.413, Best loss: 3.405, cov loss: 0.169\n",
      "    [batch 2233]: seen 223300 examples : 67.5 eps, Loss: 3.284, Avg loss: 3.413, Best loss: 3.405, cov loss: 0.145\n",
      "    [batch 2240]: seen 224000 examples : 67.5 eps, Loss: 3.406, Avg loss: 3.418, Best loss: 3.405, cov loss: 0.180\n",
      "    [batch 2247]: seen 224700 examples : 67.5 eps, Loss: 3.399, Avg loss: 3.419, Best loss: 3.405, cov loss: 0.174\n",
      "    [batch 2254]: seen 225400 examples : 67.5 eps, Loss: 3.618, Avg loss: 3.425, Best loss: 3.405, cov loss: 0.185\n",
      "    [batch 2261]: seen 226100 examples : 67.5 eps, Loss: 3.403, Avg loss: 3.423, Best loss: 3.405, cov loss: 0.161\n",
      "    [batch 2268]: seen 226800 examples : 67.5 eps, Loss: 3.446, Avg loss: 3.426, Best loss: 3.405, cov loss: 0.165\n",
      "    [batch 2275]: seen 227500 examples : 67.5 eps, Loss: 3.583, Avg loss: 3.427, Best loss: 3.405, cov loss: 0.175\n",
      "    [batch 2282]: seen 228200 examples : 67.5 eps, Loss: 3.354, Avg loss: 3.423, Best loss: 3.405, cov loss: 0.167\n",
      "    [batch 2289]: seen 228900 examples : 67.5 eps, Loss: 3.362, Avg loss: 3.425, Best loss: 3.405, cov loss: 0.171\n",
      "    [batch 2296]: seen 229600 examples : 67.6 eps, Loss: 3.422, Avg loss: 3.426, Best loss: 3.405, cov loss: 0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2303]: seen 230300 examples : 67.6 eps, Loss: 3.481, Avg loss: 3.428, Best loss: 3.405, cov loss: 0.165\n",
      "    [batch 2310]: seen 231000 examples : 67.6 eps, Loss: 3.451, Avg loss: 3.429, Best loss: 3.405, cov loss: 0.166\n",
      "    [batch 2317]: seen 231700 examples : 67.6 eps, Loss: 3.483, Avg loss: 3.434, Best loss: 3.405, cov loss: 0.179\n",
      "    [batch 2324]: seen 232400 examples : 67.6 eps, Loss: 3.234, Avg loss: 3.430, Best loss: 3.405, cov loss: 0.150\n",
      "    [batch 2331]: seen 233100 examples : 67.6 eps, Loss: 3.405, Avg loss: 3.431, Best loss: 3.405, cov loss: 0.175\n",
      "    [batch 2338]: seen 233800 examples : 67.6 eps, Loss: 3.542, Avg loss: 3.435, Best loss: 3.405, cov loss: 0.174\n",
      "    [batch 2345]: seen 234500 examples : 67.6 eps, Loss: 3.469, Avg loss: 3.435, Best loss: 3.405, cov loss: 0.171\n",
      "    [batch 2352]: seen 235200 examples : 67.6 eps, Loss: 3.420, Avg loss: 3.432, Best loss: 3.405, cov loss: 0.170\n",
      "    [batch 2359]: seen 235900 examples : 67.6 eps, Loss: 3.484, Avg loss: 3.432, Best loss: 3.405, cov loss: 0.185\n",
      "    [batch 2366]: seen 236600 examples : 67.6 eps, Loss: 3.283, Avg loss: 3.426, Best loss: 3.405, cov loss: 0.165\n",
      "    [batch 2373]: seen 237300 examples : 67.6 eps, Loss: 3.336, Avg loss: 3.417, Best loss: 3.405, cov loss: 0.164\n",
      "    [batch 2380]: seen 238000 examples : 67.6 eps, Loss: 3.157, Avg loss: 3.416, Best loss: 3.405, cov loss: 0.164\n",
      "    [batch 2387]: seen 238700 examples : 67.6 eps, Loss: 3.420, Avg loss: 3.421, Best loss: 3.405, cov loss: 0.169\n",
      "    [batch 2394]: seen 239400 examples : 67.6 eps, Loss: 3.680, Avg loss: 3.424, Best loss: 3.405, cov loss: 0.192\n",
      "    [batch 2401]: seen 240100 examples : 67.6 eps, Loss: 3.348, Avg loss: 3.423, Best loss: 3.405, cov loss: 0.161\n",
      "    [batch 2408]: seen 240800 examples : 67.6 eps, Loss: 3.480, Avg loss: 3.423, Best loss: 3.405, cov loss: 0.181\n",
      "    [batch 2415]: seen 241500 examples : 67.6 eps, Loss: 3.421, Avg loss: 3.422, Best loss: 3.405, cov loss: 0.160\n",
      "    [batch 2422]: seen 242200 examples : 67.6 eps, Loss: 3.386, Avg loss: 3.422, Best loss: 3.405, cov loss: 0.181\n",
      "    [batch 2429]: seen 242900 examples : 67.6 eps, Loss: 3.385, Avg loss: 3.424, Best loss: 3.405, cov loss: 0.181\n",
      "    [batch 2436]: seen 243600 examples : 67.6 eps, Loss: 3.338, Avg loss: 3.420, Best loss: 3.405, cov loss: 0.160\n",
      "    [batch 2443]: seen 244300 examples : 67.6 eps, Loss: 3.458, Avg loss: 3.421, Best loss: 3.405, cov loss: 0.177\n",
      "    [batch 2450]: seen 245000 examples : 67.6 eps, Loss: 3.470, Avg loss: 3.421, Best loss: 3.405, cov loss: 0.179\n",
      "    [batch 2457]: seen 245700 examples : 67.6 eps, Loss: 3.377, Avg loss: 3.417, Best loss: 3.405, cov loss: 0.176\n",
      "    [batch 2464]: seen 246400 examples : 67.6 eps, Loss: 3.553, Avg loss: 3.414, Best loss: 3.405, cov loss: 0.183\n",
      "    [batch 2471]: seen 247100 examples : 67.6 eps, Loss: 3.462, Avg loss: 3.414, Best loss: 3.405, cov loss: 0.180\n",
      "    [batch 2478]: seen 247800 examples : 67.7 eps, Loss: 3.295, Avg loss: 3.412, Best loss: 3.405, cov loss: 0.167\n",
      "    [batch 2485]: seen 248500 examples : 67.7 eps, Loss: 3.378, Avg loss: 3.411, Best loss: 3.405, cov loss: 0.172\n",
      "    [batch 2492]: seen 249200 examples : 67.7 eps, Loss: 3.474, Avg loss: 3.413, Best loss: 3.405, cov loss: 0.180\n",
      "    [batch 2499]: seen 249900 examples : 67.7 eps, Loss: 3.634, Avg loss: 3.417, Best loss: 3.405, cov loss: 0.184\n",
      "    [batch 2506]: seen 250600 examples : 67.7 eps, Loss: 3.235, Avg loss: 3.418, Best loss: 3.405, cov loss: 0.148\n",
      "    [batch 2513]: seen 251300 examples : 67.7 eps, Loss: 3.335, Avg loss: 3.418, Best loss: 3.405, cov loss: 0.176\n",
      "    [batch 2520]: seen 252000 examples : 67.7 eps, Loss: 3.425, Avg loss: 3.419, Best loss: 3.405, cov loss: 0.158\n",
      "    [batch 2527]: seen 252700 examples : 67.7 eps, Loss: 3.227, Avg loss: 3.418, Best loss: 3.405, cov loss: 0.163\n",
      "    [batch 2534]: seen 253400 examples : 67.7 eps, Loss: 3.306, Avg loss: 3.418, Best loss: 3.405, cov loss: 0.186\n",
      "    [batch 2541]: seen 254100 examples : 67.7 eps, Loss: 3.519, Avg loss: 3.425, Best loss: 3.405, cov loss: 0.190\n",
      "    [batch 2548]: seen 254800 examples : 67.7 eps, Loss: 3.447, Avg loss: 3.421, Best loss: 3.405, cov loss: 0.177\n",
      "    [batch 2555]: seen 255500 examples : 67.7 eps, Loss: 3.424, Avg loss: 3.421, Best loss: 3.405, cov loss: 0.174\n",
      "    [batch 2562]: seen 256200 examples : 67.7 eps, Loss: 3.379, Avg loss: 3.422, Best loss: 3.405, cov loss: 0.186\n",
      "    [batch 2569]: seen 256900 examples : 67.7 eps, Loss: 3.366, Avg loss: 3.424, Best loss: 3.405, cov loss: 0.175\n",
      "    [batch 2576]: seen 257600 examples : 67.7 eps, Loss: 3.491, Avg loss: 3.426, Best loss: 3.405, cov loss: 0.167\n",
      "    [batch 2583]: seen 258300 examples : 67.7 eps, Loss: 3.438, Avg loss: 3.427, Best loss: 3.405, cov loss: 0.173\n",
      "    [batch 2590]: seen 259000 examples : 67.7 eps, Loss: 3.392, Avg loss: 3.427, Best loss: 3.405, cov loss: 0.176\n",
      "    [batch 2597]: seen 259700 examples : 67.7 eps, Loss: 3.173, Avg loss: 3.426, Best loss: 3.405, cov loss: 0.159\n",
      "    [batch 2604]: seen 260400 examples : 67.7 eps, Loss: 3.733, Avg loss: 3.422, Best loss: 3.405, cov loss: 0.176\n",
      "    [batch 2611]: seen 261100 examples : 67.7 eps, Loss: 3.425, Avg loss: 3.417, Best loss: 3.405, cov loss: 0.169\n",
      "    [batch 2618]: seen 261800 examples : 67.7 eps, Loss: 3.267, Avg loss: 3.413, Best loss: 3.405, cov loss: 0.166\n",
      "    [batch 2625]: seen 262500 examples : 67.7 eps, Loss: 3.483, Avg loss: 3.412, Best loss: 3.405, cov loss: 0.168\n",
      "    [batch 2632]: seen 263200 examples : 67.7 eps, Loss: 3.344, Avg loss: 3.412, Best loss: 3.405, cov loss: 0.169\n",
      "    [batch 2639]: seen 263900 examples : 67.7 eps, Loss: 3.354, Avg loss: 3.410, Best loss: 3.405, cov loss: 0.175\n",
      "    [batch 2646]: seen 264600 examples : 67.7 eps, Loss: 3.425, Avg loss: 3.414, Best loss: 3.405, cov loss: 0.173\n",
      "    [batch 2653]: seen 265300 examples : 67.7 eps, Loss: 3.602, Avg loss: 3.419, Best loss: 3.405, cov loss: 0.185\n",
      "    [batch 2660]: seen 266000 examples : 67.7 eps, Loss: 3.627, Avg loss: 3.420, Best loss: 3.405, cov loss: 0.183\n",
      "    [batch 2667]: seen 266700 examples : 67.7 eps, Loss: 3.340, Avg loss: 3.420, Best loss: 3.405, cov loss: 0.168\n",
      "    [batch 2674]: seen 267400 examples : 67.7 eps, Loss: 3.665, Avg loss: 3.422, Best loss: 3.405, cov loss: 0.184\n",
      "    [batch 2681]: seen 268100 examples : 67.8 eps, Loss: 3.289, Avg loss: 3.421, Best loss: 3.405, cov loss: 0.164\n",
      "    [batch 2688]: seen 268800 examples : 67.8 eps, Loss: 3.502, Avg loss: 3.416, Best loss: 3.405, cov loss: 0.175\n",
      "    [batch 2695]: seen 269500 examples : 67.8 eps, Loss: 3.106, Avg loss: 3.411, Best loss: 3.405, cov loss: 0.166\n",
      "    [batch 2702]: seen 270200 examples : 67.8 eps, Loss: 3.588, Avg loss: 3.412, Best loss: 3.405, cov loss: 0.173\n",
      "    [batch 2709]: seen 270900 examples : 67.8 eps, Loss: 3.574, Avg loss: 3.410, Best loss: 3.405, cov loss: 0.167\n",
      "    [batch 2716]: seen 271600 examples : 67.8 eps, Loss: 3.540, Avg loss: 3.414, Best loss: 3.405, cov loss: 0.161\n",
      "    [batch 2723]: seen 272300 examples : 67.8 eps, Loss: 3.482, Avg loss: 3.411, Best loss: 3.405, cov loss: 0.170\n",
      "    [batch 2730]: seen 273000 examples : 67.8 eps, Loss: 3.392, Avg loss: 3.410, Best loss: 3.405, cov loss: 0.182\n",
      "    [batch 2737]: seen 273700 examples : 67.8 eps, Loss: 3.503, Avg loss: 3.409, Best loss: 3.405, cov loss: 0.184\n",
      "    [batch 2744]: seen 274400 examples : 67.8 eps, Loss: 3.427, Avg loss: 3.411, Best loss: 3.405, cov loss: 0.193\n",
      "    [batch 2751]: seen 275100 examples : 67.8 eps, Loss: 3.695, Avg loss: 3.417, Best loss: 3.405, cov loss: 0.184\n",
      "    [batch 2758]: seen 275800 examples : 67.8 eps, Loss: 3.436, Avg loss: 3.416, Best loss: 3.405, cov loss: 0.171\n",
      "    [batch 2765]: seen 276500 examples : 67.8 eps, Loss: 3.290, Avg loss: 3.412, Best loss: 3.405, cov loss: 0.166\n",
      "    [batch 2772]: seen 277200 examples : 67.8 eps, Loss: 3.420, Avg loss: 3.414, Best loss: 3.405, cov loss: 0.178\n",
      "    [batch 2779]: seen 277900 examples : 67.8 eps, Loss: 3.516, Avg loss: 3.410, Best loss: 3.405, cov loss: 0.171\n",
      "    [batch 2786]: seen 278600 examples : 67.8 eps, Loss: 3.476, Avg loss: 3.408, Best loss: 3.405, cov loss: 0.154\n",
      "    [batch 2793]: seen 279300 examples : 67.8 eps, Loss: 3.392, Avg loss: 3.411, Best loss: 3.405, cov loss: 0.158\n",
      "    [batch 2800]: seen 280000 examples : 67.8 eps, Loss: 3.415, Avg loss: 3.409, Best loss: 3.405, cov loss: 0.170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2807]: seen 280700 examples : 67.8 eps, Loss: 3.397, Avg loss: 3.413, Best loss: 3.405, cov loss: 0.167\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:08:59\n",
      "[EPOCH 20] Complete. Avg Loss: 3.4130309822976876; Best Loss: 3.4051710227025076\n",
      "[EPOCH 21] Starting training..\n",
      "    [batch 7]: seen 700 examples : 69.1 eps, Loss: 3.026, Avg loss: 3.408, Best loss: 3.405, cov loss: 0.173\n",
      "    [batch 14]: seen 1400 examples : 69.0 eps, Loss: 3.313, Avg loss: 3.408, Best loss: 3.405, cov loss: 0.170\n",
      "    [batch 21]: seen 2100 examples : 69.1 eps, Loss: 3.480, Avg loss: 3.409, Best loss: 3.405, cov loss: 0.165\n",
      "    [batch 28]: seen 2800 examples : 69.1 eps, Loss: 3.278, Avg loss: 3.407, Best loss: 3.405, cov loss: 0.176\n",
      "    [batch 33]: seen 3300 examples : 60.3 eps, Loss: 3.323, Avg loss: 3.404, Best loss: 3.404, cov loss: 0.180\n",
      "    [batch 36]: seen 3600 examples : 54.6 eps, Loss: 3.425, Avg loss: 3.403, Best loss: 3.402, cov loss: 0.170\n",
      "    [batch 39]: seen 3900 examples : 50.0 eps, Loss: 3.215, Avg loss: 3.400, Best loss: 3.400, cov loss: 0.162\n",
      "    [batch 42]: seen 4200 examples : 47.1 eps, Loss: 3.254, Avg loss: 3.399, Best loss: 3.399, cov loss: 0.167\n",
      "    [batch 45]: seen 4500 examples : 44.8 eps, Loss: 3.462, Avg loss: 3.398, Best loss: 3.398, cov loss: 0.167\n",
      "    [batch 49]: seen 4900 examples : 43.3 eps, Loss: 3.233, Avg loss: 3.396, Best loss: 3.396, cov loss: 0.172\n",
      "    [batch 54]: seen 5400 examples : 43.6 eps, Loss: 3.531, Avg loss: 3.395, Best loss: 3.394, cov loss: 0.183\n",
      "    [batch 60]: seen 6000 examples : 44.1 eps, Loss: 3.059, Avg loss: 3.392, Best loss: 3.392, cov loss: 0.163\n",
      "    [batch 67]: seen 6700 examples : 45.8 eps, Loss: 3.387, Avg loss: 3.394, Best loss: 3.392, cov loss: 0.177\n",
      "    [batch 73]: seen 7300 examples : 46.1 eps, Loss: 3.113, Avg loss: 3.392, Best loss: 3.392, cov loss: 0.158\n",
      "    [batch 80]: seen 8000 examples : 47.5 eps, Loss: 3.303, Avg loss: 3.392, Best loss: 3.392, cov loss: 0.173\n",
      "    [batch 85]: seen 8500 examples : 47.4 eps, Loss: 3.531, Avg loss: 3.392, Best loss: 3.390, cov loss: 0.159\n",
      "    [batch 88]: seen 8800 examples : 45.4 eps, Loss: 3.331, Avg loss: 3.389, Best loss: 3.389, cov loss: 0.189\n",
      "    [batch 92]: seen 9200 examples : 44.5 eps, Loss: 3.244, Avg loss: 3.387, Best loss: 3.387, cov loss: 0.162\n",
      "    [batch 97]: seen 9700 examples : 44.7 eps, Loss: 3.305, Avg loss: 3.387, Best loss: 3.386, cov loss: 0.156\n",
      "    [batch 104]: seen 10400 examples : 45.7 eps, Loss: 3.429, Avg loss: 3.391, Best loss: 3.386, cov loss: 0.176\n",
      "    [batch 111]: seen 11100 examples : 46.7 eps, Loss: 3.393, Avg loss: 3.394, Best loss: 3.386, cov loss: 0.168\n",
      "    [batch 118]: seen 11800 examples : 47.6 eps, Loss: 3.381, Avg loss: 3.389, Best loss: 3.386, cov loss: 0.171\n",
      "    [batch 125]: seen 12500 examples : 48.5 eps, Loss: 3.415, Avg loss: 3.388, Best loss: 3.386, cov loss: 0.186\n",
      "    [batch 130]: seen 13000 examples : 48.4 eps, Loss: 3.490, Avg loss: 3.386, Best loss: 3.385, cov loss: 0.193\n",
      "    [batch 136]: seen 13600 examples : 48.3 eps, Loss: 3.259, Avg loss: 3.385, Best loss: 3.385, cov loss: 0.171\n",
      "    [batch 141]: seen 14100 examples : 48.3 eps, Loss: 3.226, Avg loss: 3.384, Best loss: 3.384, cov loss: 0.167\n",
      "    [batch 146]: seen 14600 examples : 48.2 eps, Loss: 3.385, Avg loss: 3.384, Best loss: 3.383, cov loss: 0.160\n",
      "    [batch 153]: seen 15300 examples : 48.9 eps, Loss: 3.313, Avg loss: 3.385, Best loss: 3.383, cov loss: 0.163\n",
      "    [batch 160]: seen 16000 examples : 49.5 eps, Loss: 3.258, Avg loss: 3.387, Best loss: 3.383, cov loss: 0.151\n",
      "    [batch 167]: seen 16700 examples : 50.1 eps, Loss: 3.552, Avg loss: 3.387, Best loss: 3.383, cov loss: 0.174\n",
      "    [batch 174]: seen 17400 examples : 50.6 eps, Loss: 3.348, Avg loss: 3.385, Best loss: 3.383, cov loss: 0.174\n",
      "    [batch 179]: seen 17900 examples : 50.5 eps, Loss: 3.373, Avg loss: 3.385, Best loss: 3.382, cov loss: 0.180\n",
      "    [batch 186]: seen 18600 examples : 51.0 eps, Loss: 3.466, Avg loss: 3.388, Best loss: 3.382, cov loss: 0.160\n",
      "    [batch 193]: seen 19300 examples : 51.5 eps, Loss: 3.336, Avg loss: 3.390, Best loss: 3.382, cov loss: 0.172\n",
      "    [batch 200]: seen 20000 examples : 52.0 eps, Loss: 3.273, Avg loss: 3.385, Best loss: 3.382, cov loss: 0.156\n",
      "    [batch 207]: seen 20700 examples : 52.4 eps, Loss: 3.326, Avg loss: 3.383, Best loss: 3.382, cov loss: 0.166\n",
      "    [batch 214]: seen 21400 examples : 52.8 eps, Loss: 3.359, Avg loss: 3.386, Best loss: 3.382, cov loss: 0.163\n",
      "    [batch 221]: seen 22100 examples : 53.2 eps, Loss: 3.454, Avg loss: 3.390, Best loss: 3.382, cov loss: 0.181\n",
      "    [batch 228]: seen 22800 examples : 53.6 eps, Loss: 3.451, Avg loss: 3.387, Best loss: 3.382, cov loss: 0.168\n",
      "    [batch 235]: seen 23500 examples : 54.0 eps, Loss: 3.394, Avg loss: 3.386, Best loss: 3.382, cov loss: 0.171\n",
      "    [batch 242]: seen 24200 examples : 54.3 eps, Loss: 3.318, Avg loss: 3.384, Best loss: 3.382, cov loss: 0.179\n",
      "    [batch 249]: seen 24900 examples : 54.7 eps, Loss: 3.358, Avg loss: 3.384, Best loss: 3.382, cov loss: 0.151\n",
      "    [batch 253]: seen 25300 examples : 54.0 eps, Loss: 3.288, Avg loss: 3.381, Best loss: 3.381, cov loss: 0.161\n",
      "    [batch 257]: seen 25700 examples : 53.4 eps, Loss: 3.307, Avg loss: 3.380, Best loss: 3.380, cov loss: 0.171\n",
      "    [batch 262]: seen 26200 examples : 53.3 eps, Loss: 3.330, Avg loss: 3.379, Best loss: 3.377, cov loss: 0.153\n",
      "    [batch 269]: seen 26900 examples : 53.6 eps, Loss: 3.258, Avg loss: 3.378, Best loss: 3.377, cov loss: 0.155\n",
      "    [batch 274]: seen 27400 examples : 53.5 eps, Loss: 3.384, Avg loss: 3.376, Best loss: 3.376, cov loss: 0.175\n",
      "    [batch 280]: seen 28000 examples : 53.4 eps, Loss: 3.258, Avg loss: 3.376, Best loss: 3.376, cov loss: 0.167\n",
      "    [batch 283]: seen 28300 examples : 52.8 eps, Loss: 3.445, Avg loss: 3.374, Best loss: 3.374, cov loss: 0.176\n",
      "    [batch 286]: seen 28600 examples : 52.3 eps, Loss: 3.557, Avg loss: 3.374, Best loss: 3.372, cov loss: 0.170\n",
      "    [batch 292]: seen 29200 examples : 52.2 eps, Loss: 3.159, Avg loss: 3.372, Best loss: 3.372, cov loss: 0.163\n",
      "    [batch 295]: seen 29500 examples : 51.7 eps, Loss: 3.512, Avg loss: 3.372, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 302]: seen 30200 examples : 52.0 eps, Loss: 3.305, Avg loss: 3.374, Best loss: 3.371, cov loss: 0.156\n",
      "    [batch 309]: seen 30900 examples : 52.3 eps, Loss: 3.022, Avg loss: 3.373, Best loss: 3.371, cov loss: 0.149\n",
      "    [batch 316]: seen 31600 examples : 52.6 eps, Loss: 3.452, Avg loss: 3.375, Best loss: 3.371, cov loss: 0.192\n",
      "    [batch 323]: seen 32300 examples : 52.9 eps, Loss: 3.575, Avg loss: 3.380, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 330]: seen 33000 examples : 53.1 eps, Loss: 3.466, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.180\n",
      "    [batch 337]: seen 33700 examples : 53.4 eps, Loss: 3.157, Avg loss: 3.378, Best loss: 3.371, cov loss: 0.159\n",
      "    [batch 344]: seen 34400 examples : 53.6 eps, Loss: 3.308, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 351]: seen 35100 examples : 53.9 eps, Loss: 3.398, Avg loss: 3.380, Best loss: 3.371, cov loss: 0.165\n",
      "    [batch 358]: seen 35800 examples : 54.1 eps, Loss: 3.460, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.188\n",
      "    [batch 365]: seen 36500 examples : 54.3 eps, Loss: 3.324, Avg loss: 3.390, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 372]: seen 37200 examples : 54.5 eps, Loss: 3.610, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 379]: seen 37900 examples : 54.8 eps, Loss: 3.495, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 386]: seen 38600 examples : 55.0 eps, Loss: 3.473, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 393]: seen 39300 examples : 55.2 eps, Loss: 3.562, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.180\n",
      "    [batch 400]: seen 40000 examples : 55.4 eps, Loss: 3.348, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.143\n",
      "    [batch 407]: seen 40700 examples : 55.5 eps, Loss: 3.451, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 414]: seen 41400 examples : 55.7 eps, Loss: 3.409, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.172\n",
      "    [batch 421]: seen 42100 examples : 55.9 eps, Loss: 3.332, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 428]: seen 42800 examples : 56.1 eps, Loss: 3.501, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 435]: seen 43500 examples : 56.2 eps, Loss: 3.384, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 442]: seen 44200 examples : 56.4 eps, Loss: 3.384, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 449]: seen 44900 examples : 56.6 eps, Loss: 3.444, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.185\n",
      "    [batch 456]: seen 45600 examples : 56.7 eps, Loss: 3.251, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.147\n",
      "    [batch 463]: seen 46300 examples : 56.9 eps, Loss: 3.171, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.159\n",
      "    [batch 470]: seen 47000 examples : 57.0 eps, Loss: 3.359, Avg loss: 3.391, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 477]: seen 47700 examples : 57.2 eps, Loss: 3.561, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.172\n",
      "    [batch 484]: seen 48400 examples : 57.3 eps, Loss: 3.449, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.157\n",
      "    [batch 491]: seen 49100 examples : 57.5 eps, Loss: 3.652, Avg loss: 3.390, Best loss: 3.371, cov loss: 0.198\n",
      "    [batch 498]: seen 49800 examples : 57.6 eps, Loss: 3.434, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 505]: seen 50500 examples : 57.7 eps, Loss: 3.336, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 512]: seen 51200 examples : 57.9 eps, Loss: 3.412, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.178\n",
      "    [batch 519]: seen 51900 examples : 58.0 eps, Loss: 3.420, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.172\n",
      "    [batch 526]: seen 52600 examples : 58.1 eps, Loss: 3.443, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 533]: seen 53300 examples : 58.2 eps, Loss: 3.336, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 540]: seen 54000 examples : 58.3 eps, Loss: 3.351, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.179\n",
      "    [batch 547]: seen 54700 examples : 58.5 eps, Loss: 3.230, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 554]: seen 55400 examples : 58.6 eps, Loss: 3.345, Avg loss: 3.384, Best loss: 3.371, cov loss: 0.179\n",
      "    [batch 561]: seen 56100 examples : 58.7 eps, Loss: 3.434, Avg loss: 3.383, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 568]: seen 56800 examples : 58.8 eps, Loss: 3.430, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.190\n",
      "    [batch 575]: seen 57500 examples : 58.9 eps, Loss: 3.403, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 582]: seen 58200 examples : 59.0 eps, Loss: 3.452, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 589]: seen 58900 examples : 59.1 eps, Loss: 3.483, Avg loss: 3.400, Best loss: 3.371, cov loss: 0.193\n",
      "    [batch 596]: seen 59600 examples : 59.2 eps, Loss: 3.281, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 603]: seen 60300 examples : 59.3 eps, Loss: 3.523, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.178\n",
      "    [batch 610]: seen 61000 examples : 59.4 eps, Loss: 3.460, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.178\n",
      "    [batch 617]: seen 61700 examples : 59.5 eps, Loss: 3.271, Avg loss: 3.403, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 624]: seen 62400 examples : 59.6 eps, Loss: 3.466, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.185\n",
      "    [batch 631]: seen 63100 examples : 59.7 eps, Loss: 3.323, Avg loss: 3.402, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 638]: seen 63800 examples : 59.8 eps, Loss: 3.445, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 645]: seen 64500 examples : 59.9 eps, Loss: 3.356, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.158\n",
      "    [batch 652]: seen 65200 examples : 59.9 eps, Loss: 3.375, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.178\n",
      "    [batch 659]: seen 65900 examples : 60.0 eps, Loss: 3.337, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.145\n",
      "    [batch 666]: seen 66600 examples : 60.1 eps, Loss: 3.507, Avg loss: 3.400, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 673]: seen 67300 examples : 60.2 eps, Loss: 3.414, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.159\n",
      "    [batch 680]: seen 68000 examples : 60.3 eps, Loss: 3.365, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.187\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-36106\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-36106\n",
      "    [batch 686]: seen 68600 examples : 60.3 eps, Loss: 3.242, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.158\n",
      "    [batch 693]: seen 69300 examples : 60.3 eps, Loss: 3.251, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.159\n",
      "    [batch 700]: seen 70000 examples : 60.4 eps, Loss: 3.641, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.188\n",
      "    [batch 707]: seen 70700 examples : 60.5 eps, Loss: 3.568, Avg loss: 3.403, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 714]: seen 71400 examples : 60.6 eps, Loss: 3.469, Avg loss: 3.407, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 721]: seen 72100 examples : 60.6 eps, Loss: 3.469, Avg loss: 3.402, Best loss: 3.371, cov loss: 0.197\n",
      "    [batch 728]: seen 72800 examples : 60.7 eps, Loss: 3.381, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 735]: seen 73500 examples : 60.8 eps, Loss: 3.328, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.160\n",
      "    [batch 742]: seen 74200 examples : 60.8 eps, Loss: 3.408, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 749]: seen 74900 examples : 60.9 eps, Loss: 3.512, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.187\n",
      "    [batch 756]: seen 75600 examples : 61.0 eps, Loss: 3.474, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.179\n",
      "    [batch 763]: seen 76300 examples : 61.0 eps, Loss: 3.386, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 770]: seen 77000 examples : 61.1 eps, Loss: 3.228, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.161\n",
      "    [batch 777]: seen 77700 examples : 61.2 eps, Loss: 3.415, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.194\n",
      "    [batch 784]: seen 78400 examples : 61.2 eps, Loss: 3.572, Avg loss: 3.391, Best loss: 3.371, cov loss: 0.188\n",
      "    [batch 791]: seen 79100 examples : 61.3 eps, Loss: 3.367, Avg loss: 3.391, Best loss: 3.371, cov loss: 0.158\n",
      "    [batch 798]: seen 79800 examples : 61.4 eps, Loss: 3.644, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 805]: seen 80500 examples : 61.4 eps, Loss: 3.495, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.182\n",
      "    [batch 812]: seen 81200 examples : 61.5 eps, Loss: 3.449, Avg loss: 3.402, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 819]: seen 81900 examples : 61.5 eps, Loss: 3.436, Avg loss: 3.400, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 826]: seen 82600 examples : 61.6 eps, Loss: 3.413, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.160\n",
      "    [batch 833]: seen 83300 examples : 61.6 eps, Loss: 3.572, Avg loss: 3.400, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 840]: seen 84000 examples : 61.7 eps, Loss: 3.431, Avg loss: 3.406, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 847]: seen 84700 examples : 61.8 eps, Loss: 3.380, Avg loss: 3.404, Best loss: 3.371, cov loss: 0.187\n",
      "    [batch 854]: seen 85400 examples : 61.8 eps, Loss: 3.184, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.161\n",
      "    [batch 861]: seen 86100 examples : 61.9 eps, Loss: 3.339, Avg loss: 3.400, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 868]: seen 86800 examples : 61.9 eps, Loss: 3.315, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 875]: seen 87500 examples : 62.0 eps, Loss: 3.521, Avg loss: 3.399, Best loss: 3.371, cov loss: 0.187\n",
      "    [batch 882]: seen 88200 examples : 62.0 eps, Loss: 3.398, Avg loss: 3.399, Best loss: 3.371, cov loss: 0.184\n",
      "    [batch 889]: seen 88900 examples : 62.1 eps, Loss: 3.247, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.162\n",
      "    [batch 896]: seen 89600 examples : 62.1 eps, Loss: 3.404, Avg loss: 3.402, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 903]: seen 90300 examples : 62.2 eps, Loss: 3.374, Avg loss: 3.403, Best loss: 3.371, cov loss: 0.163\n",
      "    [batch 910]: seen 91000 examples : 62.2 eps, Loss: 3.274, Avg loss: 3.399, Best loss: 3.371, cov loss: 0.172\n",
      "    [batch 917]: seen 91700 examples : 62.3 eps, Loss: 3.501, Avg loss: 3.400, Best loss: 3.371, cov loss: 0.179\n",
      "    [batch 924]: seen 92400 examples : 62.3 eps, Loss: 3.478, Avg loss: 3.400, Best loss: 3.371, cov loss: 0.172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 931]: seen 93100 examples : 62.3 eps, Loss: 3.272, Avg loss: 3.403, Best loss: 3.371, cov loss: 0.149\n",
      "    [batch 938]: seen 93800 examples : 62.4 eps, Loss: 3.404, Avg loss: 3.404, Best loss: 3.371, cov loss: 0.186\n",
      "    [batch 945]: seen 94500 examples : 62.4 eps, Loss: 3.404, Avg loss: 3.407, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 952]: seen 95200 examples : 62.5 eps, Loss: 3.426, Avg loss: 3.403, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 959]: seen 95900 examples : 62.5 eps, Loss: 3.569, Avg loss: 3.406, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 966]: seen 96600 examples : 62.6 eps, Loss: 3.437, Avg loss: 3.408, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 973]: seen 97300 examples : 62.6 eps, Loss: 3.209, Avg loss: 3.405, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 980]: seen 98000 examples : 62.6 eps, Loss: 3.366, Avg loss: 3.402, Best loss: 3.371, cov loss: 0.172\n",
      "    [batch 987]: seen 98700 examples : 62.7 eps, Loss: 3.460, Avg loss: 3.403, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 994]: seen 99400 examples : 62.7 eps, Loss: 3.398, Avg loss: 3.404, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 1001]: seen 100100 examples : 62.8 eps, Loss: 3.446, Avg loss: 3.400, Best loss: 3.371, cov loss: 0.163\n",
      "    [batch 1008]: seen 100800 examples : 62.8 eps, Loss: 3.393, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.150\n",
      "    [batch 1015]: seen 101500 examples : 62.8 eps, Loss: 3.294, Avg loss: 3.402, Best loss: 3.371, cov loss: 0.150\n",
      "    [batch 1022]: seen 102200 examples : 62.9 eps, Loss: 3.658, Avg loss: 3.407, Best loss: 3.371, cov loss: 0.184\n",
      "    [batch 1029]: seen 102900 examples : 62.9 eps, Loss: 3.420, Avg loss: 3.405, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 1036]: seen 103600 examples : 63.0 eps, Loss: 3.309, Avg loss: 3.407, Best loss: 3.371, cov loss: 0.148\n",
      "    [batch 1043]: seen 104300 examples : 63.0 eps, Loss: 3.420, Avg loss: 3.409, Best loss: 3.371, cov loss: 0.176\n",
      "    [batch 1050]: seen 105000 examples : 63.0 eps, Loss: 3.435, Avg loss: 3.407, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 1057]: seen 105700 examples : 63.1 eps, Loss: 3.275, Avg loss: 3.410, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 1064]: seen 106400 examples : 63.1 eps, Loss: 3.347, Avg loss: 3.409, Best loss: 3.371, cov loss: 0.160\n",
      "    [batch 1071]: seen 107100 examples : 63.1 eps, Loss: 3.443, Avg loss: 3.412, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 1078]: seen 107800 examples : 63.2 eps, Loss: 3.381, Avg loss: 3.414, Best loss: 3.371, cov loss: 0.179\n",
      "    [batch 1085]: seen 108500 examples : 63.2 eps, Loss: 3.592, Avg loss: 3.411, Best loss: 3.371, cov loss: 0.193\n",
      "    [batch 1092]: seen 109200 examples : 63.2 eps, Loss: 3.497, Avg loss: 3.410, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 1099]: seen 109900 examples : 63.3 eps, Loss: 3.383, Avg loss: 3.409, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 1106]: seen 110600 examples : 63.3 eps, Loss: 3.723, Avg loss: 3.409, Best loss: 3.371, cov loss: 0.193\n",
      "    [batch 1113]: seen 111300 examples : 63.3 eps, Loss: 3.477, Avg loss: 3.412, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 1120]: seen 112000 examples : 63.4 eps, Loss: 3.524, Avg loss: 3.408, Best loss: 3.371, cov loss: 0.182\n",
      "    [batch 1127]: seen 112700 examples : 63.4 eps, Loss: 3.495, Avg loss: 3.407, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 1134]: seen 113400 examples : 63.4 eps, Loss: 3.572, Avg loss: 3.410, Best loss: 3.371, cov loss: 0.185\n",
      "    [batch 1141]: seen 114100 examples : 63.5 eps, Loss: 3.559, Avg loss: 3.412, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 1148]: seen 114800 examples : 63.5 eps, Loss: 3.433, Avg loss: 3.416, Best loss: 3.371, cov loss: 0.160\n",
      "    [batch 1155]: seen 115500 examples : 63.5 eps, Loss: 3.279, Avg loss: 3.410, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 1162]: seen 116200 examples : 63.6 eps, Loss: 3.277, Avg loss: 3.409, Best loss: 3.371, cov loss: 0.142\n",
      "    [batch 1169]: seen 116900 examples : 63.6 eps, Loss: 3.337, Avg loss: 3.405, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 1176]: seen 117600 examples : 63.6 eps, Loss: 3.503, Avg loss: 3.403, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 1183]: seen 118300 examples : 63.7 eps, Loss: 3.305, Avg loss: 3.402, Best loss: 3.371, cov loss: 0.160\n",
      "    [batch 1190]: seen 119000 examples : 63.7 eps, Loss: 3.526, Avg loss: 3.407, Best loss: 3.371, cov loss: 0.172\n",
      "    [batch 1197]: seen 119700 examples : 63.7 eps, Loss: 3.492, Avg loss: 3.408, Best loss: 3.371, cov loss: 0.178\n",
      "    [batch 1204]: seen 120400 examples : 63.7 eps, Loss: 3.578, Avg loss: 3.412, Best loss: 3.371, cov loss: 0.187\n",
      "    [batch 1211]: seen 121100 examples : 63.8 eps, Loss: 3.377, Avg loss: 3.414, Best loss: 3.371, cov loss: 0.190\n",
      "    [batch 1218]: seen 121800 examples : 63.8 eps, Loss: 3.437, Avg loss: 3.414, Best loss: 3.371, cov loss: 0.159\n",
      "    [batch 1225]: seen 122500 examples : 63.8 eps, Loss: 3.312, Avg loss: 3.410, Best loss: 3.371, cov loss: 0.160\n",
      "    [batch 1232]: seen 123200 examples : 63.9 eps, Loss: 3.264, Avg loss: 3.404, Best loss: 3.371, cov loss: 0.183\n",
      "    [batch 1239]: seen 123900 examples : 63.9 eps, Loss: 3.453, Avg loss: 3.410, Best loss: 3.371, cov loss: 0.179\n",
      "    [batch 1246]: seen 124600 examples : 63.9 eps, Loss: 3.571, Avg loss: 3.412, Best loss: 3.371, cov loss: 0.182\n",
      "    [batch 1253]: seen 125300 examples : 63.9 eps, Loss: 3.387, Avg loss: 3.412, Best loss: 3.371, cov loss: 0.159\n",
      "    [batch 1260]: seen 126000 examples : 64.0 eps, Loss: 3.442, Avg loss: 3.409, Best loss: 3.371, cov loss: 0.200\n",
      "    [batch 1267]: seen 126700 examples : 64.0 eps, Loss: 3.589, Avg loss: 3.410, Best loss: 3.371, cov loss: 0.154\n",
      "    [batch 1274]: seen 127400 examples : 64.0 eps, Loss: 3.374, Avg loss: 3.411, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 1281]: seen 128100 examples : 64.0 eps, Loss: 3.402, Avg loss: 3.410, Best loss: 3.371, cov loss: 0.155\n",
      "    [batch 1288]: seen 128800 examples : 64.1 eps, Loss: 3.234, Avg loss: 3.403, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 1295]: seen 129500 examples : 64.1 eps, Loss: 3.420, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.156\n",
      "    [batch 1302]: seen 130200 examples : 64.1 eps, Loss: 3.436, Avg loss: 3.400, Best loss: 3.371, cov loss: 0.179\n",
      "    [batch 1309]: seen 130900 examples : 64.1 eps, Loss: 3.494, Avg loss: 3.402, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 1316]: seen 131600 examples : 64.2 eps, Loss: 3.345, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.179\n",
      "    [batch 1323]: seen 132300 examples : 64.2 eps, Loss: 3.260, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.190\n",
      "    [batch 1330]: seen 133000 examples : 64.2 eps, Loss: 3.377, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.176\n",
      "    [batch 1337]: seen 133700 examples : 64.2 eps, Loss: 3.596, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.181\n",
      "    [batch 1344]: seen 134400 examples : 64.3 eps, Loss: 3.603, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.180\n",
      "    [batch 1351]: seen 135100 examples : 64.3 eps, Loss: 3.471, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 1358]: seen 135800 examples : 64.3 eps, Loss: 3.515, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 1365]: seen 136500 examples : 64.3 eps, Loss: 3.434, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.183\n",
      "    [batch 1372]: seen 137200 examples : 64.3 eps, Loss: 3.416, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 1379]: seen 137900 examples : 64.4 eps, Loss: 3.276, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 1386]: seen 138600 examples : 64.4 eps, Loss: 3.321, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.163\n",
      "    [batch 1393]: seen 139300 examples : 64.4 eps, Loss: 3.433, Avg loss: 3.390, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 1400]: seen 140000 examples : 64.4 eps, Loss: 3.299, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 1407]: seen 140700 examples : 64.5 eps, Loss: 3.317, Avg loss: 3.386, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 1414]: seen 141400 examples : 64.5 eps, Loss: 3.355, Avg loss: 3.386, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 1421]: seen 142100 examples : 64.5 eps, Loss: 3.366, Avg loss: 3.384, Best loss: 3.371, cov loss: 0.179\n",
      "    [batch 1428]: seen 142800 examples : 64.5 eps, Loss: 3.198, Avg loss: 3.378, Best loss: 3.371, cov loss: 0.170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1435]: seen 143500 examples : 64.5 eps, Loss: 3.362, Avg loss: 3.380, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 1442]: seen 144200 examples : 64.6 eps, Loss: 3.449, Avg loss: 3.379, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 1449]: seen 144900 examples : 64.6 eps, Loss: 3.542, Avg loss: 3.376, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 1456]: seen 145600 examples : 64.6 eps, Loss: 3.349, Avg loss: 3.379, Best loss: 3.371, cov loss: 0.163\n",
      "    [batch 1463]: seen 146300 examples : 64.6 eps, Loss: 3.407, Avg loss: 3.378, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 1470]: seen 147000 examples : 64.6 eps, Loss: 3.392, Avg loss: 3.381, Best loss: 3.371, cov loss: 0.155\n",
      "    [batch 1477]: seen 147700 examples : 64.7 eps, Loss: 3.400, Avg loss: 3.380, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 1484]: seen 148400 examples : 64.7 eps, Loss: 3.127, Avg loss: 3.378, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 1491]: seen 149100 examples : 64.7 eps, Loss: 3.335, Avg loss: 3.376, Best loss: 3.371, cov loss: 0.158\n",
      "    [batch 1498]: seen 149800 examples : 64.7 eps, Loss: 3.396, Avg loss: 3.376, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 1505]: seen 150500 examples : 64.7 eps, Loss: 3.318, Avg loss: 3.373, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 1512]: seen 151200 examples : 64.7 eps, Loss: 3.216, Avg loss: 3.373, Best loss: 3.371, cov loss: 0.161\n",
      "    [batch 1519]: seen 151900 examples : 64.8 eps, Loss: 3.419, Avg loss: 3.375, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 1526]: seen 152600 examples : 64.8 eps, Loss: 3.677, Avg loss: 3.374, Best loss: 3.371, cov loss: 0.178\n",
      "    [batch 1533]: seen 153300 examples : 64.8 eps, Loss: 3.447, Avg loss: 3.377, Best loss: 3.371, cov loss: 0.160\n",
      "    [batch 1540]: seen 154000 examples : 64.8 eps, Loss: 3.449, Avg loss: 3.376, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 1547]: seen 154700 examples : 64.8 eps, Loss: 3.081, Avg loss: 3.374, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 1554]: seen 155400 examples : 64.9 eps, Loss: 3.442, Avg loss: 3.380, Best loss: 3.371, cov loss: 0.205\n",
      "    [batch 1561]: seen 156100 examples : 64.9 eps, Loss: 3.486, Avg loss: 3.380, Best loss: 3.371, cov loss: 0.172\n",
      "    [batch 1568]: seen 156800 examples : 64.9 eps, Loss: 3.383, Avg loss: 3.380, Best loss: 3.371, cov loss: 0.165\n",
      "    [batch 1575]: seen 157500 examples : 64.9 eps, Loss: 3.347, Avg loss: 3.378, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 1582]: seen 158200 examples : 64.9 eps, Loss: 3.489, Avg loss: 3.376, Best loss: 3.371, cov loss: 0.181\n",
      "    [batch 1589]: seen 158900 examples : 64.9 eps, Loss: 3.659, Avg loss: 3.380, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 1596]: seen 159600 examples : 65.0 eps, Loss: 3.463, Avg loss: 3.380, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 1603]: seen 160300 examples : 65.0 eps, Loss: 3.639, Avg loss: 3.386, Best loss: 3.371, cov loss: 0.202\n",
      "    [batch 1610]: seen 161000 examples : 65.0 eps, Loss: 3.327, Avg loss: 3.387, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 1617]: seen 161700 examples : 65.0 eps, Loss: 3.404, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.163\n",
      "    [batch 1624]: seen 162400 examples : 65.0 eps, Loss: 3.228, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.176\n",
      "    [batch 1631]: seen 163100 examples : 65.0 eps, Loss: 3.632, Avg loss: 3.399, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 1638]: seen 163800 examples : 65.1 eps, Loss: 3.501, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 1645]: seen 164500 examples : 65.1 eps, Loss: 3.327, Avg loss: 3.400, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 1652]: seen 165200 examples : 65.1 eps, Loss: 3.386, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.163\n",
      "    [batch 1659]: seen 165900 examples : 65.1 eps, Loss: 3.549, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.165\n",
      "    [batch 1666]: seen 166600 examples : 65.1 eps, Loss: 3.476, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.184\n",
      "    [batch 1673]: seen 167300 examples : 65.1 eps, Loss: 3.346, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 1680]: seen 168000 examples : 65.2 eps, Loss: 3.361, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 1687]: seen 168700 examples : 65.2 eps, Loss: 3.501, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 1694]: seen 169400 examples : 65.2 eps, Loss: 3.263, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.148\n",
      "    [batch 1701]: seen 170100 examples : 65.2 eps, Loss: 3.342, Avg loss: 3.399, Best loss: 3.371, cov loss: 0.172\n",
      "    [batch 1708]: seen 170800 examples : 65.2 eps, Loss: 3.358, Avg loss: 3.400, Best loss: 3.371, cov loss: 0.161\n",
      "    [batch 1715]: seen 171500 examples : 65.2 eps, Loss: 3.393, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 1722]: seen 172200 examples : 65.2 eps, Loss: 3.344, Avg loss: 3.399, Best loss: 3.371, cov loss: 0.181\n",
      "    [batch 1729]: seen 172900 examples : 65.3 eps, Loss: 3.385, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.181\n",
      "    [batch 1736]: seen 173600 examples : 65.3 eps, Loss: 3.472, Avg loss: 3.400, Best loss: 3.371, cov loss: 0.187\n",
      "    [batch 1743]: seen 174300 examples : 65.3 eps, Loss: 3.545, Avg loss: 3.402, Best loss: 3.371, cov loss: 0.197\n",
      "    [batch 1750]: seen 175000 examples : 65.3 eps, Loss: 3.254, Avg loss: 3.399, Best loss: 3.371, cov loss: 0.153\n",
      "    [batch 1757]: seen 175700 examples : 65.3 eps, Loss: 3.139, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.160\n",
      "    [batch 1764]: seen 176400 examples : 65.3 eps, Loss: 3.461, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.176\n",
      "    [batch 1771]: seen 177100 examples : 65.3 eps, Loss: 3.219, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.163\n",
      "    [batch 1778]: seen 177800 examples : 65.4 eps, Loss: 3.653, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.191\n",
      "    [batch 1785]: seen 178500 examples : 65.4 eps, Loss: 3.359, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.165\n",
      "    [batch 1792]: seen 179200 examples : 65.4 eps, Loss: 3.410, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 1799]: seen 179900 examples : 65.4 eps, Loss: 3.516, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 1806]: seen 180600 examples : 65.4 eps, Loss: 3.521, Avg loss: 3.399, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 1813]: seen 181300 examples : 65.4 eps, Loss: 3.388, Avg loss: 3.399, Best loss: 3.371, cov loss: 0.172\n",
      "    [batch 1820]: seen 182000 examples : 65.4 eps, Loss: 3.265, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 1827]: seen 182700 examples : 65.4 eps, Loss: 3.409, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.154\n",
      "    [batch 1834]: seen 183400 examples : 65.5 eps, Loss: 3.433, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.182\n",
      "    [batch 1841]: seen 184100 examples : 65.5 eps, Loss: 3.322, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.190\n",
      "    [batch 1848]: seen 184800 examples : 65.5 eps, Loss: 3.395, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.165\n",
      "    [batch 1855]: seen 185500 examples : 65.5 eps, Loss: 3.453, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 1862]: seen 186200 examples : 65.5 eps, Loss: 3.388, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.172\n",
      "    [batch 1869]: seen 186900 examples : 65.5 eps, Loss: 3.234, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 1876]: seen 187600 examples : 65.5 eps, Loss: 3.394, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.183\n",
      "    [batch 1883]: seen 188300 examples : 65.5 eps, Loss: 3.554, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.154\n",
      "    [batch 1890]: seen 189000 examples : 65.6 eps, Loss: 3.264, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 1897]: seen 189700 examples : 65.6 eps, Loss: 3.344, Avg loss: 3.391, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 1904]: seen 190400 examples : 65.6 eps, Loss: 3.282, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 1911]: seen 191100 examples : 65.6 eps, Loss: 3.364, Avg loss: 3.391, Best loss: 3.371, cov loss: 0.179\n",
      "    [batch 1918]: seen 191800 examples : 65.6 eps, Loss: 3.540, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.178\n",
      "    [batch 1925]: seen 192500 examples : 65.6 eps, Loss: 3.393, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.163\n",
      "    [batch 1932]: seen 193200 examples : 65.6 eps, Loss: 3.443, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1939]: seen 193900 examples : 65.6 eps, Loss: 3.349, Avg loss: 3.399, Best loss: 3.371, cov loss: 0.198\n",
      "    [batch 1946]: seen 194600 examples : 65.7 eps, Loss: 3.342, Avg loss: 3.399, Best loss: 3.371, cov loss: 0.158\n",
      "    [batch 1953]: seen 195300 examples : 65.7 eps, Loss: 3.573, Avg loss: 3.399, Best loss: 3.371, cov loss: 0.190\n",
      "    [batch 1960]: seen 196000 examples : 65.7 eps, Loss: 3.421, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 1967]: seen 196700 examples : 65.7 eps, Loss: 3.435, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 1974]: seen 197400 examples : 65.7 eps, Loss: 3.372, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 1981]: seen 198100 examples : 65.7 eps, Loss: 3.367, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 1988]: seen 198800 examples : 65.7 eps, Loss: 3.275, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.157\n",
      "    [batch 1995]: seen 199500 examples : 65.7 eps, Loss: 3.358, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.154\n",
      "    [batch 2002]: seen 200200 examples : 65.7 eps, Loss: 3.462, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.189\n",
      "    [batch 2009]: seen 200900 examples : 65.8 eps, Loss: 3.319, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.172\n",
      "    [batch 2016]: seen 201600 examples : 65.8 eps, Loss: 3.318, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.165\n",
      "    [batch 2023]: seen 202300 examples : 65.8 eps, Loss: 3.118, Avg loss: 3.390, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 2030]: seen 203000 examples : 65.8 eps, Loss: 3.303, Avg loss: 3.386, Best loss: 3.371, cov loss: 0.161\n",
      "    [batch 2037]: seen 203700 examples : 65.8 eps, Loss: 3.321, Avg loss: 3.382, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 2044]: seen 204400 examples : 65.8 eps, Loss: 3.346, Avg loss: 3.382, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 2051]: seen 205100 examples : 65.8 eps, Loss: 3.451, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 2058]: seen 205800 examples : 65.8 eps, Loss: 3.524, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.187\n",
      "    [batch 2065]: seen 206500 examples : 65.8 eps, Loss: 3.456, Avg loss: 3.387, Best loss: 3.371, cov loss: 0.192\n",
      "    [batch 2072]: seen 207200 examples : 65.8 eps, Loss: 3.277, Avg loss: 3.390, Best loss: 3.371, cov loss: 0.161\n",
      "    [batch 2079]: seen 207900 examples : 65.9 eps, Loss: 3.372, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.181\n",
      "    [batch 2086]: seen 208600 examples : 65.9 eps, Loss: 3.632, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 2093]: seen 209300 examples : 65.9 eps, Loss: 3.318, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.163\n",
      "    [batch 2100]: seen 210000 examples : 65.9 eps, Loss: 3.391, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 2107]: seen 210700 examples : 65.9 eps, Loss: 3.172, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 2114]: seen 211400 examples : 65.9 eps, Loss: 3.378, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.143\n",
      "    [batch 2121]: seen 212100 examples : 65.9 eps, Loss: 3.027, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 2128]: seen 212800 examples : 65.9 eps, Loss: 3.348, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 2135]: seen 213500 examples : 65.9 eps, Loss: 3.293, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 2142]: seen 214200 examples : 66.0 eps, Loss: 3.584, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.157\n",
      "    [batch 2149]: seen 214900 examples : 66.0 eps, Loss: 3.253, Avg loss: 3.390, Best loss: 3.371, cov loss: 0.162\n",
      "    [batch 2156]: seen 215600 examples : 66.0 eps, Loss: 3.357, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.180\n",
      "    [batch 2163]: seen 216300 examples : 66.0 eps, Loss: 3.527, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 2170]: seen 217000 examples : 66.0 eps, Loss: 3.212, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.156\n",
      "    [batch 2177]: seen 217700 examples : 66.0 eps, Loss: 3.447, Avg loss: 3.390, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 2184]: seen 218400 examples : 66.0 eps, Loss: 3.252, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 2191]: seen 219100 examples : 66.0 eps, Loss: 3.492, Avg loss: 3.391, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 2198]: seen 219800 examples : 66.0 eps, Loss: 3.348, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.182\n",
      "    [batch 2205]: seen 220500 examples : 66.0 eps, Loss: 3.419, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 2212]: seen 221200 examples : 66.0 eps, Loss: 3.452, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.181\n",
      "    [batch 2219]: seen 221900 examples : 66.1 eps, Loss: 3.482, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.189\n",
      "    [batch 2226]: seen 222600 examples : 66.1 eps, Loss: 3.339, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 2233]: seen 223300 examples : 66.1 eps, Loss: 3.419, Avg loss: 3.401, Best loss: 3.371, cov loss: 0.156\n",
      "    [batch 2240]: seen 224000 examples : 66.1 eps, Loss: 3.399, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 2247]: seen 224700 examples : 66.1 eps, Loss: 3.394, Avg loss: 3.398, Best loss: 3.371, cov loss: 0.176\n",
      "    [batch 2254]: seen 225400 examples : 66.1 eps, Loss: 3.424, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 2261]: seen 226100 examples : 66.1 eps, Loss: 3.431, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.171\n",
      "    [batch 2268]: seen 226800 examples : 66.1 eps, Loss: 3.332, Avg loss: 3.384, Best loss: 3.371, cov loss: 0.178\n",
      "    [batch 2275]: seen 227500 examples : 66.1 eps, Loss: 3.473, Avg loss: 3.383, Best loss: 3.371, cov loss: 0.195\n",
      "    [batch 2282]: seen 228200 examples : 66.1 eps, Loss: 3.505, Avg loss: 3.381, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 2289]: seen 228900 examples : 66.1 eps, Loss: 3.388, Avg loss: 3.381, Best loss: 3.371, cov loss: 0.155\n",
      "    [batch 2296]: seen 229600 examples : 66.1 eps, Loss: 3.559, Avg loss: 3.386, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 2303]: seen 230300 examples : 66.2 eps, Loss: 3.589, Avg loss: 3.387, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 2310]: seen 231000 examples : 66.2 eps, Loss: 3.170, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 2317]: seen 231700 examples : 66.2 eps, Loss: 3.225, Avg loss: 3.387, Best loss: 3.371, cov loss: 0.165\n",
      "    [batch 2324]: seen 232400 examples : 66.2 eps, Loss: 3.228, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.154\n",
      "    [batch 2331]: seen 233100 examples : 66.2 eps, Loss: 3.333, Avg loss: 3.386, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 2338]: seen 233800 examples : 66.2 eps, Loss: 3.665, Avg loss: 3.386, Best loss: 3.371, cov loss: 0.187\n",
      "    [batch 2345]: seen 234500 examples : 66.2 eps, Loss: 3.365, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.178\n",
      "    [batch 2352]: seen 235200 examples : 66.2 eps, Loss: 3.115, Avg loss: 3.380, Best loss: 3.371, cov loss: 0.162\n",
      "    [batch 2359]: seen 235900 examples : 66.2 eps, Loss: 3.501, Avg loss: 3.383, Best loss: 3.371, cov loss: 0.194\n",
      "    [batch 2366]: seen 236600 examples : 66.2 eps, Loss: 3.480, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.178\n",
      "    [batch 2373]: seen 237300 examples : 66.2 eps, Loss: 3.497, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 2380]: seen 238000 examples : 66.2 eps, Loss: 3.353, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 2387]: seen 238700 examples : 66.3 eps, Loss: 3.289, Avg loss: 3.383, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 2394]: seen 239400 examples : 66.3 eps, Loss: 3.432, Avg loss: 3.383, Best loss: 3.371, cov loss: 0.186\n",
      "    [batch 2401]: seen 240100 examples : 66.3 eps, Loss: 3.407, Avg loss: 3.386, Best loss: 3.371, cov loss: 0.180\n",
      "    [batch 2408]: seen 240800 examples : 66.3 eps, Loss: 3.476, Avg loss: 3.386, Best loss: 3.371, cov loss: 0.160\n",
      "    [batch 2415]: seen 241500 examples : 66.3 eps, Loss: 3.405, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.179\n",
      "    [batch 2422]: seen 242200 examples : 66.3 eps, Loss: 3.466, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.160\n",
      "    [batch 2429]: seen 242900 examples : 66.3 eps, Loss: 3.444, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.162\n",
      "    [batch 2436]: seen 243600 examples : 66.3 eps, Loss: 3.200, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2443]: seen 244300 examples : 66.3 eps, Loss: 3.213, Avg loss: 3.391, Best loss: 3.371, cov loss: 0.153\n",
      "    [batch 2450]: seen 245000 examples : 66.3 eps, Loss: 3.587, Avg loss: 3.391, Best loss: 3.371, cov loss: 0.167\n",
      "    [batch 2457]: seen 245700 examples : 66.3 eps, Loss: 3.324, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 2464]: seen 246400 examples : 66.3 eps, Loss: 3.353, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.180\n",
      "    [batch 2471]: seen 247100 examples : 66.3 eps, Loss: 3.422, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 2478]: seen 247800 examples : 66.3 eps, Loss: 3.163, Avg loss: 3.391, Best loss: 3.371, cov loss: 0.158\n",
      "    [batch 2485]: seen 248500 examples : 66.4 eps, Loss: 3.283, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.165\n",
      "    [batch 2492]: seen 249200 examples : 66.4 eps, Loss: 3.516, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.156\n",
      "    [batch 2499]: seen 249900 examples : 66.4 eps, Loss: 3.378, Avg loss: 3.387, Best loss: 3.371, cov loss: 0.160\n",
      "    [batch 2506]: seen 250600 examples : 66.4 eps, Loss: 3.452, Avg loss: 3.390, Best loss: 3.371, cov loss: 0.158\n",
      "    [batch 2513]: seen 251300 examples : 66.4 eps, Loss: 3.362, Avg loss: 3.386, Best loss: 3.371, cov loss: 0.189\n",
      "    [batch 2520]: seen 252000 examples : 66.4 eps, Loss: 3.187, Avg loss: 3.384, Best loss: 3.371, cov loss: 0.158\n",
      "    [batch 2527]: seen 252700 examples : 66.4 eps, Loss: 3.375, Avg loss: 3.387, Best loss: 3.371, cov loss: 0.183\n",
      "    [batch 2534]: seen 253400 examples : 66.4 eps, Loss: 3.369, Avg loss: 3.387, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 2541]: seen 254100 examples : 66.4 eps, Loss: 3.440, Avg loss: 3.387, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 2548]: seen 254800 examples : 66.4 eps, Loss: 3.697, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.206\n",
      "    [batch 2555]: seen 255500 examples : 66.4 eps, Loss: 3.532, Avg loss: 3.386, Best loss: 3.371, cov loss: 0.154\n",
      "    [batch 2562]: seen 256200 examples : 66.4 eps, Loss: 3.318, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 2569]: seen 256900 examples : 66.4 eps, Loss: 3.549, Avg loss: 3.384, Best loss: 3.371, cov loss: 0.185\n",
      "    [batch 2576]: seen 257600 examples : 66.4 eps, Loss: 3.494, Avg loss: 3.383, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 2583]: seen 258300 examples : 66.5 eps, Loss: 3.340, Avg loss: 3.382, Best loss: 3.371, cov loss: 0.172\n",
      "    [batch 2590]: seen 259000 examples : 66.5 eps, Loss: 3.384, Avg loss: 3.383, Best loss: 3.371, cov loss: 0.160\n",
      "    [batch 2597]: seen 259700 examples : 66.5 eps, Loss: 3.388, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.190\n",
      "    [batch 2604]: seen 260400 examples : 66.5 eps, Loss: 3.390, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 2611]: seen 261100 examples : 66.5 eps, Loss: 3.439, Avg loss: 3.390, Best loss: 3.371, cov loss: 0.187\n",
      "    [batch 2618]: seen 261800 examples : 66.5 eps, Loss: 3.442, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.163\n",
      "    [batch 2625]: seen 262500 examples : 66.5 eps, Loss: 3.436, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 2632]: seen 263200 examples : 66.5 eps, Loss: 3.350, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.177\n",
      "    [batch 2639]: seen 263900 examples : 66.5 eps, Loss: 3.395, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 2646]: seen 264600 examples : 66.5 eps, Loss: 3.255, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 2653]: seen 265300 examples : 66.5 eps, Loss: 3.235, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.157\n",
      "    [batch 2660]: seen 266000 examples : 66.5 eps, Loss: 3.398, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.157\n",
      "    [batch 2667]: seen 266700 examples : 66.5 eps, Loss: 3.525, Avg loss: 3.387, Best loss: 3.371, cov loss: 0.194\n",
      "    [batch 2674]: seen 267400 examples : 66.5 eps, Loss: 3.471, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.158\n",
      "    [batch 2681]: seen 268100 examples : 66.5 eps, Loss: 3.520, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.187\n",
      "    [batch 2688]: seen 268800 examples : 66.6 eps, Loss: 3.477, Avg loss: 3.391, Best loss: 3.371, cov loss: 0.155\n",
      "    [batch 2695]: seen 269500 examples : 66.6 eps, Loss: 3.493, Avg loss: 3.390, Best loss: 3.371, cov loss: 0.193\n",
      "    [batch 2702]: seen 270200 examples : 66.6 eps, Loss: 3.468, Avg loss: 3.391, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 2709]: seen 270900 examples : 66.6 eps, Loss: 3.227, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.162\n",
      "    [batch 2716]: seen 271600 examples : 66.6 eps, Loss: 3.357, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 2723]: seen 272300 examples : 66.6 eps, Loss: 3.519, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 2730]: seen 273000 examples : 66.6 eps, Loss: 3.310, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 2737]: seen 273700 examples : 66.6 eps, Loss: 3.538, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 2744]: seen 274400 examples : 66.6 eps, Loss: 3.254, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.178\n",
      "    [batch 2751]: seen 275100 examples : 66.6 eps, Loss: 3.383, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 2758]: seen 275800 examples : 66.6 eps, Loss: 3.515, Avg loss: 3.395, Best loss: 3.371, cov loss: 0.181\n",
      "    [batch 2765]: seen 276500 examples : 66.6 eps, Loss: 3.683, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 2772]: seen 277200 examples : 66.6 eps, Loss: 3.290, Avg loss: 3.390, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 2779]: seen 277900 examples : 66.6 eps, Loss: 3.310, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.153\n",
      "    [batch 2786]: seen 278600 examples : 66.6 eps, Loss: 3.207, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.154\n",
      "    [batch 2793]: seen 279300 examples : 66.6 eps, Loss: 3.253, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.155\n",
      "    [batch 2800]: seen 280000 examples : 66.6 eps, Loss: 3.326, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 2807]: seen 280700 examples : 66.7 eps, Loss: 3.408, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.170\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:10:11\n",
      "[EPOCH 21] Complete. Avg Loss: 3.3961969918671047; Best Loss: 3.3708799741833415\n",
      "[EPOCH 22] Starting training..\n",
      "    [batch 7]: seen 700 examples : 69.1 eps, Loss: 3.370, Avg loss: 3.392, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 14]: seen 1400 examples : 69.0 eps, Loss: 3.538, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.156\n",
      "    [batch 21]: seen 2100 examples : 69.0 eps, Loss: 3.479, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.183\n",
      "    [batch 28]: seen 2800 examples : 69.0 eps, Loss: 3.393, Avg loss: 3.397, Best loss: 3.371, cov loss: 0.182\n",
      "    [batch 35]: seen 3500 examples : 69.1 eps, Loss: 3.217, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 42]: seen 4200 examples : 69.0 eps, Loss: 3.319, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 49]: seen 4900 examples : 69.0 eps, Loss: 3.442, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.148\n",
      "    [batch 56]: seen 5600 examples : 69.0 eps, Loss: 3.484, Avg loss: 3.391, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 63]: seen 6300 examples : 69.1 eps, Loss: 3.310, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 70]: seen 7000 examples : 69.1 eps, Loss: 3.399, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.178\n",
      "    [batch 77]: seen 7700 examples : 69.0 eps, Loss: 3.361, Avg loss: 3.388, Best loss: 3.371, cov loss: 0.180\n",
      "    [batch 84]: seen 8400 examples : 69.1 eps, Loss: 3.383, Avg loss: 3.387, Best loss: 3.371, cov loss: 0.148\n",
      "    [batch 91]: seen 9100 examples : 69.0 eps, Loss: 3.273, Avg loss: 3.385, Best loss: 3.371, cov loss: 0.169\n",
      "    [batch 98]: seen 9800 examples : 69.0 eps, Loss: 3.271, Avg loss: 3.382, Best loss: 3.371, cov loss: 0.166\n",
      "    [batch 105]: seen 10500 examples : 69.0 eps, Loss: 3.206, Avg loss: 3.375, Best loss: 3.371, cov loss: 0.161\n",
      "    [batch 112]: seen 11200 examples : 69.0 eps, Loss: 3.445, Avg loss: 3.379, Best loss: 3.371, cov loss: 0.182\n",
      "    [batch 119]: seen 11900 examples : 69.0 eps, Loss: 3.206, Avg loss: 3.379, Best loss: 3.371, cov loss: 0.163\n",
      "    [batch 126]: seen 12600 examples : 69.0 eps, Loss: 3.434, Avg loss: 3.383, Best loss: 3.371, cov loss: 0.177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 133]: seen 13300 examples : 69.0 eps, Loss: 3.203, Avg loss: 3.382, Best loss: 3.371, cov loss: 0.164\n",
      "    [batch 140]: seen 14000 examples : 69.0 eps, Loss: 3.366, Avg loss: 3.381, Best loss: 3.371, cov loss: 0.168\n",
      "    [batch 147]: seen 14700 examples : 69.0 eps, Loss: 3.399, Avg loss: 3.381, Best loss: 3.371, cov loss: 0.170\n",
      "    [batch 154]: seen 15400 examples : 69.0 eps, Loss: 3.465, Avg loss: 3.383, Best loss: 3.371, cov loss: 0.186\n",
      "    [batch 161]: seen 16100 examples : 69.0 eps, Loss: 3.466, Avg loss: 3.386, Best loss: 3.371, cov loss: 0.165\n",
      "    [batch 168]: seen 16800 examples : 69.0 eps, Loss: 3.661, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.176\n",
      "    [batch 175]: seen 17500 examples : 69.0 eps, Loss: 3.328, Avg loss: 3.396, Best loss: 3.371, cov loss: 0.144\n",
      "    [batch 182]: seen 18200 examples : 69.0 eps, Loss: 3.393, Avg loss: 3.393, Best loss: 3.371, cov loss: 0.165\n",
      "    [batch 189]: seen 18900 examples : 69.0 eps, Loss: 3.589, Avg loss: 3.394, Best loss: 3.371, cov loss: 0.175\n",
      "    [batch 196]: seen 19600 examples : 69.0 eps, Loss: 3.353, Avg loss: 3.390, Best loss: 3.371, cov loss: 0.154\n",
      "    [batch 203]: seen 20300 examples : 69.0 eps, Loss: 3.288, Avg loss: 3.389, Best loss: 3.371, cov loss: 0.147\n",
      "    [batch 210]: seen 21000 examples : 69.0 eps, Loss: 3.309, Avg loss: 3.383, Best loss: 3.371, cov loss: 0.165\n",
      "    [batch 217]: seen 21700 examples : 69.0 eps, Loss: 3.368, Avg loss: 3.380, Best loss: 3.371, cov loss: 0.176\n",
      "    [batch 224]: seen 22400 examples : 69.0 eps, Loss: 3.292, Avg loss: 3.377, Best loss: 3.371, cov loss: 0.173\n",
      "    [batch 231]: seen 23100 examples : 69.0 eps, Loss: 3.391, Avg loss: 3.378, Best loss: 3.371, cov loss: 0.180\n",
      "    [batch 238]: seen 23800 examples : 69.0 eps, Loss: 3.255, Avg loss: 3.374, Best loss: 3.371, cov loss: 0.181\n",
      "    [batch 245]: seen 24500 examples : 69.0 eps, Loss: 3.381, Avg loss: 3.373, Best loss: 3.371, cov loss: 0.174\n",
      "    [batch 249]: seen 24900 examples : 67.6 eps, Loss: 3.246, Avg loss: 3.369, Best loss: 3.369, cov loss: 0.171\n",
      "    [batch 254]: seen 25400 examples : 67.0 eps, Loss: 3.314, Avg loss: 3.369, Best loss: 3.368, cov loss: 0.164\n",
      "    [batch 261]: seen 26100 examples : 67.0 eps, Loss: 3.349, Avg loss: 3.370, Best loss: 3.368, cov loss: 0.169\n",
      "    [batch 265]: seen 26500 examples : 65.9 eps, Loss: 3.351, Avg loss: 3.367, Best loss: 3.367, cov loss: 0.184\n",
      "    [batch 269]: seen 26900 examples : 64.9 eps, Loss: 3.320, Avg loss: 3.365, Best loss: 3.365, cov loss: 0.178\n",
      "    [batch 274]: seen 27400 examples : 64.4 eps, Loss: 3.529, Avg loss: 3.368, Best loss: 3.365, cov loss: 0.154\n",
      "    [batch 281]: seen 28100 examples : 64.5 eps, Loss: 3.389, Avg loss: 3.370, Best loss: 3.365, cov loss: 0.168\n",
      "    [batch 288]: seen 28800 examples : 64.6 eps, Loss: 3.241, Avg loss: 3.368, Best loss: 3.365, cov loss: 0.154\n",
      "    [batch 295]: seen 29500 examples : 64.7 eps, Loss: 3.305, Avg loss: 3.365, Best loss: 3.365, cov loss: 0.148\n",
      "    [batch 302]: seen 30200 examples : 64.8 eps, Loss: 3.372, Avg loss: 3.370, Best loss: 3.365, cov loss: 0.183\n",
      "    [batch 309]: seen 30900 examples : 64.9 eps, Loss: 3.423, Avg loss: 3.373, Best loss: 3.365, cov loss: 0.176\n",
      "    [batch 316]: seen 31600 examples : 65.0 eps, Loss: 3.369, Avg loss: 3.370, Best loss: 3.365, cov loss: 0.163\n",
      "    [batch 323]: seen 32300 examples : 65.1 eps, Loss: 3.476, Avg loss: 3.377, Best loss: 3.365, cov loss: 0.149\n",
      "    [batch 330]: seen 33000 examples : 65.1 eps, Loss: 3.311, Avg loss: 3.375, Best loss: 3.365, cov loss: 0.158\n",
      "    [batch 337]: seen 33700 examples : 65.2 eps, Loss: 3.547, Avg loss: 3.377, Best loss: 3.365, cov loss: 0.165\n",
      "    [batch 344]: seen 34400 examples : 65.3 eps, Loss: 3.543, Avg loss: 3.380, Best loss: 3.365, cov loss: 0.163\n",
      "    [batch 351]: seen 35100 examples : 65.4 eps, Loss: 3.397, Avg loss: 3.379, Best loss: 3.365, cov loss: 0.178\n",
      "    [batch 358]: seen 35800 examples : 65.4 eps, Loss: 3.459, Avg loss: 3.383, Best loss: 3.365, cov loss: 0.185\n",
      "    [batch 365]: seen 36500 examples : 65.5 eps, Loss: 3.241, Avg loss: 3.382, Best loss: 3.365, cov loss: 0.158\n",
      "    [batch 372]: seen 37200 examples : 65.6 eps, Loss: 3.330, Avg loss: 3.378, Best loss: 3.365, cov loss: 0.166\n",
      "    [batch 379]: seen 37900 examples : 65.6 eps, Loss: 3.415, Avg loss: 3.375, Best loss: 3.365, cov loss: 0.164\n",
      "    [batch 386]: seen 38600 examples : 65.7 eps, Loss: 3.242, Avg loss: 3.372, Best loss: 3.365, cov loss: 0.170\n",
      "    [batch 393]: seen 39300 examples : 65.7 eps, Loss: 3.248, Avg loss: 3.369, Best loss: 3.365, cov loss: 0.183\n",
      "    [batch 400]: seen 40000 examples : 65.8 eps, Loss: 3.429, Avg loss: 3.374, Best loss: 3.365, cov loss: 0.161\n",
      "    [batch 407]: seen 40700 examples : 65.8 eps, Loss: 3.437, Avg loss: 3.374, Best loss: 3.365, cov loss: 0.174\n",
      "    [batch 414]: seen 41400 examples : 65.9 eps, Loss: 3.296, Avg loss: 3.378, Best loss: 3.365, cov loss: 0.166\n",
      "    [batch 421]: seen 42100 examples : 65.9 eps, Loss: 3.471, Avg loss: 3.379, Best loss: 3.365, cov loss: 0.169\n",
      "    [batch 428]: seen 42800 examples : 66.0 eps, Loss: 3.359, Avg loss: 3.375, Best loss: 3.365, cov loss: 0.161\n",
      "    [batch 435]: seen 43500 examples : 66.0 eps, Loss: 3.429, Avg loss: 3.376, Best loss: 3.365, cov loss: 0.168\n",
      "    [batch 442]: seen 44200 examples : 66.1 eps, Loss: 3.364, Avg loss: 3.376, Best loss: 3.365, cov loss: 0.152\n",
      "    [batch 449]: seen 44900 examples : 66.1 eps, Loss: 3.282, Avg loss: 3.376, Best loss: 3.365, cov loss: 0.155\n",
      "    [batch 456]: seen 45600 examples : 66.2 eps, Loss: 3.297, Avg loss: 3.377, Best loss: 3.365, cov loss: 0.168\n",
      "    [batch 463]: seen 46300 examples : 66.2 eps, Loss: 3.211, Avg loss: 3.376, Best loss: 3.365, cov loss: 0.171\n",
      "    [batch 470]: seen 47000 examples : 66.2 eps, Loss: 3.627, Avg loss: 3.376, Best loss: 3.365, cov loss: 0.188\n",
      "    [batch 477]: seen 47700 examples : 66.3 eps, Loss: 3.354, Avg loss: 3.376, Best loss: 3.365, cov loss: 0.173\n",
      "    [batch 484]: seen 48400 examples : 66.3 eps, Loss: 3.313, Avg loss: 3.375, Best loss: 3.365, cov loss: 0.163\n",
      "    [batch 491]: seen 49100 examples : 66.4 eps, Loss: 3.332, Avg loss: 3.369, Best loss: 3.365, cov loss: 0.171\n",
      "    [batch 498]: seen 49800 examples : 66.4 eps, Loss: 3.431, Avg loss: 3.368, Best loss: 3.365, cov loss: 0.190\n",
      "    [batch 505]: seen 50500 examples : 66.4 eps, Loss: 3.297, Avg loss: 3.366, Best loss: 3.365, cov loss: 0.149\n",
      "    [batch 512]: seen 51200 examples : 66.5 eps, Loss: 3.302, Avg loss: 3.369, Best loss: 3.365, cov loss: 0.152\n",
      "    [batch 519]: seen 51900 examples : 66.5 eps, Loss: 3.528, Avg loss: 3.367, Best loss: 3.365, cov loss: 0.179\n",
      "    [batch 523]: seen 52300 examples : 65.9 eps, Loss: 3.337, Avg loss: 3.365, Best loss: 3.365, cov loss: 0.176\n",
      "    [batch 526]: seen 52600 examples : 65.4 eps, Loss: 3.549, Avg loss: 3.365, Best loss: 3.364, cov loss: 0.176\n",
      "    [batch 533]: seen 53300 examples : 65.4 eps, Loss: 3.397, Avg loss: 3.370, Best loss: 3.364, cov loss: 0.165\n",
      "    [batch 539]: seen 53900 examples : 65.2 eps, Loss: 3.185, Avg loss: 3.363, Best loss: 3.363, cov loss: 0.173\n",
      "    [batch 544]: seen 54400 examples : 64.9 eps, Loss: 3.549, Avg loss: 3.367, Best loss: 3.363, cov loss: 0.167\n",
      "    [batch 551]: seen 55100 examples : 65.0 eps, Loss: 3.195, Avg loss: 3.365, Best loss: 3.363, cov loss: 0.183\n",
      "    [batch 556]: seen 55600 examples : 64.8 eps, Loss: 3.413, Avg loss: 3.364, Best loss: 3.363, cov loss: 0.176\n",
      "    [batch 559]: seen 55900 examples : 64.3 eps, Loss: 3.102, Avg loss: 3.360, Best loss: 3.360, cov loss: 0.151\n",
      "    [batch 562]: seen 56200 examples : 63.7 eps, Loss: 3.100, Avg loss: 3.355, Best loss: 3.355, cov loss: 0.169\n",
      "    [batch 567]: seen 56700 examples : 63.5 eps, Loss: 3.614, Avg loss: 3.357, Best loss: 3.354, cov loss: 0.175\n",
      "    [batch 574]: seen 57400 examples : 63.3 eps, Loss: 3.127, Avg loss: 3.354, Best loss: 3.354, cov loss: 0.140\n",
      "    [batch 577]: seen 57700 examples : 62.7 eps, Loss: 3.338, Avg loss: 3.353, Best loss: 3.353, cov loss: 0.170\n",
      "    [batch 580]: seen 58000 examples : 62.0 eps, Loss: 3.063, Avg loss: 3.349, Best loss: 3.349, cov loss: 0.151\n",
      "    [batch 585]: seen 58500 examples : 61.8 eps, Loss: 3.428, Avg loss: 3.351, Best loss: 3.349, cov loss: 0.191\n",
      "    [batch 592]: seen 59200 examples : 61.9 eps, Loss: 3.146, Avg loss: 3.349, Best loss: 3.349, cov loss: 0.148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 597]: seen 59700 examples : 61.5 eps, Loss: 3.247, Avg loss: 3.346, Best loss: 3.346, cov loss: 0.151\n",
      "    [batch 602]: seen 60200 examples : 61.4 eps, Loss: 3.175, Avg loss: 3.347, Best loss: 3.346, cov loss: 0.169\n",
      "    [batch 607]: seen 60700 examples : 61.2 eps, Loss: 3.382, Avg loss: 3.347, Best loss: 3.346, cov loss: 0.175\n",
      "    [batch 614]: seen 61400 examples : 61.3 eps, Loss: 3.392, Avg loss: 3.349, Best loss: 3.346, cov loss: 0.171\n",
      "    [batch 620]: seen 62000 examples : 61.1 eps, Loss: 3.151, Avg loss: 3.345, Best loss: 3.345, cov loss: 0.155\n",
      "    [batch 625]: seen 62500 examples : 61.0 eps, Loss: 3.286, Avg loss: 3.345, Best loss: 3.344, cov loss: 0.167\n",
      "    [batch 629]: seen 62900 examples : 60.6 eps, Loss: 3.278, Avg loss: 3.343, Best loss: 3.343, cov loss: 0.151\n",
      "    [batch 633]: seen 63300 examples : 60.4 eps, Loss: 3.313, Avg loss: 3.343, Best loss: 3.342, cov loss: 0.163\n",
      "    [batch 638]: seen 63800 examples : 60.3 eps, Loss: 3.260, Avg loss: 3.342, Best loss: 3.340, cov loss: 0.152\n",
      "    [batch 642]: seen 64200 examples : 59.9 eps, Loss: 3.253, Avg loss: 3.340, Best loss: 3.340, cov loss: 0.175\n",
      "    [batch 645]: seen 64500 examples : 59.6 eps, Loss: 3.418, Avg loss: 3.339, Best loss: 3.338, cov loss: 0.163\n",
      "    [batch 652]: seen 65200 examples : 59.7 eps, Loss: 3.337, Avg loss: 3.342, Best loss: 3.338, cov loss: 0.158\n",
      "    [batch 659]: seen 65900 examples : 59.8 eps, Loss: 3.259, Avg loss: 3.341, Best loss: 3.338, cov loss: 0.155\n",
      "    [batch 666]: seen 66600 examples : 59.9 eps, Loss: 3.502, Avg loss: 3.345, Best loss: 3.338, cov loss: 0.185\n",
      "    [batch 673]: seen 67300 examples : 59.9 eps, Loss: 3.423, Avg loss: 3.347, Best loss: 3.338, cov loss: 0.166\n",
      "    [batch 680]: seen 68000 examples : 60.0 eps, Loss: 3.183, Avg loss: 3.348, Best loss: 3.338, cov loss: 0.173\n",
      "    [batch 687]: seen 68700 examples : 60.1 eps, Loss: 3.534, Avg loss: 3.351, Best loss: 3.338, cov loss: 0.189\n",
      "    [batch 694]: seen 69400 examples : 60.2 eps, Loss: 3.347, Avg loss: 3.354, Best loss: 3.338, cov loss: 0.157\n",
      "    [batch 701]: seen 70100 examples : 60.3 eps, Loss: 3.463, Avg loss: 3.354, Best loss: 3.338, cov loss: 0.182\n",
      "    [batch 708]: seen 70800 examples : 60.3 eps, Loss: 3.244, Avg loss: 3.352, Best loss: 3.338, cov loss: 0.166\n",
      "    [batch 715]: seen 71500 examples : 60.4 eps, Loss: 3.337, Avg loss: 3.350, Best loss: 3.338, cov loss: 0.155\n",
      "    [batch 722]: seen 72200 examples : 60.5 eps, Loss: 3.268, Avg loss: 3.351, Best loss: 3.338, cov loss: 0.178\n",
      "    [batch 729]: seen 72900 examples : 60.6 eps, Loss: 3.159, Avg loss: 3.347, Best loss: 3.338, cov loss: 0.152\n",
      "    [batch 736]: seen 73600 examples : 60.6 eps, Loss: 3.190, Avg loss: 3.349, Best loss: 3.338, cov loss: 0.174\n",
      "    [batch 743]: seen 74300 examples : 60.7 eps, Loss: 3.509, Avg loss: 3.352, Best loss: 3.338, cov loss: 0.182\n",
      "    [batch 750]: seen 75000 examples : 60.8 eps, Loss: 3.405, Avg loss: 3.357, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 757]: seen 75700 examples : 60.8 eps, Loss: 3.358, Avg loss: 3.353, Best loss: 3.338, cov loss: 0.158\n",
      "    [batch 764]: seen 76400 examples : 60.9 eps, Loss: 3.124, Avg loss: 3.349, Best loss: 3.338, cov loss: 0.146\n",
      "    [batch 771]: seen 77100 examples : 61.0 eps, Loss: 3.337, Avg loss: 3.352, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 778]: seen 77800 examples : 61.0 eps, Loss: 3.378, Avg loss: 3.353, Best loss: 3.338, cov loss: 0.161\n",
      "    [batch 785]: seen 78500 examples : 61.1 eps, Loss: 3.476, Avg loss: 3.351, Best loss: 3.338, cov loss: 0.190\n",
      "    [batch 792]: seen 79200 examples : 61.1 eps, Loss: 3.449, Avg loss: 3.353, Best loss: 3.338, cov loss: 0.156\n",
      "    [batch 799]: seen 79900 examples : 61.2 eps, Loss: 3.343, Avg loss: 3.352, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 806]: seen 80600 examples : 61.3 eps, Loss: 3.407, Avg loss: 3.354, Best loss: 3.338, cov loss: 0.162\n",
      "    [batch 813]: seen 81300 examples : 61.3 eps, Loss: 3.457, Avg loss: 3.351, Best loss: 3.338, cov loss: 0.174\n",
      "    [batch 820]: seen 82000 examples : 61.4 eps, Loss: 3.382, Avg loss: 3.355, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 827]: seen 82700 examples : 61.4 eps, Loss: 3.397, Avg loss: 3.358, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 834]: seen 83400 examples : 61.5 eps, Loss: 3.329, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.151\n",
      "    [batch 841]: seen 84100 examples : 61.6 eps, Loss: 3.435, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.177\n",
      "    [batch 848]: seen 84800 examples : 61.6 eps, Loss: 3.384, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.158\n",
      "    [batch 855]: seen 85500 examples : 61.7 eps, Loss: 3.178, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.155\n",
      "    [batch 862]: seen 86200 examples : 61.7 eps, Loss: 3.454, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.164\n",
      "    [batch 869]: seen 86900 examples : 61.8 eps, Loss: 3.429, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 876]: seen 87600 examples : 61.8 eps, Loss: 3.296, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 883]: seen 88300 examples : 61.9 eps, Loss: 3.521, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.180\n",
      "    [batch 890]: seen 89000 examples : 61.9 eps, Loss: 3.260, Avg loss: 3.373, Best loss: 3.338, cov loss: 0.171\n",
      "    [batch 897]: seen 89700 examples : 62.0 eps, Loss: 3.194, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 904]: seen 90400 examples : 62.0 eps, Loss: 3.365, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.152\n",
      "    [batch 911]: seen 91100 examples : 62.1 eps, Loss: 3.309, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.176\n",
      "    [batch 918]: seen 91800 examples : 62.1 eps, Loss: 3.429, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.161\n",
      "    [batch 925]: seen 92500 examples : 62.2 eps, Loss: 3.521, Avg loss: 3.371, Best loss: 3.338, cov loss: 0.181\n",
      "    [batch 932]: seen 93200 examples : 62.2 eps, Loss: 3.353, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.148\n",
      "    [batch 939]: seen 93900 examples : 62.3 eps, Loss: 3.404, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.174\n",
      "    [batch 946]: seen 94600 examples : 62.3 eps, Loss: 3.431, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.154\n",
      "    [batch 953]: seen 95300 examples : 62.4 eps, Loss: 3.447, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 960]: seen 96000 examples : 62.4 eps, Loss: 3.290, Avg loss: 3.369, Best loss: 3.338, cov loss: 0.171\n",
      "    [batch 967]: seen 96700 examples : 62.4 eps, Loss: 3.433, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 974]: seen 97400 examples : 62.5 eps, Loss: 3.492, Avg loss: 3.369, Best loss: 3.338, cov loss: 0.176\n",
      "    [batch 981]: seen 98100 examples : 62.5 eps, Loss: 3.310, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.177\n",
      "    [batch 988]: seen 98800 examples : 62.6 eps, Loss: 3.318, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.157\n",
      "    [batch 995]: seen 99500 examples : 62.6 eps, Loss: 3.345, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.171\n",
      "    [batch 1002]: seen 100200 examples : 62.7 eps, Loss: 3.344, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 1009]: seen 100900 examples : 62.7 eps, Loss: 3.250, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.154\n",
      "    [batch 1016]: seen 101600 examples : 62.7 eps, Loss: 3.360, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.177\n",
      "    [batch 1023]: seen 102300 examples : 62.8 eps, Loss: 3.309, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.166\n",
      "    [batch 1030]: seen 103000 examples : 62.8 eps, Loss: 3.383, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 1037]: seen 103700 examples : 62.8 eps, Loss: 3.473, Avg loss: 3.373, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 1044]: seen 104400 examples : 62.9 eps, Loss: 3.279, Avg loss: 3.372, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 1051]: seen 105100 examples : 62.9 eps, Loss: 3.374, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.163\n",
      "    [batch 1058]: seen 105800 examples : 63.0 eps, Loss: 3.374, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.173\n",
      "    [batch 1065]: seen 106500 examples : 63.0 eps, Loss: 3.402, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.163\n",
      "    [batch 1072]: seen 107200 examples : 63.0 eps, Loss: 3.216, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 1079]: seen 107900 examples : 63.1 eps, Loss: 3.285, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1086]: seen 108600 examples : 63.1 eps, Loss: 3.342, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.149\n",
      "    [batch 1093]: seen 109300 examples : 63.1 eps, Loss: 3.559, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.179\n",
      "    [batch 1100]: seen 110000 examples : 63.2 eps, Loss: 3.385, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.174\n",
      "    [batch 1107]: seen 110700 examples : 63.2 eps, Loss: 3.473, Avg loss: 3.369, Best loss: 3.338, cov loss: 0.177\n",
      "    [batch 1114]: seen 111400 examples : 63.2 eps, Loss: 3.384, Avg loss: 3.373, Best loss: 3.338, cov loss: 0.183\n",
      "    [batch 1121]: seen 112100 examples : 63.3 eps, Loss: 3.364, Avg loss: 3.373, Best loss: 3.338, cov loss: 0.155\n",
      "    [batch 1128]: seen 112800 examples : 63.3 eps, Loss: 3.334, Avg loss: 3.372, Best loss: 3.338, cov loss: 0.153\n",
      "    [batch 1135]: seen 113500 examples : 63.3 eps, Loss: 3.343, Avg loss: 3.374, Best loss: 3.338, cov loss: 0.142\n",
      "    [batch 1142]: seen 114200 examples : 63.4 eps, Loss: 3.278, Avg loss: 3.375, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 1149]: seen 114900 examples : 63.4 eps, Loss: 3.432, Avg loss: 3.381, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 1156]: seen 115600 examples : 63.4 eps, Loss: 3.337, Avg loss: 3.383, Best loss: 3.338, cov loss: 0.158\n",
      "    [batch 1163]: seen 116300 examples : 63.5 eps, Loss: 3.581, Avg loss: 3.382, Best loss: 3.338, cov loss: 0.162\n",
      "    [batch 1170]: seen 117000 examples : 63.5 eps, Loss: 3.458, Avg loss: 3.379, Best loss: 3.338, cov loss: 0.165\n",
      "    [batch 1177]: seen 117700 examples : 63.5 eps, Loss: 3.312, Avg loss: 3.376, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 1184]: seen 118400 examples : 63.6 eps, Loss: 3.546, Avg loss: 3.375, Best loss: 3.338, cov loss: 0.155\n",
      "    [batch 1191]: seen 119100 examples : 63.6 eps, Loss: 3.475, Avg loss: 3.374, Best loss: 3.338, cov loss: 0.168\n",
      "    [batch 1198]: seen 119800 examples : 63.6 eps, Loss: 3.245, Avg loss: 3.376, Best loss: 3.338, cov loss: 0.141\n",
      "    [batch 1205]: seen 120500 examples : 63.6 eps, Loss: 3.305, Avg loss: 3.373, Best loss: 3.338, cov loss: 0.175\n",
      "    [batch 1212]: seen 121200 examples : 63.7 eps, Loss: 3.295, Avg loss: 3.376, Best loss: 3.338, cov loss: 0.165\n",
      "    [batch 1219]: seen 121900 examples : 63.7 eps, Loss: 3.413, Avg loss: 3.376, Best loss: 3.338, cov loss: 0.171\n",
      "    [batch 1226]: seen 122600 examples : 63.7 eps, Loss: 3.425, Avg loss: 3.373, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 1233]: seen 123300 examples : 63.8 eps, Loss: 3.336, Avg loss: 3.373, Best loss: 3.338, cov loss: 0.179\n",
      "    [batch 1240]: seen 124000 examples : 63.8 eps, Loss: 3.097, Avg loss: 3.371, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 1247]: seen 124700 examples : 63.8 eps, Loss: 3.499, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 1254]: seen 125400 examples : 63.8 eps, Loss: 3.424, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 1261]: seen 126100 examples : 63.9 eps, Loss: 3.611, Avg loss: 3.371, Best loss: 3.338, cov loss: 0.179\n",
      "    [batch 1268]: seen 126800 examples : 63.9 eps, Loss: 3.333, Avg loss: 3.371, Best loss: 3.338, cov loss: 0.156\n",
      "    [batch 1275]: seen 127500 examples : 63.9 eps, Loss: 3.483, Avg loss: 3.372, Best loss: 3.338, cov loss: 0.175\n",
      "    [batch 1282]: seen 128200 examples : 63.9 eps, Loss: 3.276, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.156\n",
      "    [batch 1289]: seen 128900 examples : 64.0 eps, Loss: 3.453, Avg loss: 3.369, Best loss: 3.338, cov loss: 0.157\n",
      "    [batch 1296]: seen 129600 examples : 64.0 eps, Loss: 3.179, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.178\n",
      "    [batch 1303]: seen 130300 examples : 64.0 eps, Loss: 3.517, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.185\n",
      "    [batch 1310]: seen 131000 examples : 64.0 eps, Loss: 3.221, Avg loss: 3.369, Best loss: 3.338, cov loss: 0.144\n",
      "    [batch 1317]: seen 131700 examples : 64.1 eps, Loss: 3.109, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.171\n",
      "    [batch 1324]: seen 132400 examples : 64.1 eps, Loss: 3.542, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.163\n",
      "    [batch 1331]: seen 133100 examples : 64.1 eps, Loss: 3.624, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.181\n",
      "    [batch 1338]: seen 133800 examples : 64.1 eps, Loss: 3.505, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.187\n",
      "    [batch 1345]: seen 134500 examples : 64.2 eps, Loss: 3.384, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.153\n",
      "    [batch 1352]: seen 135200 examples : 64.2 eps, Loss: 3.295, Avg loss: 3.358, Best loss: 3.338, cov loss: 0.171\n",
      "    [batch 1359]: seen 135900 examples : 64.2 eps, Loss: 3.472, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.182\n",
      "    [batch 1366]: seen 136600 examples : 64.2 eps, Loss: 3.159, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.145\n",
      "    [batch 1373]: seen 137300 examples : 64.3 eps, Loss: 3.278, Avg loss: 3.359, Best loss: 3.338, cov loss: 0.186\n",
      "    [batch 1380]: seen 138000 examples : 64.3 eps, Loss: 3.509, Avg loss: 3.360, Best loss: 3.338, cov loss: 0.175\n",
      "    [batch 1387]: seen 138700 examples : 64.3 eps, Loss: 3.362, Avg loss: 3.356, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 1394]: seen 139400 examples : 64.3 eps, Loss: 3.474, Avg loss: 3.354, Best loss: 3.338, cov loss: 0.194\n",
      "    [batch 1401]: seen 140100 examples : 64.3 eps, Loss: 3.369, Avg loss: 3.350, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 1408]: seen 140800 examples : 64.4 eps, Loss: 3.398, Avg loss: 3.351, Best loss: 3.338, cov loss: 0.179\n",
      "    [batch 1415]: seen 141500 examples : 64.4 eps, Loss: 3.146, Avg loss: 3.349, Best loss: 3.338, cov loss: 0.151\n",
      "    [batch 1422]: seen 142200 examples : 64.4 eps, Loss: 3.287, Avg loss: 3.348, Best loss: 3.338, cov loss: 0.162\n",
      "    [batch 1429]: seen 142900 examples : 64.4 eps, Loss: 3.283, Avg loss: 3.350, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 1436]: seen 143600 examples : 64.4 eps, Loss: 3.548, Avg loss: 3.354, Best loss: 3.338, cov loss: 0.174\n",
      "    [batch 1443]: seen 144300 examples : 64.5 eps, Loss: 3.432, Avg loss: 3.358, Best loss: 3.338, cov loss: 0.182\n",
      "    [batch 1450]: seen 145000 examples : 64.5 eps, Loss: 3.452, Avg loss: 3.359, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 1457]: seen 145700 examples : 64.5 eps, Loss: 3.368, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.155\n",
      "    [batch 1464]: seen 146400 examples : 64.5 eps, Loss: 3.372, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.184\n",
      "    [batch 1471]: seen 147100 examples : 64.6 eps, Loss: 3.330, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.149\n",
      "    [batch 1478]: seen 147800 examples : 64.6 eps, Loss: 3.349, Avg loss: 3.359, Best loss: 3.338, cov loss: 0.188\n",
      "    [batch 1485]: seen 148500 examples : 64.6 eps, Loss: 3.462, Avg loss: 3.358, Best loss: 3.338, cov loss: 0.157\n",
      "    [batch 1492]: seen 149200 examples : 64.6 eps, Loss: 3.316, Avg loss: 3.354, Best loss: 3.338, cov loss: 0.184\n",
      "    [batch 1499]: seen 149900 examples : 64.6 eps, Loss: 3.384, Avg loss: 3.353, Best loss: 3.338, cov loss: 0.166\n",
      "    [batch 1506]: seen 150600 examples : 64.6 eps, Loss: 3.631, Avg loss: 3.357, Best loss: 3.338, cov loss: 0.178\n",
      "    [batch 1513]: seen 151300 examples : 64.7 eps, Loss: 3.476, Avg loss: 3.360, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 1520]: seen 152000 examples : 64.7 eps, Loss: 3.453, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 1527]: seen 152700 examples : 64.7 eps, Loss: 3.509, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 1534]: seen 153400 examples : 64.7 eps, Loss: 3.371, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.179\n",
      "    [batch 1541]: seen 154100 examples : 64.7 eps, Loss: 3.406, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.155\n",
      "    [batch 1548]: seen 154800 examples : 64.8 eps, Loss: 3.431, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.187\n",
      "    [batch 1555]: seen 155500 examples : 64.8 eps, Loss: 3.500, Avg loss: 3.360, Best loss: 3.338, cov loss: 0.150\n",
      "    [batch 1562]: seen 156200 examples : 64.8 eps, Loss: 3.266, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.165\n",
      "    [batch 1569]: seen 156900 examples : 64.8 eps, Loss: 3.391, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.173\n",
      "    [batch 1576]: seen 157600 examples : 64.8 eps, Loss: 3.083, Avg loss: 3.356, Best loss: 3.338, cov loss: 0.164\n",
      "    [batch 1583]: seen 158300 examples : 64.8 eps, Loss: 3.273, Avg loss: 3.356, Best loss: 3.338, cov loss: 0.156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1590]: seen 159000 examples : 64.9 eps, Loss: 3.395, Avg loss: 3.357, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 1597]: seen 159700 examples : 64.9 eps, Loss: 3.399, Avg loss: 3.360, Best loss: 3.338, cov loss: 0.175\n",
      "    [batch 1604]: seen 160400 examples : 64.9 eps, Loss: 3.439, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.164\n",
      "    [batch 1611]: seen 161100 examples : 64.9 eps, Loss: 3.338, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 1618]: seen 161800 examples : 64.9 eps, Loss: 3.152, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.152\n",
      "    [batch 1625]: seen 162500 examples : 65.0 eps, Loss: 3.300, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.175\n",
      "    [batch 1632]: seen 163200 examples : 65.0 eps, Loss: 3.549, Avg loss: 3.373, Best loss: 3.338, cov loss: 0.187\n",
      "    [batch 1639]: seen 163900 examples : 65.0 eps, Loss: 3.527, Avg loss: 3.374, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 1646]: seen 164600 examples : 65.0 eps, Loss: 3.352, Avg loss: 3.374, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 1653]: seen 165300 examples : 65.0 eps, Loss: 3.247, Avg loss: 3.373, Best loss: 3.338, cov loss: 0.157\n",
      "    [batch 1660]: seen 166000 examples : 65.0 eps, Loss: 3.417, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.168\n",
      "    [batch 1667]: seen 166700 examples : 65.0 eps, Loss: 3.248, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 1674]: seen 167400 examples : 65.1 eps, Loss: 3.480, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.166\n",
      "    [batch 1681]: seen 168100 examples : 65.1 eps, Loss: 3.370, Avg loss: 3.371, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 1688]: seen 168800 examples : 65.1 eps, Loss: 3.563, Avg loss: 3.371, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 1695]: seen 169500 examples : 65.1 eps, Loss: 3.141, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.165\n",
      "    [batch 1702]: seen 170200 examples : 65.1 eps, Loss: 3.458, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.185\n",
      "    [batch 1709]: seen 170900 examples : 65.1 eps, Loss: 3.787, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.198\n",
      "    [batch 1716]: seen 171600 examples : 65.2 eps, Loss: 3.392, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.156\n",
      "    [batch 1723]: seen 172300 examples : 65.2 eps, Loss: 3.304, Avg loss: 3.369, Best loss: 3.338, cov loss: 0.157\n",
      "    [batch 1730]: seen 173000 examples : 65.2 eps, Loss: 3.228, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 1737]: seen 173700 examples : 65.2 eps, Loss: 3.312, Avg loss: 3.372, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 1744]: seen 174400 examples : 65.2 eps, Loss: 3.457, Avg loss: 3.374, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 1751]: seen 175100 examples : 65.2 eps, Loss: 3.200, Avg loss: 3.377, Best loss: 3.338, cov loss: 0.153\n",
      "    [batch 1758]: seen 175800 examples : 65.2 eps, Loss: 3.244, Avg loss: 3.377, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 1765]: seen 176500 examples : 65.3 eps, Loss: 3.502, Avg loss: 3.380, Best loss: 3.338, cov loss: 0.193\n",
      "    [batch 1772]: seen 177200 examples : 65.3 eps, Loss: 3.399, Avg loss: 3.381, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 1779]: seen 177900 examples : 65.3 eps, Loss: 3.272, Avg loss: 3.382, Best loss: 3.338, cov loss: 0.165\n",
      "    [batch 1786]: seen 178600 examples : 65.3 eps, Loss: 3.006, Avg loss: 3.384, Best loss: 3.338, cov loss: 0.152\n",
      "    [batch 1793]: seen 179300 examples : 65.3 eps, Loss: 3.423, Avg loss: 3.384, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 1800]: seen 180000 examples : 65.3 eps, Loss: 3.402, Avg loss: 3.381, Best loss: 3.338, cov loss: 0.166\n",
      "    [batch 1807]: seen 180700 examples : 65.3 eps, Loss: 3.348, Avg loss: 3.379, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 1814]: seen 181400 examples : 65.4 eps, Loss: 3.485, Avg loss: 3.378, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 1821]: seen 182100 examples : 65.4 eps, Loss: 3.259, Avg loss: 3.377, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 1828]: seen 182800 examples : 65.4 eps, Loss: 3.302, Avg loss: 3.382, Best loss: 3.338, cov loss: 0.180\n",
      "    [batch 1835]: seen 183500 examples : 65.4 eps, Loss: 3.361, Avg loss: 3.379, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 1842]: seen 184200 examples : 65.4 eps, Loss: 3.278, Avg loss: 3.376, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 1849]: seen 184900 examples : 65.4 eps, Loss: 3.250, Avg loss: 3.376, Best loss: 3.338, cov loss: 0.166\n",
      "    [batch 1856]: seen 185600 examples : 65.4 eps, Loss: 3.264, Avg loss: 3.372, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 1863]: seen 186300 examples : 65.4 eps, Loss: 3.482, Avg loss: 3.376, Best loss: 3.338, cov loss: 0.175\n",
      "    [batch 1870]: seen 187000 examples : 65.5 eps, Loss: 3.222, Avg loss: 3.375, Best loss: 3.338, cov loss: 0.147\n",
      "    [batch 1877]: seen 187700 examples : 65.5 eps, Loss: 3.310, Avg loss: 3.376, Best loss: 3.338, cov loss: 0.158\n",
      "    [batch 1884]: seen 188400 examples : 65.5 eps, Loss: 3.282, Avg loss: 3.375, Best loss: 3.338, cov loss: 0.181\n",
      "    [batch 1891]: seen 189100 examples : 65.5 eps, Loss: 3.180, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.165\n",
      "    [batch 1898]: seen 189800 examples : 65.5 eps, Loss: 3.444, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 1905]: seen 190500 examples : 65.5 eps, Loss: 3.558, Avg loss: 3.371, Best loss: 3.338, cov loss: 0.166\n",
      "    [batch 1912]: seen 191200 examples : 65.5 eps, Loss: 3.309, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.181\n",
      "    [batch 1919]: seen 191900 examples : 65.5 eps, Loss: 3.510, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.156\n",
      "    [batch 1926]: seen 192600 examples : 65.6 eps, Loss: 3.240, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 1933]: seen 193300 examples : 65.6 eps, Loss: 3.362, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.166\n",
      "    [batch 1940]: seen 194000 examples : 65.6 eps, Loss: 3.246, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 1947]: seen 194700 examples : 65.6 eps, Loss: 3.509, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.176\n",
      "    [batch 1954]: seen 195400 examples : 65.6 eps, Loss: 3.424, Avg loss: 3.360, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 1961]: seen 196100 examples : 65.6 eps, Loss: 3.444, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.179\n",
      "    [batch 1968]: seen 196800 examples : 65.6 eps, Loss: 3.351, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 1975]: seen 197500 examples : 65.6 eps, Loss: 3.447, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.176\n",
      "    [batch 1982]: seen 198200 examples : 65.6 eps, Loss: 3.245, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.161\n",
      "    [batch 1989]: seen 198900 examples : 65.7 eps, Loss: 3.517, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 1996]: seen 199600 examples : 65.7 eps, Loss: 3.392, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.179\n",
      "    [batch 2003]: seen 200300 examples : 65.7 eps, Loss: 3.551, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.179\n",
      "    [batch 2010]: seen 201000 examples : 65.7 eps, Loss: 3.374, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 2017]: seen 201700 examples : 65.7 eps, Loss: 3.346, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 2024]: seen 202400 examples : 65.7 eps, Loss: 3.372, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.183\n",
      "    [batch 2031]: seen 203100 examples : 65.7 eps, Loss: 3.326, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.169\n",
      "    [batch 2038]: seen 203800 examples : 65.7 eps, Loss: 3.426, Avg loss: 3.359, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 2045]: seen 204500 examples : 65.7 eps, Loss: 3.341, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.162\n",
      "    [batch 2052]: seen 205200 examples : 65.8 eps, Loss: 3.452, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.168\n",
      "    [batch 2059]: seen 205900 examples : 65.8 eps, Loss: 3.328, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.176\n",
      "    [batch 2066]: seen 206600 examples : 65.8 eps, Loss: 3.368, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.180\n",
      "    [batch 2073]: seen 207300 examples : 65.8 eps, Loss: 3.348, Avg loss: 3.360, Best loss: 3.338, cov loss: 0.178\n",
      "    [batch 2080]: seen 208000 examples : 65.8 eps, Loss: 3.431, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.179\n",
      "    [batch 2087]: seen 208700 examples : 65.8 eps, Loss: 3.208, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2094]: seen 209400 examples : 65.8 eps, Loss: 3.485, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.173\n",
      "    [batch 2101]: seen 210100 examples : 65.8 eps, Loss: 3.275, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.156\n",
      "    [batch 2108]: seen 210800 examples : 65.8 eps, Loss: 3.572, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 2115]: seen 211500 examples : 65.8 eps, Loss: 3.418, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 2122]: seen 212200 examples : 65.9 eps, Loss: 3.296, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.181\n",
      "    [batch 2129]: seen 212900 examples : 65.9 eps, Loss: 3.317, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.156\n",
      "    [batch 2136]: seen 213600 examples : 65.9 eps, Loss: 3.321, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 2143]: seen 214300 examples : 65.9 eps, Loss: 3.246, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.161\n",
      "    [batch 2150]: seen 215000 examples : 65.9 eps, Loss: 3.352, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.168\n",
      "    [batch 2157]: seen 215700 examples : 65.9 eps, Loss: 3.339, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.158\n",
      "    [batch 2164]: seen 216400 examples : 65.9 eps, Loss: 3.368, Avg loss: 3.369, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 2171]: seen 217100 examples : 65.9 eps, Loss: 3.214, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.163\n",
      "    [batch 2178]: seen 217800 examples : 65.9 eps, Loss: 3.432, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.151\n",
      "    [batch 2185]: seen 218500 examples : 65.9 eps, Loss: 3.305, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.168\n",
      "    [batch 2192]: seen 219200 examples : 66.0 eps, Loss: 3.086, Avg loss: 3.360, Best loss: 3.338, cov loss: 0.156\n",
      "    [batch 2199]: seen 219900 examples : 66.0 eps, Loss: 3.384, Avg loss: 3.355, Best loss: 3.338, cov loss: 0.178\n",
      "    [batch 2206]: seen 220600 examples : 66.0 eps, Loss: 3.308, Avg loss: 3.359, Best loss: 3.338, cov loss: 0.173\n",
      "    [batch 2213]: seen 221300 examples : 66.0 eps, Loss: 3.374, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.157\n",
      "    [batch 2220]: seen 222000 examples : 66.0 eps, Loss: 3.535, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.182\n",
      "    [batch 2227]: seen 222700 examples : 66.0 eps, Loss: 3.327, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.157\n",
      "    [batch 2234]: seen 223400 examples : 66.0 eps, Loss: 3.275, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.183\n",
      "    [batch 2241]: seen 224100 examples : 66.0 eps, Loss: 3.349, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 2248]: seen 224800 examples : 66.0 eps, Loss: 3.327, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.183\n",
      "    [batch 2255]: seen 225500 examples : 66.0 eps, Loss: 3.434, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 2262]: seen 226200 examples : 66.0 eps, Loss: 3.205, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 2269]: seen 226900 examples : 66.1 eps, Loss: 3.136, Avg loss: 3.356, Best loss: 3.338, cov loss: 0.153\n",
      "    [batch 2276]: seen 227600 examples : 66.1 eps, Loss: 3.404, Avg loss: 3.358, Best loss: 3.338, cov loss: 0.180\n",
      "    [batch 2283]: seen 228300 examples : 66.1 eps, Loss: 3.422, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.162\n",
      "    [batch 2290]: seen 229000 examples : 66.1 eps, Loss: 3.228, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.163\n",
      "    [batch 2297]: seen 229700 examples : 66.1 eps, Loss: 3.351, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 2304]: seen 230400 examples : 66.1 eps, Loss: 3.431, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 2311]: seen 231100 examples : 66.1 eps, Loss: 3.479, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 2318]: seen 231800 examples : 66.1 eps, Loss: 3.399, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.180\n",
      "    [batch 2325]: seen 232500 examples : 66.1 eps, Loss: 3.325, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 2332]: seen 233200 examples : 66.1 eps, Loss: 3.344, Avg loss: 3.359, Best loss: 3.338, cov loss: 0.176\n",
      "    [batch 2339]: seen 233900 examples : 66.1 eps, Loss: 3.340, Avg loss: 3.358, Best loss: 3.338, cov loss: 0.180\n",
      "    [batch 2346]: seen 234600 examples : 66.1 eps, Loss: 3.428, Avg loss: 3.356, Best loss: 3.338, cov loss: 0.165\n",
      "    [batch 2353]: seen 235300 examples : 66.2 eps, Loss: 3.291, Avg loss: 3.357, Best loss: 3.338, cov loss: 0.154\n",
      "    [batch 2360]: seen 236000 examples : 66.2 eps, Loss: 3.526, Avg loss: 3.358, Best loss: 3.338, cov loss: 0.155\n",
      "    [batch 2367]: seen 236700 examples : 66.2 eps, Loss: 3.452, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.166\n",
      "    [batch 2374]: seen 237400 examples : 66.2 eps, Loss: 3.437, Avg loss: 3.359, Best loss: 3.338, cov loss: 0.162\n",
      "    [batch 2381]: seen 238100 examples : 66.2 eps, Loss: 3.298, Avg loss: 3.357, Best loss: 3.338, cov loss: 0.157\n",
      "    [batch 2388]: seen 238800 examples : 66.2 eps, Loss: 3.536, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.190\n",
      "    [batch 2395]: seen 239500 examples : 66.2 eps, Loss: 3.218, Avg loss: 3.358, Best loss: 3.338, cov loss: 0.158\n",
      "    [batch 2402]: seen 240200 examples : 66.2 eps, Loss: 3.390, Avg loss: 3.358, Best loss: 3.338, cov loss: 0.158\n",
      "    [batch 2409]: seen 240900 examples : 66.2 eps, Loss: 3.540, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.178\n",
      "    [batch 2416]: seen 241600 examples : 66.2 eps, Loss: 3.212, Avg loss: 3.360, Best loss: 3.338, cov loss: 0.160\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-38874\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-38874\n",
      "    [batch 2422]: seen 242200 examples : 66.2 eps, Loss: 3.289, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.155\n",
      "    [batch 2429]: seen 242900 examples : 66.2 eps, Loss: 3.318, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.157\n",
      "    [batch 2436]: seen 243600 examples : 66.2 eps, Loss: 3.423, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.179\n",
      "    [batch 2443]: seen 244300 examples : 66.2 eps, Loss: 3.330, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 2450]: seen 245000 examples : 66.2 eps, Loss: 3.427, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 2457]: seen 245700 examples : 66.2 eps, Loss: 3.196, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.186\n",
      "    [batch 2464]: seen 246400 examples : 66.2 eps, Loss: 3.278, Avg loss: 3.360, Best loss: 3.338, cov loss: 0.168\n",
      "    [batch 2471]: seen 247100 examples : 66.3 eps, Loss: 3.422, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 2478]: seen 247800 examples : 66.3 eps, Loss: 3.347, Avg loss: 3.369, Best loss: 3.338, cov loss: 0.153\n",
      "    [batch 2485]: seen 248500 examples : 66.3 eps, Loss: 3.403, Avg loss: 3.368, Best loss: 3.338, cov loss: 0.158\n",
      "    [batch 2492]: seen 249200 examples : 66.3 eps, Loss: 3.512, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.164\n",
      "    [batch 2499]: seen 249900 examples : 66.3 eps, Loss: 3.238, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 2506]: seen 250600 examples : 66.3 eps, Loss: 3.515, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.175\n",
      "    [batch 2513]: seen 251300 examples : 66.3 eps, Loss: 3.000, Avg loss: 3.356, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 2520]: seen 252000 examples : 66.3 eps, Loss: 3.352, Avg loss: 3.354, Best loss: 3.338, cov loss: 0.183\n",
      "    [batch 2527]: seen 252700 examples : 66.3 eps, Loss: 3.380, Avg loss: 3.356, Best loss: 3.338, cov loss: 0.177\n",
      "    [batch 2534]: seen 253400 examples : 66.3 eps, Loss: 3.381, Avg loss: 3.358, Best loss: 3.338, cov loss: 0.177\n",
      "    [batch 2541]: seen 254100 examples : 66.3 eps, Loss: 3.479, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.175\n",
      "    [batch 2548]: seen 254800 examples : 66.3 eps, Loss: 3.449, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.172\n",
      "    [batch 2555]: seen 255500 examples : 66.3 eps, Loss: 3.178, Avg loss: 3.363, Best loss: 3.338, cov loss: 0.148\n",
      "    [batch 2562]: seen 256200 examples : 66.3 eps, Loss: 3.412, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.163\n",
      "    [batch 2569]: seen 256900 examples : 66.4 eps, Loss: 3.389, Avg loss: 3.369, Best loss: 3.338, cov loss: 0.163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2576]: seen 257600 examples : 66.4 eps, Loss: 3.417, Avg loss: 3.371, Best loss: 3.338, cov loss: 0.182\n",
      "    [batch 2583]: seen 258300 examples : 66.4 eps, Loss: 3.377, Avg loss: 3.372, Best loss: 3.338, cov loss: 0.164\n",
      "    [batch 2590]: seen 259000 examples : 66.4 eps, Loss: 3.385, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 2597]: seen 259700 examples : 66.4 eps, Loss: 3.221, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.160\n",
      "    [batch 2604]: seen 260400 examples : 66.4 eps, Loss: 3.405, Avg loss: 3.371, Best loss: 3.338, cov loss: 0.190\n",
      "    [batch 2611]: seen 261100 examples : 66.4 eps, Loss: 3.584, Avg loss: 3.373, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 2618]: seen 261800 examples : 66.4 eps, Loss: 3.259, Avg loss: 3.372, Best loss: 3.338, cov loss: 0.161\n",
      "    [batch 2625]: seen 262500 examples : 66.4 eps, Loss: 3.371, Avg loss: 3.371, Best loss: 3.338, cov loss: 0.164\n",
      "    [batch 2632]: seen 263200 examples : 66.4 eps, Loss: 3.328, Avg loss: 3.372, Best loss: 3.338, cov loss: 0.140\n",
      "    [batch 2639]: seen 263900 examples : 66.4 eps, Loss: 3.438, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.193\n",
      "    [batch 2646]: seen 264600 examples : 66.4 eps, Loss: 3.393, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.171\n",
      "    [batch 2653]: seen 265300 examples : 66.4 eps, Loss: 3.401, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.164\n",
      "    [batch 2660]: seen 266000 examples : 66.4 eps, Loss: 3.197, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 2667]: seen 266700 examples : 66.4 eps, Loss: 3.285, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.168\n",
      "    [batch 2674]: seen 267400 examples : 66.5 eps, Loss: 3.331, Avg loss: 3.364, Best loss: 3.338, cov loss: 0.168\n",
      "    [batch 2681]: seen 268100 examples : 66.5 eps, Loss: 3.316, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.161\n",
      "    [batch 2688]: seen 268800 examples : 66.5 eps, Loss: 3.335, Avg loss: 3.360, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 2695]: seen 269500 examples : 66.5 eps, Loss: 3.425, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.177\n",
      "    [batch 2702]: seen 270200 examples : 66.5 eps, Loss: 3.069, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.164\n",
      "    [batch 2709]: seen 270900 examples : 66.5 eps, Loss: 3.444, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 2716]: seen 271600 examples : 66.5 eps, Loss: 3.462, Avg loss: 3.369, Best loss: 3.338, cov loss: 0.168\n",
      "    [batch 2723]: seen 272300 examples : 66.5 eps, Loss: 3.149, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.178\n",
      "    [batch 2730]: seen 273000 examples : 66.5 eps, Loss: 3.317, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 2737]: seen 273700 examples : 66.5 eps, Loss: 3.412, Avg loss: 3.362, Best loss: 3.338, cov loss: 0.159\n",
      "    [batch 2744]: seen 274400 examples : 66.5 eps, Loss: 3.448, Avg loss: 3.365, Best loss: 3.338, cov loss: 0.175\n",
      "    [batch 2751]: seen 275100 examples : 66.5 eps, Loss: 3.414, Avg loss: 3.361, Best loss: 3.338, cov loss: 0.155\n",
      "    [batch 2758]: seen 275800 examples : 66.5 eps, Loss: 3.447, Avg loss: 3.366, Best loss: 3.338, cov loss: 0.174\n",
      "    [batch 2765]: seen 276500 examples : 66.5 eps, Loss: 3.324, Avg loss: 3.371, Best loss: 3.338, cov loss: 0.168\n",
      "    [batch 2772]: seen 277200 examples : 66.5 eps, Loss: 3.497, Avg loss: 3.372, Best loss: 3.338, cov loss: 0.170\n",
      "    [batch 2779]: seen 277900 examples : 66.5 eps, Loss: 3.332, Avg loss: 3.371, Best loss: 3.338, cov loss: 0.167\n",
      "    [batch 2786]: seen 278600 examples : 66.6 eps, Loss: 3.493, Avg loss: 3.370, Best loss: 3.338, cov loss: 0.184\n",
      "    [batch 2793]: seen 279300 examples : 66.6 eps, Loss: 3.329, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.173\n",
      "    [batch 2800]: seen 280000 examples : 66.6 eps, Loss: 3.406, Avg loss: 3.367, Best loss: 3.338, cov loss: 0.150\n",
      "    [batch 2807]: seen 280700 examples : 66.6 eps, Loss: 3.463, Avg loss: 3.372, Best loss: 3.338, cov loss: 0.171\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:10:16\n",
      "[EPOCH 22] Complete. Avg Loss: 3.371766867781611; Best Loss: 3.337834712368996\n",
      "[END] Training complete: Best Loss: 3.337834712368996; Total time: 7:05:05\n"
     ]
    }
   ],
   "source": [
    "avg_loss = 3.6\n",
    "best_loss = 3.6\n",
    "curr_best = best_loss\n",
    "train_step = 26879\n",
    "epochs = 6\n",
    "restore = True\n",
    "epoch_start = 16\n",
    "\n",
    "train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train session 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 252\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 110\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 2 1/2 76\n",
      "\n",
      "\n",
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: 16:03\n",
      "INFO:tensorflow:Fetching data..\n",
      "INFO:tensorflow:Creating batches..\n",
      "INFO:tensorflow:[TOTAL Batches]  : 2808\n",
      "INFO:tensorflow:[TOTAL Examples] : 280778\n",
      "INFO:tensorflow:Creating batches..COMPLETE\n",
      "INFO:tensorflow:Building core graph...\n",
      "INFO:tensorflow:Adding attention_decoder timestep 0 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 1 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 2 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 3 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 4 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 5 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 6 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 7 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 8 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 9 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 10 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 11 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 12 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 13 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 14 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 15 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 16 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 17 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 18 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 19 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 20 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 21 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 22 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 23 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 24 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 25 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 26 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 27 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 28 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 29 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 30 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 31 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 32 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 33 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 34 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 35 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 36 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 37 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 38 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 39 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 40 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 41 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 42 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 43 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 44 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 45 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 46 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 47 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 48 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 49 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 50 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 51 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 52 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 53 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 54 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 55 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 56 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 57 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 58 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 59 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 60 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 61 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 62 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 63 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 64 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 65 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 66 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 67 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 68 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 69 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 70 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 71 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 72 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 73 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 74 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 75 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 76 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 77 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 78 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 79 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 80 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 81 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 82 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 83 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 84 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 85 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 86 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 87 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 88 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 89 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 90 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 91 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 92 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 93 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 94 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 95 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 96 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 97 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 98 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 99 of 100\n",
      "INFO:tensorflow:Building projection graph...\n",
      "INFO:tensorflow:Building projection graph...COMPLETE\n",
      "INFO:tensorflow:Building Loss graph...\n",
      "INFO:tensorflow:Building Loss graph...COMPLETE\n",
      "INFO:tensorflow:Building core graph...COMPLETE\n",
      "INFO:tensorflow:Building train graph...\n",
      "INFO:tensorflow:Building train graph...COMPLETE\n",
      "INFO:tensorflow:Building summary graph...\n",
      "INFO:tensorflow:Building summary graph...COMPLETE\n",
      "WARNING:tensorflow:From <ipython-input-5-17e3ed376236>:18: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-38874\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-38874\n",
      "[EPOCH 23] Starting training..\n",
      "    [batch 1]: seen 100 examples : 3.4 eps, Loss: 3.398, Avg loss: 3.371, Best loss: 3.337, cov loss: 0.155\n",
      "    [batch 8]: seen 800 examples : 20.1 eps, Loss: 3.463, Avg loss: 3.369, Best loss: 3.337, cov loss: 0.176\n",
      "    [batch 15]: seen 1500 examples : 29.9 eps, Loss: 3.397, Avg loss: 3.371, Best loss: 3.337, cov loss: 0.167\n",
      "    [batch 22]: seen 2200 examples : 36.5 eps, Loss: 3.517, Avg loss: 3.371, Best loss: 3.337, cov loss: 0.155\n",
      "    [batch 29]: seen 2900 examples : 41.2 eps, Loss: 3.287, Avg loss: 3.371, Best loss: 3.337, cov loss: 0.178\n",
      "    [batch 36]: seen 3600 examples : 44.6 eps, Loss: 3.415, Avg loss: 3.373, Best loss: 3.337, cov loss: 0.163\n",
      "    [batch 43]: seen 4300 examples : 47.3 eps, Loss: 3.377, Avg loss: 3.369, Best loss: 3.337, cov loss: 0.171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 50]: seen 5000 examples : 49.5 eps, Loss: 3.443, Avg loss: 3.371, Best loss: 3.337, cov loss: 0.175\n",
      "    [batch 57]: seen 5700 examples : 51.2 eps, Loss: 3.474, Avg loss: 3.373, Best loss: 3.337, cov loss: 0.181\n",
      "    [batch 64]: seen 6400 examples : 52.7 eps, Loss: 3.348, Avg loss: 3.377, Best loss: 3.337, cov loss: 0.158\n",
      "    [batch 71]: seen 7100 examples : 53.9 eps, Loss: 3.364, Avg loss: 3.374, Best loss: 3.337, cov loss: 0.168\n",
      "    [batch 78]: seen 7800 examples : 55.0 eps, Loss: 3.547, Avg loss: 3.374, Best loss: 3.337, cov loss: 0.162\n",
      "    [batch 85]: seen 8500 examples : 55.9 eps, Loss: 3.284, Avg loss: 3.376, Best loss: 3.337, cov loss: 0.168\n",
      "    [batch 92]: seen 9200 examples : 56.7 eps, Loss: 3.320, Avg loss: 3.376, Best loss: 3.337, cov loss: 0.166\n",
      "    [batch 99]: seen 9900 examples : 57.4 eps, Loss: 3.505, Avg loss: 3.377, Best loss: 3.337, cov loss: 0.176\n",
      "    [batch 106]: seen 10600 examples : 58.0 eps, Loss: 3.357, Avg loss: 3.377, Best loss: 3.337, cov loss: 0.176\n",
      "    [batch 113]: seen 11300 examples : 58.5 eps, Loss: 3.314, Avg loss: 3.374, Best loss: 3.337, cov loss: 0.190\n",
      "    [batch 120]: seen 12000 examples : 59.0 eps, Loss: 3.380, Avg loss: 3.372, Best loss: 3.337, cov loss: 0.166\n",
      "    [batch 127]: seen 12700 examples : 59.5 eps, Loss: 3.350, Avg loss: 3.367, Best loss: 3.337, cov loss: 0.181\n",
      "    [batch 134]: seen 13400 examples : 59.9 eps, Loss: 3.225, Avg loss: 3.364, Best loss: 3.337, cov loss: 0.157\n",
      "    [batch 141]: seen 14100 examples : 60.3 eps, Loss: 3.174, Avg loss: 3.365, Best loss: 3.337, cov loss: 0.170\n",
      "    [batch 148]: seen 14800 examples : 60.7 eps, Loss: 3.153, Avg loss: 3.365, Best loss: 3.337, cov loss: 0.146\n",
      "    [batch 155]: seen 15500 examples : 61.0 eps, Loss: 3.489, Avg loss: 3.372, Best loss: 3.337, cov loss: 0.163\n",
      "    [batch 162]: seen 16200 examples : 61.3 eps, Loss: 3.362, Avg loss: 3.370, Best loss: 3.337, cov loss: 0.164\n",
      "    [batch 169]: seen 16900 examples : 61.5 eps, Loss: 3.553, Avg loss: 3.371, Best loss: 3.337, cov loss: 0.181\n",
      "    [batch 176]: seen 17600 examples : 61.8 eps, Loss: 3.589, Avg loss: 3.377, Best loss: 3.337, cov loss: 0.193\n",
      "    [batch 183]: seen 18300 examples : 62.0 eps, Loss: 3.332, Avg loss: 3.375, Best loss: 3.337, cov loss: 0.170\n",
      "    [batch 190]: seen 19000 examples : 62.3 eps, Loss: 3.397, Avg loss: 3.375, Best loss: 3.337, cov loss: 0.180\n",
      "    [batch 197]: seen 19700 examples : 62.5 eps, Loss: 3.326, Avg loss: 3.376, Best loss: 3.337, cov loss: 0.187\n",
      "    [batch 204]: seen 20400 examples : 62.7 eps, Loss: 3.235, Avg loss: 3.370, Best loss: 3.337, cov loss: 0.154\n",
      "    [batch 211]: seen 21100 examples : 62.8 eps, Loss: 3.512, Avg loss: 3.369, Best loss: 3.337, cov loss: 0.176\n",
      "    [batch 218]: seen 21800 examples : 63.0 eps, Loss: 3.453, Avg loss: 3.368, Best loss: 3.337, cov loss: 0.165\n",
      "    [batch 225]: seen 22500 examples : 63.2 eps, Loss: 3.357, Avg loss: 3.371, Best loss: 3.337, cov loss: 0.159\n",
      "    [batch 232]: seen 23200 examples : 63.3 eps, Loss: 3.425, Avg loss: 3.378, Best loss: 3.337, cov loss: 0.181\n",
      "    [batch 239]: seen 23900 examples : 63.5 eps, Loss: 3.410, Avg loss: 3.376, Best loss: 3.337, cov loss: 0.179\n",
      "    [batch 246]: seen 24600 examples : 63.6 eps, Loss: 3.330, Avg loss: 3.372, Best loss: 3.337, cov loss: 0.159\n",
      "    [batch 253]: seen 25300 examples : 63.7 eps, Loss: 3.535, Avg loss: 3.370, Best loss: 3.337, cov loss: 0.164\n",
      "    [batch 260]: seen 26000 examples : 63.9 eps, Loss: 3.530, Avg loss: 3.368, Best loss: 3.337, cov loss: 0.178\n",
      "    [batch 267]: seen 26700 examples : 64.0 eps, Loss: 3.347, Avg loss: 3.369, Best loss: 3.337, cov loss: 0.170\n",
      "    [batch 274]: seen 27400 examples : 64.1 eps, Loss: 3.448, Avg loss: 3.374, Best loss: 3.337, cov loss: 0.180\n",
      "    [batch 281]: seen 28100 examples : 64.2 eps, Loss: 3.455, Avg loss: 3.371, Best loss: 3.337, cov loss: 0.178\n",
      "    [batch 288]: seen 28800 examples : 64.3 eps, Loss: 3.352, Avg loss: 3.372, Best loss: 3.337, cov loss: 0.156\n",
      "    [batch 295]: seen 29500 examples : 64.4 eps, Loss: 3.273, Avg loss: 3.372, Best loss: 3.337, cov loss: 0.158\n",
      "    [batch 302]: seen 30200 examples : 64.5 eps, Loss: 3.421, Avg loss: 3.372, Best loss: 3.337, cov loss: 0.190\n",
      "    [batch 309]: seen 30900 examples : 64.6 eps, Loss: 3.409, Avg loss: 3.370, Best loss: 3.337, cov loss: 0.169\n",
      "    [batch 316]: seen 31600 examples : 64.7 eps, Loss: 3.359, Avg loss: 3.367, Best loss: 3.337, cov loss: 0.150\n",
      "    [batch 323]: seen 32300 examples : 64.7 eps, Loss: 3.195, Avg loss: 3.364, Best loss: 3.337, cov loss: 0.166\n",
      "    [batch 330]: seen 33000 examples : 64.8 eps, Loss: 3.304, Avg loss: 3.361, Best loss: 3.337, cov loss: 0.169\n",
      "    [batch 337]: seen 33700 examples : 64.9 eps, Loss: 3.232, Avg loss: 3.357, Best loss: 3.337, cov loss: 0.166\n",
      "    [batch 344]: seen 34400 examples : 65.0 eps, Loss: 3.352, Avg loss: 3.356, Best loss: 3.337, cov loss: 0.177\n",
      "    [batch 351]: seen 35100 examples : 65.0 eps, Loss: 3.244, Avg loss: 3.354, Best loss: 3.337, cov loss: 0.168\n",
      "    [batch 358]: seen 35800 examples : 65.1 eps, Loss: 3.326, Avg loss: 3.358, Best loss: 3.337, cov loss: 0.152\n",
      "    [batch 365]: seen 36500 examples : 65.2 eps, Loss: 3.184, Avg loss: 3.359, Best loss: 3.337, cov loss: 0.153\n",
      "    [batch 372]: seen 37200 examples : 65.2 eps, Loss: 3.403, Avg loss: 3.361, Best loss: 3.337, cov loss: 0.165\n",
      "    [batch 379]: seen 37900 examples : 65.3 eps, Loss: 3.076, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.168\n",
      "    [batch 386]: seen 38600 examples : 65.4 eps, Loss: 3.399, Avg loss: 3.364, Best loss: 3.337, cov loss: 0.181\n",
      "    [batch 393]: seen 39300 examples : 65.4 eps, Loss: 3.424, Avg loss: 3.362, Best loss: 3.337, cov loss: 0.169\n",
      "    [batch 400]: seen 40000 examples : 65.5 eps, Loss: 3.378, Avg loss: 3.363, Best loss: 3.337, cov loss: 0.155\n",
      "    [batch 407]: seen 40700 examples : 65.5 eps, Loss: 3.265, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.158\n",
      "    [batch 414]: seen 41400 examples : 65.6 eps, Loss: 3.314, Avg loss: 3.359, Best loss: 3.337, cov loss: 0.169\n",
      "    [batch 421]: seen 42100 examples : 65.6 eps, Loss: 3.462, Avg loss: 3.361, Best loss: 3.337, cov loss: 0.171\n",
      "    [batch 428]: seen 42800 examples : 65.7 eps, Loss: 3.273, Avg loss: 3.362, Best loss: 3.337, cov loss: 0.186\n",
      "    [batch 435]: seen 43500 examples : 65.7 eps, Loss: 3.555, Avg loss: 3.359, Best loss: 3.337, cov loss: 0.163\n",
      "    [batch 442]: seen 44200 examples : 65.8 eps, Loss: 3.358, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.183\n",
      "    [batch 449]: seen 44900 examples : 65.8 eps, Loss: 3.136, Avg loss: 3.355, Best loss: 3.337, cov loss: 0.160\n",
      "    [batch 456]: seen 45600 examples : 65.9 eps, Loss: 3.432, Avg loss: 3.358, Best loss: 3.337, cov loss: 0.156\n",
      "    [batch 463]: seen 46300 examples : 65.9 eps, Loss: 3.236, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.169\n",
      "    [batch 470]: seen 47000 examples : 65.9 eps, Loss: 3.444, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.168\n",
      "    [batch 477]: seen 47700 examples : 66.0 eps, Loss: 3.597, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.181\n",
      "    [batch 484]: seen 48400 examples : 66.0 eps, Loss: 3.467, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.177\n",
      "    [batch 491]: seen 49100 examples : 66.1 eps, Loss: 3.463, Avg loss: 3.351, Best loss: 3.337, cov loss: 0.173\n",
      "    [batch 498]: seen 49800 examples : 66.1 eps, Loss: 3.228, Avg loss: 3.350, Best loss: 3.337, cov loss: 0.166\n",
      "    [batch 505]: seen 50500 examples : 66.1 eps, Loss: 3.366, Avg loss: 3.349, Best loss: 3.337, cov loss: 0.159\n",
      "    [batch 512]: seen 51200 examples : 66.2 eps, Loss: 3.249, Avg loss: 3.345, Best loss: 3.337, cov loss: 0.167\n",
      "    [batch 519]: seen 51900 examples : 66.2 eps, Loss: 3.368, Avg loss: 3.348, Best loss: 3.337, cov loss: 0.171\n",
      "    [batch 526]: seen 52600 examples : 66.2 eps, Loss: 3.522, Avg loss: 3.350, Best loss: 3.337, cov loss: 0.163\n",
      "    [batch 533]: seen 53300 examples : 66.3 eps, Loss: 3.443, Avg loss: 3.348, Best loss: 3.337, cov loss: 0.173\n",
      "    [batch 540]: seen 54000 examples : 66.3 eps, Loss: 3.386, Avg loss: 3.352, Best loss: 3.337, cov loss: 0.177\n",
      "    [batch 547]: seen 54700 examples : 66.3 eps, Loss: 3.404, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.177\n",
      "    [batch 554]: seen 55400 examples : 66.3 eps, Loss: 3.526, Avg loss: 3.355, Best loss: 3.337, cov loss: 0.187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 561]: seen 56100 examples : 66.4 eps, Loss: 3.690, Avg loss: 3.361, Best loss: 3.337, cov loss: 0.186\n",
      "    [batch 568]: seen 56800 examples : 66.4 eps, Loss: 3.458, Avg loss: 3.363, Best loss: 3.337, cov loss: 0.176\n",
      "    [batch 575]: seen 57500 examples : 66.4 eps, Loss: 3.292, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.178\n",
      "    [batch 582]: seen 58200 examples : 66.5 eps, Loss: 3.286, Avg loss: 3.361, Best loss: 3.337, cov loss: 0.150\n",
      "    [batch 589]: seen 58900 examples : 66.5 eps, Loss: 3.353, Avg loss: 3.359, Best loss: 3.337, cov loss: 0.173\n",
      "    [batch 596]: seen 59600 examples : 66.5 eps, Loss: 3.601, Avg loss: 3.357, Best loss: 3.337, cov loss: 0.177\n",
      "    [batch 603]: seen 60300 examples : 66.5 eps, Loss: 3.497, Avg loss: 3.354, Best loss: 3.337, cov loss: 0.171\n",
      "    [batch 610]: seen 61000 examples : 66.6 eps, Loss: 3.341, Avg loss: 3.357, Best loss: 3.337, cov loss: 0.171\n",
      "    [batch 617]: seen 61700 examples : 66.6 eps, Loss: 3.290, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.161\n",
      "    [batch 624]: seen 62400 examples : 66.6 eps, Loss: 3.389, Avg loss: 3.351, Best loss: 3.337, cov loss: 0.148\n",
      "    [batch 631]: seen 63100 examples : 66.6 eps, Loss: 3.455, Avg loss: 3.351, Best loss: 3.337, cov loss: 0.170\n",
      "    [batch 638]: seen 63800 examples : 66.7 eps, Loss: 3.496, Avg loss: 3.354, Best loss: 3.337, cov loss: 0.177\n",
      "    [batch 645]: seen 64500 examples : 66.7 eps, Loss: 3.211, Avg loss: 3.358, Best loss: 3.337, cov loss: 0.156\n",
      "    [batch 652]: seen 65200 examples : 66.7 eps, Loss: 3.238, Avg loss: 3.359, Best loss: 3.337, cov loss: 0.156\n",
      "    [batch 659]: seen 65900 examples : 66.7 eps, Loss: 3.481, Avg loss: 3.359, Best loss: 3.337, cov loss: 0.168\n",
      "    [batch 666]: seen 66600 examples : 66.7 eps, Loss: 3.266, Avg loss: 3.356, Best loss: 3.337, cov loss: 0.160\n",
      "    [batch 673]: seen 67300 examples : 66.8 eps, Loss: 3.454, Avg loss: 3.358, Best loss: 3.337, cov loss: 0.180\n",
      "    [batch 680]: seen 68000 examples : 66.8 eps, Loss: 3.278, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.171\n",
      "    [batch 687]: seen 68700 examples : 66.8 eps, Loss: 3.363, Avg loss: 3.356, Best loss: 3.337, cov loss: 0.181\n",
      "    [batch 694]: seen 69400 examples : 66.8 eps, Loss: 3.353, Avg loss: 3.358, Best loss: 3.337, cov loss: 0.159\n",
      "    [batch 701]: seen 70100 examples : 66.8 eps, Loss: 3.306, Avg loss: 3.364, Best loss: 3.337, cov loss: 0.154\n",
      "    [batch 708]: seen 70800 examples : 66.9 eps, Loss: 3.286, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.167\n",
      "    [batch 715]: seen 71500 examples : 66.9 eps, Loss: 3.475, Avg loss: 3.362, Best loss: 3.337, cov loss: 0.159\n",
      "    [batch 722]: seen 72200 examples : 66.9 eps, Loss: 3.459, Avg loss: 3.363, Best loss: 3.337, cov loss: 0.170\n",
      "    [batch 729]: seen 72900 examples : 66.9 eps, Loss: 3.212, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.181\n",
      "    [batch 736]: seen 73600 examples : 66.9 eps, Loss: 3.413, Avg loss: 3.364, Best loss: 3.337, cov loss: 0.179\n",
      "    [batch 743]: seen 74300 examples : 67.0 eps, Loss: 3.303, Avg loss: 3.361, Best loss: 3.337, cov loss: 0.160\n",
      "    [batch 750]: seen 75000 examples : 67.0 eps, Loss: 3.336, Avg loss: 3.363, Best loss: 3.337, cov loss: 0.165\n",
      "    [batch 757]: seen 75700 examples : 67.0 eps, Loss: 3.467, Avg loss: 3.365, Best loss: 3.337, cov loss: 0.177\n",
      "    [batch 764]: seen 76400 examples : 67.0 eps, Loss: 3.431, Avg loss: 3.362, Best loss: 3.337, cov loss: 0.162\n",
      "    [batch 771]: seen 77100 examples : 67.0 eps, Loss: 3.512, Avg loss: 3.361, Best loss: 3.337, cov loss: 0.168\n",
      "    [batch 778]: seen 77800 examples : 67.0 eps, Loss: 3.381, Avg loss: 3.362, Best loss: 3.337, cov loss: 0.164\n",
      "    [batch 785]: seen 78500 examples : 67.1 eps, Loss: 3.340, Avg loss: 3.364, Best loss: 3.337, cov loss: 0.170\n",
      "    [batch 792]: seen 79200 examples : 67.1 eps, Loss: 3.407, Avg loss: 3.365, Best loss: 3.337, cov loss: 0.175\n",
      "    [batch 799]: seen 79900 examples : 67.1 eps, Loss: 3.242, Avg loss: 3.365, Best loss: 3.337, cov loss: 0.158\n",
      "    [batch 806]: seen 80600 examples : 67.1 eps, Loss: 3.394, Avg loss: 3.364, Best loss: 3.337, cov loss: 0.165\n",
      "    [batch 813]: seen 81300 examples : 67.1 eps, Loss: 3.323, Avg loss: 3.366, Best loss: 3.337, cov loss: 0.184\n",
      "    [batch 820]: seen 82000 examples : 67.1 eps, Loss: 3.312, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.179\n",
      "    [batch 827]: seen 82700 examples : 67.1 eps, Loss: 3.085, Avg loss: 3.359, Best loss: 3.337, cov loss: 0.150\n",
      "    [batch 834]: seen 83400 examples : 67.2 eps, Loss: 3.336, Avg loss: 3.364, Best loss: 3.337, cov loss: 0.174\n",
      "    [batch 841]: seen 84100 examples : 67.2 eps, Loss: 3.237, Avg loss: 3.362, Best loss: 3.337, cov loss: 0.162\n",
      "    [batch 848]: seen 84800 examples : 67.2 eps, Loss: 3.288, Avg loss: 3.364, Best loss: 3.337, cov loss: 0.173\n",
      "    [batch 855]: seen 85500 examples : 67.2 eps, Loss: 3.553, Avg loss: 3.370, Best loss: 3.337, cov loss: 0.176\n",
      "    [batch 862]: seen 86200 examples : 67.2 eps, Loss: 3.216, Avg loss: 3.369, Best loss: 3.337, cov loss: 0.181\n",
      "    [batch 869]: seen 86900 examples : 67.2 eps, Loss: 3.241, Avg loss: 3.370, Best loss: 3.337, cov loss: 0.169\n",
      "    [batch 876]: seen 87600 examples : 67.2 eps, Loss: 3.320, Avg loss: 3.366, Best loss: 3.337, cov loss: 0.174\n",
      "    [batch 883]: seen 88300 examples : 67.3 eps, Loss: 3.561, Avg loss: 3.365, Best loss: 3.337, cov loss: 0.162\n",
      "    [batch 890]: seen 89000 examples : 67.3 eps, Loss: 3.338, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.179\n",
      "    [batch 897]: seen 89700 examples : 67.3 eps, Loss: 3.571, Avg loss: 3.357, Best loss: 3.337, cov loss: 0.175\n",
      "    [batch 904]: seen 90400 examples : 67.3 eps, Loss: 3.507, Avg loss: 3.357, Best loss: 3.337, cov loss: 0.180\n",
      "    [batch 911]: seen 91100 examples : 67.3 eps, Loss: 3.072, Avg loss: 3.354, Best loss: 3.337, cov loss: 0.151\n",
      "    [batch 918]: seen 91800 examples : 67.3 eps, Loss: 3.381, Avg loss: 3.349, Best loss: 3.337, cov loss: 0.162\n",
      "    [batch 925]: seen 92500 examples : 67.3 eps, Loss: 3.208, Avg loss: 3.347, Best loss: 3.337, cov loss: 0.157\n",
      "    [batch 932]: seen 93200 examples : 67.3 eps, Loss: 3.360, Avg loss: 3.349, Best loss: 3.337, cov loss: 0.166\n",
      "    [batch 939]: seen 93900 examples : 67.3 eps, Loss: 3.317, Avg loss: 3.348, Best loss: 3.337, cov loss: 0.159\n",
      "    [batch 946]: seen 94600 examples : 67.4 eps, Loss: 3.460, Avg loss: 3.352, Best loss: 3.337, cov loss: 0.196\n",
      "    [batch 953]: seen 95300 examples : 67.4 eps, Loss: 3.340, Avg loss: 3.346, Best loss: 3.337, cov loss: 0.163\n",
      "    [batch 960]: seen 96000 examples : 67.4 eps, Loss: 3.416, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.163\n",
      "    [batch 967]: seen 96700 examples : 67.4 eps, Loss: 3.325, Avg loss: 3.355, Best loss: 3.337, cov loss: 0.158\n",
      "    [batch 974]: seen 97400 examples : 67.4 eps, Loss: 3.409, Avg loss: 3.359, Best loss: 3.337, cov loss: 0.182\n",
      "    [batch 981]: seen 98100 examples : 67.4 eps, Loss: 3.400, Avg loss: 3.361, Best loss: 3.337, cov loss: 0.164\n",
      "    [batch 988]: seen 98800 examples : 67.4 eps, Loss: 3.185, Avg loss: 3.365, Best loss: 3.337, cov loss: 0.154\n",
      "    [batch 995]: seen 99500 examples : 67.4 eps, Loss: 3.323, Avg loss: 3.367, Best loss: 3.337, cov loss: 0.177\n",
      "    [batch 1002]: seen 100200 examples : 67.4 eps, Loss: 3.342, Avg loss: 3.367, Best loss: 3.337, cov loss: 0.170\n",
      "    [batch 1009]: seen 100900 examples : 67.5 eps, Loss: 3.166, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.169\n",
      "    [batch 1016]: seen 101600 examples : 67.5 eps, Loss: 3.248, Avg loss: 3.358, Best loss: 3.337, cov loss: 0.150\n",
      "    [batch 1023]: seen 102300 examples : 67.5 eps, Loss: 3.418, Avg loss: 3.358, Best loss: 3.337, cov loss: 0.165\n",
      "    [batch 1030]: seen 103000 examples : 67.5 eps, Loss: 3.380, Avg loss: 3.358, Best loss: 3.337, cov loss: 0.173\n",
      "    [batch 1037]: seen 103700 examples : 67.5 eps, Loss: 3.376, Avg loss: 3.362, Best loss: 3.337, cov loss: 0.175\n",
      "    [batch 1044]: seen 104400 examples : 67.5 eps, Loss: 3.332, Avg loss: 3.363, Best loss: 3.337, cov loss: 0.159\n",
      "    [batch 1051]: seen 105100 examples : 67.5 eps, Loss: 3.339, Avg loss: 3.365, Best loss: 3.337, cov loss: 0.152\n",
      "    [batch 1058]: seen 105800 examples : 67.5 eps, Loss: 3.438, Avg loss: 3.367, Best loss: 3.337, cov loss: 0.182\n",
      "    [batch 1065]: seen 106500 examples : 67.5 eps, Loss: 3.455, Avg loss: 3.367, Best loss: 3.337, cov loss: 0.166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1072]: seen 107200 examples : 67.5 eps, Loss: 3.390, Avg loss: 3.364, Best loss: 3.337, cov loss: 0.181\n",
      "    [batch 1079]: seen 107900 examples : 67.5 eps, Loss: 3.328, Avg loss: 3.368, Best loss: 3.337, cov loss: 0.163\n",
      "    [batch 1086]: seen 108600 examples : 67.6 eps, Loss: 3.336, Avg loss: 3.365, Best loss: 3.337, cov loss: 0.172\n",
      "    [batch 1093]: seen 109300 examples : 67.6 eps, Loss: 3.418, Avg loss: 3.364, Best loss: 3.337, cov loss: 0.168\n",
      "    [batch 1100]: seen 110000 examples : 67.6 eps, Loss: 3.531, Avg loss: 3.362, Best loss: 3.337, cov loss: 0.186\n",
      "    [batch 1107]: seen 110700 examples : 67.6 eps, Loss: 3.394, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.188\n",
      "    [batch 1114]: seen 111400 examples : 67.6 eps, Loss: 3.194, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.153\n",
      "    [batch 1121]: seen 112100 examples : 67.6 eps, Loss: 3.441, Avg loss: 3.358, Best loss: 3.337, cov loss: 0.166\n",
      "    [batch 1128]: seen 112800 examples : 67.6 eps, Loss: 3.285, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.160\n",
      "    [batch 1135]: seen 113500 examples : 67.6 eps, Loss: 3.160, Avg loss: 3.362, Best loss: 3.337, cov loss: 0.158\n",
      "    [batch 1142]: seen 114200 examples : 67.6 eps, Loss: 3.261, Avg loss: 3.359, Best loss: 3.337, cov loss: 0.171\n",
      "    [batch 1149]: seen 114900 examples : 67.6 eps, Loss: 3.378, Avg loss: 3.360, Best loss: 3.337, cov loss: 0.173\n",
      "    [batch 1156]: seen 115600 examples : 67.6 eps, Loss: 3.341, Avg loss: 3.359, Best loss: 3.337, cov loss: 0.170\n",
      "    [batch 1163]: seen 116300 examples : 67.6 eps, Loss: 3.327, Avg loss: 3.356, Best loss: 3.337, cov loss: 0.147\n",
      "    [batch 1170]: seen 117000 examples : 67.7 eps, Loss: 3.286, Avg loss: 3.357, Best loss: 3.337, cov loss: 0.168\n",
      "    [batch 1177]: seen 117700 examples : 67.7 eps, Loss: 3.297, Avg loss: 3.352, Best loss: 3.337, cov loss: 0.182\n",
      "    [batch 1184]: seen 118400 examples : 67.7 eps, Loss: 3.395, Avg loss: 3.349, Best loss: 3.337, cov loss: 0.157\n",
      "    [batch 1191]: seen 119100 examples : 67.7 eps, Loss: 3.424, Avg loss: 3.348, Best loss: 3.337, cov loss: 0.163\n",
      "    [batch 1198]: seen 119800 examples : 67.7 eps, Loss: 3.437, Avg loss: 3.349, Best loss: 3.337, cov loss: 0.186\n",
      "    [batch 1205]: seen 120500 examples : 67.7 eps, Loss: 3.399, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.201\n",
      "    [batch 1212]: seen 121200 examples : 67.7 eps, Loss: 3.436, Avg loss: 3.351, Best loss: 3.337, cov loss: 0.171\n",
      "    [batch 1219]: seen 121900 examples : 67.7 eps, Loss: 3.289, Avg loss: 3.351, Best loss: 3.337, cov loss: 0.159\n",
      "    [batch 1226]: seen 122600 examples : 67.7 eps, Loss: 3.387, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.173\n",
      "    [batch 1233]: seen 123300 examples : 67.7 eps, Loss: 3.445, Avg loss: 3.356, Best loss: 3.337, cov loss: 0.167\n",
      "    [batch 1240]: seen 124000 examples : 67.7 eps, Loss: 3.368, Avg loss: 3.356, Best loss: 3.337, cov loss: 0.153\n",
      "    [batch 1247]: seen 124700 examples : 67.7 eps, Loss: 3.333, Avg loss: 3.359, Best loss: 3.337, cov loss: 0.171\n",
      "    [batch 1254]: seen 125400 examples : 67.7 eps, Loss: 3.270, Avg loss: 3.358, Best loss: 3.337, cov loss: 0.153\n",
      "    [batch 1261]: seen 126100 examples : 67.7 eps, Loss: 3.311, Avg loss: 3.355, Best loss: 3.337, cov loss: 0.151\n",
      "    [batch 1268]: seen 126800 examples : 67.8 eps, Loss: 3.155, Avg loss: 3.350, Best loss: 3.337, cov loss: 0.168\n",
      "    [batch 1275]: seen 127500 examples : 67.8 eps, Loss: 3.484, Avg loss: 3.350, Best loss: 3.337, cov loss: 0.165\n",
      "    [batch 1282]: seen 128200 examples : 67.8 eps, Loss: 3.289, Avg loss: 3.354, Best loss: 3.337, cov loss: 0.169\n",
      "    [batch 1289]: seen 128900 examples : 67.8 eps, Loss: 3.288, Avg loss: 3.348, Best loss: 3.337, cov loss: 0.163\n",
      "    [batch 1296]: seen 129600 examples : 67.8 eps, Loss: 3.335, Avg loss: 3.345, Best loss: 3.337, cov loss: 0.177\n",
      "    [batch 1303]: seen 130300 examples : 67.8 eps, Loss: 3.325, Avg loss: 3.339, Best loss: 3.337, cov loss: 0.168\n",
      "    [batch 1310]: seen 131000 examples : 67.8 eps, Loss: 3.270, Avg loss: 3.340, Best loss: 3.337, cov loss: 0.161\n",
      "    [batch 1317]: seen 131700 examples : 67.8 eps, Loss: 3.402, Avg loss: 3.348, Best loss: 3.337, cov loss: 0.154\n",
      "    [batch 1324]: seen 132400 examples : 67.8 eps, Loss: 3.380, Avg loss: 3.347, Best loss: 3.337, cov loss: 0.159\n",
      "    [batch 1331]: seen 133100 examples : 67.8 eps, Loss: 3.325, Avg loss: 3.349, Best loss: 3.337, cov loss: 0.170\n",
      "    [batch 1338]: seen 133800 examples : 67.8 eps, Loss: 3.483, Avg loss: 3.350, Best loss: 3.337, cov loss: 0.167\n",
      "    [batch 1345]: seen 134500 examples : 67.8 eps, Loss: 3.455, Avg loss: 3.352, Best loss: 3.337, cov loss: 0.190\n",
      "    [batch 1352]: seen 135200 examples : 67.8 eps, Loss: 3.351, Avg loss: 3.352, Best loss: 3.337, cov loss: 0.166\n",
      "    [batch 1359]: seen 135900 examples : 67.8 eps, Loss: 3.240, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.177\n",
      "    [batch 1366]: seen 136600 examples : 67.8 eps, Loss: 3.503, Avg loss: 3.355, Best loss: 3.337, cov loss: 0.164\n",
      "    [batch 1373]: seen 137300 examples : 67.9 eps, Loss: 3.295, Avg loss: 3.355, Best loss: 3.337, cov loss: 0.162\n",
      "    [batch 1380]: seen 138000 examples : 67.9 eps, Loss: 3.259, Avg loss: 3.354, Best loss: 3.337, cov loss: 0.163\n",
      "    [batch 1387]: seen 138700 examples : 67.9 eps, Loss: 3.341, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.162\n",
      "    [batch 1394]: seen 139400 examples : 67.9 eps, Loss: 3.342, Avg loss: 3.350, Best loss: 3.337, cov loss: 0.157\n",
      "    [batch 1401]: seen 140100 examples : 67.9 eps, Loss: 3.392, Avg loss: 3.347, Best loss: 3.337, cov loss: 0.172\n",
      "    [batch 1408]: seen 140800 examples : 67.9 eps, Loss: 3.509, Avg loss: 3.346, Best loss: 3.337, cov loss: 0.171\n",
      "    [batch 1415]: seen 141500 examples : 67.9 eps, Loss: 3.425, Avg loss: 3.346, Best loss: 3.337, cov loss: 0.176\n",
      "    [batch 1422]: seen 142200 examples : 67.9 eps, Loss: 3.380, Avg loss: 3.349, Best loss: 3.337, cov loss: 0.170\n",
      "    [batch 1429]: seen 142900 examples : 67.9 eps, Loss: 3.302, Avg loss: 3.344, Best loss: 3.337, cov loss: 0.169\n",
      "    [batch 1436]: seen 143600 examples : 67.9 eps, Loss: 3.409, Avg loss: 3.348, Best loss: 3.337, cov loss: 0.156\n",
      "    [batch 1443]: seen 144300 examples : 67.9 eps, Loss: 3.397, Avg loss: 3.352, Best loss: 3.337, cov loss: 0.155\n",
      "    [batch 1450]: seen 145000 examples : 67.9 eps, Loss: 3.348, Avg loss: 3.355, Best loss: 3.337, cov loss: 0.171\n",
      "    [batch 1457]: seen 145700 examples : 67.9 eps, Loss: 3.344, Avg loss: 3.355, Best loss: 3.337, cov loss: 0.155\n",
      "    [batch 1464]: seen 146400 examples : 67.9 eps, Loss: 3.282, Avg loss: 3.356, Best loss: 3.337, cov loss: 0.180\n",
      "    [batch 1471]: seen 147100 examples : 67.9 eps, Loss: 3.294, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.174\n",
      "    [batch 1478]: seen 147800 examples : 67.9 eps, Loss: 3.423, Avg loss: 3.353, Best loss: 3.337, cov loss: 0.174\n",
      "    [batch 1485]: seen 148500 examples : 67.9 eps, Loss: 3.519, Avg loss: 3.354, Best loss: 3.337, cov loss: 0.168\n",
      "    [batch 1492]: seen 149200 examples : 67.9 eps, Loss: 3.086, Avg loss: 3.350, Best loss: 3.337, cov loss: 0.155\n",
      "    [batch 1499]: seen 149900 examples : 67.9 eps, Loss: 3.286, Avg loss: 3.349, Best loss: 3.337, cov loss: 0.171\n",
      "    [batch 1506]: seen 150600 examples : 68.0 eps, Loss: 3.051, Avg loss: 3.345, Best loss: 3.337, cov loss: 0.163\n",
      "    [batch 1513]: seen 151300 examples : 68.0 eps, Loss: 3.211, Avg loss: 3.347, Best loss: 3.337, cov loss: 0.163\n",
      "    [batch 1520]: seen 152000 examples : 68.0 eps, Loss: 3.275, Avg loss: 3.346, Best loss: 3.337, cov loss: 0.170\n",
      "    [batch 1527]: seen 152700 examples : 68.0 eps, Loss: 3.425, Avg loss: 3.343, Best loss: 3.337, cov loss: 0.183\n",
      "    [batch 1534]: seen 153400 examples : 68.0 eps, Loss: 3.083, Avg loss: 3.339, Best loss: 3.337, cov loss: 0.158\n",
      "    [batch 1538]: seen 153800 examples : 67.8 eps, Loss: 3.232, Avg loss: 3.336, Best loss: 3.336, cov loss: 0.169\n",
      "    [batch 1541]: seen 154100 examples : 67.6 eps, Loss: 3.283, Avg loss: 3.336, Best loss: 3.336, cov loss: 0.175\n",
      "    [batch 1548]: seen 154800 examples : 67.7 eps, Loss: 3.308, Avg loss: 3.339, Best loss: 3.336, cov loss: 0.148\n",
      "    [batch 1555]: seen 155500 examples : 67.7 eps, Loss: 3.298, Avg loss: 3.337, Best loss: 3.336, cov loss: 0.170\n",
      "    [batch 1562]: seen 156200 examples : 67.7 eps, Loss: 3.215, Avg loss: 3.338, Best loss: 3.336, cov loss: 0.158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1569]: seen 156900 examples : 67.7 eps, Loss: 3.411, Avg loss: 3.348, Best loss: 3.336, cov loss: 0.180\n",
      "    [batch 1576]: seen 157600 examples : 67.7 eps, Loss: 3.357, Avg loss: 3.349, Best loss: 3.336, cov loss: 0.167\n",
      "    [batch 1583]: seen 158300 examples : 67.7 eps, Loss: 3.334, Avg loss: 3.348, Best loss: 3.336, cov loss: 0.177\n",
      "    [batch 1590]: seen 159000 examples : 67.7 eps, Loss: 3.342, Avg loss: 3.347, Best loss: 3.336, cov loss: 0.162\n",
      "    [batch 1597]: seen 159700 examples : 67.7 eps, Loss: 3.576, Avg loss: 3.352, Best loss: 3.336, cov loss: 0.173\n",
      "    [batch 1604]: seen 160400 examples : 67.7 eps, Loss: 3.175, Avg loss: 3.349, Best loss: 3.336, cov loss: 0.170\n",
      "    [batch 1611]: seen 161100 examples : 67.7 eps, Loss: 3.412, Avg loss: 3.352, Best loss: 3.336, cov loss: 0.162\n",
      "    [batch 1618]: seen 161800 examples : 67.7 eps, Loss: 3.545, Avg loss: 3.357, Best loss: 3.336, cov loss: 0.169\n",
      "    [batch 1625]: seen 162500 examples : 67.7 eps, Loss: 3.269, Avg loss: 3.358, Best loss: 3.336, cov loss: 0.174\n",
      "    [batch 1632]: seen 163200 examples : 67.7 eps, Loss: 3.392, Avg loss: 3.363, Best loss: 3.336, cov loss: 0.166\n",
      "    [batch 1639]: seen 163900 examples : 67.7 eps, Loss: 3.406, Avg loss: 3.362, Best loss: 3.336, cov loss: 0.169\n",
      "    [batch 1646]: seen 164600 examples : 67.7 eps, Loss: 3.390, Avg loss: 3.365, Best loss: 3.336, cov loss: 0.157\n",
      "    [batch 1653]: seen 165300 examples : 67.7 eps, Loss: 3.355, Avg loss: 3.363, Best loss: 3.336, cov loss: 0.164\n",
      "    [batch 1660]: seen 166000 examples : 67.7 eps, Loss: 3.375, Avg loss: 3.359, Best loss: 3.336, cov loss: 0.164\n",
      "    [batch 1667]: seen 166700 examples : 67.7 eps, Loss: 3.285, Avg loss: 3.359, Best loss: 3.336, cov loss: 0.180\n",
      "    [batch 1674]: seen 167400 examples : 67.8 eps, Loss: 3.159, Avg loss: 3.349, Best loss: 3.336, cov loss: 0.186\n",
      "    [batch 1681]: seen 168100 examples : 67.8 eps, Loss: 3.403, Avg loss: 3.351, Best loss: 3.336, cov loss: 0.156\n",
      "    [batch 1688]: seen 168800 examples : 67.8 eps, Loss: 3.157, Avg loss: 3.351, Best loss: 3.336, cov loss: 0.148\n",
      "    [batch 1695]: seen 169500 examples : 67.8 eps, Loss: 3.273, Avg loss: 3.346, Best loss: 3.336, cov loss: 0.178\n",
      "    [batch 1702]: seen 170200 examples : 67.8 eps, Loss: 3.356, Avg loss: 3.349, Best loss: 3.336, cov loss: 0.181\n",
      "    [batch 1709]: seen 170900 examples : 67.8 eps, Loss: 3.275, Avg loss: 3.345, Best loss: 3.336, cov loss: 0.162\n",
      "    [batch 1716]: seen 171600 examples : 67.8 eps, Loss: 3.320, Avg loss: 3.341, Best loss: 3.336, cov loss: 0.175\n",
      "    [batch 1723]: seen 172300 examples : 67.8 eps, Loss: 3.313, Avg loss: 3.341, Best loss: 3.336, cov loss: 0.147\n",
      "    [batch 1730]: seen 173000 examples : 67.8 eps, Loss: 3.312, Avg loss: 3.342, Best loss: 3.336, cov loss: 0.166\n",
      "    [batch 1737]: seen 173700 examples : 67.8 eps, Loss: 3.419, Avg loss: 3.344, Best loss: 3.336, cov loss: 0.170\n",
      "    [batch 1744]: seen 174400 examples : 67.8 eps, Loss: 3.332, Avg loss: 3.348, Best loss: 3.336, cov loss: 0.160\n",
      "    [batch 1751]: seen 175100 examples : 67.8 eps, Loss: 3.464, Avg loss: 3.350, Best loss: 3.336, cov loss: 0.175\n",
      "    [batch 1758]: seen 175800 examples : 67.8 eps, Loss: 3.269, Avg loss: 3.350, Best loss: 3.336, cov loss: 0.165\n",
      "    [batch 1765]: seen 176500 examples : 67.8 eps, Loss: 3.398, Avg loss: 3.345, Best loss: 3.336, cov loss: 0.163\n",
      "    [batch 1772]: seen 177200 examples : 67.8 eps, Loss: 3.382, Avg loss: 3.348, Best loss: 3.336, cov loss: 0.166\n",
      "    [batch 1779]: seen 177900 examples : 67.8 eps, Loss: 3.391, Avg loss: 3.348, Best loss: 3.336, cov loss: 0.165\n",
      "    [batch 1786]: seen 178600 examples : 67.8 eps, Loss: 3.298, Avg loss: 3.350, Best loss: 3.336, cov loss: 0.170\n",
      "    [batch 1793]: seen 179300 examples : 67.8 eps, Loss: 3.496, Avg loss: 3.348, Best loss: 3.336, cov loss: 0.189\n",
      "    [batch 1800]: seen 180000 examples : 67.8 eps, Loss: 3.457, Avg loss: 3.348, Best loss: 3.336, cov loss: 0.160\n",
      "    [batch 1807]: seen 180700 examples : 67.8 eps, Loss: 3.232, Avg loss: 3.345, Best loss: 3.336, cov loss: 0.169\n",
      "    [batch 1814]: seen 181400 examples : 67.8 eps, Loss: 3.357, Avg loss: 3.344, Best loss: 3.336, cov loss: 0.161\n",
      "    [batch 1821]: seen 182100 examples : 67.9 eps, Loss: 3.239, Avg loss: 3.341, Best loss: 3.336, cov loss: 0.167\n",
      "    [batch 1828]: seen 182800 examples : 67.9 eps, Loss: 3.417, Avg loss: 3.345, Best loss: 3.336, cov loss: 0.169\n",
      "    [batch 1835]: seen 183500 examples : 67.9 eps, Loss: 3.447, Avg loss: 3.344, Best loss: 3.336, cov loss: 0.172\n",
      "    [batch 1842]: seen 184200 examples : 67.9 eps, Loss: 3.418, Avg loss: 3.348, Best loss: 3.336, cov loss: 0.174\n",
      "    [batch 1849]: seen 184900 examples : 67.9 eps, Loss: 3.605, Avg loss: 3.349, Best loss: 3.336, cov loss: 0.181\n",
      "    [batch 1856]: seen 185600 examples : 67.9 eps, Loss: 3.192, Avg loss: 3.348, Best loss: 3.336, cov loss: 0.172\n",
      "    [batch 1863]: seen 186300 examples : 67.9 eps, Loss: 3.428, Avg loss: 3.348, Best loss: 3.336, cov loss: 0.177\n",
      "    [batch 1870]: seen 187000 examples : 67.9 eps, Loss: 3.376, Avg loss: 3.354, Best loss: 3.336, cov loss: 0.174\n",
      "    [batch 1877]: seen 187700 examples : 67.9 eps, Loss: 3.359, Avg loss: 3.355, Best loss: 3.336, cov loss: 0.174\n",
      "    [batch 1884]: seen 188400 examples : 67.9 eps, Loss: 3.317, Avg loss: 3.354, Best loss: 3.336, cov loss: 0.145\n",
      "    [batch 1891]: seen 189100 examples : 67.9 eps, Loss: 3.469, Avg loss: 3.356, Best loss: 3.336, cov loss: 0.159\n",
      "    [batch 1898]: seen 189800 examples : 67.9 eps, Loss: 3.342, Avg loss: 3.354, Best loss: 3.336, cov loss: 0.192\n",
      "    [batch 1905]: seen 190500 examples : 67.9 eps, Loss: 3.263, Avg loss: 3.351, Best loss: 3.336, cov loss: 0.147\n",
      "    [batch 1912]: seen 191200 examples : 67.9 eps, Loss: 3.296, Avg loss: 3.353, Best loss: 3.336, cov loss: 0.160\n",
      "    [batch 1919]: seen 191900 examples : 67.9 eps, Loss: 3.485, Avg loss: 3.357, Best loss: 3.336, cov loss: 0.169\n",
      "    [batch 1926]: seen 192600 examples : 67.9 eps, Loss: 3.290, Avg loss: 3.359, Best loss: 3.336, cov loss: 0.159\n",
      "    [batch 1933]: seen 193300 examples : 67.9 eps, Loss: 3.450, Avg loss: 3.358, Best loss: 3.336, cov loss: 0.174\n",
      "    [batch 1940]: seen 194000 examples : 67.9 eps, Loss: 3.370, Avg loss: 3.359, Best loss: 3.336, cov loss: 0.170\n",
      "    [batch 1947]: seen 194700 examples : 67.9 eps, Loss: 3.580, Avg loss: 3.362, Best loss: 3.336, cov loss: 0.166\n",
      "    [batch 1954]: seen 195400 examples : 67.9 eps, Loss: 3.466, Avg loss: 3.365, Best loss: 3.336, cov loss: 0.188\n",
      "    [batch 1961]: seen 196100 examples : 67.9 eps, Loss: 3.218, Avg loss: 3.361, Best loss: 3.336, cov loss: 0.145\n",
      "    [batch 1968]: seen 196800 examples : 67.9 eps, Loss: 3.466, Avg loss: 3.360, Best loss: 3.336, cov loss: 0.173\n",
      "    [batch 1975]: seen 197500 examples : 67.9 eps, Loss: 3.281, Avg loss: 3.357, Best loss: 3.336, cov loss: 0.171\n",
      "    [batch 1982]: seen 198200 examples : 67.9 eps, Loss: 3.401, Avg loss: 3.360, Best loss: 3.336, cov loss: 0.161\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-40415\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-40415\n",
      "    [batch 1989]: seen 198900 examples : 67.9 eps, Loss: 3.311, Avg loss: 3.362, Best loss: 3.336, cov loss: 0.161\n",
      "    [batch 1996]: seen 199600 examples : 67.9 eps, Loss: 3.469, Avg loss: 3.365, Best loss: 3.336, cov loss: 0.165\n",
      "    [batch 2003]: seen 200300 examples : 67.9 eps, Loss: 3.365, Avg loss: 3.363, Best loss: 3.336, cov loss: 0.169\n",
      "    [batch 2010]: seen 201000 examples : 67.9 eps, Loss: 3.153, Avg loss: 3.362, Best loss: 3.336, cov loss: 0.153\n",
      "    [batch 2017]: seen 201700 examples : 67.9 eps, Loss: 3.455, Avg loss: 3.365, Best loss: 3.336, cov loss: 0.162\n",
      "    [batch 2024]: seen 202400 examples : 67.9 eps, Loss: 3.367, Avg loss: 3.364, Best loss: 3.336, cov loss: 0.151\n",
      "    [batch 2031]: seen 203100 examples : 67.9 eps, Loss: 3.415, Avg loss: 3.367, Best loss: 3.336, cov loss: 0.165\n",
      "    [batch 2038]: seen 203800 examples : 67.9 eps, Loss: 3.500, Avg loss: 3.363, Best loss: 3.336, cov loss: 0.166\n",
      "    [batch 2045]: seen 204500 examples : 67.9 eps, Loss: 3.562, Avg loss: 3.362, Best loss: 3.336, cov loss: 0.182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2052]: seen 205200 examples : 67.9 eps, Loss: 3.484, Avg loss: 3.364, Best loss: 3.336, cov loss: 0.159\n",
      "    [batch 2059]: seen 205900 examples : 67.9 eps, Loss: 3.272, Avg loss: 3.360, Best loss: 3.336, cov loss: 0.162\n",
      "    [batch 2066]: seen 206600 examples : 68.0 eps, Loss: 3.349, Avg loss: 3.360, Best loss: 3.336, cov loss: 0.162\n",
      "    [batch 2073]: seen 207300 examples : 68.0 eps, Loss: 3.441, Avg loss: 3.363, Best loss: 3.336, cov loss: 0.167\n",
      "    [batch 2080]: seen 208000 examples : 68.0 eps, Loss: 3.403, Avg loss: 3.364, Best loss: 3.336, cov loss: 0.173\n",
      "    [batch 2087]: seen 208700 examples : 68.0 eps, Loss: 3.321, Avg loss: 3.367, Best loss: 3.336, cov loss: 0.168\n",
      "    [batch 2094]: seen 209400 examples : 68.0 eps, Loss: 3.444, Avg loss: 3.372, Best loss: 3.336, cov loss: 0.181\n",
      "    [batch 2101]: seen 210100 examples : 68.0 eps, Loss: 3.627, Avg loss: 3.373, Best loss: 3.336, cov loss: 0.165\n",
      "    [batch 2108]: seen 210800 examples : 68.0 eps, Loss: 3.487, Avg loss: 3.373, Best loss: 3.336, cov loss: 0.163\n",
      "    [batch 2115]: seen 211500 examples : 68.0 eps, Loss: 3.323, Avg loss: 3.373, Best loss: 3.336, cov loss: 0.166\n",
      "    [batch 2122]: seen 212200 examples : 68.0 eps, Loss: 3.477, Avg loss: 3.371, Best loss: 3.336, cov loss: 0.171\n",
      "    [batch 2129]: seen 212900 examples : 68.0 eps, Loss: 3.404, Avg loss: 3.371, Best loss: 3.336, cov loss: 0.170\n",
      "    [batch 2136]: seen 213600 examples : 68.0 eps, Loss: 3.299, Avg loss: 3.368, Best loss: 3.336, cov loss: 0.177\n",
      "    [batch 2143]: seen 214300 examples : 68.0 eps, Loss: 3.478, Avg loss: 3.376, Best loss: 3.336, cov loss: 0.149\n",
      "    [batch 2150]: seen 215000 examples : 68.0 eps, Loss: 3.314, Avg loss: 3.374, Best loss: 3.336, cov loss: 0.168\n",
      "    [batch 2157]: seen 215700 examples : 68.0 eps, Loss: 3.493, Avg loss: 3.372, Best loss: 3.336, cov loss: 0.179\n",
      "    [batch 2164]: seen 216400 examples : 68.0 eps, Loss: 3.478, Avg loss: 3.372, Best loss: 3.336, cov loss: 0.172\n",
      "    [batch 2171]: seen 217100 examples : 68.0 eps, Loss: 3.315, Avg loss: 3.372, Best loss: 3.336, cov loss: 0.168\n",
      "    [batch 2178]: seen 217800 examples : 68.0 eps, Loss: 3.411, Avg loss: 3.371, Best loss: 3.336, cov loss: 0.177\n",
      "    [batch 2185]: seen 218500 examples : 68.0 eps, Loss: 3.267, Avg loss: 3.373, Best loss: 3.336, cov loss: 0.161\n",
      "    [batch 2192]: seen 219200 examples : 68.0 eps, Loss: 3.335, Avg loss: 3.366, Best loss: 3.336, cov loss: 0.176\n",
      "    [batch 2199]: seen 219900 examples : 68.0 eps, Loss: 3.381, Avg loss: 3.368, Best loss: 3.336, cov loss: 0.184\n",
      "    [batch 2206]: seen 220600 examples : 68.0 eps, Loss: 3.302, Avg loss: 3.367, Best loss: 3.336, cov loss: 0.165\n",
      "    [batch 2213]: seen 221300 examples : 68.0 eps, Loss: 3.296, Avg loss: 3.367, Best loss: 3.336, cov loss: 0.173\n",
      "    [batch 2220]: seen 222000 examples : 68.0 eps, Loss: 3.406, Avg loss: 3.364, Best loss: 3.336, cov loss: 0.167\n",
      "    [batch 2227]: seen 222700 examples : 68.0 eps, Loss: 3.350, Avg loss: 3.365, Best loss: 3.336, cov loss: 0.164\n",
      "    [batch 2234]: seen 223400 examples : 68.0 eps, Loss: 3.317, Avg loss: 3.359, Best loss: 3.336, cov loss: 0.177\n",
      "    [batch 2241]: seen 224100 examples : 68.0 eps, Loss: 3.387, Avg loss: 3.361, Best loss: 3.336, cov loss: 0.159\n",
      "    [batch 2248]: seen 224800 examples : 68.0 eps, Loss: 3.332, Avg loss: 3.361, Best loss: 3.336, cov loss: 0.173\n",
      "    [batch 2255]: seen 225500 examples : 68.0 eps, Loss: 3.378, Avg loss: 3.362, Best loss: 3.336, cov loss: 0.160\n",
      "    [batch 2262]: seen 226200 examples : 68.0 eps, Loss: 3.692, Avg loss: 3.369, Best loss: 3.336, cov loss: 0.189\n",
      "    [batch 2269]: seen 226900 examples : 68.0 eps, Loss: 3.479, Avg loss: 3.368, Best loss: 3.336, cov loss: 0.180\n",
      "    [batch 2276]: seen 227600 examples : 68.0 eps, Loss: 3.353, Avg loss: 3.368, Best loss: 3.336, cov loss: 0.145\n",
      "    [batch 2283]: seen 228300 examples : 68.1 eps, Loss: 3.333, Avg loss: 3.365, Best loss: 3.336, cov loss: 0.164\n",
      "    [batch 2290]: seen 229000 examples : 68.1 eps, Loss: 3.394, Avg loss: 3.364, Best loss: 3.336, cov loss: 0.171\n",
      "    [batch 2297]: seen 229700 examples : 68.1 eps, Loss: 3.336, Avg loss: 3.369, Best loss: 3.336, cov loss: 0.158\n",
      "    [batch 2304]: seen 230400 examples : 68.1 eps, Loss: 3.410, Avg loss: 3.369, Best loss: 3.336, cov loss: 0.158\n",
      "    [batch 2311]: seen 231100 examples : 68.1 eps, Loss: 3.366, Avg loss: 3.370, Best loss: 3.336, cov loss: 0.159\n",
      "    [batch 2318]: seen 231800 examples : 68.1 eps, Loss: 3.374, Avg loss: 3.369, Best loss: 3.336, cov loss: 0.170\n",
      "    [batch 2325]: seen 232500 examples : 68.1 eps, Loss: 3.285, Avg loss: 3.365, Best loss: 3.336, cov loss: 0.178\n",
      "    [batch 2332]: seen 233200 examples : 68.1 eps, Loss: 3.580, Avg loss: 3.369, Best loss: 3.336, cov loss: 0.182\n",
      "    [batch 2339]: seen 233900 examples : 68.1 eps, Loss: 3.401, Avg loss: 3.370, Best loss: 3.336, cov loss: 0.172\n",
      "    [batch 2346]: seen 234600 examples : 68.1 eps, Loss: 3.148, Avg loss: 3.368, Best loss: 3.336, cov loss: 0.148\n",
      "    [batch 2353]: seen 235300 examples : 68.1 eps, Loss: 3.305, Avg loss: 3.366, Best loss: 3.336, cov loss: 0.158\n",
      "    [batch 2360]: seen 236000 examples : 68.1 eps, Loss: 3.427, Avg loss: 3.363, Best loss: 3.336, cov loss: 0.182\n",
      "    [batch 2367]: seen 236700 examples : 68.1 eps, Loss: 3.277, Avg loss: 3.363, Best loss: 3.336, cov loss: 0.150\n",
      "    [batch 2374]: seen 237400 examples : 68.1 eps, Loss: 3.403, Avg loss: 3.362, Best loss: 3.336, cov loss: 0.167\n",
      "    [batch 2381]: seen 238100 examples : 68.1 eps, Loss: 3.043, Avg loss: 3.360, Best loss: 3.336, cov loss: 0.158\n",
      "    [batch 2388]: seen 238800 examples : 68.1 eps, Loss: 3.390, Avg loss: 3.359, Best loss: 3.336, cov loss: 0.179\n",
      "    [batch 2395]: seen 239500 examples : 68.1 eps, Loss: 3.296, Avg loss: 3.360, Best loss: 3.336, cov loss: 0.163\n",
      "    [batch 2402]: seen 240200 examples : 68.1 eps, Loss: 3.333, Avg loss: 3.362, Best loss: 3.336, cov loss: 0.170\n",
      "    [batch 2409]: seen 240900 examples : 68.1 eps, Loss: 3.272, Avg loss: 3.360, Best loss: 3.336, cov loss: 0.170\n",
      "    [batch 2416]: seen 241600 examples : 68.1 eps, Loss: 3.388, Avg loss: 3.365, Best loss: 3.336, cov loss: 0.146\n",
      "    [batch 2423]: seen 242300 examples : 68.1 eps, Loss: 3.221, Avg loss: 3.363, Best loss: 3.336, cov loss: 0.171\n",
      "    [batch 2430]: seen 243000 examples : 68.1 eps, Loss: 3.480, Avg loss: 3.363, Best loss: 3.336, cov loss: 0.171\n",
      "    [batch 2437]: seen 243700 examples : 68.1 eps, Loss: 3.380, Avg loss: 3.361, Best loss: 3.336, cov loss: 0.177\n",
      "    [batch 2444]: seen 244400 examples : 68.1 eps, Loss: 3.280, Avg loss: 3.363, Best loss: 3.336, cov loss: 0.172\n",
      "    [batch 2451]: seen 245100 examples : 68.1 eps, Loss: 3.381, Avg loss: 3.361, Best loss: 3.336, cov loss: 0.169\n",
      "    [batch 2458]: seen 245800 examples : 68.1 eps, Loss: 3.482, Avg loss: 3.361, Best loss: 3.336, cov loss: 0.188\n",
      "    [batch 2465]: seen 246500 examples : 68.1 eps, Loss: 3.432, Avg loss: 3.362, Best loss: 3.336, cov loss: 0.175\n",
      "    [batch 2472]: seen 247200 examples : 68.1 eps, Loss: 3.318, Avg loss: 3.363, Best loss: 3.336, cov loss: 0.177\n",
      "    [batch 2479]: seen 247900 examples : 68.2 eps, Loss: 3.406, Avg loss: 3.366, Best loss: 3.336, cov loss: 0.194\n",
      "    [batch 2486]: seen 248600 examples : 68.2 eps, Loss: 3.283, Avg loss: 3.368, Best loss: 3.336, cov loss: 0.159\n",
      "    [batch 2493]: seen 249300 examples : 68.2 eps, Loss: 3.420, Avg loss: 3.367, Best loss: 3.336, cov loss: 0.178\n",
      "    [batch 2500]: seen 250000 examples : 68.2 eps, Loss: 3.272, Avg loss: 3.365, Best loss: 3.336, cov loss: 0.164\n",
      "    [batch 2507]: seen 250700 examples : 68.2 eps, Loss: 3.440, Avg loss: 3.370, Best loss: 3.336, cov loss: 0.186\n",
      "    [batch 2514]: seen 251400 examples : 68.2 eps, Loss: 3.295, Avg loss: 3.366, Best loss: 3.336, cov loss: 0.169\n",
      "    [batch 2521]: seen 252100 examples : 68.2 eps, Loss: 3.354, Avg loss: 3.371, Best loss: 3.336, cov loss: 0.172\n",
      "    [batch 2528]: seen 252800 examples : 68.2 eps, Loss: 3.452, Avg loss: 3.374, Best loss: 3.336, cov loss: 0.162\n",
      "    [batch 2535]: seen 253500 examples : 68.2 eps, Loss: 3.410, Avg loss: 3.375, Best loss: 3.336, cov loss: 0.168\n",
      "    [batch 2605]: seen 260500 examples : 68.2 eps, Loss: 3.392, Avg loss: 3.368, Best loss: 3.336, cov loss: 0.160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f43c3341d8a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurr_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-17e3ed376236>\u001b[0m in \u001b[0;36mtrain_continue\u001b[0;34m(hps, epochs, train_step, curr_best, best_loss, avg_loss, restore, epoch_start)\u001b[0m\n\u001b[1;32m     36\u001b[0m                                                             \u001b[0mhps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                                             \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                                                             avg_loss)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcurr_best\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266/final_0/W266_Final/model_4/training_util.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(lm, session, batches, summary_writer, train_dir, train_step, saver, hps, best_loss, avg_loss)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunTrainStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266/final_0/W266_Final/model_4/training_util.py\u001b[0m in \u001b[0;36mrunTrainStep\u001b[0;34m(lm, session, batch)\u001b[0m\n\u001b[1;32m     87\u001b[0m     }\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_loss = 3.371\n",
    "best_loss = 3.337\n",
    "curr_best = best_loss\n",
    "train_step = 38874\n",
    "epochs = 3\n",
    "restore = True\n",
    "epoch_start = 22\n",
    "\n",
    "train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train session 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 252\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 110\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 2 1/2 76\n",
      "\n",
      "\n",
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: 16:03\n",
      "INFO:tensorflow:Fetching data..\n",
      "INFO:tensorflow:Creating batches..\n",
      "INFO:tensorflow:[TOTAL Batches]  : 2808\n",
      "INFO:tensorflow:[TOTAL Examples] : 280778\n",
      "INFO:tensorflow:Creating batches..COMPLETE\n",
      "INFO:tensorflow:Building core graph...\n",
      "INFO:tensorflow:Adding attention_decoder timestep 0 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 1 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 2 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 3 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 4 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 5 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 6 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 7 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 8 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 9 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 10 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 11 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 12 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 13 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 14 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 15 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 16 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 17 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 18 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 19 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 20 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 21 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 22 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 23 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 24 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 25 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 26 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 27 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 28 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 29 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 30 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 31 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 32 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 33 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 34 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 35 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 36 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 37 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 38 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 39 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 40 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 41 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 42 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 43 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 44 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 45 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 46 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 47 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 48 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 49 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 50 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 51 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 52 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 53 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 54 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 55 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 56 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 57 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 58 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 59 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 60 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 61 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 62 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 63 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 64 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 65 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 66 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 67 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 68 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 69 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 70 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 71 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 72 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 73 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 74 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 75 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 76 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 77 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 78 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 79 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 80 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 81 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 82 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 83 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 84 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 85 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 86 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 87 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 88 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 89 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 90 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 91 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 92 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 93 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 94 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 95 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 96 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 97 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 98 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 99 of 100\n",
      "INFO:tensorflow:Building projection graph...\n",
      "INFO:tensorflow:Building projection graph...COMPLETE\n",
      "INFO:tensorflow:Building Loss graph...\n",
      "INFO:tensorflow:Building Loss graph...COMPLETE\n",
      "INFO:tensorflow:Building core graph...COMPLETE\n",
      "INFO:tensorflow:Building train graph...\n",
      "INFO:tensorflow:Building train graph...COMPLETE\n",
      "INFO:tensorflow:Building summary graph...\n",
      "INFO:tensorflow:Building summary graph...COMPLETE\n",
      "WARNING:tensorflow:From <ipython-input-6-17e3ed376236>:18: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-40426\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-40426\n",
      "[EPOCH 23] Starting training..\n",
      "    [batch 1]: seen 100 examples : 3.0 eps, Loss: 3.311, Avg loss: 3.311, Best loss: 3.311, cov loss: 0.156\n",
      "    [batch 8]: seen 800 examples : 18.5 eps, Loss: 3.427, Avg loss: 3.317, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 15]: seen 1500 examples : 28.1 eps, Loss: 3.180, Avg loss: 3.316, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 22]: seen 2200 examples : 34.6 eps, Loss: 3.348, Avg loss: 3.316, Best loss: 3.311, cov loss: 0.174\n",
      "    [batch 29]: seen 2900 examples : 39.2 eps, Loss: 3.371, Avg loss: 3.323, Best loss: 3.311, cov loss: 0.181\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-40427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-40427\n",
      "    [batch 35]: seen 3500 examples : 41.5 eps, Loss: 3.312, Avg loss: 3.325, Best loss: 3.311, cov loss: 0.152\n",
      "    [batch 42]: seen 4200 examples : 44.4 eps, Loss: 3.338, Avg loss: 3.324, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 49]: seen 4900 examples : 46.7 eps, Loss: 3.348, Avg loss: 3.325, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 56]: seen 5600 examples : 48.6 eps, Loss: 3.407, Avg loss: 3.326, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 63]: seen 6300 examples : 50.2 eps, Loss: 3.150, Avg loss: 3.323, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 70]: seen 7000 examples : 51.6 eps, Loss: 3.242, Avg loss: 3.323, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 77]: seen 7700 examples : 52.7 eps, Loss: 3.279, Avg loss: 3.323, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 84]: seen 8400 examples : 53.8 eps, Loss: 3.242, Avg loss: 3.326, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 91]: seen 9100 examples : 54.6 eps, Loss: 3.205, Avg loss: 3.323, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 98]: seen 9800 examples : 55.4 eps, Loss: 3.357, Avg loss: 3.327, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 105]: seen 10500 examples : 56.1 eps, Loss: 3.403, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.163\n",
      "    [batch 112]: seen 11200 examples : 56.8 eps, Loss: 3.283, Avg loss: 3.333, Best loss: 3.311, cov loss: 0.182\n",
      "    [batch 119]: seen 11900 examples : 57.3 eps, Loss: 3.393, Avg loss: 3.331, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 126]: seen 12600 examples : 57.8 eps, Loss: 3.381, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.178\n",
      "    [batch 133]: seen 13300 examples : 58.3 eps, Loss: 3.588, Avg loss: 3.337, Best loss: 3.311, cov loss: 0.160\n",
      "    [batch 140]: seen 14000 examples : 58.7 eps, Loss: 3.263, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 147]: seen 14700 examples : 59.1 eps, Loss: 3.315, Avg loss: 3.337, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 154]: seen 15400 examples : 59.5 eps, Loss: 3.280, Avg loss: 3.341, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 161]: seen 16100 examples : 59.8 eps, Loss: 3.258, Avg loss: 3.341, Best loss: 3.311, cov loss: 0.161\n",
      "    [batch 168]: seen 16800 examples : 60.1 eps, Loss: 3.369, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 175]: seen 17500 examples : 60.4 eps, Loss: 3.407, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.182\n",
      "    [batch 182]: seen 18200 examples : 60.7 eps, Loss: 3.353, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.170\n",
      "    [batch 189]: seen 18900 examples : 60.9 eps, Loss: 3.410, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.189\n",
      "    [batch 196]: seen 19600 examples : 61.1 eps, Loss: 3.298, Avg loss: 3.338, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 203]: seen 20300 examples : 61.4 eps, Loss: 3.450, Avg loss: 3.338, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 210]: seen 21000 examples : 61.6 eps, Loss: 3.359, Avg loss: 3.340, Best loss: 3.311, cov loss: 0.160\n",
      "    [batch 217]: seen 21700 examples : 61.8 eps, Loss: 3.456, Avg loss: 3.340, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 224]: seen 22400 examples : 61.9 eps, Loss: 3.361, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.137\n",
      "    [batch 231]: seen 23100 examples : 62.1 eps, Loss: 3.389, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.185\n",
      "    [batch 238]: seen 23800 examples : 62.3 eps, Loss: 3.264, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 245]: seen 24500 examples : 62.4 eps, Loss: 3.233, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.159\n",
      "    [batch 252]: seen 25200 examples : 62.6 eps, Loss: 3.378, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 259]: seen 25900 examples : 62.7 eps, Loss: 3.480, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.174\n",
      "    [batch 266]: seen 26600 examples : 62.8 eps, Loss: 3.431, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 273]: seen 27300 examples : 63.0 eps, Loss: 3.276, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 280]: seen 28000 examples : 63.1 eps, Loss: 3.204, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.170\n",
      "    [batch 287]: seen 28700 examples : 63.2 eps, Loss: 3.360, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.154\n",
      "    [batch 294]: seen 29400 examples : 63.3 eps, Loss: 3.416, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.177\n",
      "    [batch 301]: seen 30100 examples : 63.4 eps, Loss: 3.505, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.178\n",
      "    [batch 308]: seen 30800 examples : 63.5 eps, Loss: 3.477, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 315]: seen 31500 examples : 63.6 eps, Loss: 3.346, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.153\n",
      "    [batch 322]: seen 32200 examples : 63.7 eps, Loss: 3.396, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.186\n",
      "    [batch 329]: seen 32900 examples : 63.8 eps, Loss: 3.368, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.172\n",
      "    [batch 336]: seen 33600 examples : 63.9 eps, Loss: 3.394, Avg loss: 3.354, Best loss: 3.311, cov loss: 0.163\n",
      "    [batch 343]: seen 34300 examples : 64.0 eps, Loss: 3.496, Avg loss: 3.356, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 350]: seen 35000 examples : 64.1 eps, Loss: 3.402, Avg loss: 3.357, Best loss: 3.311, cov loss: 0.159\n",
      "    [batch 357]: seen 35700 examples : 64.1 eps, Loss: 3.438, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 364]: seen 36400 examples : 64.2 eps, Loss: 3.324, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.174\n",
      "    [batch 371]: seen 37100 examples : 64.3 eps, Loss: 3.645, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 378]: seen 37800 examples : 64.4 eps, Loss: 3.247, Avg loss: 3.355, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 385]: seen 38500 examples : 64.4 eps, Loss: 3.389, Avg loss: 3.351, Best loss: 3.311, cov loss: 0.157\n",
      "    [batch 392]: seen 39200 examples : 64.5 eps, Loss: 3.407, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.189\n",
      "    [batch 399]: seen 39900 examples : 64.6 eps, Loss: 3.509, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 406]: seen 40600 examples : 64.6 eps, Loss: 3.293, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.151\n",
      "    [batch 413]: seen 41300 examples : 64.7 eps, Loss: 3.389, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.192\n",
      "    [batch 420]: seen 42000 examples : 64.7 eps, Loss: 3.311, Avg loss: 3.340, Best loss: 3.311, cov loss: 0.152\n",
      "    [batch 427]: seen 42700 examples : 64.8 eps, Loss: 3.370, Avg loss: 3.341, Best loss: 3.311, cov loss: 0.172\n",
      "    [batch 434]: seen 43400 examples : 64.8 eps, Loss: 3.447, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.170\n",
      "    [batch 441]: seen 44100 examples : 64.9 eps, Loss: 3.126, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.152\n",
      "    [batch 448]: seen 44800 examples : 64.9 eps, Loss: 3.205, Avg loss: 3.337, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 455]: seen 45500 examples : 65.0 eps, Loss: 3.103, Avg loss: 3.331, Best loss: 3.311, cov loss: 0.157\n",
      "    [batch 462]: seen 46200 examples : 65.0 eps, Loss: 3.354, Avg loss: 3.331, Best loss: 3.311, cov loss: 0.178\n",
      "    [batch 469]: seen 46900 examples : 65.1 eps, Loss: 3.396, Avg loss: 3.336, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 476]: seen 47600 examples : 65.1 eps, Loss: 3.513, Avg loss: 3.336, Best loss: 3.311, cov loss: 0.181\n",
      "    [batch 483]: seen 48300 examples : 65.2 eps, Loss: 3.363, Avg loss: 3.335, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 490]: seen 49000 examples : 65.2 eps, Loss: 3.354, Avg loss: 3.335, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 497]: seen 49700 examples : 65.3 eps, Loss: 3.372, Avg loss: 3.333, Best loss: 3.311, cov loss: 0.170\n",
      "    [batch 504]: seen 50400 examples : 65.3 eps, Loss: 3.510, Avg loss: 3.336, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 511]: seen 51100 examples : 65.3 eps, Loss: 3.527, Avg loss: 3.336, Best loss: 3.311, cov loss: 0.183\n",
      "    [batch 518]: seen 51800 examples : 65.4 eps, Loss: 3.432, Avg loss: 3.337, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 525]: seen 52500 examples : 65.4 eps, Loss: 3.479, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.177\n",
      "    [batch 532]: seen 53200 examples : 65.4 eps, Loss: 3.460, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 539]: seen 53900 examples : 65.5 eps, Loss: 3.469, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 546]: seen 54600 examples : 65.5 eps, Loss: 3.429, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 553]: seen 55300 examples : 65.6 eps, Loss: 3.412, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 560]: seen 56000 examples : 65.6 eps, Loss: 3.215, Avg loss: 3.341, Best loss: 3.311, cov loss: 0.149\n",
      "    [batch 567]: seen 56700 examples : 65.6 eps, Loss: 3.258, Avg loss: 3.336, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 574]: seen 57400 examples : 65.6 eps, Loss: 3.512, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.182\n",
      "    [batch 581]: seen 58100 examples : 65.7 eps, Loss: 3.346, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.157\n",
      "    [batch 588]: seen 58800 examples : 65.7 eps, Loss: 3.235, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 595]: seen 59500 examples : 65.7 eps, Loss: 3.424, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 602]: seen 60200 examples : 65.8 eps, Loss: 3.216, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.155\n",
      "    [batch 609]: seen 60900 examples : 65.8 eps, Loss: 3.218, Avg loss: 3.341, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 616]: seen 61600 examples : 65.8 eps, Loss: 3.599, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.170\n",
      "    [batch 623]: seen 62300 examples : 65.9 eps, Loss: 3.440, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 630]: seen 63000 examples : 65.9 eps, Loss: 3.462, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.182\n",
      "    [batch 637]: seen 63700 examples : 65.9 eps, Loss: 3.283, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.151\n",
      "    [batch 644]: seen 64400 examples : 65.9 eps, Loss: 3.302, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 651]: seen 65100 examples : 66.0 eps, Loss: 3.386, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.178\n",
      "    [batch 658]: seen 65800 examples : 66.0 eps, Loss: 3.185, Avg loss: 3.351, Best loss: 3.311, cov loss: 0.150\n",
      "    [batch 665]: seen 66500 examples : 66.0 eps, Loss: 3.499, Avg loss: 3.352, Best loss: 3.311, cov loss: 0.184\n",
      "    [batch 672]: seen 67200 examples : 66.0 eps, Loss: 3.484, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.177\n",
      "    [batch 679]: seen 67900 examples : 66.0 eps, Loss: 3.472, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.189\n",
      "    [batch 686]: seen 68600 examples : 66.1 eps, Loss: 3.389, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.177\n",
      "    [batch 693]: seen 69300 examples : 66.1 eps, Loss: 3.505, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 700]: seen 70000 examples : 66.1 eps, Loss: 3.309, Avg loss: 3.351, Best loss: 3.311, cov loss: 0.154\n",
      "    [batch 707]: seen 70700 examples : 66.1 eps, Loss: 3.400, Avg loss: 3.351, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 714]: seen 71400 examples : 66.2 eps, Loss: 3.334, Avg loss: 3.355, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 721]: seen 72100 examples : 66.2 eps, Loss: 3.680, Avg loss: 3.360, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 728]: seen 72800 examples : 66.2 eps, Loss: 3.633, Avg loss: 3.367, Best loss: 3.311, cov loss: 0.182\n",
      "    [batch 735]: seen 73500 examples : 66.2 eps, Loss: 3.136, Avg loss: 3.360, Best loss: 3.311, cov loss: 0.153\n",
      "    [batch 742]: seen 74200 examples : 66.2 eps, Loss: 3.334, Avg loss: 3.360, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 749]: seen 74900 examples : 66.3 eps, Loss: 3.341, Avg loss: 3.360, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 756]: seen 75600 examples : 66.3 eps, Loss: 3.364, Avg loss: 3.360, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 763]: seen 76300 examples : 66.3 eps, Loss: 3.493, Avg loss: 3.358, Best loss: 3.311, cov loss: 0.174\n",
      "    [batch 770]: seen 77000 examples : 66.3 eps, Loss: 3.363, Avg loss: 3.357, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 777]: seen 77700 examples : 66.3 eps, Loss: 3.162, Avg loss: 3.354, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 784]: seen 78400 examples : 66.3 eps, Loss: 3.424, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.183\n",
      "    [batch 791]: seen 79100 examples : 66.4 eps, Loss: 3.445, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 798]: seen 79800 examples : 66.4 eps, Loss: 3.265, Avg loss: 3.351, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 805]: seen 80500 examples : 66.4 eps, Loss: 3.363, Avg loss: 3.352, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 812]: seen 81200 examples : 66.4 eps, Loss: 3.417, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.179\n",
      "    [batch 819]: seen 81900 examples : 66.4 eps, Loss: 3.234, Avg loss: 3.351, Best loss: 3.311, cov loss: 0.157\n",
      "    [batch 826]: seen 82600 examples : 66.4 eps, Loss: 3.356, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 833]: seen 83300 examples : 66.5 eps, Loss: 3.440, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.174\n",
      "    [batch 840]: seen 84000 examples : 66.5 eps, Loss: 3.292, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 847]: seen 84700 examples : 66.5 eps, Loss: 3.198, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.155\n",
      "    [batch 854]: seen 85400 examples : 66.5 eps, Loss: 3.320, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.150\n",
      "    [batch 861]: seen 86100 examples : 66.5 eps, Loss: 3.166, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.156\n",
      "    [batch 868]: seen 86800 examples : 66.5 eps, Loss: 3.431, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.155\n",
      "    [batch 875]: seen 87500 examples : 66.6 eps, Loss: 3.398, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.151\n",
      "    [batch 882]: seen 88200 examples : 66.6 eps, Loss: 3.264, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.170\n",
      "    [batch 889]: seen 88900 examples : 66.6 eps, Loss: 3.382, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 896]: seen 89600 examples : 66.6 eps, Loss: 3.671, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 903]: seen 90300 examples : 66.6 eps, Loss: 3.378, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.143\n",
      "    [batch 910]: seen 91000 examples : 66.6 eps, Loss: 3.296, Avg loss: 3.355, Best loss: 3.311, cov loss: 0.160\n",
      "    [batch 917]: seen 91700 examples : 66.6 eps, Loss: 3.316, Avg loss: 3.354, Best loss: 3.311, cov loss: 0.163\n",
      "    [batch 924]: seen 92400 examples : 66.6 eps, Loss: 3.277, Avg loss: 3.354, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 931]: seen 93100 examples : 66.7 eps, Loss: 3.271, Avg loss: 3.355, Best loss: 3.311, cov loss: 0.163\n",
      "    [batch 938]: seen 93800 examples : 66.7 eps, Loss: 3.458, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 945]: seen 94500 examples : 66.7 eps, Loss: 3.371, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.152\n",
      "    [batch 952]: seen 95200 examples : 66.7 eps, Loss: 3.353, Avg loss: 3.352, Best loss: 3.311, cov loss: 0.200\n",
      "    [batch 959]: seen 95900 examples : 66.7 eps, Loss: 3.463, Avg loss: 3.355, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 966]: seen 96600 examples : 66.7 eps, Loss: 3.192, Avg loss: 3.354, Best loss: 3.311, cov loss: 0.156\n",
      "    [batch 973]: seen 97300 examples : 66.7 eps, Loss: 3.292, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.154\n",
      "    [batch 980]: seen 98000 examples : 66.7 eps, Loss: 3.393, Avg loss: 3.354, Best loss: 3.311, cov loss: 0.152\n",
      "    [batch 987]: seen 98700 examples : 66.8 eps, Loss: 3.105, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 994]: seen 99400 examples : 66.8 eps, Loss: 3.135, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.160\n",
      "    [batch 1001]: seen 100100 examples : 66.8 eps, Loss: 3.381, Avg loss: 3.341, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 1008]: seen 100800 examples : 66.8 eps, Loss: 3.224, Avg loss: 3.341, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 1015]: seen 101500 examples : 66.8 eps, Loss: 3.189, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.147\n",
      "    [batch 1022]: seen 102200 examples : 66.8 eps, Loss: 3.340, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 1029]: seen 102900 examples : 66.8 eps, Loss: 3.455, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 1036]: seen 103600 examples : 66.8 eps, Loss: 3.314, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.170\n",
      "    [batch 1043]: seen 104300 examples : 66.8 eps, Loss: 3.183, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1050]: seen 105000 examples : 66.9 eps, Loss: 3.405, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.157\n",
      "    [batch 1057]: seen 105700 examples : 66.9 eps, Loss: 3.250, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.160\n",
      "    [batch 1064]: seen 106400 examples : 66.9 eps, Loss: 3.519, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 1071]: seen 107100 examples : 66.9 eps, Loss: 3.443, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 1078]: seen 107800 examples : 66.9 eps, Loss: 3.429, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.177\n",
      "    [batch 1085]: seen 108500 examples : 66.9 eps, Loss: 3.440, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.161\n",
      "    [batch 1092]: seen 109200 examples : 66.9 eps, Loss: 3.428, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 1099]: seen 109900 examples : 66.9 eps, Loss: 3.362, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.160\n",
      "    [batch 1106]: seen 110600 examples : 66.9 eps, Loss: 3.400, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.154\n",
      "    [batch 1113]: seen 111300 examples : 66.9 eps, Loss: 3.323, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 1120]: seen 112000 examples : 67.0 eps, Loss: 3.266, Avg loss: 3.351, Best loss: 3.311, cov loss: 0.156\n",
      "    [batch 1127]: seen 112700 examples : 67.0 eps, Loss: 3.104, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.154\n",
      "    [batch 1134]: seen 113400 examples : 67.0 eps, Loss: 3.214, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.145\n",
      "    [batch 1141]: seen 114100 examples : 67.0 eps, Loss: 3.413, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.184\n",
      "    [batch 1148]: seen 114800 examples : 67.0 eps, Loss: 3.365, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.187\n",
      "    [batch 1155]: seen 115500 examples : 67.0 eps, Loss: 3.517, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 1162]: seen 116200 examples : 67.0 eps, Loss: 3.336, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.179\n",
      "    [batch 1169]: seen 116900 examples : 67.0 eps, Loss: 3.332, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 1176]: seen 117600 examples : 67.0 eps, Loss: 3.340, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 1183]: seen 118300 examples : 67.0 eps, Loss: 3.364, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 1190]: seen 119000 examples : 67.0 eps, Loss: 3.326, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.149\n",
      "    [batch 1197]: seen 119700 examples : 67.1 eps, Loss: 3.567, Avg loss: 3.355, Best loss: 3.311, cov loss: 0.178\n",
      "    [batch 1204]: seen 120400 examples : 67.1 eps, Loss: 3.469, Avg loss: 3.356, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 1211]: seen 121100 examples : 67.1 eps, Loss: 3.510, Avg loss: 3.359, Best loss: 3.311, cov loss: 0.163\n",
      "    [batch 1218]: seen 121800 examples : 67.1 eps, Loss: 3.388, Avg loss: 3.360, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 1225]: seen 122500 examples : 67.1 eps, Loss: 3.422, Avg loss: 3.363, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 1232]: seen 123200 examples : 67.1 eps, Loss: 3.352, Avg loss: 3.359, Best loss: 3.311, cov loss: 0.190\n",
      "    [batch 1239]: seen 123900 examples : 67.1 eps, Loss: 3.375, Avg loss: 3.358, Best loss: 3.311, cov loss: 0.159\n",
      "    [batch 1246]: seen 124600 examples : 67.1 eps, Loss: 3.359, Avg loss: 3.360, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 1253]: seen 125300 examples : 67.1 eps, Loss: 3.267, Avg loss: 3.357, Best loss: 3.311, cov loss: 0.183\n",
      "    [batch 1260]: seen 126000 examples : 67.1 eps, Loss: 3.247, Avg loss: 3.356, Best loss: 3.311, cov loss: 0.147\n",
      "    [batch 1267]: seen 126700 examples : 67.1 eps, Loss: 3.468, Avg loss: 3.354, Best loss: 3.311, cov loss: 0.163\n",
      "    [batch 1274]: seen 127400 examples : 67.1 eps, Loss: 3.152, Avg loss: 3.351, Best loss: 3.311, cov loss: 0.174\n",
      "    [batch 1281]: seen 128100 examples : 67.1 eps, Loss: 3.274, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 1288]: seen 128800 examples : 67.2 eps, Loss: 3.144, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.150\n",
      "    [batch 1295]: seen 129500 examples : 67.2 eps, Loss: 3.405, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.177\n",
      "    [batch 1302]: seen 130200 examples : 67.2 eps, Loss: 3.417, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 1309]: seen 130900 examples : 67.2 eps, Loss: 3.408, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 1316]: seen 131600 examples : 67.2 eps, Loss: 3.482, Avg loss: 3.352, Best loss: 3.311, cov loss: 0.174\n",
      "    [batch 1323]: seen 132300 examples : 67.2 eps, Loss: 3.404, Avg loss: 3.352, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 1330]: seen 133000 examples : 67.2 eps, Loss: 3.380, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.181\n",
      "    [batch 1337]: seen 133700 examples : 67.2 eps, Loss: 3.437, Avg loss: 3.354, Best loss: 3.311, cov loss: 0.179\n",
      "    [batch 1344]: seen 134400 examples : 67.2 eps, Loss: 3.223, Avg loss: 3.355, Best loss: 3.311, cov loss: 0.161\n",
      "    [batch 1351]: seen 135100 examples : 67.2 eps, Loss: 3.470, Avg loss: 3.355, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 1358]: seen 135800 examples : 67.2 eps, Loss: 3.201, Avg loss: 3.357, Best loss: 3.311, cov loss: 0.183\n",
      "    [batch 1365]: seen 136500 examples : 67.2 eps, Loss: 3.405, Avg loss: 3.355, Best loss: 3.311, cov loss: 0.186\n",
      "    [batch 1372]: seen 137200 examples : 67.2 eps, Loss: 3.129, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.143\n",
      "    [batch 1379]: seen 137900 examples : 67.2 eps, Loss: 3.275, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.159\n",
      "    [batch 1386]: seen 138600 examples : 67.2 eps, Loss: 3.531, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.154\n",
      "    [batch 1393]: seen 139300 examples : 67.3 eps, Loss: 3.533, Avg loss: 3.355, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 1400]: seen 140000 examples : 67.3 eps, Loss: 3.147, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 1407]: seen 140700 examples : 67.3 eps, Loss: 3.218, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.170\n",
      "    [batch 1414]: seen 141400 examples : 67.3 eps, Loss: 3.369, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.149\n",
      "    [batch 1421]: seen 142100 examples : 67.3 eps, Loss: 3.388, Avg loss: 3.357, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 1428]: seen 142800 examples : 67.3 eps, Loss: 3.183, Avg loss: 3.359, Best loss: 3.311, cov loss: 0.172\n",
      "    [batch 1435]: seen 143500 examples : 67.3 eps, Loss: 3.270, Avg loss: 3.361, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 1442]: seen 144200 examples : 67.3 eps, Loss: 3.394, Avg loss: 3.366, Best loss: 3.311, cov loss: 0.161\n",
      "    [batch 1449]: seen 144900 examples : 67.3 eps, Loss: 3.284, Avg loss: 3.364, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 1456]: seen 145600 examples : 67.3 eps, Loss: 3.349, Avg loss: 3.362, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 1463]: seen 146300 examples : 67.3 eps, Loss: 3.367, Avg loss: 3.361, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 1470]: seen 147000 examples : 67.3 eps, Loss: 3.390, Avg loss: 3.361, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 1477]: seen 147700 examples : 67.3 eps, Loss: 3.187, Avg loss: 3.361, Best loss: 3.311, cov loss: 0.150\n",
      "    [batch 1484]: seen 148400 examples : 67.3 eps, Loss: 3.528, Avg loss: 3.360, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 1491]: seen 149100 examples : 67.3 eps, Loss: 3.437, Avg loss: 3.358, Best loss: 3.311, cov loss: 0.181\n",
      "    [batch 1498]: seen 149800 examples : 67.3 eps, Loss: 3.155, Avg loss: 3.354, Best loss: 3.311, cov loss: 0.151\n",
      "    [batch 1505]: seen 150500 examples : 67.3 eps, Loss: 3.403, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.177\n",
      "    [batch 1512]: seen 151200 examples : 67.4 eps, Loss: 3.163, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 1519]: seen 151900 examples : 67.4 eps, Loss: 3.146, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.157\n",
      "    [batch 1526]: seen 152600 examples : 67.4 eps, Loss: 3.444, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.179\n",
      "    [batch 1533]: seen 153300 examples : 67.4 eps, Loss: 3.332, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 1540]: seen 154000 examples : 67.4 eps, Loss: 3.392, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.159\n",
      "    [batch 1547]: seen 154700 examples : 67.4 eps, Loss: 3.307, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1554]: seen 155400 examples : 67.4 eps, Loss: 3.191, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.150\n",
      "    [batch 1561]: seen 156100 examples : 67.4 eps, Loss: 3.447, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 1568]: seen 156800 examples : 67.4 eps, Loss: 3.345, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 1575]: seen 157500 examples : 67.4 eps, Loss: 3.512, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.161\n",
      "    [batch 1582]: seen 158200 examples : 67.4 eps, Loss: 3.376, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 1589]: seen 158900 examples : 67.4 eps, Loss: 3.415, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 1596]: seen 159600 examples : 67.4 eps, Loss: 3.246, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 1603]: seen 160300 examples : 67.4 eps, Loss: 3.200, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.155\n",
      "    [batch 1610]: seen 161000 examples : 67.4 eps, Loss: 3.342, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 1617]: seen 161700 examples : 67.4 eps, Loss: 3.476, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 1624]: seen 162400 examples : 67.4 eps, Loss: 3.433, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.140\n",
      "    [batch 1631]: seen 163100 examples : 67.4 eps, Loss: 3.460, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.178\n",
      "    [batch 1638]: seen 163800 examples : 67.4 eps, Loss: 3.377, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 1645]: seen 164500 examples : 67.4 eps, Loss: 3.276, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.163\n",
      "    [batch 1652]: seen 165200 examples : 67.5 eps, Loss: 3.394, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.142\n",
      "    [batch 1659]: seen 165900 examples : 67.5 eps, Loss: 3.293, Avg loss: 3.353, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 1666]: seen 166600 examples : 67.5 eps, Loss: 3.255, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.152\n",
      "    [batch 1673]: seen 167300 examples : 67.5 eps, Loss: 3.539, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.181\n",
      "    [batch 1680]: seen 168000 examples : 67.5 eps, Loss: 3.311, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 1687]: seen 168700 examples : 67.5 eps, Loss: 3.301, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 1694]: seen 169400 examples : 67.5 eps, Loss: 3.536, Avg loss: 3.350, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 1701]: seen 170100 examples : 67.5 eps, Loss: 3.511, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.193\n",
      "    [batch 1708]: seen 170800 examples : 67.5 eps, Loss: 3.557, Avg loss: 3.352, Best loss: 3.311, cov loss: 0.161\n",
      "    [batch 1715]: seen 171500 examples : 67.5 eps, Loss: 3.237, Avg loss: 3.351, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 1722]: seen 172200 examples : 67.5 eps, Loss: 3.262, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 1729]: seen 172900 examples : 67.5 eps, Loss: 3.251, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 1736]: seen 173600 examples : 67.5 eps, Loss: 3.464, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.189\n",
      "    [batch 1743]: seen 174300 examples : 67.5 eps, Loss: 3.394, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 1750]: seen 175000 examples : 67.5 eps, Loss: 3.262, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 1757]: seen 175700 examples : 67.5 eps, Loss: 3.385, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 1764]: seen 176400 examples : 67.5 eps, Loss: 3.281, Avg loss: 3.341, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 1771]: seen 177100 examples : 67.5 eps, Loss: 3.296, Avg loss: 3.336, Best loss: 3.311, cov loss: 0.159\n",
      "    [batch 1778]: seen 177800 examples : 67.5 eps, Loss: 3.275, Avg loss: 3.334, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 1785]: seen 178500 examples : 67.5 eps, Loss: 3.324, Avg loss: 3.333, Best loss: 3.311, cov loss: 0.182\n",
      "    [batch 1792]: seen 179200 examples : 67.5 eps, Loss: 3.190, Avg loss: 3.335, Best loss: 3.311, cov loss: 0.160\n",
      "    [batch 1799]: seen 179900 examples : 67.5 eps, Loss: 3.396, Avg loss: 3.335, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 1806]: seen 180600 examples : 67.5 eps, Loss: 3.250, Avg loss: 3.334, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 1813]: seen 181300 examples : 67.5 eps, Loss: 3.592, Avg loss: 3.341, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 1820]: seen 182000 examples : 67.6 eps, Loss: 3.396, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.157\n",
      "    [batch 1827]: seen 182700 examples : 67.6 eps, Loss: 3.376, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 1834]: seen 183400 examples : 67.6 eps, Loss: 3.401, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 1841]: seen 184100 examples : 67.6 eps, Loss: 3.453, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.184\n",
      "    [batch 1848]: seen 184800 examples : 67.6 eps, Loss: 3.501, Avg loss: 3.352, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 1855]: seen 185500 examples : 67.6 eps, Loss: 3.391, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.179\n",
      "    [batch 1862]: seen 186200 examples : 67.6 eps, Loss: 3.221, Avg loss: 3.352, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 1869]: seen 186900 examples : 67.6 eps, Loss: 3.235, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.155\n",
      "    [batch 1876]: seen 187600 examples : 67.6 eps, Loss: 3.235, Avg loss: 3.349, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 1883]: seen 188300 examples : 67.6 eps, Loss: 3.147, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 1890]: seen 189000 examples : 67.6 eps, Loss: 3.420, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 1897]: seen 189700 examples : 67.6 eps, Loss: 3.169, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 1904]: seen 190400 examples : 67.6 eps, Loss: 3.317, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.159\n",
      "    [batch 1911]: seen 191100 examples : 67.6 eps, Loss: 3.258, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 1918]: seen 191800 examples : 67.6 eps, Loss: 3.258, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 1925]: seen 192500 examples : 67.6 eps, Loss: 3.184, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 1932]: seen 193200 examples : 67.6 eps, Loss: 3.338, Avg loss: 3.341, Best loss: 3.311, cov loss: 0.185\n",
      "    [batch 1939]: seen 193900 examples : 67.6 eps, Loss: 3.441, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 1946]: seen 194600 examples : 67.6 eps, Loss: 3.159, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.153\n",
      "    [batch 1953]: seen 195300 examples : 67.6 eps, Loss: 3.227, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 1960]: seen 196000 examples : 67.6 eps, Loss: 3.286, Avg loss: 3.335, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 1967]: seen 196700 examples : 67.6 eps, Loss: 3.049, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 1974]: seen 197400 examples : 67.6 eps, Loss: 3.277, Avg loss: 3.329, Best loss: 3.311, cov loss: 0.172\n",
      "    [batch 1981]: seen 198100 examples : 67.6 eps, Loss: 3.431, Avg loss: 3.335, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 1988]: seen 198800 examples : 67.6 eps, Loss: 3.358, Avg loss: 3.331, Best loss: 3.311, cov loss: 0.160\n",
      "    [batch 1995]: seen 199500 examples : 67.6 eps, Loss: 3.253, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.155\n",
      "    [batch 2002]: seen 200200 examples : 67.6 eps, Loss: 3.524, Avg loss: 3.331, Best loss: 3.311, cov loss: 0.184\n",
      "    [batch 2009]: seen 200900 examples : 67.6 eps, Loss: 3.403, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.182\n",
      "    [batch 2016]: seen 201600 examples : 67.6 eps, Loss: 3.294, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 2023]: seen 202300 examples : 67.7 eps, Loss: 3.268, Avg loss: 3.328, Best loss: 3.311, cov loss: 0.149\n",
      "    [batch 2030]: seen 203000 examples : 67.7 eps, Loss: 3.289, Avg loss: 3.327, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 2037]: seen 203700 examples : 67.7 eps, Loss: 3.308, Avg loss: 3.332, Best loss: 3.311, cov loss: 0.155\n",
      "    [batch 2044]: seen 204400 examples : 67.7 eps, Loss: 3.292, Avg loss: 3.337, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 2051]: seen 205100 examples : 67.7 eps, Loss: 3.436, Avg loss: 3.340, Best loss: 3.311, cov loss: 0.174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2058]: seen 205800 examples : 67.7 eps, Loss: 3.263, Avg loss: 3.336, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 2065]: seen 206500 examples : 67.7 eps, Loss: 3.272, Avg loss: 3.338, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 2072]: seen 207200 examples : 67.7 eps, Loss: 3.296, Avg loss: 3.341, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 2079]: seen 207900 examples : 67.7 eps, Loss: 3.397, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.183\n",
      "    [batch 2086]: seen 208600 examples : 67.7 eps, Loss: 3.256, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 2093]: seen 209300 examples : 67.7 eps, Loss: 3.489, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 2100]: seen 210000 examples : 67.7 eps, Loss: 3.244, Avg loss: 3.340, Best loss: 3.311, cov loss: 0.159\n",
      "    [batch 2107]: seen 210700 examples : 67.7 eps, Loss: 3.304, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.160\n",
      "    [batch 2114]: seen 211400 examples : 67.7 eps, Loss: 3.493, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.174\n",
      "    [batch 2121]: seen 212100 examples : 67.7 eps, Loss: 3.215, Avg loss: 3.340, Best loss: 3.311, cov loss: 0.155\n",
      "    [batch 2128]: seen 212800 examples : 67.7 eps, Loss: 3.308, Avg loss: 3.331, Best loss: 3.311, cov loss: 0.178\n",
      "    [batch 2135]: seen 213500 examples : 67.7 eps, Loss: 3.282, Avg loss: 3.328, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 2142]: seen 214200 examples : 67.7 eps, Loss: 3.359, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 2149]: seen 214900 examples : 67.7 eps, Loss: 3.229, Avg loss: 3.325, Best loss: 3.311, cov loss: 0.153\n",
      "    [batch 2156]: seen 215600 examples : 67.7 eps, Loss: 3.485, Avg loss: 3.329, Best loss: 3.311, cov loss: 0.185\n",
      "    [batch 2163]: seen 216300 examples : 67.7 eps, Loss: 3.518, Avg loss: 3.336, Best loss: 3.311, cov loss: 0.174\n",
      "    [batch 2170]: seen 217000 examples : 67.7 eps, Loss: 3.563, Avg loss: 3.338, Best loss: 3.311, cov loss: 0.191\n",
      "    [batch 2177]: seen 217700 examples : 67.7 eps, Loss: 3.485, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 2184]: seen 218400 examples : 67.7 eps, Loss: 3.245, Avg loss: 3.332, Best loss: 3.311, cov loss: 0.152\n",
      "    [batch 2191]: seen 219100 examples : 67.7 eps, Loss: 3.452, Avg loss: 3.331, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 2198]: seen 219800 examples : 67.7 eps, Loss: 3.340, Avg loss: 3.329, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 2205]: seen 220500 examples : 67.7 eps, Loss: 3.273, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 2212]: seen 221200 examples : 67.7 eps, Loss: 3.392, Avg loss: 3.332, Best loss: 3.311, cov loss: 0.163\n",
      "    [batch 2219]: seen 221900 examples : 67.7 eps, Loss: 3.318, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.157\n",
      "    [batch 2226]: seen 222600 examples : 67.7 eps, Loss: 3.185, Avg loss: 3.328, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 2233]: seen 223300 examples : 67.7 eps, Loss: 3.440, Avg loss: 3.328, Best loss: 3.311, cov loss: 0.189\n",
      "    [batch 2240]: seen 224000 examples : 67.7 eps, Loss: 3.315, Avg loss: 3.328, Best loss: 3.311, cov loss: 0.173\n",
      "    [batch 2247]: seen 224700 examples : 67.7 eps, Loss: 3.255, Avg loss: 3.333, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 2254]: seen 225400 examples : 67.7 eps, Loss: 3.194, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 2261]: seen 226100 examples : 67.7 eps, Loss: 3.449, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 2268]: seen 226800 examples : 67.7 eps, Loss: 3.389, Avg loss: 3.325, Best loss: 3.311, cov loss: 0.161\n",
      "    [batch 2275]: seen 227500 examples : 67.7 eps, Loss: 3.234, Avg loss: 3.323, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 2282]: seen 228200 examples : 67.8 eps, Loss: 3.472, Avg loss: 3.328, Best loss: 3.311, cov loss: 0.148\n",
      "    [batch 2289]: seen 228900 examples : 67.8 eps, Loss: 3.435, Avg loss: 3.329, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 2296]: seen 229600 examples : 67.8 eps, Loss: 3.230, Avg loss: 3.327, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 2303]: seen 230300 examples : 67.8 eps, Loss: 3.389, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 2310]: seen 231000 examples : 67.8 eps, Loss: 3.475, Avg loss: 3.329, Best loss: 3.311, cov loss: 0.180\n",
      "    [batch 2317]: seen 231700 examples : 67.8 eps, Loss: 3.228, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.164\n",
      "    [batch 2324]: seen 232400 examples : 67.8 eps, Loss: 3.372, Avg loss: 3.331, Best loss: 3.311, cov loss: 0.153\n",
      "    [batch 2331]: seen 233100 examples : 67.8 eps, Loss: 3.353, Avg loss: 3.337, Best loss: 3.311, cov loss: 0.178\n",
      "    [batch 2338]: seen 233800 examples : 67.8 eps, Loss: 3.294, Avg loss: 3.334, Best loss: 3.311, cov loss: 0.156\n",
      "    [batch 2345]: seen 234500 examples : 67.8 eps, Loss: 3.563, Avg loss: 3.335, Best loss: 3.311, cov loss: 0.186\n",
      "    [batch 2352]: seen 235200 examples : 67.8 eps, Loss: 3.305, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 2359]: seen 235900 examples : 67.8 eps, Loss: 3.187, Avg loss: 3.338, Best loss: 3.311, cov loss: 0.153\n",
      "    [batch 2366]: seen 236600 examples : 67.8 eps, Loss: 3.223, Avg loss: 3.332, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 2373]: seen 237300 examples : 67.8 eps, Loss: 3.312, Avg loss: 3.334, Best loss: 3.311, cov loss: 0.156\n",
      "    [batch 2380]: seen 238000 examples : 67.8 eps, Loss: 3.318, Avg loss: 3.333, Best loss: 3.311, cov loss: 0.155\n",
      "    [batch 2387]: seen 238700 examples : 67.8 eps, Loss: 3.129, Avg loss: 3.332, Best loss: 3.311, cov loss: 0.151\n",
      "    [batch 2394]: seen 239400 examples : 67.8 eps, Loss: 3.297, Avg loss: 3.334, Best loss: 3.311, cov loss: 0.183\n",
      "    [batch 2401]: seen 240100 examples : 67.8 eps, Loss: 3.119, Avg loss: 3.331, Best loss: 3.311, cov loss: 0.152\n",
      "    [batch 2408]: seen 240800 examples : 67.8 eps, Loss: 3.517, Avg loss: 3.337, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 2415]: seen 241500 examples : 67.8 eps, Loss: 3.421, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 2422]: seen 242200 examples : 67.8 eps, Loss: 3.513, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.183\n",
      "    [batch 2429]: seen 242900 examples : 67.8 eps, Loss: 3.441, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.178\n",
      "    [batch 2436]: seen 243600 examples : 67.8 eps, Loss: 3.409, Avg loss: 3.348, Best loss: 3.311, cov loss: 0.183\n",
      "    [batch 2443]: seen 244300 examples : 67.8 eps, Loss: 3.215, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.172\n",
      "    [batch 2450]: seen 245000 examples : 67.8 eps, Loss: 3.417, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.179\n",
      "    [batch 2457]: seen 245700 examples : 67.8 eps, Loss: 3.309, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.155\n",
      "    [batch 2464]: seen 246400 examples : 67.8 eps, Loss: 3.282, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.159\n",
      "    [batch 2471]: seen 247100 examples : 67.8 eps, Loss: 3.379, Avg loss: 3.347, Best loss: 3.311, cov loss: 0.163\n",
      "    [batch 2478]: seen 247800 examples : 67.8 eps, Loss: 3.358, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 2485]: seen 248500 examples : 67.8 eps, Loss: 3.487, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 2492]: seen 249200 examples : 67.8 eps, Loss: 3.344, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 2499]: seen 249900 examples : 67.8 eps, Loss: 3.357, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.178\n",
      "    [batch 2506]: seen 250600 examples : 67.8 eps, Loss: 3.213, Avg loss: 3.337, Best loss: 3.311, cov loss: 0.156\n",
      "    [batch 2513]: seen 251300 examples : 67.9 eps, Loss: 3.277, Avg loss: 3.334, Best loss: 3.311, cov loss: 0.182\n",
      "    [batch 2520]: seen 252000 examples : 67.9 eps, Loss: 3.175, Avg loss: 3.331, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 2527]: seen 252700 examples : 67.9 eps, Loss: 3.249, Avg loss: 3.333, Best loss: 3.311, cov loss: 0.161\n",
      "    [batch 2534]: seen 253400 examples : 67.9 eps, Loss: 3.460, Avg loss: 3.335, Best loss: 3.311, cov loss: 0.170\n",
      "    [batch 2541]: seen 254100 examples : 67.9 eps, Loss: 3.357, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.149\n",
      "    [batch 2548]: seen 254800 examples : 67.9 eps, Loss: 3.380, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.154\n",
      "    [batch 2555]: seen 255500 examples : 67.9 eps, Loss: 3.308, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2562]: seen 256200 examples : 67.9 eps, Loss: 3.340, Avg loss: 3.335, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 2569]: seen 256900 examples : 67.9 eps, Loss: 3.267, Avg loss: 3.332, Best loss: 3.311, cov loss: 0.170\n",
      "    [batch 2576]: seen 257600 examples : 67.9 eps, Loss: 3.468, Avg loss: 3.334, Best loss: 3.311, cov loss: 0.181\n",
      "    [batch 2583]: seen 258300 examples : 67.9 eps, Loss: 3.307, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 2590]: seen 259000 examples : 67.9 eps, Loss: 3.481, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.161\n",
      "    [batch 2597]: seen 259700 examples : 67.9 eps, Loss: 3.146, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 2604]: seen 260400 examples : 67.9 eps, Loss: 3.417, Avg loss: 3.329, Best loss: 3.311, cov loss: 0.172\n",
      "    [batch 2611]: seen 261100 examples : 67.9 eps, Loss: 3.221, Avg loss: 3.328, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 2618]: seen 261800 examples : 67.9 eps, Loss: 3.284, Avg loss: 3.325, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 2625]: seen 262500 examples : 67.9 eps, Loss: 3.211, Avg loss: 3.323, Best loss: 3.311, cov loss: 0.144\n",
      "    [batch 2632]: seen 263200 examples : 67.9 eps, Loss: 3.053, Avg loss: 3.320, Best loss: 3.311, cov loss: 0.144\n",
      "    [batch 2639]: seen 263900 examples : 67.9 eps, Loss: 3.390, Avg loss: 3.321, Best loss: 3.311, cov loss: 0.178\n",
      "    [batch 2646]: seen 264600 examples : 67.9 eps, Loss: 3.496, Avg loss: 3.327, Best loss: 3.311, cov loss: 0.188\n",
      "    [batch 2653]: seen 265300 examples : 67.9 eps, Loss: 3.194, Avg loss: 3.332, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 2660]: seen 266000 examples : 67.9 eps, Loss: 3.356, Avg loss: 3.333, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 2667]: seen 266700 examples : 67.9 eps, Loss: 3.350, Avg loss: 3.334, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 2674]: seen 267400 examples : 67.9 eps, Loss: 3.371, Avg loss: 3.332, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 2681]: seen 268100 examples : 67.9 eps, Loss: 3.482, Avg loss: 3.337, Best loss: 3.311, cov loss: 0.174\n",
      "    [batch 2688]: seen 268800 examples : 67.9 eps, Loss: 3.304, Avg loss: 3.334, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 2695]: seen 269500 examples : 67.9 eps, Loss: 3.433, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.149\n",
      "    [batch 2702]: seen 270200 examples : 67.9 eps, Loss: 3.359, Avg loss: 3.337, Best loss: 3.311, cov loss: 0.160\n",
      "    [batch 2709]: seen 270900 examples : 67.9 eps, Loss: 3.393, Avg loss: 3.334, Best loss: 3.311, cov loss: 0.179\n",
      "    [batch 2716]: seen 271600 examples : 67.9 eps, Loss: 3.351, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 2723]: seen 272300 examples : 67.9 eps, Loss: 3.354, Avg loss: 3.337, Best loss: 3.311, cov loss: 0.172\n",
      "    [batch 2730]: seen 273000 examples : 67.9 eps, Loss: 3.481, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 2737]: seen 273700 examples : 67.9 eps, Loss: 3.452, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 2744]: seen 274400 examples : 67.9 eps, Loss: 3.333, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.171\n",
      "    [batch 2751]: seen 275100 examples : 67.9 eps, Loss: 3.261, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 2758]: seen 275800 examples : 68.0 eps, Loss: 3.360, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.151\n",
      "    [batch 2765]: seen 276500 examples : 68.0 eps, Loss: 3.350, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 2772]: seen 277200 examples : 68.0 eps, Loss: 3.547, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.179\n",
      "    [batch 2779]: seen 277900 examples : 68.0 eps, Loss: 3.300, Avg loss: 3.346, Best loss: 3.311, cov loss: 0.151\n",
      "    [batch 2786]: seen 278600 examples : 68.0 eps, Loss: 3.321, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 2793]: seen 279300 examples : 68.0 eps, Loss: 3.450, Avg loss: 3.345, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 2800]: seen 280000 examples : 68.0 eps, Loss: 3.409, Avg loss: 3.344, Best loss: 3.311, cov loss: 0.155\n",
      "    [batch 2807]: seen 280700 examples : 68.0 eps, Loss: 3.449, Avg loss: 3.343, Best loss: 3.311, cov loss: 0.164\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:08:49\n",
      "[EPOCH 23] Complete. Avg Loss: 3.343346171146398; Best Loss: 3.3108396530151367\n",
      "[EPOCH 24] Starting training..\n",
      "    [batch 7]: seen 700 examples : 69.0 eps, Loss: 3.272, Avg loss: 3.342, Best loss: 3.311, cov loss: 0.153\n",
      "    [batch 14]: seen 1400 examples : 69.0 eps, Loss: 3.388, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.162\n",
      "    [batch 21]: seen 2100 examples : 69.0 eps, Loss: 3.306, Avg loss: 3.338, Best loss: 3.311, cov loss: 0.147\n",
      "    [batch 28]: seen 2800 examples : 69.0 eps, Loss: 3.218, Avg loss: 3.339, Best loss: 3.311, cov loss: 0.168\n",
      "    [batch 35]: seen 3500 examples : 69.0 eps, Loss: 3.258, Avg loss: 3.335, Best loss: 3.311, cov loss: 0.160\n",
      "    [batch 42]: seen 4200 examples : 68.9 eps, Loss: 3.239, Avg loss: 3.330, Best loss: 3.311, cov loss: 0.176\n",
      "    [batch 49]: seen 4900 examples : 68.9 eps, Loss: 3.351, Avg loss: 3.326, Best loss: 3.311, cov loss: 0.165\n",
      "    [batch 56]: seen 5600 examples : 68.9 eps, Loss: 3.321, Avg loss: 3.322, Best loss: 3.311, cov loss: 0.167\n",
      "    [batch 63]: seen 6300 examples : 68.9 eps, Loss: 3.320, Avg loss: 3.316, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 70]: seen 7000 examples : 68.9 eps, Loss: 3.379, Avg loss: 3.320, Best loss: 3.311, cov loss: 0.172\n",
      "    [batch 77]: seen 7700 examples : 68.9 eps, Loss: 3.505, Avg loss: 3.321, Best loss: 3.311, cov loss: 0.174\n",
      "    [batch 84]: seen 8400 examples : 68.9 eps, Loss: 3.294, Avg loss: 3.319, Best loss: 3.311, cov loss: 0.158\n",
      "    [batch 91]: seen 9100 examples : 68.9 eps, Loss: 3.473, Avg loss: 3.323, Best loss: 3.311, cov loss: 0.166\n",
      "    [batch 98]: seen 9800 examples : 68.9 eps, Loss: 3.515, Avg loss: 3.327, Best loss: 3.311, cov loss: 0.180\n",
      "    [batch 105]: seen 10500 examples : 68.9 eps, Loss: 3.132, Avg loss: 3.326, Best loss: 3.311, cov loss: 0.161\n",
      "    [batch 112]: seen 11200 examples : 68.9 eps, Loss: 3.482, Avg loss: 3.323, Best loss: 3.311, cov loss: 0.157\n",
      "    [batch 119]: seen 11900 examples : 68.9 eps, Loss: 3.253, Avg loss: 3.325, Best loss: 3.311, cov loss: 0.157\n",
      "    [batch 126]: seen 12600 examples : 68.9 eps, Loss: 3.193, Avg loss: 3.329, Best loss: 3.311, cov loss: 0.159\n",
      "    [batch 133]: seen 13300 examples : 68.9 eps, Loss: 3.274, Avg loss: 3.328, Best loss: 3.311, cov loss: 0.156\n",
      "    [batch 140]: seen 14000 examples : 68.9 eps, Loss: 3.179, Avg loss: 3.327, Best loss: 3.311, cov loss: 0.163\n",
      "    [batch 147]: seen 14700 examples : 68.9 eps, Loss: 3.258, Avg loss: 3.326, Best loss: 3.311, cov loss: 0.161\n",
      "    [batch 154]: seen 15400 examples : 69.0 eps, Loss: 3.258, Avg loss: 3.320, Best loss: 3.311, cov loss: 0.175\n",
      "    [batch 161]: seen 16100 examples : 69.0 eps, Loss: 3.204, Avg loss: 3.313, Best loss: 3.311, cov loss: 0.169\n",
      "    [batch 167]: seen 16700 examples : 68.0 eps, Loss: 3.045, Avg loss: 3.310, Best loss: 3.310, cov loss: 0.145\n",
      "    [batch 170]: seen 17000 examples : 65.4 eps, Loss: 3.288, Avg loss: 3.308, Best loss: 3.308, cov loss: 0.159\n",
      "    [batch 173]: seen 17300 examples : 63.8 eps, Loss: 3.262, Avg loss: 3.306, Best loss: 3.306, cov loss: 0.173\n",
      "    [batch 178]: seen 17800 examples : 63.2 eps, Loss: 3.259, Avg loss: 3.307, Best loss: 3.305, cov loss: 0.164\n",
      "    [batch 185]: seen 18500 examples : 63.4 eps, Loss: 3.323, Avg loss: 3.306, Best loss: 3.305, cov loss: 0.163\n",
      "    [batch 192]: seen 19200 examples : 63.6 eps, Loss: 3.227, Avg loss: 3.308, Best loss: 3.305, cov loss: 0.164\n",
      "    [batch 199]: seen 19900 examples : 63.8 eps, Loss: 3.338, Avg loss: 3.306, Best loss: 3.305, cov loss: 0.163\n",
      "    [batch 206]: seen 20600 examples : 63.9 eps, Loss: 3.251, Avg loss: 3.308, Best loss: 3.305, cov loss: 0.152\n",
      "    [batch 213]: seen 21300 examples : 64.1 eps, Loss: 3.111, Avg loss: 3.310, Best loss: 3.305, cov loss: 0.175\n",
      "    [batch 218]: seen 21800 examples : 63.5 eps, Loss: 3.308, Avg loss: 3.306, Best loss: 3.305, cov loss: 0.159\n",
      "    [batch 225]: seen 22500 examples : 63.7 eps, Loss: 3.290, Avg loss: 3.308, Best loss: 3.305, cov loss: 0.170\n",
      "    [batch 232]: seen 23200 examples : 63.8 eps, Loss: 3.218, Avg loss: 3.307, Best loss: 3.305, cov loss: 0.164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 239]: seen 23900 examples : 64.0 eps, Loss: 3.295, Avg loss: 3.307, Best loss: 3.305, cov loss: 0.170\n",
      "    [batch 244]: seen 24400 examples : 63.0 eps, Loss: 3.260, Avg loss: 3.304, Best loss: 3.304, cov loss: 0.157\n",
      "    [batch 251]: seen 25100 examples : 63.1 eps, Loss: 3.489, Avg loss: 3.306, Best loss: 3.304, cov loss: 0.162\n",
      "    [batch 258]: seen 25800 examples : 63.3 eps, Loss: 3.292, Avg loss: 3.310, Best loss: 3.304, cov loss: 0.169\n",
      "    [batch 265]: seen 26500 examples : 63.4 eps, Loss: 3.166, Avg loss: 3.307, Best loss: 3.304, cov loss: 0.151\n",
      "    [batch 272]: seen 27200 examples : 63.5 eps, Loss: 3.414, Avg loss: 3.310, Best loss: 3.304, cov loss: 0.162\n",
      "    [batch 279]: seen 27900 examples : 63.7 eps, Loss: 3.209, Avg loss: 3.307, Best loss: 3.304, cov loss: 0.159\n",
      "    [batch 286]: seen 28600 examples : 63.8 eps, Loss: 3.329, Avg loss: 3.307, Best loss: 3.304, cov loss: 0.183\n",
      "    [batch 293]: seen 29300 examples : 63.9 eps, Loss: 3.275, Avg loss: 3.308, Best loss: 3.304, cov loss: 0.170\n",
      "    [batch 300]: seen 30000 examples : 64.0 eps, Loss: 3.116, Avg loss: 3.307, Best loss: 3.304, cov loss: 0.181\n",
      "    [batch 307]: seen 30700 examples : 64.1 eps, Loss: 3.324, Avg loss: 3.308, Best loss: 3.304, cov loss: 0.157\n",
      "    [batch 314]: seen 31400 examples : 64.2 eps, Loss: 3.196, Avg loss: 3.310, Best loss: 3.304, cov loss: 0.158\n",
      "    [batch 321]: seen 32100 examples : 64.3 eps, Loss: 3.431, Avg loss: 3.306, Best loss: 3.304, cov loss: 0.150\n",
      "    [batch 328]: seen 32800 examples : 64.4 eps, Loss: 3.347, Avg loss: 3.307, Best loss: 3.304, cov loss: 0.186\n",
      "    [batch 335]: seen 33500 examples : 64.5 eps, Loss: 3.134, Avg loss: 3.308, Best loss: 3.304, cov loss: 0.140\n",
      "    [batch 342]: seen 34200 examples : 64.6 eps, Loss: 3.218, Avg loss: 3.310, Best loss: 3.304, cov loss: 0.159\n",
      "    [batch 349]: seen 34900 examples : 64.6 eps, Loss: 3.299, Avg loss: 3.308, Best loss: 3.304, cov loss: 0.163\n",
      "    [batch 356]: seen 35600 examples : 64.7 eps, Loss: 3.172, Avg loss: 3.311, Best loss: 3.304, cov loss: 0.153\n",
      "    [batch 363]: seen 36300 examples : 64.8 eps, Loss: 3.328, Avg loss: 3.316, Best loss: 3.304, cov loss: 0.151\n",
      "    [batch 370]: seen 37000 examples : 64.9 eps, Loss: 3.279, Avg loss: 3.311, Best loss: 3.304, cov loss: 0.168\n",
      "    [batch 377]: seen 37700 examples : 64.9 eps, Loss: 3.390, Avg loss: 3.312, Best loss: 3.304, cov loss: 0.182\n",
      "    [batch 384]: seen 38400 examples : 64.6 eps, Loss: 3.138, Avg loss: 3.304, Best loss: 3.304, cov loss: 0.153\n",
      "    [batch 391]: seen 39100 examples : 64.7 eps, Loss: 3.286, Avg loss: 3.310, Best loss: 3.304, cov loss: 0.162\n",
      "    [batch 398]: seen 39800 examples : 64.8 eps, Loss: 3.327, Avg loss: 3.312, Best loss: 3.304, cov loss: 0.173\n",
      "    [batch 405]: seen 40500 examples : 64.8 eps, Loss: 3.314, Avg loss: 3.314, Best loss: 3.304, cov loss: 0.164\n",
      "    [batch 412]: seen 41200 examples : 64.9 eps, Loss: 3.367, Avg loss: 3.320, Best loss: 3.304, cov loss: 0.152\n",
      "    [batch 419]: seen 41900 examples : 65.0 eps, Loss: 3.408, Avg loss: 3.323, Best loss: 3.304, cov loss: 0.164\n",
      "    [batch 426]: seen 42600 examples : 65.0 eps, Loss: 3.358, Avg loss: 3.321, Best loss: 3.304, cov loss: 0.158\n",
      "    [batch 433]: seen 43300 examples : 65.1 eps, Loss: 3.114, Avg loss: 3.319, Best loss: 3.304, cov loss: 0.153\n",
      "    [batch 440]: seen 44000 examples : 65.2 eps, Loss: 3.381, Avg loss: 3.322, Best loss: 3.304, cov loss: 0.167\n",
      "    [batch 447]: seen 44700 examples : 65.2 eps, Loss: 3.257, Avg loss: 3.324, Best loss: 3.304, cov loss: 0.148\n",
      "    [batch 454]: seen 45400 examples : 65.3 eps, Loss: 3.311, Avg loss: 3.326, Best loss: 3.304, cov loss: 0.151\n",
      "    [batch 461]: seen 46100 examples : 65.3 eps, Loss: 3.292, Avg loss: 3.326, Best loss: 3.304, cov loss: 0.144\n",
      "    [batch 468]: seen 46800 examples : 65.4 eps, Loss: 3.468, Avg loss: 3.324, Best loss: 3.304, cov loss: 0.181\n",
      "    [batch 475]: seen 47500 examples : 65.4 eps, Loss: 3.354, Avg loss: 3.319, Best loss: 3.304, cov loss: 0.168\n",
      "    [batch 482]: seen 48200 examples : 65.5 eps, Loss: 3.118, Avg loss: 3.319, Best loss: 3.304, cov loss: 0.145\n",
      "    [batch 489]: seen 48900 examples : 65.5 eps, Loss: 3.325, Avg loss: 3.317, Best loss: 3.304, cov loss: 0.177\n",
      "    [batch 496]: seen 49600 examples : 65.6 eps, Loss: 3.281, Avg loss: 3.313, Best loss: 3.304, cov loss: 0.167\n",
      "    [batch 503]: seen 50300 examples : 65.6 eps, Loss: 3.435, Avg loss: 3.319, Best loss: 3.304, cov loss: 0.162\n",
      "    [batch 510]: seen 51000 examples : 65.7 eps, Loss: 3.279, Avg loss: 3.317, Best loss: 3.304, cov loss: 0.169\n",
      "    [batch 517]: seen 51700 examples : 65.7 eps, Loss: 3.314, Avg loss: 3.318, Best loss: 3.304, cov loss: 0.164\n",
      "    [batch 524]: seen 52400 examples : 65.7 eps, Loss: 3.195, Avg loss: 3.316, Best loss: 3.304, cov loss: 0.150\n",
      "    [batch 531]: seen 53100 examples : 65.8 eps, Loss: 3.219, Avg loss: 3.316, Best loss: 3.304, cov loss: 0.135\n",
      "    [batch 538]: seen 53800 examples : 65.8 eps, Loss: 3.279, Avg loss: 3.315, Best loss: 3.304, cov loss: 0.160\n",
      "    [batch 545]: seen 54500 examples : 65.9 eps, Loss: 3.200, Avg loss: 3.315, Best loss: 3.304, cov loss: 0.160\n",
      "    [batch 552]: seen 55200 examples : 65.9 eps, Loss: 3.169, Avg loss: 3.314, Best loss: 3.304, cov loss: 0.153\n",
      "    [batch 559]: seen 55900 examples : 65.9 eps, Loss: 3.144, Avg loss: 3.310, Best loss: 3.304, cov loss: 0.153\n",
      "    [batch 566]: seen 56600 examples : 66.0 eps, Loss: 3.197, Avg loss: 3.311, Best loss: 3.304, cov loss: 0.151\n",
      "    [batch 573]: seen 57300 examples : 66.0 eps, Loss: 3.307, Avg loss: 3.305, Best loss: 3.304, cov loss: 0.152\n",
      "    [batch 578]: seen 57800 examples : 65.8 eps, Loss: 3.420, Avg loss: 3.307, Best loss: 3.303, cov loss: 0.178\n",
      "    [batch 585]: seen 58500 examples : 65.8 eps, Loss: 3.559, Avg loss: 3.308, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 592]: seen 59200 examples : 65.9 eps, Loss: 3.390, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 599]: seen 59900 examples : 65.9 eps, Loss: 3.403, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.151\n",
      "    [batch 606]: seen 60600 examples : 65.9 eps, Loss: 3.236, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.170\n",
      "    [batch 613]: seen 61300 examples : 66.0 eps, Loss: 3.284, Avg loss: 3.310, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 620]: seen 62000 examples : 66.0 eps, Loss: 3.486, Avg loss: 3.311, Best loss: 3.303, cov loss: 0.165\n",
      "    [batch 627]: seen 62700 examples : 66.0 eps, Loss: 3.407, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.150\n",
      "    [batch 634]: seen 63400 examples : 66.1 eps, Loss: 3.312, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.174\n",
      "    [batch 641]: seen 64100 examples : 66.1 eps, Loss: 3.363, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 648]: seen 64800 examples : 66.1 eps, Loss: 3.432, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 655]: seen 65500 examples : 66.1 eps, Loss: 3.238, Avg loss: 3.310, Best loss: 3.303, cov loss: 0.181\n",
      "    [batch 662]: seen 66200 examples : 66.2 eps, Loss: 3.253, Avg loss: 3.310, Best loss: 3.303, cov loss: 0.157\n",
      "    [batch 669]: seen 66900 examples : 66.2 eps, Loss: 3.369, Avg loss: 3.311, Best loss: 3.303, cov loss: 0.157\n",
      "    [batch 676]: seen 67600 examples : 66.2 eps, Loss: 3.406, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.177\n",
      "    [batch 683]: seen 68300 examples : 66.3 eps, Loss: 3.060, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.156\n",
      "    [batch 690]: seen 69000 examples : 66.3 eps, Loss: 3.232, Avg loss: 3.314, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 697]: seen 69700 examples : 66.3 eps, Loss: 3.104, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.154\n",
      "    [batch 704]: seen 70400 examples : 66.3 eps, Loss: 3.401, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.172\n",
      "    [batch 711]: seen 71100 examples : 66.4 eps, Loss: 3.395, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 718]: seen 71800 examples : 66.4 eps, Loss: 3.212, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.171\n",
      "    [batch 725]: seen 72500 examples : 66.4 eps, Loss: 3.359, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.170\n",
      "    [batch 732]: seen 73200 examples : 66.4 eps, Loss: 3.367, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.157\n",
      "    [batch 739]: seen 73900 examples : 66.5 eps, Loss: 3.366, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 746]: seen 74600 examples : 66.5 eps, Loss: 3.300, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.170\n",
      "    [batch 753]: seen 75300 examples : 66.5 eps, Loss: 3.307, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.158\n",
      "    [batch 760]: seen 76000 examples : 66.5 eps, Loss: 3.287, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.156\n",
      "    [batch 767]: seen 76700 examples : 66.5 eps, Loss: 3.240, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.170\n",
      "    [batch 774]: seen 77400 examples : 66.6 eps, Loss: 3.331, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.178\n",
      "    [batch 781]: seen 78100 examples : 66.6 eps, Loss: 3.267, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 788]: seen 78800 examples : 66.6 eps, Loss: 3.405, Avg loss: 3.311, Best loss: 3.303, cov loss: 0.175\n",
      "    [batch 795]: seen 79500 examples : 66.6 eps, Loss: 3.302, Avg loss: 3.305, Best loss: 3.303, cov loss: 0.155\n",
      "    [batch 802]: seen 80200 examples : 66.6 eps, Loss: 3.194, Avg loss: 3.304, Best loss: 3.303, cov loss: 0.152\n",
      "    [batch 809]: seen 80900 examples : 66.7 eps, Loss: 3.434, Avg loss: 3.308, Best loss: 3.303, cov loss: 0.169\n",
      "    [batch 816]: seen 81600 examples : 66.7 eps, Loss: 3.438, Avg loss: 3.307, Best loss: 3.303, cov loss: 0.186\n",
      "    [batch 823]: seen 82300 examples : 66.7 eps, Loss: 3.418, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.185\n",
      "    [batch 830]: seen 83000 examples : 66.7 eps, Loss: 3.200, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.143\n",
      "    [batch 837]: seen 83700 examples : 66.7 eps, Loss: 3.162, Avg loss: 3.310, Best loss: 3.303, cov loss: 0.147\n",
      "    [batch 844]: seen 84400 examples : 66.8 eps, Loss: 3.360, Avg loss: 3.308, Best loss: 3.303, cov loss: 0.171\n",
      "    [batch 851]: seen 85100 examples : 66.8 eps, Loss: 3.188, Avg loss: 3.306, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 858]: seen 85800 examples : 66.8 eps, Loss: 3.390, Avg loss: 3.309, Best loss: 3.303, cov loss: 0.165\n",
      "    [batch 865]: seen 86500 examples : 66.8 eps, Loss: 3.303, Avg loss: 3.311, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 872]: seen 87200 examples : 66.8 eps, Loss: 3.333, Avg loss: 3.308, Best loss: 3.303, cov loss: 0.168\n",
      "    [batch 879]: seen 87900 examples : 66.8 eps, Loss: 3.352, Avg loss: 3.310, Best loss: 3.303, cov loss: 0.159\n",
      "    [batch 886]: seen 88600 examples : 66.9 eps, Loss: 3.443, Avg loss: 3.309, Best loss: 3.303, cov loss: 0.158\n",
      "    [batch 893]: seen 89300 examples : 66.9 eps, Loss: 3.361, Avg loss: 3.310, Best loss: 3.303, cov loss: 0.177\n",
      "    [batch 900]: seen 90000 examples : 66.9 eps, Loss: 3.359, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 907]: seen 90700 examples : 66.9 eps, Loss: 3.370, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.170\n",
      "    [batch 914]: seen 91400 examples : 66.9 eps, Loss: 3.352, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.170\n",
      "    [batch 921]: seen 92100 examples : 66.9 eps, Loss: 3.283, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 928]: seen 92800 examples : 67.0 eps, Loss: 3.566, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.179\n",
      "    [batch 935]: seen 93500 examples : 67.0 eps, Loss: 3.341, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.154\n",
      "    [batch 942]: seen 94200 examples : 67.0 eps, Loss: 3.223, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.149\n",
      "    [batch 949]: seen 94900 examples : 67.0 eps, Loss: 3.297, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.152\n",
      "    [batch 956]: seen 95600 examples : 67.0 eps, Loss: 3.279, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.151\n",
      "    [batch 963]: seen 96300 examples : 67.0 eps, Loss: 3.258, Avg loss: 3.314, Best loss: 3.303, cov loss: 0.165\n",
      "    [batch 970]: seen 97000 examples : 67.0 eps, Loss: 3.350, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.175\n",
      "    [batch 977]: seen 97700 examples : 67.0 eps, Loss: 3.326, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.166\n",
      "    [batch 984]: seen 98400 examples : 67.1 eps, Loss: 3.389, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.169\n",
      "    [batch 991]: seen 99100 examples : 67.1 eps, Loss: 3.518, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.154\n",
      "    [batch 998]: seen 99800 examples : 67.1 eps, Loss: 3.259, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.157\n",
      "    [batch 1005]: seen 100500 examples : 67.1 eps, Loss: 3.377, Avg loss: 3.323, Best loss: 3.303, cov loss: 0.170\n",
      "    [batch 1012]: seen 101200 examples : 67.1 eps, Loss: 3.127, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.151\n",
      "    [batch 1019]: seen 101900 examples : 67.1 eps, Loss: 3.166, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 1026]: seen 102600 examples : 67.1 eps, Loss: 3.247, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.165\n",
      "    [batch 1033]: seen 103300 examples : 67.1 eps, Loss: 3.368, Avg loss: 3.323, Best loss: 3.303, cov loss: 0.155\n",
      "    [batch 1040]: seen 104000 examples : 67.2 eps, Loss: 3.619, Avg loss: 3.327, Best loss: 3.303, cov loss: 0.165\n",
      "    [batch 1047]: seen 104700 examples : 67.2 eps, Loss: 3.188, Avg loss: 3.327, Best loss: 3.303, cov loss: 0.157\n",
      "    [batch 1054]: seen 105400 examples : 67.2 eps, Loss: 3.450, Avg loss: 3.328, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 1061]: seen 106100 examples : 67.2 eps, Loss: 3.162, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.154\n",
      "    [batch 1068]: seen 106800 examples : 67.2 eps, Loss: 3.336, Avg loss: 3.323, Best loss: 3.303, cov loss: 0.159\n",
      "    [batch 1075]: seen 107500 examples : 67.2 eps, Loss: 3.503, Avg loss: 3.326, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 1082]: seen 108200 examples : 67.2 eps, Loss: 3.359, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.176\n",
      "    [batch 1089]: seen 108900 examples : 67.2 eps, Loss: 3.167, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 1096]: seen 109600 examples : 67.3 eps, Loss: 3.178, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 1103]: seen 110300 examples : 67.3 eps, Loss: 3.414, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.189\n",
      "    [batch 1110]: seen 111000 examples : 67.3 eps, Loss: 3.514, Avg loss: 3.323, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 1117]: seen 111700 examples : 67.3 eps, Loss: 3.214, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.153\n",
      "    [batch 1124]: seen 112400 examples : 67.3 eps, Loss: 3.452, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.179\n",
      "    [batch 1131]: seen 113100 examples : 67.3 eps, Loss: 3.196, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 1138]: seen 113800 examples : 67.3 eps, Loss: 3.402, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 1145]: seen 114500 examples : 67.3 eps, Loss: 3.549, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.198\n",
      "    [batch 1152]: seen 115200 examples : 67.3 eps, Loss: 3.131, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.166\n",
      "    [batch 1159]: seen 115900 examples : 67.3 eps, Loss: 3.337, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.156\n",
      "    [batch 1166]: seen 116600 examples : 67.3 eps, Loss: 3.108, Avg loss: 3.323, Best loss: 3.303, cov loss: 0.150\n",
      "    [batch 1173]: seen 117300 examples : 67.4 eps, Loss: 3.430, Avg loss: 3.323, Best loss: 3.303, cov loss: 0.174\n",
      "    [batch 1180]: seen 118000 examples : 67.4 eps, Loss: 3.369, Avg loss: 3.325, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 1187]: seen 118700 examples : 67.4 eps, Loss: 3.195, Avg loss: 3.326, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 1194]: seen 119400 examples : 67.4 eps, Loss: 3.217, Avg loss: 3.326, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 1201]: seen 120100 examples : 67.4 eps, Loss: 3.298, Avg loss: 3.326, Best loss: 3.303, cov loss: 0.148\n",
      "    [batch 1208]: seen 120800 examples : 67.4 eps, Loss: 3.422, Avg loss: 3.328, Best loss: 3.303, cov loss: 0.172\n",
      "    [batch 1215]: seen 121500 examples : 67.4 eps, Loss: 3.248, Avg loss: 3.330, Best loss: 3.303, cov loss: 0.175\n",
      "    [batch 1222]: seen 122200 examples : 67.4 eps, Loss: 3.339, Avg loss: 3.334, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 1229]: seen 122900 examples : 67.4 eps, Loss: 3.276, Avg loss: 3.335, Best loss: 3.303, cov loss: 0.158\n",
      "    [batch 1236]: seen 123600 examples : 67.4 eps, Loss: 3.307, Avg loss: 3.330, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 1243]: seen 124300 examples : 67.4 eps, Loss: 3.552, Avg loss: 3.330, Best loss: 3.303, cov loss: 0.179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1250]: seen 125000 examples : 67.5 eps, Loss: 3.463, Avg loss: 3.333, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 1257]: seen 125700 examples : 67.5 eps, Loss: 3.236, Avg loss: 3.331, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 1264]: seen 126400 examples : 67.5 eps, Loss: 3.114, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.155\n",
      "    [batch 1271]: seen 127100 examples : 67.5 eps, Loss: 3.209, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 1278]: seen 127800 examples : 67.5 eps, Loss: 3.344, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 1285]: seen 128500 examples : 67.5 eps, Loss: 3.369, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.177\n",
      "    [batch 1292]: seen 129200 examples : 67.5 eps, Loss: 3.279, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.176\n",
      "    [batch 1299]: seen 129900 examples : 67.5 eps, Loss: 3.356, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.159\n",
      "    [batch 1306]: seen 130600 examples : 67.5 eps, Loss: 3.372, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.183\n",
      "    [batch 1313]: seen 131300 examples : 67.5 eps, Loss: 3.439, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.171\n",
      "    [batch 1320]: seen 132000 examples : 67.5 eps, Loss: 3.371, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.156\n",
      "    [batch 1327]: seen 132700 examples : 67.5 eps, Loss: 3.310, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.157\n",
      "    [batch 1334]: seen 133400 examples : 67.5 eps, Loss: 3.223, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.171\n",
      "    [batch 1341]: seen 134100 examples : 67.6 eps, Loss: 3.312, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.165\n",
      "    [batch 1348]: seen 134800 examples : 67.6 eps, Loss: 3.291, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 1355]: seen 135500 examples : 67.6 eps, Loss: 3.308, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.156\n",
      "    [batch 1362]: seen 136200 examples : 67.6 eps, Loss: 3.361, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 1369]: seen 136900 examples : 67.6 eps, Loss: 3.211, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 1376]: seen 137600 examples : 67.6 eps, Loss: 3.345, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.195\n",
      "    [batch 1383]: seen 138300 examples : 67.6 eps, Loss: 3.466, Avg loss: 3.323, Best loss: 3.303, cov loss: 0.178\n",
      "    [batch 1390]: seen 139000 examples : 67.6 eps, Loss: 3.355, Avg loss: 3.324, Best loss: 3.303, cov loss: 0.157\n",
      "    [batch 1397]: seen 139700 examples : 67.6 eps, Loss: 3.229, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 1404]: seen 140400 examples : 67.6 eps, Loss: 3.305, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.169\n",
      "    [batch 1411]: seen 141100 examples : 67.6 eps, Loss: 3.335, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.159\n",
      "    [batch 1418]: seen 141800 examples : 67.6 eps, Loss: 3.308, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 1425]: seen 142500 examples : 67.6 eps, Loss: 3.284, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.154\n",
      "    [batch 1432]: seen 143200 examples : 67.6 eps, Loss: 3.315, Avg loss: 3.308, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 1439]: seen 143900 examples : 67.7 eps, Loss: 3.383, Avg loss: 3.310, Best loss: 3.303, cov loss: 0.150\n",
      "    [batch 1446]: seen 144600 examples : 67.7 eps, Loss: 3.413, Avg loss: 3.310, Best loss: 3.303, cov loss: 0.182\n",
      "    [batch 1453]: seen 145300 examples : 67.7 eps, Loss: 3.354, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.158\n",
      "    [batch 1460]: seen 146000 examples : 67.7 eps, Loss: 3.359, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.176\n",
      "    [batch 1467]: seen 146700 examples : 67.7 eps, Loss: 3.466, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.184\n",
      "    [batch 1474]: seen 147400 examples : 67.7 eps, Loss: 3.478, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.173\n",
      "    [batch 1481]: seen 148100 examples : 67.7 eps, Loss: 3.289, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.155\n",
      "    [batch 1488]: seen 148800 examples : 67.7 eps, Loss: 3.408, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.157\n",
      "    [batch 1495]: seen 149500 examples : 67.7 eps, Loss: 3.159, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.151\n",
      "    [batch 1502]: seen 150200 examples : 67.7 eps, Loss: 3.285, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 1509]: seen 150900 examples : 67.7 eps, Loss: 3.327, Avg loss: 3.314, Best loss: 3.303, cov loss: 0.159\n",
      "    [batch 1516]: seen 151600 examples : 67.7 eps, Loss: 3.117, Avg loss: 3.310, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 1523]: seen 152300 examples : 67.7 eps, Loss: 3.536, Avg loss: 3.314, Best loss: 3.303, cov loss: 0.170\n",
      "    [batch 1530]: seen 153000 examples : 67.7 eps, Loss: 3.282, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.169\n",
      "    [batch 1537]: seen 153700 examples : 67.7 eps, Loss: 3.366, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.168\n",
      "    [batch 1544]: seen 154400 examples : 67.7 eps, Loss: 3.362, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.180\n",
      "    [batch 1551]: seen 155100 examples : 67.7 eps, Loss: 3.231, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 1558]: seen 155800 examples : 67.8 eps, Loss: 3.242, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.153\n",
      "    [batch 1565]: seen 156500 examples : 67.8 eps, Loss: 3.314, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.179\n",
      "    [batch 1572]: seen 157200 examples : 67.8 eps, Loss: 3.306, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.172\n",
      "    [batch 1579]: seen 157900 examples : 67.8 eps, Loss: 3.410, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.166\n",
      "    [batch 1586]: seen 158600 examples : 67.8 eps, Loss: 3.211, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.154\n",
      "    [batch 1593]: seen 159300 examples : 67.8 eps, Loss: 3.455, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.181\n",
      "    [batch 1600]: seen 160000 examples : 67.8 eps, Loss: 3.281, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 1607]: seen 160700 examples : 67.8 eps, Loss: 3.298, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 1614]: seen 161400 examples : 67.8 eps, Loss: 3.337, Avg loss: 3.325, Best loss: 3.303, cov loss: 0.178\n",
      "    [batch 1621]: seen 162100 examples : 67.8 eps, Loss: 3.380, Avg loss: 3.325, Best loss: 3.303, cov loss: 0.168\n",
      "    [batch 1628]: seen 162800 examples : 67.8 eps, Loss: 3.300, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.170\n",
      "    [batch 1635]: seen 163500 examples : 67.8 eps, Loss: 3.369, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.166\n",
      "    [batch 1642]: seen 164200 examples : 67.8 eps, Loss: 3.447, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.176\n",
      "    [batch 1649]: seen 164900 examples : 67.8 eps, Loss: 3.339, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.158\n",
      "    [batch 1656]: seen 165600 examples : 67.8 eps, Loss: 3.368, Avg loss: 3.325, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 1663]: seen 166300 examples : 67.8 eps, Loss: 3.333, Avg loss: 3.324, Best loss: 3.303, cov loss: 0.149\n",
      "    [batch 1670]: seen 167000 examples : 67.8 eps, Loss: 3.401, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.172\n",
      "    [batch 1677]: seen 167700 examples : 67.8 eps, Loss: 3.363, Avg loss: 3.324, Best loss: 3.303, cov loss: 0.182\n",
      "    [batch 1684]: seen 168400 examples : 67.8 eps, Loss: 3.389, Avg loss: 3.324, Best loss: 3.303, cov loss: 0.148\n",
      "    [batch 1691]: seen 169100 examples : 67.8 eps, Loss: 3.267, Avg loss: 3.325, Best loss: 3.303, cov loss: 0.165\n",
      "    [batch 1698]: seen 169800 examples : 67.9 eps, Loss: 3.356, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.173\n",
      "    [batch 1705]: seen 170500 examples : 67.9 eps, Loss: 3.208, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.154\n",
      "    [batch 1712]: seen 171200 examples : 67.9 eps, Loss: 3.516, Avg loss: 3.326, Best loss: 3.303, cov loss: 0.174\n",
      "    [batch 1719]: seen 171900 examples : 67.9 eps, Loss: 3.287, Avg loss: 3.326, Best loss: 3.303, cov loss: 0.141\n",
      "    [batch 1726]: seen 172600 examples : 67.9 eps, Loss: 3.201, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.187\n",
      "    [batch 1733]: seen 173300 examples : 67.9 eps, Loss: 3.235, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.158\n",
      "    [batch 1740]: seen 174000 examples : 67.9 eps, Loss: 3.181, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.146\n",
      "    [batch 1747]: seen 174700 examples : 67.9 eps, Loss: 3.201, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1754]: seen 175400 examples : 67.9 eps, Loss: 3.292, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.153\n",
      "    [batch 1761]: seen 176100 examples : 67.9 eps, Loss: 3.383, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 1768]: seen 176800 examples : 67.9 eps, Loss: 3.376, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 1775]: seen 177500 examples : 67.9 eps, Loss: 3.323, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 1782]: seen 178200 examples : 67.9 eps, Loss: 3.201, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.149\n",
      "    [batch 1789]: seen 178900 examples : 67.9 eps, Loss: 3.366, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.168\n",
      "    [batch 1796]: seen 179600 examples : 67.9 eps, Loss: 3.442, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.180\n",
      "    [batch 1803]: seen 180300 examples : 67.9 eps, Loss: 3.263, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 1810]: seen 181000 examples : 67.9 eps, Loss: 3.345, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.168\n",
      "    [batch 1817]: seen 181700 examples : 67.9 eps, Loss: 3.397, Avg loss: 3.323, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 1824]: seen 182400 examples : 67.9 eps, Loss: 3.288, Avg loss: 3.325, Best loss: 3.303, cov loss: 0.158\n",
      "    [batch 1831]: seen 183100 examples : 67.9 eps, Loss: 3.174, Avg loss: 3.325, Best loss: 3.303, cov loss: 0.152\n",
      "    [batch 1838]: seen 183800 examples : 67.9 eps, Loss: 3.405, Avg loss: 3.326, Best loss: 3.303, cov loss: 0.174\n",
      "    [batch 1845]: seen 184500 examples : 67.9 eps, Loss: 3.231, Avg loss: 3.326, Best loss: 3.303, cov loss: 0.173\n",
      "    [batch 1852]: seen 185200 examples : 67.9 eps, Loss: 3.418, Avg loss: 3.324, Best loss: 3.303, cov loss: 0.172\n",
      "    [batch 1859]: seen 185900 examples : 67.9 eps, Loss: 3.461, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.186\n",
      "    [batch 1866]: seen 186600 examples : 67.9 eps, Loss: 3.398, Avg loss: 3.314, Best loss: 3.303, cov loss: 0.145\n",
      "    [batch 1873]: seen 187300 examples : 68.0 eps, Loss: 3.230, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.176\n",
      "    [batch 1880]: seen 188000 examples : 68.0 eps, Loss: 3.287, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 1887]: seen 188700 examples : 68.0 eps, Loss: 3.211, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 1894]: seen 189400 examples : 68.0 eps, Loss: 3.233, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.168\n",
      "    [batch 1901]: seen 190100 examples : 68.0 eps, Loss: 3.358, Avg loss: 3.324, Best loss: 3.303, cov loss: 0.170\n",
      "    [batch 1908]: seen 190800 examples : 68.0 eps, Loss: 3.401, Avg loss: 3.328, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 1915]: seen 191500 examples : 68.0 eps, Loss: 3.221, Avg loss: 3.323, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 1922]: seen 192200 examples : 68.0 eps, Loss: 3.362, Avg loss: 3.325, Best loss: 3.303, cov loss: 0.154\n",
      "    [batch 1929]: seen 192900 examples : 68.0 eps, Loss: 3.490, Avg loss: 3.325, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 1936]: seen 193600 examples : 68.0 eps, Loss: 3.322, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 1943]: seen 194300 examples : 68.0 eps, Loss: 3.354, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 1950]: seen 195000 examples : 68.0 eps, Loss: 3.344, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.174\n",
      "    [batch 1957]: seen 195700 examples : 68.0 eps, Loss: 3.372, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.154\n",
      "    [batch 1964]: seen 196400 examples : 68.0 eps, Loss: 3.307, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.174\n",
      "    [batch 1971]: seen 197100 examples : 68.0 eps, Loss: 3.288, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.176\n",
      "    [batch 1978]: seen 197800 examples : 68.0 eps, Loss: 3.242, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 1985]: seen 198500 examples : 68.0 eps, Loss: 3.211, Avg loss: 3.314, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 1992]: seen 199200 examples : 68.0 eps, Loss: 3.271, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.151\n",
      "    [batch 1999]: seen 199900 examples : 68.0 eps, Loss: 3.344, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 2006]: seen 200600 examples : 68.0 eps, Loss: 3.308, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.149\n",
      "    [batch 2013]: seen 201300 examples : 68.0 eps, Loss: 3.400, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.159\n",
      "    [batch 2020]: seen 202000 examples : 68.0 eps, Loss: 3.290, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.158\n",
      "    [batch 2027]: seen 202700 examples : 68.0 eps, Loss: 3.291, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.178\n",
      "    [batch 2034]: seen 203400 examples : 68.0 eps, Loss: 3.493, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.166\n",
      "    [batch 2041]: seen 204100 examples : 68.0 eps, Loss: 3.351, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 2048]: seen 204800 examples : 68.0 eps, Loss: 3.402, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.166\n",
      "    [batch 2055]: seen 205500 examples : 68.0 eps, Loss: 3.147, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.185\n",
      "    [batch 2062]: seen 206200 examples : 68.0 eps, Loss: 3.427, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 2069]: seen 206900 examples : 68.0 eps, Loss: 3.316, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.172\n",
      "    [batch 2076]: seen 207600 examples : 68.0 eps, Loss: 3.166, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 2083]: seen 208300 examples : 68.0 eps, Loss: 3.362, Avg loss: 3.310, Best loss: 3.303, cov loss: 0.169\n",
      "    [batch 2090]: seen 209000 examples : 68.1 eps, Loss: 3.336, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.157\n",
      "    [batch 2097]: seen 209700 examples : 68.1 eps, Loss: 3.316, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 2104]: seen 210400 examples : 68.1 eps, Loss: 3.108, Avg loss: 3.308, Best loss: 3.303, cov loss: 0.155\n",
      "    [batch 2111]: seen 211100 examples : 68.1 eps, Loss: 3.388, Avg loss: 3.308, Best loss: 3.303, cov loss: 0.159\n",
      "    [batch 2118]: seen 211800 examples : 68.1 eps, Loss: 3.305, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 2125]: seen 212500 examples : 68.1 eps, Loss: 3.238, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.170\n",
      "    [batch 2132]: seen 213200 examples : 68.1 eps, Loss: 3.190, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 2139]: seen 213900 examples : 68.1 eps, Loss: 3.188, Avg loss: 3.311, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 2146]: seen 214600 examples : 68.1 eps, Loss: 3.336, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 2153]: seen 215300 examples : 68.1 eps, Loss: 3.306, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.153\n",
      "    [batch 2160]: seen 216000 examples : 68.1 eps, Loss: 3.442, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.165\n",
      "    [batch 2167]: seen 216700 examples : 68.1 eps, Loss: 3.341, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.149\n",
      "    [batch 2174]: seen 217400 examples : 68.1 eps, Loss: 3.521, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.172\n",
      "    [batch 2181]: seen 218100 examples : 68.1 eps, Loss: 3.484, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.165\n",
      "    [batch 2188]: seen 218800 examples : 68.1 eps, Loss: 3.215, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.156\n",
      "    [batch 2195]: seen 219500 examples : 68.1 eps, Loss: 3.468, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.175\n",
      "    [batch 2202]: seen 220200 examples : 68.1 eps, Loss: 3.472, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 2209]: seen 220900 examples : 68.1 eps, Loss: 3.373, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.174\n",
      "    [batch 2216]: seen 221600 examples : 68.1 eps, Loss: 3.263, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.153\n",
      "    [batch 2223]: seen 222300 examples : 68.1 eps, Loss: 3.267, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.156\n",
      "    [batch 2230]: seen 223000 examples : 68.1 eps, Loss: 3.434, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.171\n",
      "    [batch 2237]: seen 223700 examples : 68.1 eps, Loss: 3.398, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.157\n",
      "    [batch 2244]: seen 224400 examples : 68.1 eps, Loss: 3.538, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.171\n",
      "    [batch 2251]: seen 225100 examples : 68.1 eps, Loss: 3.243, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2258]: seen 225800 examples : 68.1 eps, Loss: 3.356, Avg loss: 3.314, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 2265]: seen 226500 examples : 68.1 eps, Loss: 3.321, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.154\n",
      "    [batch 2272]: seen 227200 examples : 68.1 eps, Loss: 3.206, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 2279]: seen 227900 examples : 68.1 eps, Loss: 3.363, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.155\n",
      "    [batch 2286]: seen 228600 examples : 68.1 eps, Loss: 3.375, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.186\n",
      "    [batch 2293]: seen 229300 examples : 68.1 eps, Loss: 3.336, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.169\n",
      "    [batch 2300]: seen 230000 examples : 68.1 eps, Loss: 3.218, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.174\n",
      "    [batch 2307]: seen 230700 examples : 68.1 eps, Loss: 3.399, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.165\n",
      "    [batch 2314]: seen 231400 examples : 68.1 eps, Loss: 3.582, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.180\n",
      "    [batch 2321]: seen 232100 examples : 68.1 eps, Loss: 3.404, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.171\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-43777\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-43777\n",
      "    [batch 2327]: seen 232700 examples : 68.1 eps, Loss: 3.509, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 2334]: seen 233400 examples : 68.1 eps, Loss: 3.363, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.188\n",
      "    [batch 2341]: seen 234100 examples : 68.1 eps, Loss: 3.323, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 2348]: seen 234800 examples : 68.1 eps, Loss: 3.157, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.158\n",
      "    [batch 2355]: seen 235500 examples : 68.1 eps, Loss: 3.036, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.165\n",
      "    [batch 2362]: seen 236200 examples : 68.1 eps, Loss: 3.507, Avg loss: 3.323, Best loss: 3.303, cov loss: 0.183\n",
      "    [batch 2369]: seen 236900 examples : 68.1 eps, Loss: 3.285, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 2376]: seen 237600 examples : 68.1 eps, Loss: 3.325, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.159\n",
      "    [batch 2383]: seen 238300 examples : 68.1 eps, Loss: 3.494, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.170\n",
      "    [batch 2390]: seen 239000 examples : 68.1 eps, Loss: 3.212, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.168\n",
      "    [batch 2397]: seen 239700 examples : 68.1 eps, Loss: 3.128, Avg loss: 3.314, Best loss: 3.303, cov loss: 0.179\n",
      "    [batch 2404]: seen 240400 examples : 68.1 eps, Loss: 3.357, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.171\n",
      "    [batch 2411]: seen 241100 examples : 68.1 eps, Loss: 3.409, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.153\n",
      "    [batch 2418]: seen 241800 examples : 68.1 eps, Loss: 3.284, Avg loss: 3.314, Best loss: 3.303, cov loss: 0.156\n",
      "    [batch 2425]: seen 242500 examples : 68.1 eps, Loss: 3.297, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.178\n",
      "    [batch 2432]: seen 243200 examples : 68.1 eps, Loss: 3.380, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 2439]: seen 243900 examples : 68.1 eps, Loss: 3.309, Avg loss: 3.319, Best loss: 3.303, cov loss: 0.166\n",
      "    [batch 2446]: seen 244600 examples : 68.1 eps, Loss: 3.369, Avg loss: 3.323, Best loss: 3.303, cov loss: 0.175\n",
      "    [batch 2453]: seen 245300 examples : 68.2 eps, Loss: 3.364, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 2460]: seen 246000 examples : 68.2 eps, Loss: 3.541, Avg loss: 3.328, Best loss: 3.303, cov loss: 0.178\n",
      "    [batch 2467]: seen 246700 examples : 68.2 eps, Loss: 3.447, Avg loss: 3.330, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 2474]: seen 247400 examples : 68.2 eps, Loss: 3.416, Avg loss: 3.331, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 2481]: seen 248100 examples : 68.2 eps, Loss: 3.147, Avg loss: 3.328, Best loss: 3.303, cov loss: 0.155\n",
      "    [batch 2488]: seen 248800 examples : 68.2 eps, Loss: 3.323, Avg loss: 3.327, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 2495]: seen 249500 examples : 68.2 eps, Loss: 3.375, Avg loss: 3.325, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 2502]: seen 250200 examples : 68.2 eps, Loss: 3.278, Avg loss: 3.327, Best loss: 3.303, cov loss: 0.180\n",
      "    [batch 2509]: seen 250900 examples : 68.2 eps, Loss: 3.415, Avg loss: 3.327, Best loss: 3.303, cov loss: 0.169\n",
      "    [batch 2516]: seen 251600 examples : 68.2 eps, Loss: 3.307, Avg loss: 3.328, Best loss: 3.303, cov loss: 0.159\n",
      "    [batch 2523]: seen 252300 examples : 68.2 eps, Loss: 3.299, Avg loss: 3.331, Best loss: 3.303, cov loss: 0.188\n",
      "    [batch 2530]: seen 253000 examples : 68.2 eps, Loss: 3.412, Avg loss: 3.336, Best loss: 3.303, cov loss: 0.179\n",
      "    [batch 2537]: seen 253700 examples : 68.2 eps, Loss: 3.183, Avg loss: 3.338, Best loss: 3.303, cov loss: 0.154\n",
      "    [batch 2544]: seen 254400 examples : 68.2 eps, Loss: 3.597, Avg loss: 3.338, Best loss: 3.303, cov loss: 0.196\n",
      "    [batch 2551]: seen 255100 examples : 68.2 eps, Loss: 3.432, Avg loss: 3.336, Best loss: 3.303, cov loss: 0.168\n",
      "    [batch 2558]: seen 255800 examples : 68.2 eps, Loss: 3.448, Avg loss: 3.337, Best loss: 3.303, cov loss: 0.153\n",
      "    [batch 2565]: seen 256500 examples : 68.2 eps, Loss: 3.287, Avg loss: 3.338, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 2572]: seen 257200 examples : 68.2 eps, Loss: 3.226, Avg loss: 3.337, Best loss: 3.303, cov loss: 0.173\n",
      "    [batch 2579]: seen 257900 examples : 68.2 eps, Loss: 3.433, Avg loss: 3.338, Best loss: 3.303, cov loss: 0.149\n",
      "    [batch 2586]: seen 258600 examples : 68.2 eps, Loss: 3.351, Avg loss: 3.335, Best loss: 3.303, cov loss: 0.153\n",
      "    [batch 2593]: seen 259300 examples : 68.2 eps, Loss: 3.263, Avg loss: 3.333, Best loss: 3.303, cov loss: 0.155\n",
      "    [batch 2600]: seen 260000 examples : 68.2 eps, Loss: 3.190, Avg loss: 3.332, Best loss: 3.303, cov loss: 0.150\n",
      "    [batch 2607]: seen 260700 examples : 68.2 eps, Loss: 3.195, Avg loss: 3.328, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 2614]: seen 261400 examples : 68.2 eps, Loss: 3.307, Avg loss: 3.324, Best loss: 3.303, cov loss: 0.160\n",
      "    [batch 2621]: seen 262100 examples : 68.2 eps, Loss: 3.032, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.151\n",
      "    [batch 2628]: seen 262800 examples : 68.2 eps, Loss: 3.342, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.171\n",
      "    [batch 2635]: seen 263500 examples : 68.2 eps, Loss: 3.245, Avg loss: 3.311, Best loss: 3.303, cov loss: 0.148\n",
      "    [batch 2642]: seen 264200 examples : 68.2 eps, Loss: 3.350, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.173\n",
      "    [batch 2649]: seen 264900 examples : 68.2 eps, Loss: 3.372, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 2656]: seen 265600 examples : 68.2 eps, Loss: 3.407, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.158\n",
      "    [batch 2663]: seen 266300 examples : 68.2 eps, Loss: 3.470, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 2670]: seen 267000 examples : 68.2 eps, Loss: 3.338, Avg loss: 3.318, Best loss: 3.303, cov loss: 0.168\n",
      "    [batch 2677]: seen 267700 examples : 68.2 eps, Loss: 3.359, Avg loss: 3.316, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 2684]: seen 268400 examples : 68.2 eps, Loss: 3.227, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.159\n",
      "    [batch 2691]: seen 269100 examples : 68.2 eps, Loss: 3.329, Avg loss: 3.309, Best loss: 3.303, cov loss: 0.158\n",
      "    [batch 2698]: seen 269800 examples : 68.2 eps, Loss: 3.562, Avg loss: 3.309, Best loss: 3.303, cov loss: 0.193\n",
      "    [batch 2705]: seen 270500 examples : 68.2 eps, Loss: 3.183, Avg loss: 3.308, Best loss: 3.303, cov loss: 0.171\n",
      "    [batch 2712]: seen 271200 examples : 68.2 eps, Loss: 3.302, Avg loss: 3.309, Best loss: 3.303, cov loss: 0.172\n",
      "    [batch 2719]: seen 271900 examples : 68.2 eps, Loss: 3.449, Avg loss: 3.314, Best loss: 3.303, cov loss: 0.174\n",
      "    [batch 2726]: seen 272600 examples : 68.2 eps, Loss: 3.249, Avg loss: 3.311, Best loss: 3.303, cov loss: 0.171\n",
      "    [batch 2733]: seen 273300 examples : 68.2 eps, Loss: 3.182, Avg loss: 3.311, Best loss: 3.303, cov loss: 0.166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2740]: seen 274000 examples : 68.2 eps, Loss: 3.467, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.156\n",
      "    [batch 2747]: seen 274700 examples : 68.2 eps, Loss: 3.234, Avg loss: 3.312, Best loss: 3.303, cov loss: 0.161\n",
      "    [batch 2754]: seen 275400 examples : 68.2 eps, Loss: 3.307, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 2761]: seen 276100 examples : 68.2 eps, Loss: 3.407, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 2768]: seen 276800 examples : 68.2 eps, Loss: 3.214, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.157\n",
      "    [batch 2775]: seen 277500 examples : 68.2 eps, Loss: 3.283, Avg loss: 3.325, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 2782]: seen 278200 examples : 68.2 eps, Loss: 3.569, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.158\n",
      "    [batch 2789]: seen 278900 examples : 68.2 eps, Loss: 3.522, Avg loss: 3.324, Best loss: 3.303, cov loss: 0.172\n",
      "    [batch 2796]: seen 279600 examples : 68.2 eps, Loss: 3.217, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.180\n",
      "    [batch 2803]: seen 280300 examples : 68.2 eps, Loss: 3.199, Avg loss: 3.321, Best loss: 3.303, cov loss: 0.158\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:08:32\n",
      "[EPOCH 24] Complete. Avg Loss: 3.3230545506835565; Best Loss: 3.303084875147102\n",
      "[EPOCH 25] Starting training..\n",
      "    [batch 7]: seen 700 examples : 69.0 eps, Loss: 3.389, Avg loss: 3.326, Best loss: 3.303, cov loss: 0.171\n",
      "    [batch 14]: seen 1400 examples : 68.9 eps, Loss: 3.471, Avg loss: 3.322, Best loss: 3.303, cov loss: 0.176\n",
      "    [batch 21]: seen 2100 examples : 68.9 eps, Loss: 3.380, Avg loss: 3.324, Best loss: 3.303, cov loss: 0.162\n",
      "    [batch 28]: seen 2800 examples : 68.9 eps, Loss: 3.510, Avg loss: 3.328, Best loss: 3.303, cov loss: 0.164\n",
      "    [batch 35]: seen 3500 examples : 68.9 eps, Loss: 3.238, Avg loss: 3.324, Best loss: 3.303, cov loss: 0.140\n",
      "    [batch 42]: seen 4200 examples : 68.9 eps, Loss: 3.130, Avg loss: 3.324, Best loss: 3.303, cov loss: 0.165\n",
      "    [batch 49]: seen 4900 examples : 69.0 eps, Loss: 3.135, Avg loss: 3.320, Best loss: 3.303, cov loss: 0.154\n",
      "    [batch 56]: seen 5600 examples : 68.9 eps, Loss: 3.183, Avg loss: 3.317, Best loss: 3.303, cov loss: 0.143\n",
      "    [batch 63]: seen 6300 examples : 68.9 eps, Loss: 3.421, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.150\n",
      "    [batch 70]: seen 7000 examples : 68.9 eps, Loss: 3.340, Avg loss: 3.315, Best loss: 3.303, cov loss: 0.171\n",
      "    [batch 77]: seen 7700 examples : 68.9 eps, Loss: 3.162, Avg loss: 3.311, Best loss: 3.303, cov loss: 0.166\n",
      "    [batch 84]: seen 8400 examples : 68.9 eps, Loss: 3.213, Avg loss: 3.307, Best loss: 3.303, cov loss: 0.174\n",
      "    [batch 91]: seen 9100 examples : 68.9 eps, Loss: 3.276, Avg loss: 3.306, Best loss: 3.303, cov loss: 0.179\n",
      "    [batch 98]: seen 9800 examples : 68.9 eps, Loss: 3.155, Avg loss: 3.304, Best loss: 3.303, cov loss: 0.167\n",
      "    [batch 105]: seen 10500 examples : 68.9 eps, Loss: 3.326, Avg loss: 3.306, Best loss: 3.303, cov loss: 0.178\n",
      "    [batch 112]: seen 11200 examples : 68.9 eps, Loss: 3.315, Avg loss: 3.310, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 119]: seen 11900 examples : 68.9 eps, Loss: 3.117, Avg loss: 3.307, Best loss: 3.303, cov loss: 0.152\n",
      "    [batch 126]: seen 12600 examples : 68.9 eps, Loss: 3.319, Avg loss: 3.310, Best loss: 3.303, cov loss: 0.163\n",
      "    [batch 133]: seen 13300 examples : 68.9 eps, Loss: 3.242, Avg loss: 3.308, Best loss: 3.303, cov loss: 0.172\n",
      "    [batch 140]: seen 14000 examples : 68.9 eps, Loss: 3.498, Avg loss: 3.313, Best loss: 3.303, cov loss: 0.169\n",
      "    [batch 147]: seen 14700 examples : 68.9 eps, Loss: 3.202, Avg loss: 3.307, Best loss: 3.303, cov loss: 0.157\n",
      "    [batch 154]: seen 15400 examples : 68.9 eps, Loss: 3.354, Avg loss: 3.308, Best loss: 3.303, cov loss: 0.169\n",
      "    [batch 159]: seen 15900 examples : 67.9 eps, Loss: 3.426, Avg loss: 3.303, Best loss: 3.302, cov loss: 0.188\n",
      "    [batch 164]: seen 16400 examples : 67.0 eps, Loss: 3.062, Avg loss: 3.300, Best loss: 3.300, cov loss: 0.139\n",
      "    [batch 169]: seen 16900 examples : 66.2 eps, Loss: 3.545, Avg loss: 3.304, Best loss: 3.299, cov loss: 0.164\n",
      "    [batch 176]: seen 17600 examples : 66.3 eps, Loss: 3.425, Avg loss: 3.304, Best loss: 3.299, cov loss: 0.167\n",
      "    [batch 183]: seen 18300 examples : 66.4 eps, Loss: 3.292, Avg loss: 3.307, Best loss: 3.299, cov loss: 0.175\n",
      "    [batch 190]: seen 19000 examples : 66.5 eps, Loss: 3.283, Avg loss: 3.305, Best loss: 3.299, cov loss: 0.145\n",
      "    [batch 197]: seen 19700 examples : 66.6 eps, Loss: 3.363, Avg loss: 3.309, Best loss: 3.299, cov loss: 0.176\n",
      "    [batch 204]: seen 20400 examples : 66.7 eps, Loss: 3.500, Avg loss: 3.313, Best loss: 3.299, cov loss: 0.179\n",
      "    [batch 211]: seen 21100 examples : 66.7 eps, Loss: 3.411, Avg loss: 3.317, Best loss: 3.299, cov loss: 0.160\n",
      "    [batch 218]: seen 21800 examples : 66.8 eps, Loss: 3.297, Avg loss: 3.318, Best loss: 3.299, cov loss: 0.171\n",
      "    [batch 225]: seen 22500 examples : 66.9 eps, Loss: 3.394, Avg loss: 3.319, Best loss: 3.299, cov loss: 0.168\n",
      "    [batch 232]: seen 23200 examples : 66.9 eps, Loss: 3.313, Avg loss: 3.315, Best loss: 3.299, cov loss: 0.148\n",
      "    [batch 239]: seen 23900 examples : 67.0 eps, Loss: 3.352, Avg loss: 3.312, Best loss: 3.299, cov loss: 0.152\n",
      "    [batch 246]: seen 24600 examples : 67.0 eps, Loss: 3.173, Avg loss: 3.310, Best loss: 3.299, cov loss: 0.160\n",
      "    [batch 253]: seen 25300 examples : 67.1 eps, Loss: 3.389, Avg loss: 3.315, Best loss: 3.299, cov loss: 0.182\n",
      "    [batch 260]: seen 26000 examples : 67.1 eps, Loss: 3.405, Avg loss: 3.317, Best loss: 3.299, cov loss: 0.166\n",
      "    [batch 267]: seen 26700 examples : 67.2 eps, Loss: 3.124, Avg loss: 3.318, Best loss: 3.299, cov loss: 0.146\n",
      "    [batch 274]: seen 27400 examples : 67.2 eps, Loss: 3.348, Avg loss: 3.319, Best loss: 3.299, cov loss: 0.178\n",
      "    [batch 281]: seen 28100 examples : 67.3 eps, Loss: 3.449, Avg loss: 3.317, Best loss: 3.299, cov loss: 0.173\n",
      "    [batch 288]: seen 28800 examples : 67.3 eps, Loss: 3.327, Avg loss: 3.313, Best loss: 3.299, cov loss: 0.149\n",
      "    [batch 295]: seen 29500 examples : 67.3 eps, Loss: 3.040, Avg loss: 3.312, Best loss: 3.299, cov loss: 0.160\n",
      "    [batch 302]: seen 30200 examples : 67.4 eps, Loss: 3.111, Avg loss: 3.307, Best loss: 3.299, cov loss: 0.159\n",
      "    [batch 309]: seen 30900 examples : 67.4 eps, Loss: 3.423, Avg loss: 3.312, Best loss: 3.299, cov loss: 0.163\n",
      "    [batch 316]: seen 31600 examples : 67.4 eps, Loss: 3.300, Avg loss: 3.313, Best loss: 3.299, cov loss: 0.159\n",
      "    [batch 323]: seen 32300 examples : 67.5 eps, Loss: 3.238, Avg loss: 3.313, Best loss: 3.299, cov loss: 0.156\n",
      "    [batch 330]: seen 33000 examples : 67.5 eps, Loss: 3.271, Avg loss: 3.314, Best loss: 3.299, cov loss: 0.170\n",
      "    [batch 337]: seen 33700 examples : 67.5 eps, Loss: 3.212, Avg loss: 3.311, Best loss: 3.299, cov loss: 0.150\n",
      "    [batch 344]: seen 34400 examples : 67.6 eps, Loss: 3.179, Avg loss: 3.311, Best loss: 3.299, cov loss: 0.167\n",
      "    [batch 351]: seen 35100 examples : 67.6 eps, Loss: 3.195, Avg loss: 3.308, Best loss: 3.299, cov loss: 0.175\n",
      "    [batch 358]: seen 35800 examples : 67.6 eps, Loss: 3.419, Avg loss: 3.316, Best loss: 3.299, cov loss: 0.169\n",
      "    [batch 365]: seen 36500 examples : 67.6 eps, Loss: 3.026, Avg loss: 3.311, Best loss: 3.299, cov loss: 0.142\n",
      "    [batch 372]: seen 37200 examples : 67.7 eps, Loss: 3.207, Avg loss: 3.310, Best loss: 3.299, cov loss: 0.146\n",
      "    [batch 379]: seen 37900 examples : 67.7 eps, Loss: 3.223, Avg loss: 3.313, Best loss: 3.299, cov loss: 0.158\n",
      "    [batch 386]: seen 38600 examples : 67.7 eps, Loss: 3.423, Avg loss: 3.314, Best loss: 3.299, cov loss: 0.184\n",
      "    [batch 393]: seen 39300 examples : 67.7 eps, Loss: 3.308, Avg loss: 3.315, Best loss: 3.299, cov loss: 0.171\n",
      "    [batch 400]: seen 40000 examples : 67.7 eps, Loss: 3.203, Avg loss: 3.315, Best loss: 3.299, cov loss: 0.151\n",
      "    [batch 407]: seen 40700 examples : 67.8 eps, Loss: 3.225, Avg loss: 3.312, Best loss: 3.299, cov loss: 0.161\n",
      "    [batch 414]: seen 41400 examples : 67.8 eps, Loss: 3.295, Avg loss: 3.312, Best loss: 3.299, cov loss: 0.154\n",
      "    [batch 421]: seen 42100 examples : 67.8 eps, Loss: 3.248, Avg loss: 3.311, Best loss: 3.299, cov loss: 0.158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 428]: seen 42800 examples : 67.8 eps, Loss: 3.323, Avg loss: 3.309, Best loss: 3.299, cov loss: 0.158\n",
      "    [batch 435]: seen 43500 examples : 67.8 eps, Loss: 3.341, Avg loss: 3.312, Best loss: 3.299, cov loss: 0.167\n",
      "    [batch 442]: seen 44200 examples : 67.9 eps, Loss: 3.412, Avg loss: 3.312, Best loss: 3.299, cov loss: 0.166\n",
      "    [batch 449]: seen 44900 examples : 67.9 eps, Loss: 3.318, Avg loss: 3.306, Best loss: 3.299, cov loss: 0.158\n",
      "    [batch 456]: seen 45600 examples : 67.5 eps, Loss: 3.261, Avg loss: 3.299, Best loss: 3.299, cov loss: 0.151\n",
      "    [batch 459]: seen 45900 examples : 66.8 eps, Loss: 3.368, Avg loss: 3.298, Best loss: 3.297, cov loss: 0.174\n",
      "    [batch 466]: seen 46600 examples : 66.8 eps, Loss: 3.432, Avg loss: 3.301, Best loss: 3.297, cov loss: 0.173\n",
      "    [batch 473]: seen 47300 examples : 66.9 eps, Loss: 3.403, Avg loss: 3.307, Best loss: 3.297, cov loss: 0.171\n",
      "    [batch 480]: seen 48000 examples : 66.9 eps, Loss: 3.207, Avg loss: 3.305, Best loss: 3.297, cov loss: 0.158\n",
      "    [batch 487]: seen 48700 examples : 66.9 eps, Loss: 3.243, Avg loss: 3.297, Best loss: 3.297, cov loss: 0.163\n",
      "    [batch 494]: seen 49400 examples : 66.7 eps, Loss: 3.173, Avg loss: 3.297, Best loss: 3.297, cov loss: 0.169\n",
      "    [batch 501]: seen 50100 examples : 66.7 eps, Loss: 3.357, Avg loss: 3.299, Best loss: 3.297, cov loss: 0.155\n",
      "    [batch 508]: seen 50800 examples : 66.7 eps, Loss: 3.310, Avg loss: 3.301, Best loss: 3.297, cov loss: 0.156\n",
      "    [batch 515]: seen 51500 examples : 66.7 eps, Loss: 3.292, Avg loss: 3.299, Best loss: 3.297, cov loss: 0.153\n",
      "    [batch 522]: seen 52200 examples : 66.8 eps, Loss: 3.319, Avg loss: 3.298, Best loss: 3.297, cov loss: 0.163\n",
      "    [batch 527]: seen 52700 examples : 66.5 eps, Loss: 3.521, Avg loss: 3.299, Best loss: 3.296, cov loss: 0.157\n",
      "    [batch 532]: seen 53200 examples : 66.3 eps, Loss: 3.303, Avg loss: 3.295, Best loss: 3.295, cov loss: 0.173\n",
      "    [batch 539]: seen 53900 examples : 66.3 eps, Loss: 3.197, Avg loss: 3.298, Best loss: 3.295, cov loss: 0.149\n",
      "    [batch 544]: seen 54400 examples : 65.8 eps, Loss: 3.143, Avg loss: 3.293, Best loss: 3.293, cov loss: 0.157\n",
      "    [batch 551]: seen 55100 examples : 65.8 eps, Loss: 3.353, Avg loss: 3.295, Best loss: 3.293, cov loss: 0.163\n",
      "    [batch 558]: seen 55800 examples : 65.8 eps, Loss: 3.525, Avg loss: 3.297, Best loss: 3.293, cov loss: 0.172\n",
      "    [batch 565]: seen 56500 examples : 65.9 eps, Loss: 3.238, Avg loss: 3.297, Best loss: 3.293, cov loss: 0.171\n",
      "    [batch 572]: seen 57200 examples : 65.9 eps, Loss: 3.222, Avg loss: 3.303, Best loss: 3.293, cov loss: 0.164\n",
      "    [batch 579]: seen 57900 examples : 65.9 eps, Loss: 3.432, Avg loss: 3.310, Best loss: 3.293, cov loss: 0.183\n",
      "    [batch 586]: seen 58600 examples : 66.0 eps, Loss: 3.352, Avg loss: 3.307, Best loss: 3.293, cov loss: 0.183\n",
      "    [batch 593]: seen 59300 examples : 66.0 eps, Loss: 3.251, Avg loss: 3.306, Best loss: 3.293, cov loss: 0.160\n",
      "    [batch 600]: seen 60000 examples : 66.1 eps, Loss: 3.232, Avg loss: 3.308, Best loss: 3.293, cov loss: 0.149\n",
      "    [batch 607]: seen 60700 examples : 66.1 eps, Loss: 3.405, Avg loss: 3.306, Best loss: 3.293, cov loss: 0.170\n",
      "    [batch 614]: seen 61400 examples : 66.1 eps, Loss: 3.231, Avg loss: 3.308, Best loss: 3.293, cov loss: 0.173\n",
      "    [batch 621]: seen 62100 examples : 66.1 eps, Loss: 3.461, Avg loss: 3.309, Best loss: 3.293, cov loss: 0.153\n",
      "    [batch 628]: seen 62800 examples : 66.2 eps, Loss: 3.411, Avg loss: 3.312, Best loss: 3.293, cov loss: 0.161\n",
      "    [batch 635]: seen 63500 examples : 66.2 eps, Loss: 3.325, Avg loss: 3.315, Best loss: 3.293, cov loss: 0.176\n",
      "    [batch 642]: seen 64200 examples : 66.2 eps, Loss: 3.397, Avg loss: 3.321, Best loss: 3.293, cov loss: 0.156\n",
      "    [batch 649]: seen 64900 examples : 66.3 eps, Loss: 3.414, Avg loss: 3.325, Best loss: 3.293, cov loss: 0.180\n",
      "    [batch 656]: seen 65600 examples : 66.3 eps, Loss: 3.132, Avg loss: 3.321, Best loss: 3.293, cov loss: 0.156\n",
      "    [batch 663]: seen 66300 examples : 66.3 eps, Loss: 3.287, Avg loss: 3.320, Best loss: 3.293, cov loss: 0.170\n",
      "    [batch 670]: seen 67000 examples : 66.3 eps, Loss: 3.362, Avg loss: 3.319, Best loss: 3.293, cov loss: 0.170\n",
      "    [batch 677]: seen 67700 examples : 66.4 eps, Loss: 3.312, Avg loss: 3.321, Best loss: 3.293, cov loss: 0.158\n",
      "    [batch 684]: seen 68400 examples : 66.4 eps, Loss: 3.361, Avg loss: 3.323, Best loss: 3.293, cov loss: 0.162\n",
      "    [batch 691]: seen 69100 examples : 66.4 eps, Loss: 3.393, Avg loss: 3.322, Best loss: 3.293, cov loss: 0.152\n",
      "    [batch 698]: seen 69800 examples : 66.5 eps, Loss: 3.107, Avg loss: 3.320, Best loss: 3.293, cov loss: 0.166\n",
      "    [batch 705]: seen 70500 examples : 66.5 eps, Loss: 3.380, Avg loss: 3.322, Best loss: 3.293, cov loss: 0.165\n",
      "    [batch 712]: seen 71200 examples : 66.5 eps, Loss: 3.501, Avg loss: 3.327, Best loss: 3.293, cov loss: 0.170\n",
      "    [batch 719]: seen 71900 examples : 66.5 eps, Loss: 3.271, Avg loss: 3.326, Best loss: 3.293, cov loss: 0.162\n",
      "    [batch 726]: seen 72600 examples : 66.5 eps, Loss: 3.240, Avg loss: 3.322, Best loss: 3.293, cov loss: 0.151\n",
      "    [batch 733]: seen 73300 examples : 66.6 eps, Loss: 3.293, Avg loss: 3.325, Best loss: 3.293, cov loss: 0.167\n",
      "    [batch 740]: seen 74000 examples : 66.6 eps, Loss: 3.340, Avg loss: 3.321, Best loss: 3.293, cov loss: 0.158\n",
      "    [batch 747]: seen 74700 examples : 66.6 eps, Loss: 3.273, Avg loss: 3.319, Best loss: 3.293, cov loss: 0.168\n",
      "    [batch 754]: seen 75400 examples : 66.6 eps, Loss: 3.026, Avg loss: 3.319, Best loss: 3.293, cov loss: 0.141\n",
      "    [batch 761]: seen 76100 examples : 66.7 eps, Loss: 3.292, Avg loss: 3.319, Best loss: 3.293, cov loss: 0.162\n",
      "    [batch 768]: seen 76800 examples : 66.7 eps, Loss: 3.401, Avg loss: 3.321, Best loss: 3.293, cov loss: 0.171\n",
      "    [batch 775]: seen 77500 examples : 66.7 eps, Loss: 3.278, Avg loss: 3.315, Best loss: 3.293, cov loss: 0.169\n",
      "    [EXCEPTION]:  OOM when allocating tensor with shape[100,50014] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/final_distribution/concat_98 = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/final_distribution/mul_98, projection/final_distribution/zeros, Training/gradients/b_count)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'projection/final_distribution/concat_98', defined at:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-5de9fc5d7097>\", line 9, in <module>\n",
      "    train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)\n",
      "  File \"<ipython-input-6-17e3ed376236>\", line 3, in train_continue\n",
      "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/training_util.py\", line 37, in training_init\n",
      "    lm.BuildCoreGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 140, in BuildCoreGraph\n",
      "    self.BuildProjectionGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 177, in BuildProjectionGraph\n",
      "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 177, in <listcomp>\n",
      "    vocab_dists_extended = [tf.concat(axis=1, values=[dist, extra_zeros]) for dist in vocab_dists]\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 1189, in concat\n",
      "    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 953, in concat_v2\n",
      "    \"ConcatV2\", values=values, axis=axis, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,50014] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/final_distribution/concat_98 = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/final_distribution/mul_98, projection/final_distribution/zeros, Training/gradients/b_count)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      " ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-44806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-44806\n",
      "    [batch 782]: seen 78200 examples : 66.0 eps, Loss: 3.327, Avg loss: 3.318, Best loss: 3.293, cov loss: 0.157\n",
      "    [batch 789]: seen 78900 examples : 66.1 eps, Loss: 3.324, Avg loss: 3.316, Best loss: 3.293, cov loss: 0.165\n",
      "    [batch 796]: seen 79600 examples : 66.1 eps, Loss: 3.239, Avg loss: 3.317, Best loss: 3.293, cov loss: 0.149\n",
      "    [batch 803]: seen 80300 examples : 66.1 eps, Loss: 3.470, Avg loss: 3.320, Best loss: 3.293, cov loss: 0.177\n",
      "    [batch 810]: seen 81000 examples : 66.1 eps, Loss: 3.345, Avg loss: 3.319, Best loss: 3.293, cov loss: 0.172\n",
      "    [batch 817]: seen 81700 examples : 66.2 eps, Loss: 3.430, Avg loss: 3.320, Best loss: 3.293, cov loss: 0.156\n",
      "    [batch 824]: seen 82400 examples : 66.2 eps, Loss: 3.245, Avg loss: 3.315, Best loss: 3.293, cov loss: 0.167\n",
      "    [batch 831]: seen 83100 examples : 66.2 eps, Loss: 3.203, Avg loss: 3.310, Best loss: 3.293, cov loss: 0.149\n",
      "    [batch 838]: seen 83800 examples : 66.2 eps, Loss: 3.048, Avg loss: 3.304, Best loss: 3.293, cov loss: 0.149\n",
      "    [batch 845]: seen 84500 examples : 66.2 eps, Loss: 3.224, Avg loss: 3.302, Best loss: 3.293, cov loss: 0.166\n",
      "    [batch 852]: seen 85200 examples : 66.3 eps, Loss: 3.489, Avg loss: 3.302, Best loss: 3.293, cov loss: 0.174\n",
      "    [batch 859]: seen 85900 examples : 66.3 eps, Loss: 3.430, Avg loss: 3.303, Best loss: 3.293, cov loss: 0.185\n",
      "    [batch 866]: seen 86600 examples : 66.3 eps, Loss: 3.404, Avg loss: 3.303, Best loss: 3.293, cov loss: 0.167\n",
      "    [batch 873]: seen 87300 examples : 66.3 eps, Loss: 3.263, Avg loss: 3.302, Best loss: 3.293, cov loss: 0.163\n",
      "    [batch 880]: seen 88000 examples : 66.3 eps, Loss: 3.450, Avg loss: 3.306, Best loss: 3.293, cov loss: 0.164\n",
      "    [batch 887]: seen 88700 examples : 66.4 eps, Loss: 3.642, Avg loss: 3.311, Best loss: 3.293, cov loss: 0.190\n",
      "    [batch 894]: seen 89400 examples : 66.4 eps, Loss: 3.393, Avg loss: 3.311, Best loss: 3.293, cov loss: 0.165\n",
      "    [batch 901]: seen 90100 examples : 66.4 eps, Loss: 3.226, Avg loss: 3.310, Best loss: 3.293, cov loss: 0.164\n",
      "    [batch 908]: seen 90800 examples : 66.4 eps, Loss: 3.297, Avg loss: 3.314, Best loss: 3.293, cov loss: 0.174\n",
      "    [batch 915]: seen 91500 examples : 66.4 eps, Loss: 3.239, Avg loss: 3.311, Best loss: 3.293, cov loss: 0.170\n",
      "    [batch 922]: seen 92200 examples : 66.5 eps, Loss: 3.435, Avg loss: 3.312, Best loss: 3.293, cov loss: 0.159\n",
      "    [batch 929]: seen 92900 examples : 66.5 eps, Loss: 3.301, Avg loss: 3.312, Best loss: 3.293, cov loss: 0.155\n",
      "    [batch 936]: seen 93600 examples : 66.5 eps, Loss: 3.248, Avg loss: 3.310, Best loss: 3.293, cov loss: 0.163\n",
      "    [batch 943]: seen 94300 examples : 66.5 eps, Loss: 3.515, Avg loss: 3.316, Best loss: 3.293, cov loss: 0.174\n",
      "    [batch 950]: seen 95000 examples : 66.5 eps, Loss: 3.380, Avg loss: 3.317, Best loss: 3.293, cov loss: 0.187\n",
      "    [batch 957]: seen 95700 examples : 66.5 eps, Loss: 3.344, Avg loss: 3.312, Best loss: 3.293, cov loss: 0.168\n",
      "    [batch 964]: seen 96400 examples : 66.6 eps, Loss: 3.461, Avg loss: 3.315, Best loss: 3.293, cov loss: 0.166\n",
      "    [batch 971]: seen 97100 examples : 66.6 eps, Loss: 3.364, Avg loss: 3.314, Best loss: 3.293, cov loss: 0.172\n",
      "    [batch 978]: seen 97800 examples : 66.6 eps, Loss: 3.437, Avg loss: 3.315, Best loss: 3.293, cov loss: 0.147\n",
      "    [batch 985]: seen 98500 examples : 66.6 eps, Loss: 3.313, Avg loss: 3.317, Best loss: 3.293, cov loss: 0.171\n",
      "    [batch 992]: seen 99200 examples : 66.6 eps, Loss: 3.362, Avg loss: 3.319, Best loss: 3.293, cov loss: 0.160\n",
      "    [batch 999]: seen 99900 examples : 66.6 eps, Loss: 3.481, Avg loss: 3.318, Best loss: 3.293, cov loss: 0.160\n",
      "    [batch 1006]: seen 100600 examples : 66.7 eps, Loss: 3.204, Avg loss: 3.322, Best loss: 3.293, cov loss: 0.161\n",
      "    [batch 1013]: seen 101300 examples : 66.7 eps, Loss: 3.246, Avg loss: 3.317, Best loss: 3.293, cov loss: 0.152\n",
      "    [batch 1020]: seen 102000 examples : 66.7 eps, Loss: 3.451, Avg loss: 3.320, Best loss: 3.293, cov loss: 0.166\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-44806\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-44806\n",
      "    [batch 1026]: seen 102600 examples : 66.6 eps, Loss: 3.141, Avg loss: 3.319, Best loss: 3.293, cov loss: 0.150\n",
      "    [batch 1033]: seen 103300 examples : 66.7 eps, Loss: 3.112, Avg loss: 3.317, Best loss: 3.293, cov loss: 0.159\n",
      "    [batch 1040]: seen 104000 examples : 66.7 eps, Loss: 3.298, Avg loss: 3.320, Best loss: 3.293, cov loss: 0.182\n",
      "    [batch 1047]: seen 104700 examples : 66.7 eps, Loss: 3.338, Avg loss: 3.324, Best loss: 3.293, cov loss: 0.164\n",
      "    [batch 1054]: seen 105400 examples : 66.7 eps, Loss: 3.421, Avg loss: 3.325, Best loss: 3.293, cov loss: 0.166\n",
      "    [batch 1061]: seen 106100 examples : 66.7 eps, Loss: 3.270, Avg loss: 3.325, Best loss: 3.293, cov loss: 0.151\n",
      "    [batch 1068]: seen 106800 examples : 66.7 eps, Loss: 3.089, Avg loss: 3.323, Best loss: 3.293, cov loss: 0.149\n",
      "    [batch 1075]: seen 107500 examples : 66.7 eps, Loss: 3.278, Avg loss: 3.317, Best loss: 3.293, cov loss: 0.146\n",
      "    [batch 1082]: seen 108200 examples : 66.8 eps, Loss: 3.346, Avg loss: 3.317, Best loss: 3.293, cov loss: 0.161\n",
      "    [batch 1089]: seen 108900 examples : 66.8 eps, Loss: 3.252, Avg loss: 3.321, Best loss: 3.293, cov loss: 0.139\n",
      "    [batch 1096]: seen 109600 examples : 66.8 eps, Loss: 3.365, Avg loss: 3.317, Best loss: 3.293, cov loss: 0.168\n",
      "    [batch 1103]: seen 110300 examples : 66.8 eps, Loss: 3.239, Avg loss: 3.319, Best loss: 3.293, cov loss: 0.165\n",
      "    [batch 1110]: seen 111000 examples : 66.8 eps, Loss: 3.350, Avg loss: 3.319, Best loss: 3.293, cov loss: 0.168\n",
      "    [batch 1117]: seen 111700 examples : 66.8 eps, Loss: 3.444, Avg loss: 3.317, Best loss: 3.293, cov loss: 0.164\n",
      "    [batch 1124]: seen 112400 examples : 66.8 eps, Loss: 3.457, Avg loss: 3.316, Best loss: 3.293, cov loss: 0.169\n",
      "    [batch 1131]: seen 113100 examples : 66.8 eps, Loss: 3.307, Avg loss: 3.312, Best loss: 3.293, cov loss: 0.173\n",
      "    [batch 1138]: seen 113800 examples : 66.9 eps, Loss: 3.474, Avg loss: 3.309, Best loss: 3.293, cov loss: 0.155\n",
      "    [batch 1145]: seen 114500 examples : 66.9 eps, Loss: 3.151, Avg loss: 3.304, Best loss: 3.293, cov loss: 0.161\n",
      "    [batch 1152]: seen 115200 examples : 66.9 eps, Loss: 3.399, Avg loss: 3.307, Best loss: 3.293, cov loss: 0.174\n",
      "    [batch 1159]: seen 115900 examples : 66.9 eps, Loss: 3.303, Avg loss: 3.309, Best loss: 3.293, cov loss: 0.152\n",
      "    [batch 1166]: seen 116600 examples : 66.9 eps, Loss: 3.386, Avg loss: 3.309, Best loss: 3.293, cov loss: 0.136\n",
      "    [batch 1173]: seen 117300 examples : 66.9 eps, Loss: 3.076, Avg loss: 3.309, Best loss: 3.293, cov loss: 0.153\n",
      "    [batch 1180]: seen 118000 examples : 66.9 eps, Loss: 3.160, Avg loss: 3.305, Best loss: 3.293, cov loss: 0.159\n",
      "    [batch 1187]: seen 118700 examples : 66.9 eps, Loss: 3.200, Avg loss: 3.302, Best loss: 3.293, cov loss: 0.154\n",
      "    [batch 1194]: seen 119400 examples : 67.0 eps, Loss: 3.312, Avg loss: 3.302, Best loss: 3.293, cov loss: 0.160\n",
      "    [batch 1201]: seen 120100 examples : 67.0 eps, Loss: 3.278, Avg loss: 3.302, Best loss: 3.293, cov loss: 0.147\n",
      "    [batch 1208]: seen 120800 examples : 67.0 eps, Loss: 3.304, Avg loss: 3.309, Best loss: 3.293, cov loss: 0.147\n",
      "    [batch 1215]: seen 121500 examples : 67.0 eps, Loss: 3.390, Avg loss: 3.307, Best loss: 3.293, cov loss: 0.157\n",
      "    [batch 1222]: seen 122200 examples : 67.0 eps, Loss: 3.567, Avg loss: 3.308, Best loss: 3.293, cov loss: 0.176\n",
      "    [batch 1229]: seen 122900 examples : 67.0 eps, Loss: 3.500, Avg loss: 3.309, Best loss: 3.293, cov loss: 0.166\n",
      "    [batch 1236]: seen 123600 examples : 67.0 eps, Loss: 3.366, Avg loss: 3.304, Best loss: 3.293, cov loss: 0.156\n",
      "    [batch 1243]: seen 124300 examples : 67.0 eps, Loss: 3.300, Avg loss: 3.309, Best loss: 3.293, cov loss: 0.167\n",
      "    [batch 1250]: seen 125000 examples : 67.0 eps, Loss: 3.366, Avg loss: 3.310, Best loss: 3.293, cov loss: 0.171\n",
      "    [batch 1257]: seen 125700 examples : 67.1 eps, Loss: 3.369, Avg loss: 3.314, Best loss: 3.293, cov loss: 0.170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1264]: seen 126400 examples : 67.1 eps, Loss: 3.279, Avg loss: 3.311, Best loss: 3.293, cov loss: 0.151\n",
      "    [batch 1271]: seen 127100 examples : 67.1 eps, Loss: 3.403, Avg loss: 3.313, Best loss: 3.293, cov loss: 0.157\n",
      "    [batch 1278]: seen 127800 examples : 67.1 eps, Loss: 3.221, Avg loss: 3.312, Best loss: 3.293, cov loss: 0.158\n",
      "    [batch 1285]: seen 128500 examples : 67.1 eps, Loss: 3.396, Avg loss: 3.312, Best loss: 3.293, cov loss: 0.161\n",
      "    [batch 1292]: seen 129200 examples : 67.1 eps, Loss: 3.336, Avg loss: 3.315, Best loss: 3.293, cov loss: 0.159\n",
      "    [batch 1299]: seen 129900 examples : 67.1 eps, Loss: 3.425, Avg loss: 3.315, Best loss: 3.293, cov loss: 0.168\n",
      "    [batch 1306]: seen 130600 examples : 67.1 eps, Loss: 3.317, Avg loss: 3.316, Best loss: 3.293, cov loss: 0.171\n",
      "    [batch 1313]: seen 131300 examples : 67.1 eps, Loss: 3.246, Avg loss: 3.314, Best loss: 3.293, cov loss: 0.162\n",
      "    [batch 1320]: seen 132000 examples : 67.1 eps, Loss: 3.276, Avg loss: 3.313, Best loss: 3.293, cov loss: 0.164\n",
      "    [batch 1327]: seen 132700 examples : 67.1 eps, Loss: 3.431, Avg loss: 3.320, Best loss: 3.293, cov loss: 0.170\n",
      "    [batch 1334]: seen 133400 examples : 67.2 eps, Loss: 3.222, Avg loss: 3.316, Best loss: 3.293, cov loss: 0.169\n",
      "    [batch 1341]: seen 134100 examples : 67.2 eps, Loss: 3.366, Avg loss: 3.319, Best loss: 3.293, cov loss: 0.166\n",
      "    [batch 1348]: seen 134800 examples : 67.2 eps, Loss: 3.234, Avg loss: 3.313, Best loss: 3.293, cov loss: 0.148\n",
      "    [batch 1355]: seen 135500 examples : 67.2 eps, Loss: 3.468, Avg loss: 3.314, Best loss: 3.293, cov loss: 0.174\n",
      "    [batch 1362]: seen 136200 examples : 67.2 eps, Loss: 3.215, Avg loss: 3.314, Best loss: 3.293, cov loss: 0.146\n",
      "    [batch 1369]: seen 136900 examples : 67.2 eps, Loss: 3.131, Avg loss: 3.314, Best loss: 3.293, cov loss: 0.163\n",
      "    [batch 1376]: seen 137600 examples : 67.2 eps, Loss: 3.454, Avg loss: 3.314, Best loss: 3.293, cov loss: 0.181\n",
      "    [batch 1383]: seen 138300 examples : 67.2 eps, Loss: 3.225, Avg loss: 3.312, Best loss: 3.293, cov loss: 0.149\n",
      "    [batch 1390]: seen 139000 examples : 67.2 eps, Loss: 3.175, Avg loss: 3.312, Best loss: 3.293, cov loss: 0.149\n",
      "    [batch 1397]: seen 139700 examples : 67.2 eps, Loss: 3.347, Avg loss: 3.313, Best loss: 3.293, cov loss: 0.181\n",
      "    [batch 1404]: seen 140400 examples : 67.2 eps, Loss: 3.417, Avg loss: 3.314, Best loss: 3.293, cov loss: 0.159\n",
      "    [batch 1411]: seen 141100 examples : 67.3 eps, Loss: 3.318, Avg loss: 3.315, Best loss: 3.293, cov loss: 0.162\n",
      "    [batch 1418]: seen 141800 examples : 67.3 eps, Loss: 3.329, Avg loss: 3.317, Best loss: 3.293, cov loss: 0.178\n",
      "    [batch 1425]: seen 142500 examples : 67.3 eps, Loss: 3.087, Avg loss: 3.313, Best loss: 3.293, cov loss: 0.164\n",
      "    [batch 1432]: seen 143200 examples : 67.3 eps, Loss: 3.456, Avg loss: 3.316, Best loss: 3.293, cov loss: 0.185\n",
      "    [batch 1439]: seen 143900 examples : 67.3 eps, Loss: 3.235, Avg loss: 3.315, Best loss: 3.293, cov loss: 0.160\n",
      "    [batch 1446]: seen 144600 examples : 67.3 eps, Loss: 3.243, Avg loss: 3.312, Best loss: 3.293, cov loss: 0.168\n",
      "    [batch 1453]: seen 145300 examples : 67.3 eps, Loss: 3.154, Avg loss: 3.311, Best loss: 3.293, cov loss: 0.165\n",
      "    [batch 1460]: seen 146000 examples : 67.3 eps, Loss: 3.061, Avg loss: 3.310, Best loss: 3.293, cov loss: 0.160\n",
      "    [batch 1467]: seen 146700 examples : 67.3 eps, Loss: 3.366, Avg loss: 3.310, Best loss: 3.293, cov loss: 0.165\n",
      "    [batch 1474]: seen 147400 examples : 67.3 eps, Loss: 3.165, Avg loss: 3.309, Best loss: 3.293, cov loss: 0.152\n",
      "    [batch 1481]: seen 148100 examples : 67.3 eps, Loss: 3.195, Avg loss: 3.308, Best loss: 3.293, cov loss: 0.148\n",
      "    [batch 1488]: seen 148800 examples : 67.3 eps, Loss: 3.388, Avg loss: 3.308, Best loss: 3.293, cov loss: 0.170\n",
      "    [batch 1495]: seen 149500 examples : 67.3 eps, Loss: 3.385, Avg loss: 3.309, Best loss: 3.293, cov loss: 0.164\n",
      "    [batch 1502]: seen 150200 examples : 67.4 eps, Loss: 3.459, Avg loss: 3.311, Best loss: 3.293, cov loss: 0.167\n",
      "    [batch 1509]: seen 150900 examples : 67.4 eps, Loss: 3.538, Avg loss: 3.308, Best loss: 3.293, cov loss: 0.163\n",
      "    [batch 1516]: seen 151600 examples : 67.4 eps, Loss: 3.530, Avg loss: 3.307, Best loss: 3.293, cov loss: 0.169\n",
      "    [batch 1523]: seen 152300 examples : 67.4 eps, Loss: 3.340, Avg loss: 3.305, Best loss: 3.293, cov loss: 0.163\n",
      "    [batch 1530]: seen 153000 examples : 67.4 eps, Loss: 3.269, Avg loss: 3.303, Best loss: 3.293, cov loss: 0.151\n",
      "    [batch 1537]: seen 153700 examples : 67.4 eps, Loss: 3.364, Avg loss: 3.300, Best loss: 3.293, cov loss: 0.174\n",
      "    [batch 1544]: seen 154400 examples : 67.4 eps, Loss: 3.642, Avg loss: 3.307, Best loss: 3.293, cov loss: 0.179\n",
      "    [batch 1551]: seen 155100 examples : 67.4 eps, Loss: 3.285, Avg loss: 3.305, Best loss: 3.293, cov loss: 0.172\n",
      "    [batch 1558]: seen 155800 examples : 67.4 eps, Loss: 3.284, Avg loss: 3.305, Best loss: 3.293, cov loss: 0.157\n",
      "    [batch 1565]: seen 156500 examples : 67.4 eps, Loss: 3.341, Avg loss: 3.301, Best loss: 3.293, cov loss: 0.163\n",
      "    [batch 1572]: seen 157200 examples : 67.4 eps, Loss: 3.590, Avg loss: 3.302, Best loss: 3.293, cov loss: 0.181\n",
      "    [batch 1579]: seen 157900 examples : 67.4 eps, Loss: 3.281, Avg loss: 3.302, Best loss: 3.293, cov loss: 0.154\n",
      "    [batch 1586]: seen 158600 examples : 67.4 eps, Loss: 3.350, Avg loss: 3.300, Best loss: 3.293, cov loss: 0.158\n",
      "    [batch 1593]: seen 159300 examples : 67.4 eps, Loss: 3.413, Avg loss: 3.299, Best loss: 3.293, cov loss: 0.158\n",
      "    [batch 1598]: seen 159800 examples : 67.3 eps, Loss: 3.235, Avg loss: 3.292, Best loss: 3.292, cov loss: 0.161\n",
      "    [batch 1601]: seen 160100 examples : 67.1 eps, Loss: 3.340, Avg loss: 3.291, Best loss: 3.290, cov loss: 0.149\n",
      "    [batch 1608]: seen 160800 examples : 67.0 eps, Loss: 3.103, Avg loss: 3.289, Best loss: 3.289, cov loss: 0.152\n",
      "    [batch 1615]: seen 161500 examples : 67.0 eps, Loss: 3.137, Avg loss: 3.290, Best loss: 3.289, cov loss: 0.166\n",
      "    [batch 1622]: seen 162200 examples : 67.0 eps, Loss: 3.242, Avg loss: 3.290, Best loss: 3.289, cov loss: 0.153\n",
      "    [batch 1629]: seen 162900 examples : 67.0 eps, Loss: 3.360, Avg loss: 3.292, Best loss: 3.289, cov loss: 0.163\n",
      "    [batch 1636]: seen 163600 examples : 67.0 eps, Loss: 3.380, Avg loss: 3.295, Best loss: 3.289, cov loss: 0.158\n",
      "    [batch 1643]: seen 164300 examples : 67.0 eps, Loss: 3.304, Avg loss: 3.296, Best loss: 3.289, cov loss: 0.176\n",
      "    [batch 1650]: seen 165000 examples : 67.0 eps, Loss: 3.502, Avg loss: 3.298, Best loss: 3.289, cov loss: 0.170\n",
      "    [batch 1657]: seen 165700 examples : 67.0 eps, Loss: 3.265, Avg loss: 3.297, Best loss: 3.289, cov loss: 0.168\n",
      "    [batch 1664]: seen 166400 examples : 67.1 eps, Loss: 3.263, Avg loss: 3.297, Best loss: 3.289, cov loss: 0.173\n",
      "    [batch 1671]: seen 167100 examples : 67.1 eps, Loss: 3.402, Avg loss: 3.301, Best loss: 3.289, cov loss: 0.179\n",
      "    [batch 1678]: seen 167800 examples : 67.1 eps, Loss: 3.489, Avg loss: 3.303, Best loss: 3.289, cov loss: 0.169\n",
      "    [batch 1685]: seen 168500 examples : 67.1 eps, Loss: 3.408, Avg loss: 3.306, Best loss: 3.289, cov loss: 0.150\n",
      "    [batch 1692]: seen 169200 examples : 67.1 eps, Loss: 3.287, Avg loss: 3.308, Best loss: 3.289, cov loss: 0.149\n",
      "    [batch 1699]: seen 169900 examples : 67.1 eps, Loss: 3.291, Avg loss: 3.307, Best loss: 3.289, cov loss: 0.163\n",
      "    [batch 1706]: seen 170600 examples : 67.1 eps, Loss: 3.191, Avg loss: 3.309, Best loss: 3.289, cov loss: 0.156\n",
      "    [batch 1713]: seen 171300 examples : 67.1 eps, Loss: 3.292, Avg loss: 3.308, Best loss: 3.289, cov loss: 0.177\n",
      "    [batch 1720]: seen 172000 examples : 67.1 eps, Loss: 3.287, Avg loss: 3.305, Best loss: 3.289, cov loss: 0.157\n",
      "    [batch 1727]: seen 172700 examples : 67.1 eps, Loss: 3.163, Avg loss: 3.305, Best loss: 3.289, cov loss: 0.143\n",
      "    [batch 1734]: seen 173400 examples : 67.1 eps, Loss: 3.239, Avg loss: 3.302, Best loss: 3.289, cov loss: 0.153\n",
      "    [batch 1741]: seen 174100 examples : 67.1 eps, Loss: 3.201, Avg loss: 3.302, Best loss: 3.289, cov loss: 0.164\n",
      "    [batch 1748]: seen 174800 examples : 67.1 eps, Loss: 3.383, Avg loss: 3.304, Best loss: 3.289, cov loss: 0.176\n",
      "    [batch 1755]: seen 175500 examples : 67.2 eps, Loss: 3.248, Avg loss: 3.305, Best loss: 3.289, cov loss: 0.152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1762]: seen 176200 examples : 67.2 eps, Loss: 3.391, Avg loss: 3.302, Best loss: 3.289, cov loss: 0.167\n",
      "    [batch 1769]: seen 176900 examples : 67.2 eps, Loss: 3.110, Avg loss: 3.300, Best loss: 3.289, cov loss: 0.154\n",
      "    [batch 1776]: seen 177600 examples : 67.2 eps, Loss: 3.386, Avg loss: 3.302, Best loss: 3.289, cov loss: 0.168\n",
      "    [batch 1783]: seen 178300 examples : 67.2 eps, Loss: 3.418, Avg loss: 3.305, Best loss: 3.289, cov loss: 0.167\n",
      "    [batch 1790]: seen 179000 examples : 67.2 eps, Loss: 3.218, Avg loss: 3.305, Best loss: 3.289, cov loss: 0.172\n",
      "    [batch 1797]: seen 179700 examples : 67.2 eps, Loss: 3.345, Avg loss: 3.312, Best loss: 3.289, cov loss: 0.174\n",
      "    [batch 1804]: seen 180400 examples : 67.2 eps, Loss: 3.261, Avg loss: 3.311, Best loss: 3.289, cov loss: 0.178\n",
      "    [batch 1811]: seen 181100 examples : 67.2 eps, Loss: 3.301, Avg loss: 3.312, Best loss: 3.289, cov loss: 0.173\n",
      "    [batch 1818]: seen 181800 examples : 67.2 eps, Loss: 3.272, Avg loss: 3.317, Best loss: 3.289, cov loss: 0.162\n",
      "    [batch 1825]: seen 182500 examples : 67.2 eps, Loss: 3.320, Avg loss: 3.320, Best loss: 3.289, cov loss: 0.164\n",
      "    [batch 1832]: seen 183200 examples : 67.2 eps, Loss: 3.068, Avg loss: 3.317, Best loss: 3.289, cov loss: 0.163\n",
      "    [batch 1839]: seen 183900 examples : 67.2 eps, Loss: 3.380, Avg loss: 3.319, Best loss: 3.289, cov loss: 0.174\n",
      "    [batch 1846]: seen 184600 examples : 67.2 eps, Loss: 3.391, Avg loss: 3.317, Best loss: 3.289, cov loss: 0.171\n",
      "    [batch 1853]: seen 185300 examples : 67.2 eps, Loss: 3.309, Avg loss: 3.317, Best loss: 3.289, cov loss: 0.159\n",
      "    [batch 1860]: seen 186000 examples : 67.3 eps, Loss: 2.978, Avg loss: 3.316, Best loss: 3.289, cov loss: 0.147\n",
      "    [batch 1867]: seen 186700 examples : 67.3 eps, Loss: 3.329, Avg loss: 3.317, Best loss: 3.289, cov loss: 0.149\n",
      "    [batch 1874]: seen 187400 examples : 67.3 eps, Loss: 3.239, Avg loss: 3.315, Best loss: 3.289, cov loss: 0.164\n",
      "    [batch 1881]: seen 188100 examples : 67.3 eps, Loss: 3.241, Avg loss: 3.310, Best loss: 3.289, cov loss: 0.152\n",
      "    [batch 1888]: seen 188800 examples : 67.3 eps, Loss: 3.383, Avg loss: 3.313, Best loss: 3.289, cov loss: 0.160\n",
      "    [batch 1895]: seen 189500 examples : 67.3 eps, Loss: 3.293, Avg loss: 3.313, Best loss: 3.289, cov loss: 0.173\n",
      "    [batch 1902]: seen 190200 examples : 67.3 eps, Loss: 3.303, Avg loss: 3.313, Best loss: 3.289, cov loss: 0.165\n",
      "    [batch 1909]: seen 190900 examples : 67.3 eps, Loss: 3.436, Avg loss: 3.311, Best loss: 3.289, cov loss: 0.157\n",
      "    [batch 1916]: seen 191600 examples : 67.3 eps, Loss: 3.347, Avg loss: 3.312, Best loss: 3.289, cov loss: 0.168\n",
      "    [batch 1923]: seen 192300 examples : 67.3 eps, Loss: 3.315, Avg loss: 3.310, Best loss: 3.289, cov loss: 0.150\n",
      "    [batch 1930]: seen 193000 examples : 67.3 eps, Loss: 3.303, Avg loss: 3.307, Best loss: 3.289, cov loss: 0.165\n",
      "    [batch 1937]: seen 193700 examples : 67.3 eps, Loss: 3.527, Avg loss: 3.310, Best loss: 3.289, cov loss: 0.181\n",
      "    [batch 1944]: seen 194400 examples : 67.3 eps, Loss: 3.374, Avg loss: 3.313, Best loss: 3.289, cov loss: 0.165\n",
      "    [batch 1951]: seen 195100 examples : 67.3 eps, Loss: 3.164, Avg loss: 3.310, Best loss: 3.289, cov loss: 0.157\n",
      "    [batch 1958]: seen 195800 examples : 67.3 eps, Loss: 3.347, Avg loss: 3.312, Best loss: 3.289, cov loss: 0.155\n",
      "    [batch 1965]: seen 196500 examples : 67.3 eps, Loss: 3.528, Avg loss: 3.312, Best loss: 3.289, cov loss: 0.175\n",
      "    [batch 1972]: seen 197200 examples : 67.3 eps, Loss: 3.364, Avg loss: 3.307, Best loss: 3.289, cov loss: 0.175\n",
      "    [batch 1979]: seen 197900 examples : 67.4 eps, Loss: 3.376, Avg loss: 3.307, Best loss: 3.289, cov loss: 0.168\n",
      "    [batch 1986]: seen 198600 examples : 67.4 eps, Loss: 3.183, Avg loss: 3.310, Best loss: 3.289, cov loss: 0.155\n",
      "    [batch 1993]: seen 199300 examples : 67.4 eps, Loss: 3.409, Avg loss: 3.310, Best loss: 3.289, cov loss: 0.154\n",
      "    [batch 2000]: seen 200000 examples : 67.4 eps, Loss: 3.221, Avg loss: 3.315, Best loss: 3.289, cov loss: 0.157\n",
      "    [batch 2007]: seen 200700 examples : 67.4 eps, Loss: 3.311, Avg loss: 3.313, Best loss: 3.289, cov loss: 0.167\n",
      "    [batch 2014]: seen 201400 examples : 67.4 eps, Loss: 3.347, Avg loss: 3.312, Best loss: 3.289, cov loss: 0.167\n",
      "    [batch 2021]: seen 202100 examples : 67.4 eps, Loss: 3.278, Avg loss: 3.310, Best loss: 3.289, cov loss: 0.151\n",
      "    [batch 2028]: seen 202800 examples : 67.4 eps, Loss: 3.179, Avg loss: 3.308, Best loss: 3.289, cov loss: 0.162\n",
      "    [batch 2035]: seen 203500 examples : 67.4 eps, Loss: 3.190, Avg loss: 3.303, Best loss: 3.289, cov loss: 0.177\n",
      "    [batch 2042]: seen 204200 examples : 67.4 eps, Loss: 3.141, Avg loss: 3.304, Best loss: 3.289, cov loss: 0.161\n",
      "    [batch 2049]: seen 204900 examples : 67.4 eps, Loss: 3.095, Avg loss: 3.299, Best loss: 3.289, cov loss: 0.146\n",
      "    [batch 2056]: seen 205600 examples : 67.4 eps, Loss: 3.229, Avg loss: 3.297, Best loss: 3.289, cov loss: 0.150\n",
      "    [batch 2063]: seen 206300 examples : 67.4 eps, Loss: 3.198, Avg loss: 3.299, Best loss: 3.289, cov loss: 0.169\n",
      "    [batch 2070]: seen 207000 examples : 67.4 eps, Loss: 3.234, Avg loss: 3.297, Best loss: 3.289, cov loss: 0.161\n",
      "    [batch 2077]: seen 207700 examples : 67.4 eps, Loss: 3.237, Avg loss: 3.292, Best loss: 3.289, cov loss: 0.145\n",
      "    [batch 2084]: seen 208400 examples : 67.4 eps, Loss: 3.289, Avg loss: 3.292, Best loss: 3.289, cov loss: 0.163\n",
      "    [batch 2091]: seen 209100 examples : 67.4 eps, Loss: 3.329, Avg loss: 3.293, Best loss: 3.289, cov loss: 0.158\n",
      "    [batch 2098]: seen 209800 examples : 67.4 eps, Loss: 3.415, Avg loss: 3.293, Best loss: 3.289, cov loss: 0.163\n",
      "    [batch 2105]: seen 210500 examples : 67.4 eps, Loss: 3.342, Avg loss: 3.295, Best loss: 3.289, cov loss: 0.161\n",
      "    [batch 2112]: seen 211200 examples : 67.5 eps, Loss: 3.277, Avg loss: 3.295, Best loss: 3.289, cov loss: 0.157\n",
      "    [batch 2119]: seen 211900 examples : 67.5 eps, Loss: 3.155, Avg loss: 3.295, Best loss: 3.289, cov loss: 0.163\n",
      "    [batch 2126]: seen 212600 examples : 67.5 eps, Loss: 3.187, Avg loss: 3.294, Best loss: 3.289, cov loss: 0.159\n",
      "    [batch 2133]: seen 213300 examples : 67.5 eps, Loss: 3.361, Avg loss: 3.296, Best loss: 3.289, cov loss: 0.160\n",
      "    [batch 2140]: seen 214000 examples : 67.5 eps, Loss: 3.377, Avg loss: 3.300, Best loss: 3.289, cov loss: 0.174\n",
      "    [batch 2147]: seen 214700 examples : 67.5 eps, Loss: 3.384, Avg loss: 3.302, Best loss: 3.289, cov loss: 0.161\n",
      "    [batch 2154]: seen 215400 examples : 67.5 eps, Loss: 3.445, Avg loss: 3.300, Best loss: 3.289, cov loss: 0.162\n",
      "    [batch 2161]: seen 216100 examples : 67.5 eps, Loss: 3.295, Avg loss: 3.301, Best loss: 3.289, cov loss: 0.166\n",
      "    [batch 2168]: seen 216800 examples : 67.5 eps, Loss: 3.342, Avg loss: 3.305, Best loss: 3.289, cov loss: 0.169\n",
      "    [batch 2175]: seen 217500 examples : 67.5 eps, Loss: 3.325, Avg loss: 3.306, Best loss: 3.289, cov loss: 0.154\n",
      "    [batch 2182]: seen 218200 examples : 67.5 eps, Loss: 3.264, Avg loss: 3.307, Best loss: 3.289, cov loss: 0.170\n",
      "    [batch 2189]: seen 218900 examples : 67.5 eps, Loss: 3.271, Avg loss: 3.308, Best loss: 3.289, cov loss: 0.160\n",
      "    [batch 2196]: seen 219600 examples : 67.5 eps, Loss: 3.338, Avg loss: 3.307, Best loss: 3.289, cov loss: 0.165\n",
      "    [batch 2203]: seen 220300 examples : 67.5 eps, Loss: 3.344, Avg loss: 3.305, Best loss: 3.289, cov loss: 0.172\n",
      "    [batch 2210]: seen 221000 examples : 67.5 eps, Loss: 3.289, Avg loss: 3.302, Best loss: 3.289, cov loss: 0.167\n",
      "    [batch 2217]: seen 221700 examples : 67.5 eps, Loss: 3.238, Avg loss: 3.300, Best loss: 3.289, cov loss: 0.165\n",
      "    [batch 2224]: seen 222400 examples : 67.5 eps, Loss: 3.533, Avg loss: 3.305, Best loss: 3.289, cov loss: 0.170\n",
      "    [batch 2231]: seen 223100 examples : 67.5 eps, Loss: 3.105, Avg loss: 3.306, Best loss: 3.289, cov loss: 0.149\n",
      "    [batch 2238]: seen 223800 examples : 67.5 eps, Loss: 3.416, Avg loss: 3.307, Best loss: 3.289, cov loss: 0.173\n",
      "    [batch 2245]: seen 224500 examples : 67.5 eps, Loss: 3.320, Avg loss: 3.305, Best loss: 3.289, cov loss: 0.170\n",
      "    [batch 2252]: seen 225200 examples : 67.5 eps, Loss: 3.266, Avg loss: 3.308, Best loss: 3.289, cov loss: 0.149\n",
      "    [batch 2259]: seen 225900 examples : 67.6 eps, Loss: 3.255, Avg loss: 3.310, Best loss: 3.289, cov loss: 0.160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2266]: seen 226600 examples : 67.6 eps, Loss: 3.370, Avg loss: 3.307, Best loss: 3.289, cov loss: 0.155\n",
      "    [batch 2273]: seen 227300 examples : 67.6 eps, Loss: 3.220, Avg loss: 3.308, Best loss: 3.289, cov loss: 0.149\n",
      "    [batch 2280]: seen 228000 examples : 67.6 eps, Loss: 3.474, Avg loss: 3.310, Best loss: 3.289, cov loss: 0.170\n",
      "    [batch 2287]: seen 228700 examples : 67.6 eps, Loss: 3.176, Avg loss: 3.306, Best loss: 3.289, cov loss: 0.151\n",
      "    [batch 2294]: seen 229400 examples : 67.6 eps, Loss: 3.300, Avg loss: 3.304, Best loss: 3.289, cov loss: 0.168\n",
      "    [batch 2301]: seen 230100 examples : 67.6 eps, Loss: 3.157, Avg loss: 3.302, Best loss: 3.289, cov loss: 0.148\n",
      "    [batch 2308]: seen 230800 examples : 67.6 eps, Loss: 3.349, Avg loss: 3.305, Best loss: 3.289, cov loss: 0.163\n",
      "    [batch 2315]: seen 231500 examples : 67.6 eps, Loss: 3.480, Avg loss: 3.309, Best loss: 3.289, cov loss: 0.164\n",
      "    [batch 2322]: seen 232200 examples : 67.6 eps, Loss: 3.007, Avg loss: 3.300, Best loss: 3.289, cov loss: 0.149\n",
      "    [batch 2329]: seen 232900 examples : 67.6 eps, Loss: 3.351, Avg loss: 3.301, Best loss: 3.289, cov loss: 0.161\n",
      "    [batch 2336]: seen 233600 examples : 67.6 eps, Loss: 3.331, Avg loss: 3.301, Best loss: 3.289, cov loss: 0.156\n",
      "    [batch 2343]: seen 234300 examples : 67.6 eps, Loss: 3.361, Avg loss: 3.298, Best loss: 3.289, cov loss: 0.157\n",
      "    [batch 2350]: seen 235000 examples : 67.6 eps, Loss: 3.175, Avg loss: 3.291, Best loss: 3.289, cov loss: 0.175\n",
      "    [batch 2357]: seen 235700 examples : 67.6 eps, Loss: 3.281, Avg loss: 3.295, Best loss: 3.289, cov loss: 0.161\n",
      "    [batch 2364]: seen 236400 examples : 67.6 eps, Loss: 3.127, Avg loss: 3.294, Best loss: 3.289, cov loss: 0.136\n",
      "    [batch 2371]: seen 237100 examples : 67.6 eps, Loss: 3.252, Avg loss: 3.298, Best loss: 3.289, cov loss: 0.163\n",
      "    [batch 2378]: seen 237800 examples : 67.6 eps, Loss: 3.285, Avg loss: 3.292, Best loss: 3.289, cov loss: 0.170\n",
      "    [batch 2385]: seen 238500 examples : 67.6 eps, Loss: 3.312, Avg loss: 3.296, Best loss: 3.289, cov loss: 0.142\n",
      "    [batch 2392]: seen 239200 examples : 67.6 eps, Loss: 3.119, Avg loss: 3.291, Best loss: 3.289, cov loss: 0.156\n",
      "    [batch 2397]: seen 239700 examples : 67.5 eps, Loss: 3.120, Avg loss: 3.287, Best loss: 3.287, cov loss: 0.168\n",
      "    [batch 2402]: seen 240200 examples : 67.4 eps, Loss: 3.301, Avg loss: 3.289, Best loss: 3.286, cov loss: 0.184\n",
      "    [batch 2409]: seen 240900 examples : 67.4 eps, Loss: 3.170, Avg loss: 3.293, Best loss: 3.286, cov loss: 0.154\n",
      "    [batch 2416]: seen 241600 examples : 67.5 eps, Loss: 3.145, Avg loss: 3.294, Best loss: 3.286, cov loss: 0.158\n",
      "    [batch 2423]: seen 242300 examples : 67.5 eps, Loss: 3.205, Avg loss: 3.294, Best loss: 3.286, cov loss: 0.155\n",
      "    [batch 2430]: seen 243000 examples : 67.5 eps, Loss: 3.319, Avg loss: 3.293, Best loss: 3.286, cov loss: 0.161\n",
      "    [batch 2437]: seen 243700 examples : 67.5 eps, Loss: 3.488, Avg loss: 3.297, Best loss: 3.286, cov loss: 0.161\n",
      "    [batch 2444]: seen 244400 examples : 67.5 eps, Loss: 3.411, Avg loss: 3.303, Best loss: 3.286, cov loss: 0.147\n",
      "    [batch 2451]: seen 245100 examples : 67.5 eps, Loss: 3.571, Avg loss: 3.308, Best loss: 3.286, cov loss: 0.174\n",
      "    [batch 2458]: seen 245800 examples : 67.5 eps, Loss: 3.215, Avg loss: 3.302, Best loss: 3.286, cov loss: 0.176\n",
      "    [batch 2465]: seen 246500 examples : 67.5 eps, Loss: 3.156, Avg loss: 3.303, Best loss: 3.286, cov loss: 0.155\n",
      "    [batch 2472]: seen 247200 examples : 67.5 eps, Loss: 3.417, Avg loss: 3.303, Best loss: 3.286, cov loss: 0.169\n",
      "    [batch 2479]: seen 247900 examples : 67.5 eps, Loss: 3.364, Avg loss: 3.304, Best loss: 3.286, cov loss: 0.168\n",
      "    [batch 2486]: seen 248600 examples : 67.5 eps, Loss: 3.183, Avg loss: 3.302, Best loss: 3.286, cov loss: 0.144\n",
      "    [batch 2493]: seen 249300 examples : 67.5 eps, Loss: 3.181, Avg loss: 3.302, Best loss: 3.286, cov loss: 0.168\n",
      "    [batch 2500]: seen 250000 examples : 67.5 eps, Loss: 3.105, Avg loss: 3.300, Best loss: 3.286, cov loss: 0.160\n",
      "    [batch 2507]: seen 250700 examples : 67.5 eps, Loss: 3.376, Avg loss: 3.301, Best loss: 3.286, cov loss: 0.172\n",
      "    [batch 2514]: seen 251400 examples : 67.5 eps, Loss: 3.367, Avg loss: 3.306, Best loss: 3.286, cov loss: 0.163\n",
      "    [batch 2521]: seen 252100 examples : 67.5 eps, Loss: 3.254, Avg loss: 3.306, Best loss: 3.286, cov loss: 0.163\n",
      "    [batch 2528]: seen 252800 examples : 67.5 eps, Loss: 3.255, Avg loss: 3.304, Best loss: 3.286, cov loss: 0.163\n",
      "    [batch 2535]: seen 253500 examples : 67.5 eps, Loss: 3.331, Avg loss: 3.308, Best loss: 3.286, cov loss: 0.163\n",
      "    [batch 2542]: seen 254200 examples : 67.5 eps, Loss: 3.146, Avg loss: 3.308, Best loss: 3.286, cov loss: 0.164\n",
      "    [batch 2549]: seen 254900 examples : 67.5 eps, Loss: 3.373, Avg loss: 3.308, Best loss: 3.286, cov loss: 0.174\n",
      "    [batch 2556]: seen 255600 examples : 67.5 eps, Loss: 3.272, Avg loss: 3.308, Best loss: 3.286, cov loss: 0.158\n",
      "    [batch 2563]: seen 256300 examples : 67.5 eps, Loss: 3.039, Avg loss: 3.305, Best loss: 3.286, cov loss: 0.159\n",
      "    [batch 2570]: seen 257000 examples : 67.5 eps, Loss: 3.364, Avg loss: 3.304, Best loss: 3.286, cov loss: 0.157\n",
      "    [batch 2577]: seen 257700 examples : 67.5 eps, Loss: 3.180, Avg loss: 3.301, Best loss: 3.286, cov loss: 0.152\n",
      "    [batch 2584]: seen 258400 examples : 67.5 eps, Loss: 3.355, Avg loss: 3.307, Best loss: 3.286, cov loss: 0.152\n",
      "    [batch 2591]: seen 259100 examples : 67.6 eps, Loss: 3.433, Avg loss: 3.311, Best loss: 3.286, cov loss: 0.193\n",
      "    [batch 2598]: seen 259800 examples : 67.6 eps, Loss: 3.191, Avg loss: 3.312, Best loss: 3.286, cov loss: 0.150\n",
      "    [batch 2605]: seen 260500 examples : 67.6 eps, Loss: 3.331, Avg loss: 3.312, Best loss: 3.286, cov loss: 0.151\n",
      "    [batch 2612]: seen 261200 examples : 67.6 eps, Loss: 3.410, Avg loss: 3.312, Best loss: 3.286, cov loss: 0.192\n",
      "    [batch 2619]: seen 261900 examples : 67.6 eps, Loss: 3.281, Avg loss: 3.311, Best loss: 3.286, cov loss: 0.151\n",
      "    [batch 2626]: seen 262600 examples : 67.6 eps, Loss: 3.313, Avg loss: 3.309, Best loss: 3.286, cov loss: 0.156\n",
      "    [batch 2633]: seen 263300 examples : 67.6 eps, Loss: 3.266, Avg loss: 3.308, Best loss: 3.286, cov loss: 0.175\n",
      "    [batch 2640]: seen 264000 examples : 67.6 eps, Loss: 3.243, Avg loss: 3.307, Best loss: 3.286, cov loss: 0.167\n",
      "    [batch 2647]: seen 264700 examples : 67.6 eps, Loss: 3.293, Avg loss: 3.309, Best loss: 3.286, cov loss: 0.164\n",
      "    [batch 2654]: seen 265400 examples : 67.6 eps, Loss: 3.469, Avg loss: 3.307, Best loss: 3.286, cov loss: 0.168\n",
      "    [batch 2661]: seen 266100 examples : 67.6 eps, Loss: 3.272, Avg loss: 3.309, Best loss: 3.286, cov loss: 0.172\n",
      "    [batch 2668]: seen 266800 examples : 67.6 eps, Loss: 3.373, Avg loss: 3.313, Best loss: 3.286, cov loss: 0.170\n",
      "    [batch 2675]: seen 267500 examples : 67.6 eps, Loss: 3.438, Avg loss: 3.310, Best loss: 3.286, cov loss: 0.176\n",
      "    [batch 2682]: seen 268200 examples : 67.6 eps, Loss: 3.244, Avg loss: 3.306, Best loss: 3.286, cov loss: 0.161\n",
      "    [batch 2689]: seen 268900 examples : 67.6 eps, Loss: 3.227, Avg loss: 3.306, Best loss: 3.286, cov loss: 0.167\n",
      "    [batch 2696]: seen 269600 examples : 67.6 eps, Loss: 3.318, Avg loss: 3.309, Best loss: 3.286, cov loss: 0.160\n",
      "    [batch 2703]: seen 270300 examples : 67.6 eps, Loss: 3.060, Avg loss: 3.302, Best loss: 3.286, cov loss: 0.156\n",
      "    [batch 2710]: seen 271000 examples : 67.6 eps, Loss: 3.230, Avg loss: 3.297, Best loss: 3.286, cov loss: 0.165\n",
      "    [batch 2717]: seen 271700 examples : 67.6 eps, Loss: 3.409, Avg loss: 3.298, Best loss: 3.286, cov loss: 0.161\n",
      "    [batch 2724]: seen 272400 examples : 67.6 eps, Loss: 3.145, Avg loss: 3.301, Best loss: 3.286, cov loss: 0.169\n",
      "    [batch 2731]: seen 273100 examples : 67.6 eps, Loss: 3.351, Avg loss: 3.300, Best loss: 3.286, cov loss: 0.156\n",
      "    [batch 2738]: seen 273800 examples : 67.6 eps, Loss: 3.227, Avg loss: 3.293, Best loss: 3.286, cov loss: 0.148\n",
      "    [batch 2745]: seen 274500 examples : 67.6 eps, Loss: 3.445, Avg loss: 3.300, Best loss: 3.286, cov loss: 0.161\n",
      "    [batch 2752]: seen 275200 examples : 67.6 eps, Loss: 3.315, Avg loss: 3.299, Best loss: 3.286, cov loss: 0.165\n",
      "    [batch 2759]: seen 275900 examples : 67.6 eps, Loss: 3.475, Avg loss: 3.302, Best loss: 3.286, cov loss: 0.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2766]: seen 276600 examples : 67.6 eps, Loss: 3.333, Avg loss: 3.303, Best loss: 3.286, cov loss: 0.152\n",
      "    [batch 2773]: seen 277300 examples : 67.6 eps, Loss: 3.445, Avg loss: 3.304, Best loss: 3.286, cov loss: 0.167\n",
      "    [batch 2780]: seen 278000 examples : 67.6 eps, Loss: 3.396, Avg loss: 3.306, Best loss: 3.286, cov loss: 0.155\n",
      "    [batch 2787]: seen 278700 examples : 67.6 eps, Loss: 3.456, Avg loss: 3.308, Best loss: 3.286, cov loss: 0.160\n",
      "    [batch 2794]: seen 279400 examples : 67.7 eps, Loss: 3.535, Avg loss: 3.314, Best loss: 3.286, cov loss: 0.176\n",
      "    [batch 2801]: seen 280100 examples : 67.7 eps, Loss: 3.187, Avg loss: 3.310, Best loss: 3.286, cov loss: 0.153\n",
      "    [END] Training complete: Total examples : 280600; Total time: 1:09:07\n",
      "[EPOCH 25] Complete. Avg Loss: 3.31365515932395; Best Loss: 3.285648217685122\n",
      "[EPOCH 26] Starting training..\n",
      "    [batch 7]: seen 700 examples : 68.9 eps, Loss: 3.322, Avg loss: 3.313, Best loss: 3.286, cov loss: 0.161\n",
      "    [batch 14]: seen 1400 examples : 69.0 eps, Loss: 3.222, Avg loss: 3.306, Best loss: 3.286, cov loss: 0.156\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5de9fc5d7097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurr_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-17e3ed376236>\u001b[0m in \u001b[0;36mtrain_continue\u001b[0;34m(hps, epochs, train_step, curr_best, best_loss, avg_loss, restore, epoch_start)\u001b[0m\n\u001b[1;32m     36\u001b[0m                                                             \u001b[0mhps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                                             \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                                                             avg_loss)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcurr_best\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266/final_0/W266_Final/model_4/training_util.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(lm, session, batches, summary_writer, train_dir, train_step, saver, hps, best_loss, avg_loss)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunTrainStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266/final_0/W266_Final/model_4/training_util.py\u001b[0m in \u001b[0;36mrunTrainStep\u001b[0;34m(lm, session, batch)\u001b[0m\n\u001b[1;32m     87\u001b[0m     }\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "best_loss = None\n",
    "curr_best = best_loss\n",
    "train_step = 40416\n",
    "epochs = 8\n",
    "restore = True\n",
    "epoch_start = 22\n",
    "\n",
    "train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train session 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 252\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 110\n",
      "\n",
      "\n",
      "Warning: incorrectly formatted line in vocabulary file: 2 1/2 76\n",
      "\n",
      "\n",
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: 16:03\n",
      "INFO:tensorflow:Fetching data..\n",
      "INFO:tensorflow:Creating batches..\n",
      "INFO:tensorflow:[TOTAL Batches]  : 2808\n",
      "INFO:tensorflow:[TOTAL Examples] : 280778\n",
      "INFO:tensorflow:Creating batches..COMPLETE\n",
      "INFO:tensorflow:Building core graph...\n",
      "INFO:tensorflow:Adding attention_decoder timestep 0 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 1 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 2 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 3 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 4 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 5 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 6 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 7 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 8 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 9 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 10 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 11 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 12 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 13 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 14 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 15 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 16 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 17 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 18 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 19 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 20 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 21 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 22 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 23 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 24 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 25 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 26 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 27 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 28 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 29 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 30 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 31 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 32 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 33 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 34 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 35 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 36 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 37 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 38 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 39 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 40 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 41 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 42 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 43 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 44 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 45 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 46 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 47 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 48 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 49 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 50 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 51 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 52 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 53 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 54 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 55 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 56 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 57 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 58 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 59 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 60 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 61 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 62 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 63 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 64 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 65 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 66 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 67 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 68 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 69 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 70 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 71 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 72 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 73 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 74 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 75 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 76 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 77 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 78 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 79 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 80 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 81 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 82 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 83 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 84 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 85 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 86 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 87 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 88 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 89 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 90 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 91 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 92 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 93 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 94 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 95 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 96 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 97 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 98 of 100\n",
      "INFO:tensorflow:Adding attention_decoder timestep 99 of 100\n",
      "INFO:tensorflow:Building projection graph...\n",
      "INFO:tensorflow:Building projection graph...COMPLETE\n",
      "INFO:tensorflow:Building Loss graph...\n",
      "INFO:tensorflow:Building Loss graph...COMPLETE\n",
      "INFO:tensorflow:Building core graph...COMPLETE\n",
      "INFO:tensorflow:Building train graph...\n",
      "INFO:tensorflow:Building train graph...COMPLETE\n",
      "INFO:tensorflow:Building summary graph...\n",
      "INFO:tensorflow:Building summary graph...COMPLETE\n",
      "WARNING:tensorflow:From <ipython-input-5-17e3ed376236>:18: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-46184\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-46184\n",
      "[EPOCH 24] Starting training..\n",
      "    [batch 1]: seen 100 examples : 3.4 eps, Loss: 3.187, Avg loss: 3.187, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 5]: seen 500 examples : 12.4 eps, Loss: 3.227, Avg loss: 3.190, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 10]: seen 1000 examples : 19.5 eps, Loss: 3.129, Avg loss: 3.191, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 15]: seen 1500 examples : 24.1 eps, Loss: 3.258, Avg loss: 3.194, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 20]: seen 2000 examples : 27.4 eps, Loss: 3.451, Avg loss: 3.197, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 25]: seen 2500 examples : 29.7 eps, Loss: 3.384, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 30]: seen 3000 examples : 31.4 eps, Loss: 3.441, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 35]: seen 3500 examples : 32.9 eps, Loss: 3.311, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 40]: seen 4000 examples : 34.1 eps, Loss: 3.370, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 45]: seen 4500 examples : 35.0 eps, Loss: 3.281, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 50]: seen 5000 examples : 35.9 eps, Loss: 3.265, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 55]: seen 5500 examples : 36.6 eps, Loss: 3.195, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 60]: seen 6000 examples : 37.2 eps, Loss: 3.373, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 65]: seen 6500 examples : 37.7 eps, Loss: 3.329, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 70]: seen 7000 examples : 38.2 eps, Loss: 3.345, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 75]: seen 7500 examples : 38.6 eps, Loss: 3.274, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 80]: seen 8000 examples : 39.0 eps, Loss: 3.411, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 85]: seen 8500 examples : 39.3 eps, Loss: 3.278, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 89]: seen 8900 examples : 39.3 eps, Loss: 3.401, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 94]: seen 9400 examples : 39.6 eps, Loss: 3.216, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 99]: seen 9900 examples : 39.8 eps, Loss: 3.125, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 104]: seen 10400 examples : 40.1 eps, Loss: 3.360, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 109]: seen 10900 examples : 40.3 eps, Loss: 3.469, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 114]: seen 11400 examples : 40.5 eps, Loss: 3.268, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.200\n",
      "    [batch 119]: seen 11900 examples : 40.7 eps, Loss: 3.495, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 124]: seen 12400 examples : 40.9 eps, Loss: 3.245, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 129]: seen 12900 examples : 41.1 eps, Loss: 3.205, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 134]: seen 13400 examples : 41.2 eps, Loss: 3.356, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 139]: seen 13900 examples : 41.4 eps, Loss: 3.266, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 143]: seen 14300 examples : 41.3 eps, Loss: 3.237, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 148]: seen 14800 examples : 41.4 eps, Loss: 3.033, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 153]: seen 15300 examples : 41.6 eps, Loss: 3.260, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 158]: seen 15800 examples : 41.7 eps, Loss: 3.225, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.179\n",
      "    [batch 163]: seen 16300 examples : 41.8 eps, Loss: 3.128, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 168]: seen 16800 examples : 41.9 eps, Loss: 3.261, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 173]: seen 17300 examples : 42.0 eps, Loss: 3.125, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 177]: seen 17700 examples : 41.9 eps, Loss: 3.339, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 182]: seen 18200 examples : 42.0 eps, Loss: 3.289, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 187]: seen 18700 examples : 42.1 eps, Loss: 3.210, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 192]: seen 19200 examples : 42.2 eps, Loss: 3.343, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 197]: seen 19700 examples : 42.3 eps, Loss: 3.444, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 202]: seen 20200 examples : 42.4 eps, Loss: 3.495, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 207]: seen 20700 examples : 42.4 eps, Loss: 3.302, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 212]: seen 21200 examples : 42.5 eps, Loss: 3.222, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 217]: seen 21700 examples : 42.6 eps, Loss: 3.238, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 222]: seen 22200 examples : 42.6 eps, Loss: 3.460, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 227]: seen 22700 examples : 42.7 eps, Loss: 3.344, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 232]: seen 23200 examples : 42.8 eps, Loss: 3.212, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 237]: seen 23700 examples : 42.8 eps, Loss: 3.285, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 242]: seen 24200 examples : 42.9 eps, Loss: 3.270, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 247]: seen 24700 examples : 42.9 eps, Loss: 3.270, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 252]: seen 25200 examples : 43.0 eps, Loss: 3.244, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 257]: seen 25700 examples : 43.0 eps, Loss: 3.454, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 262]: seen 26200 examples : 43.1 eps, Loss: 3.216, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 266]: seen 26600 examples : 43.0 eps, Loss: 3.209, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 271]: seen 27100 examples : 43.0 eps, Loss: 3.221, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 276]: seen 27600 examples : 43.1 eps, Loss: 3.149, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 281]: seen 28100 examples : 43.1 eps, Loss: 3.302, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 286]: seen 28600 examples : 43.2 eps, Loss: 3.355, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 291]: seen 29100 examples : 43.2 eps, Loss: 3.321, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 296]: seen 29600 examples : 43.3 eps, Loss: 3.446, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 301]: seen 30100 examples : 43.3 eps, Loss: 3.228, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 306]: seen 30600 examples : 43.3 eps, Loss: 3.476, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 311]: seen 31100 examples : 43.4 eps, Loss: 3.149, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 316]: seen 31600 examples : 43.4 eps, Loss: 3.313, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 321]: seen 32100 examples : 43.5 eps, Loss: 3.319, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 326]: seen 32600 examples : 43.5 eps, Loss: 3.392, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 331]: seen 33100 examples : 43.5 eps, Loss: 3.239, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 336]: seen 33600 examples : 43.6 eps, Loss: 3.341, Avg loss: 3.298, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 341]: seen 34100 examples : 43.6 eps, Loss: 3.441, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 346]: seen 34600 examples : 43.6 eps, Loss: 3.333, Avg loss: 3.295, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 351]: seen 35100 examples : 43.6 eps, Loss: 3.205, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 356]: seen 35600 examples : 43.7 eps, Loss: 3.168, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 360]: seen 36000 examples : 43.6 eps, Loss: 3.219, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 365]: seen 36500 examples : 43.6 eps, Loss: 3.379, Avg loss: 3.297, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 370]: seen 37000 examples : 43.7 eps, Loss: 3.277, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 375]: seen 37500 examples : 43.7 eps, Loss: 3.416, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 380]: seen 38000 examples : 43.7 eps, Loss: 3.245, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 385]: seen 38500 examples : 43.7 eps, Loss: 3.247, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 390]: seen 39000 examples : 43.8 eps, Loss: 3.376, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 395]: seen 39500 examples : 43.8 eps, Loss: 3.500, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 400]: seen 40000 examples : 43.8 eps, Loss: 3.339, Avg loss: 3.297, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 405]: seen 40500 examples : 43.8 eps, Loss: 3.355, Avg loss: 3.300, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 410]: seen 41000 examples : 43.9 eps, Loss: 3.244, Avg loss: 3.298, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 415]: seen 41500 examples : 43.9 eps, Loss: 3.291, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 420]: seen 42000 examples : 43.9 eps, Loss: 3.375, Avg loss: 3.295, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 425]: seen 42500 examples : 43.9 eps, Loss: 3.384, Avg loss: 3.297, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 430]: seen 43000 examples : 44.0 eps, Loss: 3.321, Avg loss: 3.297, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 435]: seen 43500 examples : 44.0 eps, Loss: 3.034, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 440]: seen 44000 examples : 44.0 eps, Loss: 3.251, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 444]: seen 44400 examples : 43.9 eps, Loss: 3.292, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 449]: seen 44900 examples : 44.0 eps, Loss: 3.292, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 454]: seen 45400 examples : 44.0 eps, Loss: 3.458, Avg loss: 3.295, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 459]: seen 45900 examples : 44.0 eps, Loss: 3.503, Avg loss: 3.300, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 464]: seen 46400 examples : 44.0 eps, Loss: 3.240, Avg loss: 3.297, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 469]: seen 46900 examples : 44.0 eps, Loss: 3.299, Avg loss: 3.298, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 474]: seen 47400 examples : 44.1 eps, Loss: 3.181, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 479]: seen 47900 examples : 44.1 eps, Loss: 3.374, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 484]: seen 48400 examples : 44.1 eps, Loss: 3.516, Avg loss: 3.297, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 489]: seen 48900 examples : 44.1 eps, Loss: 3.172, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 494]: seen 49400 examples : 44.1 eps, Loss: 3.386, Avg loss: 3.295, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 499]: seen 49900 examples : 44.2 eps, Loss: 3.170, Avg loss: 3.295, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 504]: seen 50400 examples : 44.2 eps, Loss: 3.543, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 509]: seen 50900 examples : 44.2 eps, Loss: 3.319, Avg loss: 3.298, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 514]: seen 51400 examples : 44.2 eps, Loss: 3.497, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 519]: seen 51900 examples : 44.2 eps, Loss: 3.194, Avg loss: 3.297, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 524]: seen 52400 examples : 44.2 eps, Loss: 3.282, Avg loss: 3.297, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 529]: seen 52900 examples : 44.3 eps, Loss: 3.423, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.179\n",
      "    [batch 533]: seen 53300 examples : 44.2 eps, Loss: 3.230, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 538]: seen 53800 examples : 44.2 eps, Loss: 3.209, Avg loss: 3.298, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 543]: seen 54300 examples : 44.2 eps, Loss: 3.310, Avg loss: 3.301, Best loss: 3.187, cov loss: 0.181\n",
      "    [batch 548]: seen 54800 examples : 44.3 eps, Loss: 3.442, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.187\n",
      "    [batch 553]: seen 55300 examples : 44.3 eps, Loss: 3.285, Avg loss: 3.303, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 558]: seen 55800 examples : 44.3 eps, Loss: 3.398, Avg loss: 3.306, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 563]: seen 56300 examples : 44.3 eps, Loss: 3.273, Avg loss: 3.305, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 568]: seen 56800 examples : 44.3 eps, Loss: 3.304, Avg loss: 3.304, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 573]: seen 57300 examples : 44.3 eps, Loss: 3.380, Avg loss: 3.307, Best loss: 3.187, cov loss: 0.181\n",
      "    [batch 578]: seen 57800 examples : 44.3 eps, Loss: 3.440, Avg loss: 3.310, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 583]: seen 58300 examples : 44.4 eps, Loss: 3.175, Avg loss: 3.309, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 588]: seen 58800 examples : 44.4 eps, Loss: 3.239, Avg loss: 3.308, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 593]: seen 59300 examples : 44.4 eps, Loss: 3.328, Avg loss: 3.304, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 598]: seen 59800 examples : 44.4 eps, Loss: 3.352, Avg loss: 3.307, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 603]: seen 60300 examples : 44.4 eps, Loss: 3.349, Avg loss: 3.308, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 608]: seen 60800 examples : 44.4 eps, Loss: 3.329, Avg loss: 3.307, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 613]: seen 61300 examples : 44.4 eps, Loss: 3.271, Avg loss: 3.307, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 618]: seen 61800 examples : 44.5 eps, Loss: 3.395, Avg loss: 3.307, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 622]: seen 62200 examples : 44.4 eps, Loss: 3.317, Avg loss: 3.307, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 627]: seen 62700 examples : 44.4 eps, Loss: 3.139, Avg loss: 3.308, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 632]: seen 63200 examples : 44.4 eps, Loss: 3.326, Avg loss: 3.304, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 637]: seen 63700 examples : 44.5 eps, Loss: 3.227, Avg loss: 3.301, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 642]: seen 64200 examples : 44.5 eps, Loss: 3.261, Avg loss: 3.302, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 647]: seen 64700 examples : 44.5 eps, Loss: 3.149, Avg loss: 3.300, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 652]: seen 65200 examples : 44.5 eps, Loss: 3.282, Avg loss: 3.301, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 657]: seen 65700 examples : 44.5 eps, Loss: 3.088, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 662]: seen 66200 examples : 44.5 eps, Loss: 3.193, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 667]: seen 66700 examples : 44.5 eps, Loss: 3.071, Avg loss: 3.297, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 672]: seen 67200 examples : 44.5 eps, Loss: 3.086, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 677]: seen 67700 examples : 44.5 eps, Loss: 3.249, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 682]: seen 68200 examples : 44.5 eps, Loss: 3.440, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 687]: seen 68700 examples : 44.6 eps, Loss: 3.413, Avg loss: 3.295, Best loss: 3.187, cov loss: 0.185\n",
      "    [batch 692]: seen 69200 examples : 44.6 eps, Loss: 3.310, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 697]: seen 69700 examples : 44.6 eps, Loss: 3.161, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 702]: seen 70200 examples : 44.6 eps, Loss: 3.019, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 707]: seen 70700 examples : 44.6 eps, Loss: 3.241, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 711]: seen 71100 examples : 44.6 eps, Loss: 3.442, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 716]: seen 71600 examples : 44.6 eps, Loss: 3.125, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 721]: seen 72100 examples : 44.6 eps, Loss: 3.346, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 726]: seen 72600 examples : 44.6 eps, Loss: 3.165, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 731]: seen 73100 examples : 44.6 eps, Loss: 3.342, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 736]: seen 73600 examples : 44.6 eps, Loss: 3.391, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 741]: seen 74100 examples : 44.6 eps, Loss: 3.436, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 746]: seen 74600 examples : 44.6 eps, Loss: 2.979, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 751]: seen 75100 examples : 44.7 eps, Loss: 3.258, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 756]: seen 75600 examples : 44.7 eps, Loss: 3.437, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 761]: seen 76100 examples : 44.7 eps, Loss: 3.201, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 766]: seen 76600 examples : 44.7 eps, Loss: 3.099, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 771]: seen 77100 examples : 44.7 eps, Loss: 3.231, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 776]: seen 77600 examples : 44.7 eps, Loss: 3.212, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 781]: seen 78100 examples : 44.7 eps, Loss: 3.265, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 786]: seen 78600 examples : 44.7 eps, Loss: 3.397, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 791]: seen 79100 examples : 44.7 eps, Loss: 3.449, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 796]: seen 79600 examples : 44.8 eps, Loss: 3.359, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 801]: seen 80100 examples : 44.8 eps, Loss: 3.269, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 805]: seen 80500 examples : 44.7 eps, Loss: 3.234, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 810]: seen 81000 examples : 44.7 eps, Loss: 3.279, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 815]: seen 81500 examples : 44.8 eps, Loss: 3.298, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 820]: seen 82000 examples : 44.8 eps, Loss: 3.351, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 825]: seen 82500 examples : 44.8 eps, Loss: 3.288, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 830]: seen 83000 examples : 44.8 eps, Loss: 3.201, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 835]: seen 83500 examples : 44.8 eps, Loss: 3.163, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 840]: seen 84000 examples : 44.8 eps, Loss: 3.373, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 845]: seen 84500 examples : 44.8 eps, Loss: 3.113, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 850]: seen 85000 examples : 44.8 eps, Loss: 3.201, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 855]: seen 85500 examples : 44.8 eps, Loss: 3.233, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 860]: seen 86000 examples : 44.8 eps, Loss: 3.375, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 865]: seen 86500 examples : 44.8 eps, Loss: 3.369, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 870]: seen 87000 examples : 44.9 eps, Loss: 3.320, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 875]: seen 87500 examples : 44.9 eps, Loss: 3.216, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 880]: seen 88000 examples : 44.9 eps, Loss: 3.257, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 885]: seen 88500 examples : 44.9 eps, Loss: 3.458, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.181\n",
      "    [batch 889]: seen 88900 examples : 44.9 eps, Loss: 3.360, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 894]: seen 89400 examples : 44.9 eps, Loss: 3.259, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 899]: seen 89900 examples : 44.9 eps, Loss: 3.235, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 904]: seen 90400 examples : 44.9 eps, Loss: 3.264, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 909]: seen 90900 examples : 44.9 eps, Loss: 3.473, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 914]: seen 91400 examples : 44.9 eps, Loss: 3.349, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 919]: seen 91900 examples : 44.9 eps, Loss: 3.212, Avg loss: 3.295, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 924]: seen 92400 examples : 44.9 eps, Loss: 3.336, Avg loss: 3.298, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 929]: seen 92900 examples : 44.9 eps, Loss: 3.244, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 934]: seen 93400 examples : 44.9 eps, Loss: 3.247, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 939]: seen 93900 examples : 44.9 eps, Loss: 3.416, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 944]: seen 94400 examples : 44.9 eps, Loss: 3.132, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 949]: seen 94900 examples : 45.0 eps, Loss: 3.276, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 954]: seen 95400 examples : 45.0 eps, Loss: 3.372, Avg loss: 3.295, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 959]: seen 95900 examples : 45.0 eps, Loss: 3.344, Avg loss: 3.298, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 964]: seen 96400 examples : 45.0 eps, Loss: 3.406, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 969]: seen 96900 examples : 45.0 eps, Loss: 3.382, Avg loss: 3.301, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 974]: seen 97400 examples : 45.0 eps, Loss: 3.510, Avg loss: 3.306, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 978]: seen 97800 examples : 45.0 eps, Loss: 3.256, Avg loss: 3.310, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 983]: seen 98300 examples : 45.0 eps, Loss: 3.333, Avg loss: 3.309, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 988]: seen 98800 examples : 45.0 eps, Loss: 3.190, Avg loss: 3.309, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 993]: seen 99300 examples : 45.0 eps, Loss: 3.468, Avg loss: 3.309, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 998]: seen 99800 examples : 45.0 eps, Loss: 3.259, Avg loss: 3.308, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1003]: seen 100300 examples : 45.0 eps, Loss: 3.261, Avg loss: 3.306, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1008]: seen 100800 examples : 45.0 eps, Loss: 3.613, Avg loss: 3.302, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 1013]: seen 101300 examples : 45.0 eps, Loss: 3.231, Avg loss: 3.298, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1018]: seen 101800 examples : 45.0 eps, Loss: 3.247, Avg loss: 3.300, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1023]: seen 102300 examples : 45.0 eps, Loss: 3.478, Avg loss: 3.301, Best loss: 3.187, cov loss: 0.187\n",
      "    [batch 1028]: seen 102800 examples : 45.0 eps, Loss: 3.275, Avg loss: 3.301, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1033]: seen 103300 examples : 45.0 eps, Loss: 3.386, Avg loss: 3.300, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1038]: seen 103800 examples : 45.0 eps, Loss: 3.185, Avg loss: 3.297, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1043]: seen 104300 examples : 45.0 eps, Loss: 3.232, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1048]: seen 104800 examples : 45.1 eps, Loss: 3.378, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1053]: seen 105300 examples : 45.1 eps, Loss: 3.278, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 1058]: seen 105800 examples : 45.1 eps, Loss: 3.287, Avg loss: 3.297, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1063]: seen 106300 examples : 45.1 eps, Loss: 3.315, Avg loss: 3.298, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1067]: seen 106700 examples : 45.0 eps, Loss: 3.283, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1072]: seen 107200 examples : 45.1 eps, Loss: 3.437, Avg loss: 3.298, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1077]: seen 107700 examples : 45.1 eps, Loss: 3.484, Avg loss: 3.301, Best loss: 3.187, cov loss: 0.179\n",
      "    [batch 1082]: seen 108200 examples : 45.1 eps, Loss: 3.210, Avg loss: 3.300, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 1087]: seen 108700 examples : 45.1 eps, Loss: 3.393, Avg loss: 3.300, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1092]: seen 109200 examples : 45.1 eps, Loss: 3.155, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1097]: seen 109700 examples : 45.1 eps, Loss: 3.204, Avg loss: 3.295, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1102]: seen 110200 examples : 45.1 eps, Loss: 3.375, Avg loss: 3.298, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1107]: seen 110700 examples : 45.1 eps, Loss: 3.244, Avg loss: 3.295, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1112]: seen 111200 examples : 45.1 eps, Loss: 3.298, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1117]: seen 111700 examples : 45.1 eps, Loss: 3.298, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1122]: seen 112200 examples : 45.1 eps, Loss: 3.219, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1127]: seen 112700 examples : 45.1 eps, Loss: 3.163, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1132]: seen 113200 examples : 45.1 eps, Loss: 3.513, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1137]: seen 113700 examples : 45.1 eps, Loss: 3.357, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 1142]: seen 114200 examples : 45.1 eps, Loss: 3.232, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1147]: seen 114700 examples : 45.1 eps, Loss: 3.284, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 1152]: seen 115200 examples : 45.1 eps, Loss: 3.383, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1156]: seen 115600 examples : 45.1 eps, Loss: 3.527, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1161]: seen 116100 examples : 45.1 eps, Loss: 3.397, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1166]: seen 116600 examples : 45.1 eps, Loss: 3.277, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1171]: seen 117100 examples : 45.1 eps, Loss: 3.261, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1176]: seen 117600 examples : 45.1 eps, Loss: 3.467, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1181]: seen 118100 examples : 45.1 eps, Loss: 3.350, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 1186]: seen 118600 examples : 45.2 eps, Loss: 3.381, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1191]: seen 119100 examples : 45.2 eps, Loss: 3.196, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1196]: seen 119600 examples : 45.2 eps, Loss: 3.260, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1201]: seen 120100 examples : 45.2 eps, Loss: 3.329, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1206]: seen 120600 examples : 45.2 eps, Loss: 3.252, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1211]: seen 121100 examples : 45.2 eps, Loss: 3.176, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1216]: seen 121600 examples : 45.2 eps, Loss: 3.217, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1221]: seen 122100 examples : 45.2 eps, Loss: 3.200, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1226]: seen 122600 examples : 45.2 eps, Loss: 3.361, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1231]: seen 123100 examples : 45.2 eps, Loss: 3.466, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 1236]: seen 123600 examples : 45.2 eps, Loss: 3.311, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1241]: seen 124100 examples : 45.2 eps, Loss: 3.094, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.160\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-47425\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-47425\n",
      "    [batch 1245]: seen 124500 examples : 45.2 eps, Loss: 3.358, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.179\n",
      "    [batch 1249]: seen 124900 examples : 45.2 eps, Loss: 3.276, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1254]: seen 125400 examples : 45.2 eps, Loss: 3.277, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1259]: seen 125900 examples : 45.2 eps, Loss: 3.078, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1264]: seen 126400 examples : 45.2 eps, Loss: 3.225, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1269]: seen 126900 examples : 45.2 eps, Loss: 3.343, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1274]: seen 127400 examples : 45.2 eps, Loss: 3.248, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 1279]: seen 127900 examples : 45.2 eps, Loss: 3.298, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 1284]: seen 128400 examples : 45.2 eps, Loss: 3.309, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1289]: seen 128900 examples : 45.2 eps, Loss: 3.207, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1294]: seen 129400 examples : 45.2 eps, Loss: 3.396, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 1299]: seen 129900 examples : 45.2 eps, Loss: 3.293, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.187\n",
      "    [batch 1304]: seen 130400 examples : 45.2 eps, Loss: 3.337, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 1309]: seen 130900 examples : 45.2 eps, Loss: 3.301, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1314]: seen 131400 examples : 45.2 eps, Loss: 3.370, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 1319]: seen 131900 examples : 45.2 eps, Loss: 3.197, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1324]: seen 132400 examples : 45.2 eps, Loss: 3.406, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1329]: seen 132900 examples : 45.2 eps, Loss: 3.172, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1333]: seen 133300 examples : 45.2 eps, Loss: 3.188, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1338]: seen 133800 examples : 45.2 eps, Loss: 3.346, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1343]: seen 134300 examples : 45.2 eps, Loss: 3.164, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1348]: seen 134800 examples : 45.2 eps, Loss: 3.409, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1353]: seen 135300 examples : 45.2 eps, Loss: 3.461, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1358]: seen 135800 examples : 45.2 eps, Loss: 3.352, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1363]: seen 136300 examples : 45.2 eps, Loss: 3.359, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1368]: seen 136800 examples : 45.2 eps, Loss: 3.258, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 1373]: seen 137300 examples : 45.2 eps, Loss: 3.327, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1378]: seen 137800 examples : 45.2 eps, Loss: 3.340, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1383]: seen 138300 examples : 45.2 eps, Loss: 3.143, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1388]: seen 138800 examples : 45.3 eps, Loss: 3.404, Avg loss: 3.298, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 1393]: seen 139300 examples : 45.3 eps, Loss: 3.434, Avg loss: 3.300, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1398]: seen 139800 examples : 45.3 eps, Loss: 3.155, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1403]: seen 140300 examples : 45.3 eps, Loss: 3.450, Avg loss: 3.303, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1408]: seen 140800 examples : 45.3 eps, Loss: 3.110, Avg loss: 3.302, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1413]: seen 141300 examples : 45.3 eps, Loss: 3.220, Avg loss: 3.304, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1418]: seen 141800 examples : 45.3 eps, Loss: 3.347, Avg loss: 3.306, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1422]: seen 142200 examples : 45.3 eps, Loss: 3.158, Avg loss: 3.304, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 1427]: seen 142700 examples : 45.3 eps, Loss: 3.524, Avg loss: 3.308, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 1432]: seen 143200 examples : 45.3 eps, Loss: 3.411, Avg loss: 3.308, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1437]: seen 143700 examples : 45.3 eps, Loss: 3.268, Avg loss: 3.308, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1442]: seen 144200 examples : 45.3 eps, Loss: 3.347, Avg loss: 3.307, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1447]: seen 144700 examples : 45.3 eps, Loss: 3.153, Avg loss: 3.309, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1452]: seen 145200 examples : 45.3 eps, Loss: 3.279, Avg loss: 3.310, Best loss: 3.187, cov loss: 0.167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1457]: seen 145700 examples : 45.3 eps, Loss: 3.216, Avg loss: 3.306, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1462]: seen 146200 examples : 45.3 eps, Loss: 3.336, Avg loss: 3.309, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1467]: seen 146700 examples : 45.3 eps, Loss: 3.122, Avg loss: 3.306, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1472]: seen 147200 examples : 45.3 eps, Loss: 3.309, Avg loss: 3.309, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1477]: seen 147700 examples : 45.3 eps, Loss: 3.170, Avg loss: 3.305, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1482]: seen 148200 examples : 45.3 eps, Loss: 3.312, Avg loss: 3.307, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1487]: seen 148700 examples : 45.3 eps, Loss: 3.162, Avg loss: 3.302, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1492]: seen 149200 examples : 45.3 eps, Loss: 3.399, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 1497]: seen 149700 examples : 45.3 eps, Loss: 3.401, Avg loss: 3.306, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1502]: seen 150200 examples : 45.3 eps, Loss: 3.332, Avg loss: 3.304, Best loss: 3.187, cov loss: 0.206\n",
      "    [batch 1507]: seen 150700 examples : 45.3 eps, Loss: 3.409, Avg loss: 3.303, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1511]: seen 151100 examples : 45.3 eps, Loss: 3.305, Avg loss: 3.302, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 1516]: seen 151600 examples : 45.3 eps, Loss: 3.098, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1521]: seen 152100 examples : 45.3 eps, Loss: 3.353, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.186\n",
      "    [batch 1526]: seen 152600 examples : 45.3 eps, Loss: 3.120, Avg loss: 3.305, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1531]: seen 153100 examples : 45.3 eps, Loss: 3.241, Avg loss: 3.305, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1536]: seen 153600 examples : 45.3 eps, Loss: 3.351, Avg loss: 3.303, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1541]: seen 154100 examples : 45.3 eps, Loss: 3.567, Avg loss: 3.302, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 1546]: seen 154600 examples : 45.3 eps, Loss: 3.334, Avg loss: 3.304, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1551]: seen 155100 examples : 45.3 eps, Loss: 3.064, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 1556]: seen 155600 examples : 45.3 eps, Loss: 3.358, Avg loss: 3.301, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1561]: seen 156100 examples : 45.3 eps, Loss: 3.207, Avg loss: 3.302, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1566]: seen 156600 examples : 45.3 eps, Loss: 3.473, Avg loss: 3.303, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1571]: seen 157100 examples : 45.3 eps, Loss: 2.919, Avg loss: 3.301, Best loss: 3.187, cov loss: 0.137\n",
      "    [batch 1576]: seen 157600 examples : 45.3 eps, Loss: 3.280, Avg loss: 3.301, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1581]: seen 158100 examples : 45.4 eps, Loss: 3.245, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1586]: seen 158600 examples : 45.4 eps, Loss: 3.245, Avg loss: 3.300, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 1591]: seen 159100 examples : 45.4 eps, Loss: 3.229, Avg loss: 3.301, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1596]: seen 159600 examples : 45.4 eps, Loss: 3.311, Avg loss: 3.303, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1601]: seen 160100 examples : 45.4 eps, Loss: 3.303, Avg loss: 3.299, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1605]: seen 160500 examples : 45.4 eps, Loss: 3.292, Avg loss: 3.297, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1610]: seen 161000 examples : 45.4 eps, Loss: 3.141, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1615]: seen 161500 examples : 45.4 eps, Loss: 3.433, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 1620]: seen 162000 examples : 45.4 eps, Loss: 3.434, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1625]: seen 162500 examples : 45.4 eps, Loss: 3.274, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1630]: seen 163000 examples : 45.4 eps, Loss: 3.187, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1635]: seen 163500 examples : 45.4 eps, Loss: 3.249, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1640]: seen 164000 examples : 45.4 eps, Loss: 3.177, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1645]: seen 164500 examples : 45.4 eps, Loss: 3.289, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1650]: seen 165000 examples : 45.4 eps, Loss: 3.221, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1655]: seen 165500 examples : 45.4 eps, Loss: 3.205, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1660]: seen 166000 examples : 45.4 eps, Loss: 3.279, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1665]: seen 166500 examples : 45.4 eps, Loss: 3.205, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1670]: seen 167000 examples : 45.4 eps, Loss: 3.365, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1675]: seen 167500 examples : 45.4 eps, Loss: 3.234, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1680]: seen 168000 examples : 45.4 eps, Loss: 3.388, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1685]: seen 168500 examples : 45.4 eps, Loss: 3.296, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1689]: seen 168900 examples : 45.4 eps, Loss: 3.319, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1694]: seen 169400 examples : 45.4 eps, Loss: 3.367, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1699]: seen 169900 examples : 45.4 eps, Loss: 3.186, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1704]: seen 170400 examples : 45.4 eps, Loss: 3.176, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1709]: seen 170900 examples : 45.4 eps, Loss: 3.502, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1714]: seen 171400 examples : 45.4 eps, Loss: 3.335, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1719]: seen 171900 examples : 45.4 eps, Loss: 3.295, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.184\n",
      "    [batch 1724]: seen 172400 examples : 45.4 eps, Loss: 3.030, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1729]: seen 172900 examples : 45.4 eps, Loss: 3.178, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1734]: seen 173400 examples : 45.4 eps, Loss: 3.277, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1739]: seen 173900 examples : 45.4 eps, Loss: 3.299, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1744]: seen 174400 examples : 45.4 eps, Loss: 3.192, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1749]: seen 174900 examples : 45.4 eps, Loss: 3.372, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1754]: seen 175400 examples : 45.4 eps, Loss: 3.236, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1759]: seen 175900 examples : 45.4 eps, Loss: 3.290, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1764]: seen 176400 examples : 45.4 eps, Loss: 3.356, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1769]: seen 176900 examples : 45.4 eps, Loss: 3.214, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1774]: seen 177400 examples : 45.4 eps, Loss: 3.192, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1778]: seen 177800 examples : 45.4 eps, Loss: 3.269, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1783]: seen 178300 examples : 45.4 eps, Loss: 3.420, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1788]: seen 178800 examples : 45.4 eps, Loss: 3.501, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1793]: seen 179300 examples : 45.4 eps, Loss: 3.248, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1798]: seen 179800 examples : 45.4 eps, Loss: 3.363, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1803]: seen 180300 examples : 45.5 eps, Loss: 3.219, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 1808]: seen 180800 examples : 45.5 eps, Loss: 3.330, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1813]: seen 181300 examples : 45.5 eps, Loss: 3.175, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1818]: seen 181800 examples : 45.5 eps, Loss: 3.260, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 1823]: seen 182300 examples : 45.5 eps, Loss: 3.454, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1828]: seen 182800 examples : 45.5 eps, Loss: 3.427, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1833]: seen 183300 examples : 45.5 eps, Loss: 3.098, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1838]: seen 183800 examples : 45.5 eps, Loss: 3.358, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1843]: seen 184300 examples : 45.5 eps, Loss: 3.357, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.182\n",
      "    [batch 1848]: seen 184800 examples : 45.5 eps, Loss: 3.229, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1853]: seen 185300 examples : 45.5 eps, Loss: 3.217, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 1858]: seen 185800 examples : 45.5 eps, Loss: 3.365, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1863]: seen 186300 examples : 45.5 eps, Loss: 3.302, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1868]: seen 186800 examples : 45.5 eps, Loss: 3.166, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1872]: seen 187200 examples : 45.5 eps, Loss: 3.299, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 1877]: seen 187700 examples : 45.5 eps, Loss: 3.065, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1882]: seen 188200 examples : 45.5 eps, Loss: 3.261, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1887]: seen 188700 examples : 45.5 eps, Loss: 3.280, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1892]: seen 189200 examples : 45.5 eps, Loss: 3.241, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1897]: seen 189700 examples : 45.5 eps, Loss: 3.333, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1902]: seen 190200 examples : 45.5 eps, Loss: 3.370, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1907]: seen 190700 examples : 45.5 eps, Loss: 3.041, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1912]: seen 191200 examples : 45.5 eps, Loss: 3.157, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1917]: seen 191700 examples : 45.5 eps, Loss: 3.509, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1922]: seen 192200 examples : 45.5 eps, Loss: 3.271, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1927]: seen 192700 examples : 45.5 eps, Loss: 3.371, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1932]: seen 193200 examples : 45.5 eps, Loss: 3.252, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.137\n",
      "    [batch 1937]: seen 193700 examples : 45.5 eps, Loss: 3.094, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1942]: seen 194200 examples : 45.5 eps, Loss: 3.372, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1947]: seen 194700 examples : 45.5 eps, Loss: 3.100, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1952]: seen 195200 examples : 45.5 eps, Loss: 3.464, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1956]: seen 195600 examples : 45.5 eps, Loss: 3.300, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1961]: seen 196100 examples : 45.5 eps, Loss: 3.265, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1966]: seen 196600 examples : 45.5 eps, Loss: 3.317, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1971]: seen 197100 examples : 45.5 eps, Loss: 3.217, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1976]: seen 197600 examples : 45.5 eps, Loss: 3.210, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1981]: seen 198100 examples : 45.5 eps, Loss: 3.154, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1986]: seen 198600 examples : 45.5 eps, Loss: 3.284, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1991]: seen 199100 examples : 45.5 eps, Loss: 3.264, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1996]: seen 199600 examples : 45.5 eps, Loss: 3.191, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2001]: seen 200100 examples : 45.5 eps, Loss: 3.304, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2006]: seen 200600 examples : 45.5 eps, Loss: 3.367, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2011]: seen 201100 examples : 45.5 eps, Loss: 3.234, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2016]: seen 201600 examples : 45.5 eps, Loss: 3.282, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2021]: seen 202100 examples : 45.5 eps, Loss: 3.489, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.191\n",
      "    [batch 2026]: seen 202600 examples : 45.5 eps, Loss: 3.224, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.136\n",
      "    [batch 2031]: seen 203100 examples : 45.6 eps, Loss: 3.348, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2036]: seen 203600 examples : 45.6 eps, Loss: 3.427, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 2041]: seen 204100 examples : 45.6 eps, Loss: 3.255, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2045]: seen 204500 examples : 45.5 eps, Loss: 3.278, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2050]: seen 205000 examples : 45.5 eps, Loss: 3.206, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2055]: seen 205500 examples : 45.5 eps, Loss: 3.256, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2060]: seen 206000 examples : 45.5 eps, Loss: 3.272, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2065]: seen 206500 examples : 45.5 eps, Loss: 3.316, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2070]: seen 207000 examples : 45.5 eps, Loss: 3.201, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2075]: seen 207500 examples : 45.6 eps, Loss: 3.304, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2080]: seen 208000 examples : 45.6 eps, Loss: 3.318, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2085]: seen 208500 examples : 45.6 eps, Loss: 3.052, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 2090]: seen 209000 examples : 45.6 eps, Loss: 3.309, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2095]: seen 209500 examples : 45.6 eps, Loss: 3.446, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2100]: seen 210000 examples : 45.6 eps, Loss: 3.297, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2105]: seen 210500 examples : 45.6 eps, Loss: 3.493, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2110]: seen 211000 examples : 45.6 eps, Loss: 3.322, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2115]: seen 211500 examples : 45.6 eps, Loss: 3.332, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2120]: seen 212000 examples : 45.6 eps, Loss: 3.383, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2125]: seen 212500 examples : 45.6 eps, Loss: 3.107, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2130]: seen 213000 examples : 45.6 eps, Loss: 3.330, Avg loss: 3.296, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2134]: seen 213400 examples : 45.6 eps, Loss: 3.288, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2139]: seen 213900 examples : 45.6 eps, Loss: 3.279, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2144]: seen 214400 examples : 45.6 eps, Loss: 3.323, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2149]: seen 214900 examples : 45.6 eps, Loss: 3.318, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2154]: seen 215400 examples : 45.6 eps, Loss: 3.237, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 2159]: seen 215900 examples : 45.6 eps, Loss: 3.144, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2164]: seen 216400 examples : 45.6 eps, Loss: 3.337, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2169]: seen 216900 examples : 45.6 eps, Loss: 3.232, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2174]: seen 217400 examples : 45.6 eps, Loss: 3.433, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 2179]: seen 217900 examples : 45.6 eps, Loss: 3.168, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 2184]: seen 218400 examples : 45.6 eps, Loss: 3.228, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2189]: seen 218900 examples : 45.6 eps, Loss: 3.529, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2194]: seen 219400 examples : 45.6 eps, Loss: 3.291, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 2199]: seen 219900 examples : 45.6 eps, Loss: 3.125, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2204]: seen 220400 examples : 45.6 eps, Loss: 3.271, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2209]: seen 220900 examples : 45.6 eps, Loss: 3.300, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2214]: seen 221400 examples : 45.6 eps, Loss: 3.334, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 2219]: seen 221900 examples : 45.6 eps, Loss: 3.220, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2224]: seen 222400 examples : 45.6 eps, Loss: 3.330, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2228]: seen 222800 examples : 45.6 eps, Loss: 3.119, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2233]: seen 223300 examples : 45.6 eps, Loss: 3.438, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.180\n",
      "    [batch 2238]: seen 223800 examples : 45.6 eps, Loss: 3.221, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.180\n",
      "    [batch 2243]: seen 224300 examples : 45.6 eps, Loss: 3.289, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2248]: seen 224800 examples : 45.6 eps, Loss: 3.120, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2253]: seen 225300 examples : 45.6 eps, Loss: 3.139, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2258]: seen 225800 examples : 45.6 eps, Loss: 3.267, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2263]: seen 226300 examples : 45.6 eps, Loss: 3.381, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2268]: seen 226800 examples : 45.6 eps, Loss: 3.341, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2273]: seen 227300 examples : 45.6 eps, Loss: 3.311, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2278]: seen 227800 examples : 45.6 eps, Loss: 3.421, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2283]: seen 228300 examples : 45.6 eps, Loss: 3.205, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 2288]: seen 228800 examples : 45.6 eps, Loss: 3.133, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 2293]: seen 229300 examples : 45.6 eps, Loss: 3.361, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2298]: seen 229800 examples : 45.6 eps, Loss: 3.408, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2303]: seen 230300 examples : 45.6 eps, Loss: 3.356, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2308]: seen 230800 examples : 45.6 eps, Loss: 3.284, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2313]: seen 231300 examples : 45.6 eps, Loss: 3.367, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2318]: seen 231800 examples : 45.6 eps, Loss: 3.179, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2323]: seen 232300 examples : 45.6 eps, Loss: 3.256, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2328]: seen 232800 examples : 45.6 eps, Loss: 3.192, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2333]: seen 233300 examples : 45.6 eps, Loss: 3.223, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2338]: seen 233800 examples : 45.6 eps, Loss: 3.145, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2343]: seen 234300 examples : 45.6 eps, Loss: 3.194, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2348]: seen 234800 examples : 45.6 eps, Loss: 3.363, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2353]: seen 235300 examples : 45.6 eps, Loss: 3.352, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2358]: seen 235800 examples : 45.6 eps, Loss: 3.452, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2363]: seen 236300 examples : 45.6 eps, Loss: 3.345, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 2368]: seen 236800 examples : 45.7 eps, Loss: 3.310, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2373]: seen 237300 examples : 45.7 eps, Loss: 3.365, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2378]: seen 237800 examples : 45.7 eps, Loss: 3.129, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2383]: seen 238300 examples : 45.7 eps, Loss: 3.503, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2388]: seen 238800 examples : 45.7 eps, Loss: 3.222, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2393]: seen 239300 examples : 45.7 eps, Loss: 3.133, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2398]: seen 239800 examples : 45.7 eps, Loss: 3.230, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2403]: seen 240300 examples : 45.7 eps, Loss: 3.213, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2408]: seen 240800 examples : 45.7 eps, Loss: 3.126, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2413]: seen 241300 examples : 45.7 eps, Loss: 3.329, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2418]: seen 241800 examples : 45.7 eps, Loss: 3.199, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 2423]: seen 242300 examples : 45.7 eps, Loss: 3.371, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 2428]: seen 242800 examples : 45.7 eps, Loss: 3.243, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2433]: seen 243300 examples : 45.7 eps, Loss: 3.416, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2438]: seen 243800 examples : 45.7 eps, Loss: 3.253, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2443]: seen 244300 examples : 45.7 eps, Loss: 3.450, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2448]: seen 244800 examples : 45.7 eps, Loss: 3.292, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2453]: seen 245300 examples : 45.7 eps, Loss: 3.293, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 2458]: seen 245800 examples : 45.7 eps, Loss: 3.354, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 2463]: seen 246300 examples : 45.7 eps, Loss: 3.037, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2468]: seen 246800 examples : 45.7 eps, Loss: 3.146, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2473]: seen 247300 examples : 45.7 eps, Loss: 3.431, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 2478]: seen 247800 examples : 45.7 eps, Loss: 3.269, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 2483]: seen 248300 examples : 45.7 eps, Loss: 3.210, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2488]: seen 248800 examples : 45.7 eps, Loss: 3.420, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2493]: seen 249300 examples : 45.7 eps, Loss: 3.315, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 2498]: seen 249800 examples : 45.7 eps, Loss: 3.241, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2503]: seen 250300 examples : 45.7 eps, Loss: 3.372, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2508]: seen 250800 examples : 45.7 eps, Loss: 3.342, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2513]: seen 251300 examples : 45.7 eps, Loss: 3.178, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2518]: seen 251800 examples : 45.7 eps, Loss: 3.250, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2523]: seen 252300 examples : 45.7 eps, Loss: 3.231, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2528]: seen 252800 examples : 45.7 eps, Loss: 3.234, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2533]: seen 253300 examples : 45.7 eps, Loss: 3.153, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2538]: seen 253800 examples : 45.7 eps, Loss: 3.380, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2543]: seen 254300 examples : 45.7 eps, Loss: 3.395, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2548]: seen 254800 examples : 45.7 eps, Loss: 3.411, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2553]: seen 255300 examples : 45.7 eps, Loss: 3.271, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 2558]: seen 255800 examples : 45.7 eps, Loss: 3.438, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2563]: seen 256300 examples : 45.7 eps, Loss: 3.330, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 2568]: seen 256800 examples : 45.7 eps, Loss: 3.109, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2573]: seen 257300 examples : 45.7 eps, Loss: 3.260, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2578]: seen 257800 examples : 45.7 eps, Loss: 3.416, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2582]: seen 258200 examples : 45.7 eps, Loss: 3.315, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2587]: seen 258700 examples : 45.7 eps, Loss: 3.331, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2592]: seen 259200 examples : 45.7 eps, Loss: 3.366, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.185\n",
      "    [batch 2597]: seen 259700 examples : 45.7 eps, Loss: 3.238, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2602]: seen 260200 examples : 45.7 eps, Loss: 3.385, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 2607]: seen 260700 examples : 45.7 eps, Loss: 3.268, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2612]: seen 261200 examples : 45.7 eps, Loss: 3.280, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2617]: seen 261700 examples : 45.7 eps, Loss: 3.208, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2622]: seen 262200 examples : 45.7 eps, Loss: 3.236, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2627]: seen 262700 examples : 45.7 eps, Loss: 3.181, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2632]: seen 263200 examples : 45.7 eps, Loss: 3.545, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 2637]: seen 263700 examples : 45.7 eps, Loss: 3.292, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2642]: seen 264200 examples : 45.7 eps, Loss: 3.391, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2647]: seen 264700 examples : 45.7 eps, Loss: 3.447, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2652]: seen 265200 examples : 45.7 eps, Loss: 3.201, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2655]: seen 265500 examples : 45.7 eps, Loss: 3.325, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.183\n",
      "    [batch 2660]: seen 266000 examples : 45.7 eps, Loss: 3.424, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2665]: seen 266500 examples : 45.7 eps, Loss: 3.245, Avg loss: 3.293, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2669]: seen 266900 examples : 45.7 eps, Loss: 3.262, Avg loss: 3.295, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 2674]: seen 267400 examples : 45.7 eps, Loss: 3.217, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2679]: seen 267900 examples : 45.7 eps, Loss: 3.205, Avg loss: 3.288, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2684]: seen 268400 examples : 45.7 eps, Loss: 3.431, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2689]: seen 268900 examples : 45.7 eps, Loss: 3.343, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2694]: seen 269400 examples : 45.7 eps, Loss: 3.244, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2699]: seen 269900 examples : 45.7 eps, Loss: 3.278, Avg loss: 3.292, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2704]: seen 270400 examples : 45.7 eps, Loss: 3.327, Avg loss: 3.294, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 2709]: seen 270900 examples : 45.7 eps, Loss: 3.249, Avg loss: 3.291, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2714]: seen 271400 examples : 45.7 eps, Loss: 3.329, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2719]: seen 271900 examples : 45.7 eps, Loss: 3.279, Avg loss: 3.290, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2724]: seen 272400 examples : 45.7 eps, Loss: 3.182, Avg loss: 3.289, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2729]: seen 272900 examples : 45.7 eps, Loss: 3.118, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 2734]: seen 273400 examples : 45.7 eps, Loss: 3.138, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2739]: seen 273900 examples : 45.7 eps, Loss: 3.310, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2744]: seen 274400 examples : 45.7 eps, Loss: 3.403, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2749]: seen 274900 examples : 45.7 eps, Loss: 3.356, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2754]: seen 275400 examples : 45.7 eps, Loss: 3.277, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2758]: seen 275800 examples : 45.7 eps, Loss: 3.345, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2763]: seen 276300 examples : 45.7 eps, Loss: 3.364, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2768]: seen 276800 examples : 45.7 eps, Loss: 3.279, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2773]: seen 277300 examples : 45.7 eps, Loss: 3.307, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2778]: seen 277800 examples : 45.7 eps, Loss: 3.251, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2783]: seen 278300 examples : 45.7 eps, Loss: 3.534, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.188\n",
      "    [batch 2788]: seen 278800 examples : 45.7 eps, Loss: 3.235, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2793]: seen 279300 examples : 45.7 eps, Loss: 3.383, Avg loss: 3.287, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2798]: seen 279800 examples : 45.7 eps, Loss: 3.316, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 2803]: seen 280300 examples : 45.7 eps, Loss: 3.341, Avg loss: 3.285, Best loss: 3.187, cov loss: 0.166\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:42:20\n",
      "[EPOCH 24] Complete. Avg Loss: 3.2833279251050556; Best Loss: 3.187039852142334\n",
      "[EPOCH 25] Starting training..\n",
      "    [batch 7]: seen 700 examples : 68.2 eps, Loss: 3.185, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 12]: seen 1200 examples : 57.1 eps, Loss: 3.244, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 17]: seen 1700 examples : 53.5 eps, Loss: 3.325, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 22]: seen 2200 examples : 51.8 eps, Loss: 3.263, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 27]: seen 2700 examples : 50.7 eps, Loss: 3.124, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 32]: seen 3200 examples : 50.1 eps, Loss: 3.233, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 37]: seen 3700 examples : 49.6 eps, Loss: 3.252, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 42]: seen 4200 examples : 49.3 eps, Loss: 3.306, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 47]: seen 4700 examples : 48.6 eps, Loss: 3.288, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 52]: seen 5200 examples : 48.4 eps, Loss: 3.140, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 57]: seen 5700 examples : 48.2 eps, Loss: 3.260, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 62]: seen 6200 examples : 48.1 eps, Loss: 3.206, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 67]: seen 6700 examples : 48.0 eps, Loss: 3.329, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 72]: seen 7200 examples : 47.9 eps, Loss: 3.177, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 77]: seen 7700 examples : 47.8 eps, Loss: 3.225, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 82]: seen 8200 examples : 47.7 eps, Loss: 3.227, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 87]: seen 8700 examples : 47.6 eps, Loss: 3.082, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 92]: seen 9200 examples : 47.5 eps, Loss: 3.276, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 97]: seen 9700 examples : 47.5 eps, Loss: 3.468, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.188\n",
      "    [batch 102]: seen 10200 examples : 47.5 eps, Loss: 3.177, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.136\n",
      "    [batch 107]: seen 10700 examples : 47.4 eps, Loss: 3.329, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 112]: seen 11200 examples : 47.4 eps, Loss: 3.203, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 117]: seen 11700 examples : 47.4 eps, Loss: 3.368, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.179\n",
      "    [batch 122]: seen 12200 examples : 47.3 eps, Loss: 3.188, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 127]: seen 12700 examples : 47.3 eps, Loss: 3.286, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 132]: seen 13200 examples : 47.3 eps, Loss: 3.117, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 137]: seen 13700 examples : 47.3 eps, Loss: 3.432, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.184\n",
      "    [batch 141]: seen 14100 examples : 47.0 eps, Loss: 3.279, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 146]: seen 14600 examples : 47.0 eps, Loss: 3.386, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 151]: seen 15100 examples : 47.0 eps, Loss: 3.181, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 156]: seen 15600 examples : 47.0 eps, Loss: 3.239, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 161]: seen 16100 examples : 47.0 eps, Loss: 3.222, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 166]: seen 16600 examples : 47.0 eps, Loss: 3.071, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 171]: seen 17100 examples : 47.0 eps, Loss: 3.364, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 176]: seen 17600 examples : 47.0 eps, Loss: 3.280, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 181]: seen 18100 examples : 46.9 eps, Loss: 3.130, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 186]: seen 18600 examples : 46.9 eps, Loss: 3.185, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 191]: seen 19100 examples : 46.9 eps, Loss: 3.443, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 196]: seen 19600 examples : 46.9 eps, Loss: 3.460, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 201]: seen 20100 examples : 46.9 eps, Loss: 3.135, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 206]: seen 20600 examples : 46.9 eps, Loss: 3.319, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 211]: seen 21100 examples : 46.9 eps, Loss: 3.242, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 216]: seen 21600 examples : 46.9 eps, Loss: 3.214, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 221]: seen 22100 examples : 46.9 eps, Loss: 3.251, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 225]: seen 22500 examples : 46.7 eps, Loss: 3.161, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 230]: seen 23000 examples : 46.7 eps, Loss: 3.265, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 235]: seen 23500 examples : 46.7 eps, Loss: 3.182, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 240]: seen 24000 examples : 46.7 eps, Loss: 3.269, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 245]: seen 24500 examples : 46.7 eps, Loss: 3.131, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 250]: seen 25000 examples : 46.7 eps, Loss: 3.345, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 255]: seen 25500 examples : 46.7 eps, Loss: 3.258, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 260]: seen 26000 examples : 46.7 eps, Loss: 3.259, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 265]: seen 26500 examples : 46.7 eps, Loss: 3.210, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 270]: seen 27000 examples : 46.7 eps, Loss: 3.227, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 275]: seen 27500 examples : 46.7 eps, Loss: 3.309, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 280]: seen 28000 examples : 46.7 eps, Loss: 3.169, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 285]: seen 28500 examples : 46.7 eps, Loss: 3.220, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 290]: seen 29000 examples : 46.7 eps, Loss: 3.339, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 295]: seen 29500 examples : 46.7 eps, Loss: 3.366, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 300]: seen 30000 examples : 46.7 eps, Loss: 3.229, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 305]: seen 30500 examples : 46.7 eps, Loss: 3.138, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 310]: seen 31000 examples : 46.7 eps, Loss: 3.188, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 314]: seen 31400 examples : 46.6 eps, Loss: 3.402, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 319]: seen 31900 examples : 46.6 eps, Loss: 3.252, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 324]: seen 32400 examples : 46.6 eps, Loss: 3.086, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 329]: seen 32900 examples : 46.6 eps, Loss: 3.448, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.179\n",
      "    [batch 334]: seen 33400 examples : 46.6 eps, Loss: 3.187, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 339]: seen 33900 examples : 46.6 eps, Loss: 3.423, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 344]: seen 34400 examples : 46.6 eps, Loss: 3.373, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 349]: seen 34900 examples : 46.6 eps, Loss: 3.221, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 354]: seen 35400 examples : 46.6 eps, Loss: 3.197, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 359]: seen 35900 examples : 46.6 eps, Loss: 3.107, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 364]: seen 36400 examples : 46.6 eps, Loss: 3.196, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 369]: seen 36900 examples : 46.6 eps, Loss: 3.267, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 374]: seen 37400 examples : 46.6 eps, Loss: 3.232, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 379]: seen 37900 examples : 46.6 eps, Loss: 3.306, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 384]: seen 38400 examples : 46.6 eps, Loss: 3.292, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 389]: seen 38900 examples : 46.6 eps, Loss: 3.349, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 394]: seen 39400 examples : 46.6 eps, Loss: 3.132, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 399]: seen 39900 examples : 46.6 eps, Loss: 3.221, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 403]: seen 40300 examples : 46.5 eps, Loss: 3.409, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 408]: seen 40800 examples : 46.5 eps, Loss: 3.370, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 413]: seen 41300 examples : 46.5 eps, Loss: 3.269, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 418]: seen 41800 examples : 46.5 eps, Loss: 3.048, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 423]: seen 42300 examples : 46.5 eps, Loss: 3.212, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 428]: seen 42800 examples : 46.5 eps, Loss: 3.263, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 433]: seen 43300 examples : 46.5 eps, Loss: 3.083, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 438]: seen 43800 examples : 46.5 eps, Loss: 3.239, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 443]: seen 44300 examples : 46.5 eps, Loss: 3.337, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 448]: seen 44800 examples : 46.5 eps, Loss: 3.137, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 453]: seen 45300 examples : 46.5 eps, Loss: 3.200, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 458]: seen 45800 examples : 46.5 eps, Loss: 3.361, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.180\n",
      "    [batch 463]: seen 46300 examples : 46.6 eps, Loss: 3.180, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 468]: seen 46800 examples : 46.6 eps, Loss: 3.119, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 473]: seen 47300 examples : 46.6 eps, Loss: 3.301, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 478]: seen 47800 examples : 46.6 eps, Loss: 3.209, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 483]: seen 48300 examples : 46.6 eps, Loss: 3.305, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 488]: seen 48800 examples : 46.6 eps, Loss: 3.120, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 492]: seen 49200 examples : 46.5 eps, Loss: 3.256, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 497]: seen 49700 examples : 46.5 eps, Loss: 3.095, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 501]: seen 50100 examples : 46.4 eps, Loss: 3.252, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 506]: seen 50600 examples : 46.4 eps, Loss: 3.277, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 511]: seen 51100 examples : 46.4 eps, Loss: 3.310, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 515]: seen 51500 examples : 46.3 eps, Loss: 3.258, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 520]: seen 52000 examples : 46.3 eps, Loss: 3.431, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 525]: seen 52500 examples : 46.3 eps, Loss: 3.390, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 528]: seen 52800 examples : 46.2 eps, Loss: 3.270, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 533]: seen 53300 examples : 46.2 eps, Loss: 3.273, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 538]: seen 53800 examples : 46.2 eps, Loss: 3.244, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 541]: seen 54100 examples : 46.0 eps, Loss: 3.230, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 546]: seen 54600 examples : 46.0 eps, Loss: 3.253, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.130\n",
      "    [batch 551]: seen 55100 examples : 46.0 eps, Loss: 3.233, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 554]: seen 55400 examples : 45.8 eps, Loss: 3.297, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 559]: seen 55900 examples : 45.9 eps, Loss: 3.359, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 564]: seen 56400 examples : 45.9 eps, Loss: 3.273, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 568]: seen 56800 examples : 45.8 eps, Loss: 3.269, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 573]: seen 57300 examples : 45.8 eps, Loss: 3.340, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 577]: seen 57700 examples : 45.7 eps, Loss: 3.092, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 582]: seen 58200 examples : 45.7 eps, Loss: 3.197, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 586]: seen 58600 examples : 45.7 eps, Loss: 3.413, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 591]: seen 59100 examples : 45.7 eps, Loss: 3.283, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 595]: seen 59500 examples : 45.6 eps, Loss: 3.232, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 600]: seen 60000 examples : 45.6 eps, Loss: 3.206, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 605]: seen 60500 examples : 45.6 eps, Loss: 3.131, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 609]: seen 60900 examples : 45.5 eps, Loss: 3.267, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 614]: seen 61400 examples : 45.5 eps, Loss: 3.371, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 619]: seen 61900 examples : 45.5 eps, Loss: 3.165, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 624]: seen 62400 examples : 45.5 eps, Loss: 3.262, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 629]: seen 62900 examples : 45.5 eps, Loss: 3.231, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.179\n",
      "    [batch 634]: seen 63400 examples : 45.5 eps, Loss: 3.281, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 639]: seen 63900 examples : 45.5 eps, Loss: 3.451, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 644]: seen 64400 examples : 45.6 eps, Loss: 3.473, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 649]: seen 64900 examples : 45.6 eps, Loss: 3.317, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 654]: seen 65400 examples : 45.6 eps, Loss: 3.382, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 659]: seen 65900 examples : 45.6 eps, Loss: 3.295, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 664]: seen 66400 examples : 45.6 eps, Loss: 3.355, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 669]: seen 66900 examples : 45.6 eps, Loss: 3.408, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 674]: seen 67400 examples : 45.6 eps, Loss: 3.336, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 679]: seen 67900 examples : 45.6 eps, Loss: 3.296, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 684]: seen 68400 examples : 45.6 eps, Loss: 3.200, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 689]: seen 68900 examples : 45.6 eps, Loss: 3.262, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 694]: seen 69400 examples : 45.6 eps, Loss: 2.972, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.131\n",
      "    [batch 699]: seen 69900 examples : 45.6 eps, Loss: 3.200, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 704]: seen 70400 examples : 45.6 eps, Loss: 3.202, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 709]: seen 70900 examples : 45.6 eps, Loss: 3.415, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 714]: seen 71400 examples : 45.6 eps, Loss: 3.335, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 719]: seen 71900 examples : 45.6 eps, Loss: 3.330, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 724]: seen 72400 examples : 45.7 eps, Loss: 3.093, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 729]: seen 72900 examples : 45.7 eps, Loss: 3.252, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 734]: seen 73400 examples : 45.7 eps, Loss: 3.204, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 739]: seen 73900 examples : 45.7 eps, Loss: 3.238, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 744]: seen 74400 examples : 45.7 eps, Loss: 3.352, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 748]: seen 74800 examples : 45.6 eps, Loss: 3.389, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 753]: seen 75300 examples : 45.6 eps, Loss: 3.289, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 758]: seen 75800 examples : 45.6 eps, Loss: 3.474, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 763]: seen 76300 examples : 45.7 eps, Loss: 3.206, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 768]: seen 76800 examples : 45.7 eps, Loss: 3.408, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 773]: seen 77300 examples : 45.7 eps, Loss: 3.147, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 778]: seen 77800 examples : 45.7 eps, Loss: 3.200, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 783]: seen 78300 examples : 45.7 eps, Loss: 3.214, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 788]: seen 78800 examples : 45.7 eps, Loss: 3.271, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 793]: seen 79300 examples : 45.7 eps, Loss: 3.173, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 798]: seen 79800 examples : 45.7 eps, Loss: 3.201, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 803]: seen 80300 examples : 45.7 eps, Loss: 3.295, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 808]: seen 80800 examples : 45.7 eps, Loss: 3.256, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 813]: seen 81300 examples : 45.7 eps, Loss: 3.346, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 818]: seen 81800 examples : 45.7 eps, Loss: 3.463, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 823]: seen 82300 examples : 45.7 eps, Loss: 3.172, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 828]: seen 82800 examples : 45.7 eps, Loss: 3.238, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 833]: seen 83300 examples : 45.7 eps, Loss: 3.201, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 837]: seen 83700 examples : 45.7 eps, Loss: 3.296, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 842]: seen 84200 examples : 45.7 eps, Loss: 3.328, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 847]: seen 84700 examples : 45.7 eps, Loss: 3.297, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 852]: seen 85200 examples : 45.7 eps, Loss: 3.340, Avg loss: 3.286, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 857]: seen 85700 examples : 45.7 eps, Loss: 3.320, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 862]: seen 86200 examples : 45.7 eps, Loss: 3.245, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 867]: seen 86700 examples : 45.7 eps, Loss: 3.412, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 872]: seen 87200 examples : 45.7 eps, Loss: 3.251, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 877]: seen 87700 examples : 45.8 eps, Loss: 3.146, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 882]: seen 88200 examples : 45.8 eps, Loss: 3.294, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 887]: seen 88700 examples : 45.8 eps, Loss: 3.261, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 892]: seen 89200 examples : 45.8 eps, Loss: 3.230, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 897]: seen 89700 examples : 45.8 eps, Loss: 3.276, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 902]: seen 90200 examples : 45.8 eps, Loss: 3.272, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 907]: seen 90700 examples : 45.8 eps, Loss: 3.347, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 912]: seen 91200 examples : 45.8 eps, Loss: 3.186, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 917]: seen 91700 examples : 45.8 eps, Loss: 3.242, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 922]: seen 92200 examples : 45.8 eps, Loss: 3.241, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 927]: seen 92700 examples : 45.8 eps, Loss: 3.303, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 932]: seen 93200 examples : 45.8 eps, Loss: 3.336, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 937]: seen 93700 examples : 45.8 eps, Loss: 3.332, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 942]: seen 94200 examples : 45.8 eps, Loss: 3.165, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 947]: seen 94700 examples : 45.8 eps, Loss: 3.135, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 952]: seen 95200 examples : 45.8 eps, Loss: 3.192, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 957]: seen 95700 examples : 45.8 eps, Loss: 3.352, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 962]: seen 96200 examples : 45.8 eps, Loss: 3.356, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 967]: seen 96700 examples : 45.8 eps, Loss: 3.354, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 972]: seen 97200 examples : 45.8 eps, Loss: 3.487, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 977]: seen 97700 examples : 45.8 eps, Loss: 2.959, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.135\n",
      "    [batch 982]: seen 98200 examples : 45.8 eps, Loss: 3.220, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 987]: seen 98700 examples : 45.8 eps, Loss: 3.416, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 992]: seen 99200 examples : 45.8 eps, Loss: 3.386, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 997]: seen 99700 examples : 45.8 eps, Loss: 3.189, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1002]: seen 100200 examples : 45.8 eps, Loss: 3.301, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1007]: seen 100700 examples : 45.9 eps, Loss: 3.390, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1012]: seen 101200 examples : 45.9 eps, Loss: 3.324, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1016]: seen 101600 examples : 45.8 eps, Loss: 3.008, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1021]: seen 102100 examples : 45.8 eps, Loss: 3.351, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1026]: seen 102600 examples : 45.8 eps, Loss: 3.351, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1031]: seen 103100 examples : 45.8 eps, Loss: 3.260, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1036]: seen 103600 examples : 45.8 eps, Loss: 3.322, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1041]: seen 104100 examples : 45.8 eps, Loss: 3.224, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1046]: seen 104600 examples : 45.9 eps, Loss: 3.318, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1051]: seen 105100 examples : 45.9 eps, Loss: 3.280, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1056]: seen 105600 examples : 45.9 eps, Loss: 3.334, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1061]: seen 106100 examples : 45.9 eps, Loss: 3.168, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1066]: seen 106600 examples : 45.9 eps, Loss: 3.341, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1071]: seen 107100 examples : 45.9 eps, Loss: 3.114, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1076]: seen 107600 examples : 45.9 eps, Loss: 3.390, Avg loss: 3.284, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1081]: seen 108100 examples : 45.9 eps, Loss: 3.390, Avg loss: 3.283, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1086]: seen 108600 examples : 45.9 eps, Loss: 3.050, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 1091]: seen 109100 examples : 45.9 eps, Loss: 3.288, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1096]: seen 109600 examples : 45.9 eps, Loss: 3.251, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1101]: seen 110100 examples : 45.9 eps, Loss: 3.182, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1105]: seen 110500 examples : 45.9 eps, Loss: 3.347, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1110]: seen 111000 examples : 45.9 eps, Loss: 3.436, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.183\n",
      "    [batch 1115]: seen 111500 examples : 45.9 eps, Loss: 3.467, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1120]: seen 112000 examples : 45.9 eps, Loss: 3.289, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1125]: seen 112500 examples : 45.9 eps, Loss: 3.434, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1130]: seen 113000 examples : 45.9 eps, Loss: 3.137, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1135]: seen 113500 examples : 45.9 eps, Loss: 3.339, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1140]: seen 114000 examples : 45.9 eps, Loss: 3.162, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1145]: seen 114500 examples : 45.9 eps, Loss: 3.379, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1150]: seen 115000 examples : 45.9 eps, Loss: 3.088, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1155]: seen 115500 examples : 45.9 eps, Loss: 3.226, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1160]: seen 116000 examples : 45.9 eps, Loss: 3.312, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1165]: seen 116500 examples : 45.9 eps, Loss: 3.464, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1170]: seen 117000 examples : 45.9 eps, Loss: 3.208, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1175]: seen 117500 examples : 45.9 eps, Loss: 3.349, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1180]: seen 118000 examples : 45.9 eps, Loss: 3.267, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1185]: seen 118500 examples : 45.9 eps, Loss: 3.295, Avg loss: 3.282, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1190]: seen 119000 examples : 45.9 eps, Loss: 3.217, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1195]: seen 119500 examples : 45.9 eps, Loss: 3.142, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1199]: seen 119900 examples : 45.9 eps, Loss: 3.467, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1204]: seen 120400 examples : 45.9 eps, Loss: 3.162, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1209]: seen 120900 examples : 45.9 eps, Loss: 3.341, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1214]: seen 121400 examples : 45.9 eps, Loss: 3.360, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.187\n",
      "    [batch 1219]: seen 121900 examples : 45.9 eps, Loss: 3.452, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1224]: seen 122400 examples : 45.9 eps, Loss: 3.218, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1229]: seen 122900 examples : 45.9 eps, Loss: 3.273, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 1234]: seen 123400 examples : 45.9 eps, Loss: 3.194, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1239]: seen 123900 examples : 45.9 eps, Loss: 3.232, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1244]: seen 124400 examples : 45.9 eps, Loss: 3.475, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.181\n",
      "    [batch 1249]: seen 124900 examples : 45.9 eps, Loss: 3.264, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1254]: seen 125400 examples : 45.9 eps, Loss: 3.039, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1259]: seen 125900 examples : 45.9 eps, Loss: 3.377, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 1264]: seen 126400 examples : 45.9 eps, Loss: 3.171, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 1269]: seen 126900 examples : 45.9 eps, Loss: 3.148, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1274]: seen 127400 examples : 45.9 eps, Loss: 3.187, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1279]: seen 127900 examples : 45.9 eps, Loss: 3.194, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1283]: seen 128300 examples : 45.9 eps, Loss: 3.299, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 1288]: seen 128800 examples : 45.9 eps, Loss: 3.211, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1293]: seen 129300 examples : 45.9 eps, Loss: 3.134, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1298]: seen 129800 examples : 45.9 eps, Loss: 3.285, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1303]: seen 130300 examples : 45.9 eps, Loss: 3.314, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1308]: seen 130800 examples : 45.9 eps, Loss: 3.367, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1313]: seen 131300 examples : 45.9 eps, Loss: 3.287, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1318]: seen 131800 examples : 45.9 eps, Loss: 3.296, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1323]: seen 132300 examples : 45.9 eps, Loss: 3.115, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1328]: seen 132800 examples : 45.9 eps, Loss: 3.252, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1333]: seen 133300 examples : 45.9 eps, Loss: 3.330, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 1338]: seen 133800 examples : 46.0 eps, Loss: 3.317, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1343]: seen 134300 examples : 46.0 eps, Loss: 3.323, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1348]: seen 134800 examples : 46.0 eps, Loss: 3.185, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 1352]: seen 135200 examples : 45.9 eps, Loss: 3.238, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1356]: seen 135600 examples : 45.9 eps, Loss: 3.422, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1361]: seen 136100 examples : 45.9 eps, Loss: 3.292, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1366]: seen 136600 examples : 45.9 eps, Loss: 3.258, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1370]: seen 137000 examples : 45.9 eps, Loss: 3.238, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1374]: seen 137400 examples : 45.8 eps, Loss: 3.225, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1379]: seen 137900 examples : 45.8 eps, Loss: 3.181, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1384]: seen 138400 examples : 45.8 eps, Loss: 3.199, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1388]: seen 138800 examples : 45.8 eps, Loss: 3.352, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 1393]: seen 139300 examples : 45.8 eps, Loss: 3.264, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1398]: seen 139800 examples : 45.8 eps, Loss: 2.994, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 1403]: seen 140300 examples : 45.8 eps, Loss: 3.205, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1408]: seen 140800 examples : 45.8 eps, Loss: 3.126, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1413]: seen 141300 examples : 45.8 eps, Loss: 3.356, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 1418]: seen 141800 examples : 45.8 eps, Loss: 3.354, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1423]: seen 142300 examples : 45.8 eps, Loss: 3.218, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1428]: seen 142800 examples : 45.8 eps, Loss: 3.325, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1433]: seen 143300 examples : 45.8 eps, Loss: 3.274, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1438]: seen 143800 examples : 45.8 eps, Loss: 3.183, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1443]: seen 144300 examples : 45.8 eps, Loss: 3.129, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1448]: seen 144800 examples : 45.8 eps, Loss: 3.522, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1453]: seen 145300 examples : 45.8 eps, Loss: 3.111, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1458]: seen 145800 examples : 45.8 eps, Loss: 3.289, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1463]: seen 146300 examples : 45.8 eps, Loss: 3.245, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1468]: seen 146800 examples : 45.8 eps, Loss: 3.308, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1473]: seen 147300 examples : 45.8 eps, Loss: 3.233, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1478]: seen 147800 examples : 45.8 eps, Loss: 3.129, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1483]: seen 148300 examples : 45.8 eps, Loss: 3.456, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1488]: seen 148800 examples : 45.8 eps, Loss: 3.233, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1493]: seen 149300 examples : 45.8 eps, Loss: 3.447, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1498]: seen 149800 examples : 45.8 eps, Loss: 3.261, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1503]: seen 150300 examples : 45.9 eps, Loss: 3.209, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1508]: seen 150800 examples : 45.9 eps, Loss: 3.397, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1513]: seen 151300 examples : 45.9 eps, Loss: 3.219, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 1518]: seen 151800 examples : 45.9 eps, Loss: 3.431, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1523]: seen 152300 examples : 45.9 eps, Loss: 3.202, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1528]: seen 152800 examples : 45.9 eps, Loss: 3.368, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1533]: seen 153300 examples : 45.9 eps, Loss: 3.420, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1538]: seen 153800 examples : 45.9 eps, Loss: 3.321, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 1543]: seen 154300 examples : 45.9 eps, Loss: 3.415, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1548]: seen 154800 examples : 45.9 eps, Loss: 3.307, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 1553]: seen 155300 examples : 45.9 eps, Loss: 3.235, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1558]: seen 155800 examples : 45.9 eps, Loss: 3.322, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1563]: seen 156300 examples : 45.9 eps, Loss: 3.198, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1568]: seen 156800 examples : 45.9 eps, Loss: 3.251, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1573]: seen 157300 examples : 45.9 eps, Loss: 3.254, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1578]: seen 157800 examples : 45.9 eps, Loss: 3.240, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1583]: seen 158300 examples : 45.9 eps, Loss: 3.206, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 1588]: seen 158800 examples : 45.9 eps, Loss: 3.163, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1593]: seen 159300 examples : 45.9 eps, Loss: 3.023, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1598]: seen 159800 examples : 45.9 eps, Loss: 3.187, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1603]: seen 160300 examples : 45.9 eps, Loss: 3.149, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1608]: seen 160800 examples : 45.9 eps, Loss: 3.157, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1613]: seen 161300 examples : 45.9 eps, Loss: 3.453, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1618]: seen 161800 examples : 45.9 eps, Loss: 3.039, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1623]: seen 162300 examples : 45.9 eps, Loss: 3.113, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1628]: seen 162800 examples : 45.9 eps, Loss: 3.116, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1633]: seen 163300 examples : 45.9 eps, Loss: 3.136, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1637]: seen 163700 examples : 45.9 eps, Loss: 3.484, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1642]: seen 164200 examples : 45.9 eps, Loss: 3.215, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1647]: seen 164700 examples : 45.9 eps, Loss: 3.410, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 1652]: seen 165200 examples : 45.9 eps, Loss: 3.276, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 1657]: seen 165700 examples : 45.9 eps, Loss: 3.404, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1662]: seen 166200 examples : 45.9 eps, Loss: 3.442, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 1667]: seen 166700 examples : 45.9 eps, Loss: 3.167, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1672]: seen 167200 examples : 45.9 eps, Loss: 3.083, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1677]: seen 167700 examples : 45.9 eps, Loss: 3.305, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1682]: seen 168200 examples : 45.9 eps, Loss: 3.312, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1687]: seen 168700 examples : 45.9 eps, Loss: 3.221, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1692]: seen 169200 examples : 45.9 eps, Loss: 3.216, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1697]: seen 169700 examples : 45.9 eps, Loss: 3.199, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1702]: seen 170200 examples : 45.9 eps, Loss: 3.242, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1707]: seen 170700 examples : 45.9 eps, Loss: 3.376, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1712]: seen 171200 examples : 45.9 eps, Loss: 3.153, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1717]: seen 171700 examples : 45.9 eps, Loss: 3.413, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1722]: seen 172200 examples : 45.9 eps, Loss: 3.138, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1726]: seen 172600 examples : 45.9 eps, Loss: 3.081, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1731]: seen 173100 examples : 45.9 eps, Loss: 3.396, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1736]: seen 173600 examples : 45.9 eps, Loss: 3.087, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1741]: seen 174100 examples : 45.9 eps, Loss: 3.261, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.191\n",
      "    [batch 1746]: seen 174600 examples : 45.9 eps, Loss: 3.300, Avg loss: 3.280, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1751]: seen 175100 examples : 45.9 eps, Loss: 3.300, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1756]: seen 175600 examples : 45.9 eps, Loss: 3.198, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1761]: seen 176100 examples : 45.9 eps, Loss: 3.317, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1766]: seen 176600 examples : 45.9 eps, Loss: 3.205, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1771]: seen 177100 examples : 45.9 eps, Loss: 3.300, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1776]: seen 177600 examples : 45.9 eps, Loss: 3.191, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1781]: seen 178100 examples : 45.9 eps, Loss: 3.317, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1786]: seen 178600 examples : 45.9 eps, Loss: 3.335, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1791]: seen 179100 examples : 45.9 eps, Loss: 3.326, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1796]: seen 179600 examples : 45.9 eps, Loss: 3.280, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1801]: seen 180100 examples : 45.9 eps, Loss: 3.358, Avg loss: 3.278, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1806]: seen 180600 examples : 45.9 eps, Loss: 3.414, Avg loss: 3.279, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1811]: seen 181100 examples : 45.9 eps, Loss: 3.110, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1816]: seen 181600 examples : 45.9 eps, Loss: 3.210, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1820]: seen 182000 examples : 45.9 eps, Loss: 3.221, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1825]: seen 182500 examples : 45.9 eps, Loss: 3.295, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1830]: seen 183000 examples : 45.9 eps, Loss: 3.429, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1835]: seen 183500 examples : 45.9 eps, Loss: 3.074, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1840]: seen 184000 examples : 45.9 eps, Loss: 3.227, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1845]: seen 184500 examples : 45.9 eps, Loss: 3.255, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 1850]: seen 185000 examples : 45.9 eps, Loss: 3.331, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1855]: seen 185500 examples : 45.9 eps, Loss: 3.269, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1860]: seen 186000 examples : 45.9 eps, Loss: 3.285, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1865]: seen 186500 examples : 45.9 eps, Loss: 3.402, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1870]: seen 187000 examples : 45.9 eps, Loss: 3.096, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1875]: seen 187500 examples : 45.9 eps, Loss: 3.317, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1880]: seen 188000 examples : 45.9 eps, Loss: 3.107, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1885]: seen 188500 examples : 45.9 eps, Loss: 3.266, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1890]: seen 189000 examples : 45.9 eps, Loss: 3.293, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1895]: seen 189500 examples : 45.9 eps, Loss: 3.375, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1900]: seen 190000 examples : 45.9 eps, Loss: 3.218, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1904]: seen 190400 examples : 45.9 eps, Loss: 3.219, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1909]: seen 190900 examples : 45.9 eps, Loss: 3.330, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1914]: seen 191400 examples : 45.9 eps, Loss: 3.218, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 1919]: seen 191900 examples : 45.9 eps, Loss: 3.179, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 1924]: seen 192400 examples : 45.9 eps, Loss: 3.118, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 1929]: seen 192900 examples : 45.9 eps, Loss: 3.232, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1934]: seen 193400 examples : 45.9 eps, Loss: 3.299, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1939]: seen 193900 examples : 45.9 eps, Loss: 3.303, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1944]: seen 194400 examples : 45.9 eps, Loss: 3.405, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.184\n",
      "    [batch 1949]: seen 194900 examples : 46.0 eps, Loss: 3.160, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1954]: seen 195400 examples : 46.0 eps, Loss: 3.233, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1959]: seen 195900 examples : 46.0 eps, Loss: 3.320, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 1964]: seen 196400 examples : 46.0 eps, Loss: 3.128, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1969]: seen 196900 examples : 46.0 eps, Loss: 3.419, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1974]: seen 197400 examples : 46.0 eps, Loss: 3.213, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 1979]: seen 197900 examples : 46.0 eps, Loss: 3.518, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1984]: seen 198400 examples : 46.0 eps, Loss: 3.266, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1989]: seen 198900 examples : 46.0 eps, Loss: 3.130, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1993]: seen 199300 examples : 46.0 eps, Loss: 3.130, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1998]: seen 199800 examples : 46.0 eps, Loss: 3.289, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2003]: seen 200300 examples : 46.0 eps, Loss: 3.110, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2008]: seen 200800 examples : 46.0 eps, Loss: 3.361, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2013]: seen 201300 examples : 46.0 eps, Loss: 3.318, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 2018]: seen 201800 examples : 46.0 eps, Loss: 3.172, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2023]: seen 202300 examples : 46.0 eps, Loss: 3.272, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 2028]: seen 202800 examples : 46.0 eps, Loss: 3.476, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2033]: seen 203300 examples : 46.0 eps, Loss: 3.316, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.179\n",
      "    [batch 2038]: seen 203800 examples : 46.0 eps, Loss: 3.338, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2043]: seen 204300 examples : 46.0 eps, Loss: 3.352, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2048]: seen 204800 examples : 46.0 eps, Loss: 3.178, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2053]: seen 205300 examples : 46.0 eps, Loss: 3.231, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2058]: seen 205800 examples : 46.0 eps, Loss: 3.128, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2063]: seen 206300 examples : 46.0 eps, Loss: 3.319, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2068]: seen 206800 examples : 46.0 eps, Loss: 3.095, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2073]: seen 207300 examples : 46.0 eps, Loss: 3.117, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2078]: seen 207800 examples : 46.0 eps, Loss: 3.135, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2083]: seen 208300 examples : 46.0 eps, Loss: 3.301, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2088]: seen 208800 examples : 46.0 eps, Loss: 3.552, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2093]: seen 209300 examples : 46.0 eps, Loss: 3.112, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 2098]: seen 209800 examples : 46.0 eps, Loss: 3.298, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2103]: seen 210300 examples : 46.0 eps, Loss: 2.987, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2108]: seen 210800 examples : 46.0 eps, Loss: 3.357, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2113]: seen 211300 examples : 46.0 eps, Loss: 3.210, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2118]: seen 211800 examples : 46.0 eps, Loss: 3.103, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2123]: seen 212300 examples : 46.0 eps, Loss: 3.319, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2128]: seen 212800 examples : 46.0 eps, Loss: 3.396, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2133]: seen 213300 examples : 46.0 eps, Loss: 3.369, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 2138]: seen 213800 examples : 46.0 eps, Loss: 3.442, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2143]: seen 214300 examples : 46.0 eps, Loss: 3.344, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2148]: seen 214800 examples : 46.0 eps, Loss: 3.463, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2153]: seen 215300 examples : 46.0 eps, Loss: 3.289, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2158]: seen 215800 examples : 46.0 eps, Loss: 3.016, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2163]: seen 216300 examples : 46.0 eps, Loss: 3.001, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 2168]: seen 216800 examples : 46.0 eps, Loss: 3.428, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2172]: seen 217200 examples : 46.0 eps, Loss: 2.927, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2177]: seen 217700 examples : 46.0 eps, Loss: 3.075, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2182]: seen 218200 examples : 46.0 eps, Loss: 3.187, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2187]: seen 218700 examples : 46.0 eps, Loss: 3.447, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2192]: seen 219200 examples : 46.0 eps, Loss: 3.151, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2197]: seen 219700 examples : 46.0 eps, Loss: 3.307, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2202]: seen 220200 examples : 46.0 eps, Loss: 3.473, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 2207]: seen 220700 examples : 46.0 eps, Loss: 3.221, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2212]: seen 221200 examples : 46.0 eps, Loss: 2.882, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2217]: seen 221700 examples : 46.0 eps, Loss: 3.254, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2222]: seen 222200 examples : 46.0 eps, Loss: 3.199, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2227]: seen 222700 examples : 46.0 eps, Loss: 3.427, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2232]: seen 223200 examples : 46.0 eps, Loss: 3.436, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2237]: seen 223700 examples : 46.0 eps, Loss: 3.385, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 2242]: seen 224200 examples : 46.0 eps, Loss: 3.276, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2247]: seen 224700 examples : 46.0 eps, Loss: 3.293, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2252]: seen 225200 examples : 46.0 eps, Loss: 3.261, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2257]: seen 225700 examples : 46.0 eps, Loss: 3.445, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2262]: seen 226200 examples : 46.0 eps, Loss: 3.255, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2266]: seen 226600 examples : 46.0 eps, Loss: 3.246, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2271]: seen 227100 examples : 46.0 eps, Loss: 3.066, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2276]: seen 227600 examples : 46.0 eps, Loss: 3.345, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2281]: seen 228100 examples : 46.0 eps, Loss: 3.399, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2286]: seen 228600 examples : 46.0 eps, Loss: 3.181, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2291]: seen 229100 examples : 46.0 eps, Loss: 3.336, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2296]: seen 229600 examples : 46.0 eps, Loss: 3.302, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2301]: seen 230100 examples : 46.0 eps, Loss: 3.275, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2306]: seen 230600 examples : 46.0 eps, Loss: 3.216, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2311]: seen 231100 examples : 46.0 eps, Loss: 3.376, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2316]: seen 231600 examples : 46.0 eps, Loss: 3.373, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2321]: seen 232100 examples : 46.0 eps, Loss: 3.348, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2326]: seen 232600 examples : 46.0 eps, Loss: 3.356, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2331]: seen 233100 examples : 46.0 eps, Loss: 3.307, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2336]: seen 233600 examples : 46.0 eps, Loss: 3.161, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2341]: seen 234100 examples : 46.0 eps, Loss: 3.340, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2346]: seen 234600 examples : 46.0 eps, Loss: 3.241, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2351]: seen 235100 examples : 46.0 eps, Loss: 3.458, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2356]: seen 235600 examples : 46.0 eps, Loss: 3.142, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 2361]: seen 236100 examples : 46.0 eps, Loss: 3.214, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2366]: seen 236600 examples : 46.0 eps, Loss: 3.215, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 2371]: seen 237100 examples : 46.0 eps, Loss: 3.202, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2376]: seen 237600 examples : 46.0 eps, Loss: 3.235, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2381]: seen 238100 examples : 46.0 eps, Loss: 3.438, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 2386]: seen 238600 examples : 46.0 eps, Loss: 3.158, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2391]: seen 239100 examples : 46.0 eps, Loss: 3.213, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2396]: seen 239600 examples : 46.0 eps, Loss: 3.355, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2401]: seen 240100 examples : 46.0 eps, Loss: 3.281, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2406]: seen 240600 examples : 46.0 eps, Loss: 3.236, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2411]: seen 241100 examples : 46.0 eps, Loss: 3.259, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2416]: seen 241600 examples : 46.0 eps, Loss: 3.224, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2421]: seen 242100 examples : 46.0 eps, Loss: 3.161, Avg loss: 3.264, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2426]: seen 242600 examples : 46.0 eps, Loss: 3.268, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2431]: seen 243100 examples : 46.1 eps, Loss: 3.267, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2436]: seen 243600 examples : 46.1 eps, Loss: 3.301, Avg loss: 3.273, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2440]: seen 244000 examples : 46.0 eps, Loss: 3.446, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2445]: seen 244500 examples : 46.0 eps, Loss: 3.172, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 2450]: seen 245000 examples : 46.0 eps, Loss: 3.300, Avg loss: 3.276, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2455]: seen 245500 examples : 46.0 eps, Loss: 3.347, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2460]: seen 246000 examples : 46.0 eps, Loss: 3.334, Avg loss: 3.281, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2465]: seen 246500 examples : 46.0 eps, Loss: 3.118, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2470]: seen 247000 examples : 46.0 eps, Loss: 3.332, Avg loss: 3.274, Best loss: 3.187, cov loss: 0.160\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-51460\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-51460\n",
      "    [batch 2474]: seen 247400 examples : 46.0 eps, Loss: 3.366, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2479]: seen 247900 examples : 46.0 eps, Loss: 3.477, Avg loss: 3.277, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2484]: seen 248400 examples : 46.0 eps, Loss: 3.286, Avg loss: 3.275, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 2489]: seen 248900 examples : 46.0 eps, Loss: 3.274, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2494]: seen 249400 examples : 46.0 eps, Loss: 3.225, Avg loss: 3.272, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2499]: seen 249900 examples : 46.0 eps, Loss: 3.122, Avg loss: 3.267, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2504]: seen 250400 examples : 46.0 eps, Loss: 3.380, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 2509]: seen 250900 examples : 46.0 eps, Loss: 3.230, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2514]: seen 251400 examples : 46.0 eps, Loss: 3.347, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2519]: seen 251900 examples : 46.0 eps, Loss: 3.300, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2524]: seen 252400 examples : 46.0 eps, Loss: 3.151, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2529]: seen 252900 examples : 46.0 eps, Loss: 3.242, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2534]: seen 253400 examples : 46.0 eps, Loss: 3.231, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2539]: seen 253900 examples : 46.0 eps, Loss: 3.425, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2544]: seen 254400 examples : 46.0 eps, Loss: 3.218, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2549]: seen 254900 examples : 46.0 eps, Loss: 3.424, Avg loss: 3.271, Best loss: 3.187, cov loss: 0.167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2554]: seen 255400 examples : 46.0 eps, Loss: 3.309, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2559]: seen 255900 examples : 46.0 eps, Loss: 3.378, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2564]: seen 256400 examples : 46.0 eps, Loss: 3.239, Avg loss: 3.269, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2569]: seen 256900 examples : 46.0 eps, Loss: 3.236, Avg loss: 3.263, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2574]: seen 257400 examples : 46.0 eps, Loss: 3.361, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2579]: seen 257900 examples : 46.1 eps, Loss: 3.426, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 2584]: seen 258400 examples : 46.1 eps, Loss: 3.370, Avg loss: 3.270, Best loss: 3.187, cov loss: 0.184\n",
      "    [batch 2589]: seen 258900 examples : 46.1 eps, Loss: 3.276, Avg loss: 3.268, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2594]: seen 259400 examples : 46.1 eps, Loss: 3.302, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2599]: seen 259900 examples : 46.1 eps, Loss: 3.218, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2604]: seen 260400 examples : 46.1 eps, Loss: 3.653, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.181\n",
      "    [batch 2609]: seen 260900 examples : 46.1 eps, Loss: 3.305, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2614]: seen 261400 examples : 46.1 eps, Loss: 3.423, Avg loss: 3.265, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2618]: seen 261800 examples : 46.0 eps, Loss: 3.193, Avg loss: 3.266, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2623]: seen 262300 examples : 46.0 eps, Loss: 3.325, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 2628]: seen 262800 examples : 46.0 eps, Loss: 3.228, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 2633]: seen 263300 examples : 46.1 eps, Loss: 3.145, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2638]: seen 263800 examples : 46.1 eps, Loss: 3.363, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2643]: seen 264300 examples : 46.1 eps, Loss: 3.212, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2648]: seen 264800 examples : 46.1 eps, Loss: 3.263, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2653]: seen 265300 examples : 46.1 eps, Loss: 3.222, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2658]: seen 265800 examples : 46.1 eps, Loss: 3.231, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2663]: seen 266300 examples : 46.1 eps, Loss: 3.230, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 2668]: seen 266800 examples : 46.1 eps, Loss: 3.212, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2673]: seen 267300 examples : 46.1 eps, Loss: 3.264, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2678]: seen 267800 examples : 46.1 eps, Loss: 3.247, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2683]: seen 268300 examples : 46.1 eps, Loss: 3.368, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2688]: seen 268800 examples : 46.1 eps, Loss: 3.420, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 2693]: seen 269300 examples : 46.1 eps, Loss: 3.120, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2698]: seen 269800 examples : 46.1 eps, Loss: 3.198, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2703]: seen 270300 examples : 46.1 eps, Loss: 3.241, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2708]: seen 270800 examples : 46.1 eps, Loss: 3.278, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2712]: seen 271200 examples : 46.1 eps, Loss: 3.045, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2716]: seen 271600 examples : 46.0 eps, Loss: 3.197, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2721]: seen 272100 examples : 46.0 eps, Loss: 3.083, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2726]: seen 272600 examples : 46.0 eps, Loss: 3.073, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 2731]: seen 273100 examples : 46.0 eps, Loss: 3.293, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2736]: seen 273600 examples : 46.0 eps, Loss: 3.118, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2741]: seen 274100 examples : 46.0 eps, Loss: 3.301, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2746]: seen 274600 examples : 46.0 eps, Loss: 3.389, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2751]: seen 275100 examples : 46.0 eps, Loss: 3.295, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 2756]: seen 275600 examples : 46.0 eps, Loss: 3.159, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 2761]: seen 276100 examples : 46.1 eps, Loss: 3.308, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2766]: seen 276600 examples : 46.1 eps, Loss: 3.329, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2771]: seen 277100 examples : 46.1 eps, Loss: 3.206, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2776]: seen 277600 examples : 46.1 eps, Loss: 3.353, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2781]: seen 278100 examples : 46.1 eps, Loss: 3.135, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2786]: seen 278600 examples : 46.1 eps, Loss: 3.244, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 2791]: seen 279100 examples : 46.1 eps, Loss: 3.066, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2795]: seen 279500 examples : 46.0 eps, Loss: 3.365, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2800]: seen 280000 examples : 46.0 eps, Loss: 3.137, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2805]: seen 280500 examples : 46.0 eps, Loss: 3.216, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.160\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:41:37\n",
      "[EPOCH 25] Complete. Avg Loss: 3.2523632689787076; Best Loss: 3.187039852142334\n",
      "[EPOCH 26] Starting training..\n",
      "    [batch 7]: seen 700 examples : 68.2 eps, Loss: 3.269, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 12]: seen 1200 examples : 57.1 eps, Loss: 3.223, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 17]: seen 1700 examples : 53.6 eps, Loss: 3.010, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 22]: seen 2200 examples : 51.9 eps, Loss: 3.137, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 27]: seen 2700 examples : 50.8 eps, Loss: 3.206, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 32]: seen 3200 examples : 50.2 eps, Loss: 3.227, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 37]: seen 3700 examples : 49.7 eps, Loss: 3.262, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 42]: seen 4200 examples : 49.3 eps, Loss: 3.268, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 47]: seen 4700 examples : 49.0 eps, Loss: 3.194, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 52]: seen 5200 examples : 48.8 eps, Loss: 3.227, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 57]: seen 5700 examples : 48.5 eps, Loss: 3.295, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 62]: seen 6200 examples : 48.3 eps, Loss: 3.227, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 67]: seen 6700 examples : 48.2 eps, Loss: 3.243, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 72]: seen 7200 examples : 48.1 eps, Loss: 3.315, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 77]: seen 7700 examples : 48.0 eps, Loss: 3.318, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 81]: seen 8100 examples : 47.6 eps, Loss: 3.210, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 86]: seen 8600 examples : 47.5 eps, Loss: 3.367, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 91]: seen 9100 examples : 47.5 eps, Loss: 3.263, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 96]: seen 9600 examples : 47.5 eps, Loss: 3.424, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 101]: seen 10100 examples : 47.4 eps, Loss: 3.271, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 106]: seen 10600 examples : 47.4 eps, Loss: 3.251, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 111]: seen 11100 examples : 47.4 eps, Loss: 3.146, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 116]: seen 11600 examples : 47.3 eps, Loss: 3.277, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 121]: seen 12100 examples : 47.3 eps, Loss: 3.300, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 126]: seen 12600 examples : 47.3 eps, Loss: 3.292, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 131]: seen 13100 examples : 47.3 eps, Loss: 3.419, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 136]: seen 13600 examples : 47.2 eps, Loss: 3.170, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 141]: seen 14100 examples : 47.2 eps, Loss: 3.241, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.133\n",
      "    [batch 146]: seen 14600 examples : 47.2 eps, Loss: 3.149, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 151]: seen 15100 examples : 47.2 eps, Loss: 2.888, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 156]: seen 15600 examples : 47.2 eps, Loss: 3.107, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 161]: seen 16100 examples : 47.2 eps, Loss: 3.329, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 166]: seen 16600 examples : 47.1 eps, Loss: 3.082, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.135\n",
      "    [batch 170]: seen 17000 examples : 46.9 eps, Loss: 3.432, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 175]: seen 17500 examples : 46.9 eps, Loss: 3.255, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 180]: seen 18000 examples : 46.9 eps, Loss: 3.433, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 185]: seen 18500 examples : 46.9 eps, Loss: 3.179, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 190]: seen 19000 examples : 46.9 eps, Loss: 3.219, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 195]: seen 19500 examples : 46.9 eps, Loss: 3.017, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 200]: seen 20000 examples : 46.9 eps, Loss: 3.203, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 205]: seen 20500 examples : 46.9 eps, Loss: 3.272, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 210]: seen 21000 examples : 46.9 eps, Loss: 3.287, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 215]: seen 21500 examples : 46.9 eps, Loss: 3.299, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 220]: seen 22000 examples : 46.9 eps, Loss: 3.431, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 225]: seen 22500 examples : 46.9 eps, Loss: 3.224, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 230]: seen 23000 examples : 46.9 eps, Loss: 3.234, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 235]: seen 23500 examples : 46.9 eps, Loss: 3.335, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 240]: seen 24000 examples : 46.8 eps, Loss: 3.330, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 245]: seen 24500 examples : 46.8 eps, Loss: 3.280, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 250]: seen 25000 examples : 46.8 eps, Loss: 3.280, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 255]: seen 25500 examples : 46.8 eps, Loss: 3.123, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 260]: seen 26000 examples : 46.8 eps, Loss: 3.215, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 264]: seen 26400 examples : 46.7 eps, Loss: 3.261, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 269]: seen 26900 examples : 46.7 eps, Loss: 3.349, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 274]: seen 27400 examples : 46.7 eps, Loss: 3.120, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 279]: seen 27900 examples : 46.7 eps, Loss: 3.332, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 284]: seen 28400 examples : 46.7 eps, Loss: 3.246, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 289]: seen 28900 examples : 46.7 eps, Loss: 3.160, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 294]: seen 29400 examples : 46.7 eps, Loss: 3.058, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 299]: seen 29900 examples : 46.7 eps, Loss: 3.345, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 304]: seen 30400 examples : 46.7 eps, Loss: 3.197, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 309]: seen 30900 examples : 46.7 eps, Loss: 3.222, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 314]: seen 31400 examples : 46.7 eps, Loss: 3.241, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 319]: seen 31900 examples : 46.7 eps, Loss: 3.215, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.184\n",
      "    [batch 324]: seen 32400 examples : 46.7 eps, Loss: 3.080, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 329]: seen 32900 examples : 46.7 eps, Loss: 3.280, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 334]: seen 33400 examples : 46.7 eps, Loss: 3.119, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 339]: seen 33900 examples : 46.7 eps, Loss: 3.176, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 344]: seen 34400 examples : 46.7 eps, Loss: 3.283, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 348]: seen 34800 examples : 46.6 eps, Loss: 3.202, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 353]: seen 35300 examples : 46.6 eps, Loss: 3.458, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 358]: seen 35800 examples : 46.6 eps, Loss: 3.167, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 363]: seen 36300 examples : 46.6 eps, Loss: 3.447, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 368]: seen 36800 examples : 46.6 eps, Loss: 3.142, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 373]: seen 37300 examples : 46.6 eps, Loss: 3.164, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 378]: seen 37800 examples : 46.6 eps, Loss: 3.149, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 383]: seen 38300 examples : 46.6 eps, Loss: 3.288, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 388]: seen 38800 examples : 46.6 eps, Loss: 3.252, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 393]: seen 39300 examples : 46.6 eps, Loss: 3.294, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 398]: seen 39800 examples : 46.6 eps, Loss: 3.293, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 403]: seen 40300 examples : 46.6 eps, Loss: 3.280, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 408]: seen 40800 examples : 46.6 eps, Loss: 3.404, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 413]: seen 41300 examples : 46.6 eps, Loss: 3.430, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 418]: seen 41800 examples : 46.6 eps, Loss: 3.346, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 423]: seen 42300 examples : 46.6 eps, Loss: 3.202, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 428]: seen 42800 examples : 46.6 eps, Loss: 3.220, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 433]: seen 43300 examples : 46.6 eps, Loss: 3.203, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 437]: seen 43700 examples : 46.5 eps, Loss: 3.301, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 442]: seen 44200 examples : 46.5 eps, Loss: 3.216, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 447]: seen 44700 examples : 46.5 eps, Loss: 3.371, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 452]: seen 45200 examples : 46.5 eps, Loss: 3.379, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 457]: seen 45700 examples : 46.6 eps, Loss: 3.320, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 462]: seen 46200 examples : 46.6 eps, Loss: 3.208, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 467]: seen 46700 examples : 46.6 eps, Loss: 3.317, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 472]: seen 47200 examples : 46.6 eps, Loss: 3.150, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 477]: seen 47700 examples : 46.6 eps, Loss: 3.307, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 482]: seen 48200 examples : 46.6 eps, Loss: 3.241, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 487]: seen 48700 examples : 46.6 eps, Loss: 3.501, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 492]: seen 49200 examples : 46.6 eps, Loss: 3.285, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 497]: seen 49700 examples : 46.6 eps, Loss: 3.125, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 502]: seen 50200 examples : 46.6 eps, Loss: 3.212, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.179\n",
      "    [batch 507]: seen 50700 examples : 46.6 eps, Loss: 3.213, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 512]: seen 51200 examples : 46.6 eps, Loss: 3.088, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 517]: seen 51700 examples : 46.6 eps, Loss: 3.406, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 522]: seen 52200 examples : 46.6 eps, Loss: 3.151, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 527]: seen 52700 examples : 46.6 eps, Loss: 3.235, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 531]: seen 53100 examples : 46.5 eps, Loss: 3.247, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 536]: seen 53600 examples : 46.5 eps, Loss: 3.248, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 541]: seen 54100 examples : 46.5 eps, Loss: 3.360, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 546]: seen 54600 examples : 46.5 eps, Loss: 3.244, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 551]: seen 55100 examples : 46.5 eps, Loss: 3.150, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 556]: seen 55600 examples : 46.5 eps, Loss: 3.292, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 561]: seen 56100 examples : 46.5 eps, Loss: 3.328, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 566]: seen 56600 examples : 46.5 eps, Loss: 3.225, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 571]: seen 57100 examples : 46.5 eps, Loss: 3.286, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 576]: seen 57600 examples : 46.5 eps, Loss: 3.220, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 581]: seen 58100 examples : 46.5 eps, Loss: 3.092, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 586]: seen 58600 examples : 46.5 eps, Loss: 3.324, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 591]: seen 59100 examples : 46.5 eps, Loss: 3.297, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 596]: seen 59600 examples : 46.5 eps, Loss: 3.173, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 601]: seen 60100 examples : 46.5 eps, Loss: 3.101, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 606]: seen 60600 examples : 46.5 eps, Loss: 3.427, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 611]: seen 61100 examples : 46.5 eps, Loss: 3.223, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 615]: seen 61500 examples : 46.5 eps, Loss: 3.266, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 620]: seen 62000 examples : 46.5 eps, Loss: 3.329, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 625]: seen 62500 examples : 46.5 eps, Loss: 3.398, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 630]: seen 63000 examples : 46.5 eps, Loss: 3.028, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 635]: seen 63500 examples : 46.5 eps, Loss: 3.423, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 640]: seen 64000 examples : 46.5 eps, Loss: 3.388, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 645]: seen 64500 examples : 46.5 eps, Loss: 3.264, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 650]: seen 65000 examples : 46.5 eps, Loss: 3.314, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 655]: seen 65500 examples : 46.5 eps, Loss: 3.077, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 660]: seen 66000 examples : 46.5 eps, Loss: 3.343, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 665]: seen 66500 examples : 46.5 eps, Loss: 3.488, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 670]: seen 67000 examples : 46.5 eps, Loss: 3.243, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 675]: seen 67500 examples : 46.5 eps, Loss: 3.204, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 680]: seen 68000 examples : 46.5 eps, Loss: 3.256, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 685]: seen 68500 examples : 46.5 eps, Loss: 3.283, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 690]: seen 69000 examples : 46.5 eps, Loss: 3.304, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 695]: seen 69500 examples : 46.5 eps, Loss: 3.178, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 700]: seen 70000 examples : 46.5 eps, Loss: 3.248, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 704]: seen 70400 examples : 46.5 eps, Loss: 3.066, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 709]: seen 70900 examples : 46.5 eps, Loss: 3.237, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 714]: seen 71400 examples : 46.5 eps, Loss: 3.230, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 719]: seen 71900 examples : 46.5 eps, Loss: 3.208, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 724]: seen 72400 examples : 46.5 eps, Loss: 3.359, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 729]: seen 72900 examples : 46.5 eps, Loss: 3.205, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 734]: seen 73400 examples : 46.5 eps, Loss: 3.350, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.185\n",
      "    [batch 739]: seen 73900 examples : 46.5 eps, Loss: 3.048, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 744]: seen 74400 examples : 46.5 eps, Loss: 2.939, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 749]: seen 74900 examples : 46.5 eps, Loss: 3.201, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 754]: seen 75400 examples : 46.5 eps, Loss: 3.076, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 759]: seen 75900 examples : 46.5 eps, Loss: 3.202, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 764]: seen 76400 examples : 46.5 eps, Loss: 3.260, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 769]: seen 76900 examples : 46.5 eps, Loss: 3.335, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 774]: seen 77400 examples : 46.5 eps, Loss: 3.255, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 779]: seen 77900 examples : 46.5 eps, Loss: 3.230, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 784]: seen 78400 examples : 46.5 eps, Loss: 3.253, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 789]: seen 78900 examples : 46.5 eps, Loss: 3.259, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 793]: seen 79300 examples : 46.4 eps, Loss: 3.116, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 798]: seen 79800 examples : 46.4 eps, Loss: 3.118, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 803]: seen 80300 examples : 46.5 eps, Loss: 3.157, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 808]: seen 80800 examples : 46.5 eps, Loss: 3.386, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 813]: seen 81300 examples : 46.5 eps, Loss: 3.497, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.139\n",
      "    [batch 818]: seen 81800 examples : 46.5 eps, Loss: 3.096, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 823]: seen 82300 examples : 46.5 eps, Loss: 3.474, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.163\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-52618\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-52618\n",
      "    [batch 827]: seen 82700 examples : 46.4 eps, Loss: 3.137, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 832]: seen 83200 examples : 46.4 eps, Loss: 3.143, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 837]: seen 83700 examples : 46.4 eps, Loss: 3.190, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 842]: seen 84200 examples : 46.4 eps, Loss: 3.292, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 847]: seen 84700 examples : 46.4 eps, Loss: 3.351, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 852]: seen 85200 examples : 46.4 eps, Loss: 3.189, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 857]: seen 85700 examples : 46.4 eps, Loss: 3.198, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.136\n",
      "    [batch 862]: seen 86200 examples : 46.4 eps, Loss: 3.317, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 867]: seen 86700 examples : 46.4 eps, Loss: 3.418, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 872]: seen 87200 examples : 46.4 eps, Loss: 3.409, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 877]: seen 87700 examples : 46.4 eps, Loss: 3.159, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 882]: seen 88200 examples : 46.4 eps, Loss: 3.257, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 886]: seen 88600 examples : 46.4 eps, Loss: 2.983, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 891]: seen 89100 examples : 46.4 eps, Loss: 3.103, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 896]: seen 89600 examples : 46.4 eps, Loss: 3.506, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 901]: seen 90100 examples : 46.4 eps, Loss: 3.241, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 906]: seen 90600 examples : 46.4 eps, Loss: 3.365, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 911]: seen 91100 examples : 46.4 eps, Loss: 3.402, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 916]: seen 91600 examples : 46.4 eps, Loss: 3.224, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 921]: seen 92100 examples : 46.4 eps, Loss: 3.285, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 926]: seen 92600 examples : 46.4 eps, Loss: 3.212, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 931]: seen 93100 examples : 46.4 eps, Loss: 3.096, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 936]: seen 93600 examples : 46.4 eps, Loss: 3.330, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 941]: seen 94100 examples : 46.4 eps, Loss: 3.345, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.185\n",
      "    [batch 946]: seen 94600 examples : 46.4 eps, Loss: 3.373, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 951]: seen 95100 examples : 46.4 eps, Loss: 3.298, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 956]: seen 95600 examples : 46.4 eps, Loss: 3.150, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 961]: seen 96100 examples : 46.4 eps, Loss: 3.350, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 966]: seen 96600 examples : 46.4 eps, Loss: 3.086, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 970]: seen 97000 examples : 46.4 eps, Loss: 3.351, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 975]: seen 97500 examples : 46.4 eps, Loss: 3.281, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 980]: seen 98000 examples : 46.4 eps, Loss: 3.443, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 985]: seen 98500 examples : 46.4 eps, Loss: 3.252, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 990]: seen 99000 examples : 46.4 eps, Loss: 3.083, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 995]: seen 99500 examples : 46.4 eps, Loss: 3.293, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1000]: seen 100000 examples : 46.4 eps, Loss: 3.313, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1005]: seen 100500 examples : 46.4 eps, Loss: 3.088, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 1010]: seen 101000 examples : 46.4 eps, Loss: 3.228, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1015]: seen 101500 examples : 46.4 eps, Loss: 3.154, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1020]: seen 102000 examples : 46.4 eps, Loss: 3.196, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1025]: seen 102500 examples : 46.4 eps, Loss: 3.193, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1030]: seen 103000 examples : 46.4 eps, Loss: 3.179, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1035]: seen 103500 examples : 46.4 eps, Loss: 3.264, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1040]: seen 104000 examples : 46.4 eps, Loss: 3.345, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1045]: seen 104500 examples : 46.4 eps, Loss: 3.201, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1050]: seen 105000 examples : 46.4 eps, Loss: 3.217, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1055]: seen 105500 examples : 46.4 eps, Loss: 3.363, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1059]: seen 105900 examples : 46.4 eps, Loss: 2.993, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1064]: seen 106400 examples : 46.4 eps, Loss: 3.097, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 1069]: seen 106900 examples : 46.4 eps, Loss: 3.207, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1074]: seen 107400 examples : 46.4 eps, Loss: 3.292, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1079]: seen 107900 examples : 46.4 eps, Loss: 3.056, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1084]: seen 108400 examples : 46.4 eps, Loss: 3.284, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1089]: seen 108900 examples : 46.4 eps, Loss: 3.163, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 1094]: seen 109400 examples : 46.4 eps, Loss: 3.306, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 1099]: seen 109900 examples : 46.4 eps, Loss: 3.132, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1104]: seen 110400 examples : 46.4 eps, Loss: 3.147, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1109]: seen 110900 examples : 46.4 eps, Loss: 3.282, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1114]: seen 111400 examples : 46.4 eps, Loss: 3.440, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1119]: seen 111900 examples : 46.4 eps, Loss: 3.115, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1124]: seen 112400 examples : 46.4 eps, Loss: 3.365, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.180\n",
      "    [batch 1129]: seen 112900 examples : 46.4 eps, Loss: 3.167, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1134]: seen 113400 examples : 46.4 eps, Loss: 3.417, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 1139]: seen 113900 examples : 46.4 eps, Loss: 3.351, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 1144]: seen 114400 examples : 46.4 eps, Loss: 3.227, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1149]: seen 114900 examples : 46.4 eps, Loss: 3.277, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 1154]: seen 115400 examples : 46.4 eps, Loss: 3.125, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1159]: seen 115900 examples : 46.4 eps, Loss: 3.194, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1164]: seen 116400 examples : 46.4 eps, Loss: 3.258, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1169]: seen 116900 examples : 46.4 eps, Loss: 3.287, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1174]: seen 117400 examples : 46.4 eps, Loss: 3.192, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 1179]: seen 117900 examples : 46.4 eps, Loss: 3.144, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 1184]: seen 118400 examples : 46.4 eps, Loss: 3.097, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1189]: seen 118900 examples : 46.4 eps, Loss: 2.928, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1194]: seen 119400 examples : 46.4 eps, Loss: 3.230, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1199]: seen 119900 examples : 46.4 eps, Loss: 3.099, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1204]: seen 120400 examples : 46.4 eps, Loss: 3.110, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.137\n",
      "    [batch 1209]: seen 120900 examples : 46.4 eps, Loss: 3.309, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1214]: seen 121400 examples : 46.4 eps, Loss: 3.200, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1219]: seen 121900 examples : 46.4 eps, Loss: 3.202, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1224]: seen 122400 examples : 46.4 eps, Loss: 3.408, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.180\n",
      "    [batch 1229]: seen 122900 examples : 46.4 eps, Loss: 2.967, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 1234]: seen 123400 examples : 46.4 eps, Loss: 3.194, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.132\n",
      "    [batch 1238]: seen 123800 examples : 46.4 eps, Loss: 3.211, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1243]: seen 124300 examples : 46.4 eps, Loss: 3.214, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1248]: seen 124800 examples : 46.4 eps, Loss: 3.119, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1253]: seen 125300 examples : 46.4 eps, Loss: 3.256, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1258]: seen 125800 examples : 46.4 eps, Loss: 3.158, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1263]: seen 126300 examples : 46.4 eps, Loss: 3.239, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1268]: seen 126800 examples : 46.4 eps, Loss: 3.130, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1273]: seen 127300 examples : 46.4 eps, Loss: 3.261, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1278]: seen 127800 examples : 46.4 eps, Loss: 3.192, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1283]: seen 128300 examples : 46.4 eps, Loss: 3.262, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1288]: seen 128800 examples : 46.4 eps, Loss: 3.277, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1293]: seen 129300 examples : 46.4 eps, Loss: 3.216, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1298]: seen 129800 examples : 46.4 eps, Loss: 3.396, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1303]: seen 130300 examples : 46.4 eps, Loss: 3.246, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1308]: seen 130800 examples : 46.4 eps, Loss: 3.433, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1313]: seen 131300 examples : 46.4 eps, Loss: 3.104, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1318]: seen 131800 examples : 46.4 eps, Loss: 3.311, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1323]: seen 132300 examples : 46.4 eps, Loss: 3.090, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 1327]: seen 132700 examples : 46.4 eps, Loss: 3.182, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1332]: seen 133200 examples : 46.4 eps, Loss: 3.327, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1337]: seen 133700 examples : 46.4 eps, Loss: 3.274, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1342]: seen 134200 examples : 46.4 eps, Loss: 3.361, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1347]: seen 134700 examples : 46.4 eps, Loss: 3.166, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1352]: seen 135200 examples : 46.4 eps, Loss: 3.302, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 1357]: seen 135700 examples : 46.4 eps, Loss: 3.263, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1362]: seen 136200 examples : 46.4 eps, Loss: 3.186, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 1367]: seen 136700 examples : 46.4 eps, Loss: 3.049, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1372]: seen 137200 examples : 46.4 eps, Loss: 3.120, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1377]: seen 137700 examples : 46.4 eps, Loss: 3.267, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1382]: seen 138200 examples : 46.4 eps, Loss: 3.078, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.139\n",
      "    [batch 1387]: seen 138700 examples : 46.4 eps, Loss: 3.327, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1392]: seen 139200 examples : 46.4 eps, Loss: 3.232, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1397]: seen 139700 examples : 46.4 eps, Loss: 3.185, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1402]: seen 140200 examples : 46.4 eps, Loss: 3.239, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1407]: seen 140700 examples : 46.4 eps, Loss: 3.274, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.136\n",
      "    [batch 1412]: seen 141200 examples : 46.4 eps, Loss: 3.323, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1416]: seen 141600 examples : 46.4 eps, Loss: 3.280, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 1421]: seen 142100 examples : 46.4 eps, Loss: 3.192, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1426]: seen 142600 examples : 46.4 eps, Loss: 3.286, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1431]: seen 143100 examples : 46.4 eps, Loss: 3.066, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1436]: seen 143600 examples : 46.4 eps, Loss: 3.302, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 1441]: seen 144100 examples : 46.4 eps, Loss: 3.291, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1446]: seen 144600 examples : 46.4 eps, Loss: 3.401, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.183\n",
      "    [batch 1451]: seen 145100 examples : 46.4 eps, Loss: 3.245, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1456]: seen 145600 examples : 46.4 eps, Loss: 3.281, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 1461]: seen 146100 examples : 46.4 eps, Loss: 3.345, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1466]: seen 146600 examples : 46.4 eps, Loss: 3.194, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1471]: seen 147100 examples : 46.4 eps, Loss: 3.216, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1476]: seen 147600 examples : 46.4 eps, Loss: 3.401, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1481]: seen 148100 examples : 46.4 eps, Loss: 3.439, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1486]: seen 148600 examples : 46.4 eps, Loss: 3.055, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1491]: seen 149100 examples : 46.4 eps, Loss: 3.304, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1496]: seen 149600 examples : 46.4 eps, Loss: 3.248, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1501]: seen 150100 examples : 46.4 eps, Loss: 3.196, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1506]: seen 150600 examples : 46.4 eps, Loss: 3.277, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 1511]: seen 151100 examples : 46.4 eps, Loss: 3.239, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1516]: seen 151600 examples : 46.4 eps, Loss: 3.372, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1521]: seen 152100 examples : 46.4 eps, Loss: 3.148, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1526]: seen 152600 examples : 46.4 eps, Loss: 3.174, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1531]: seen 153100 examples : 46.4 eps, Loss: 3.222, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1536]: seen 153600 examples : 46.4 eps, Loss: 3.372, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1541]: seen 154100 examples : 46.4 eps, Loss: 3.232, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1546]: seen 154600 examples : 46.4 eps, Loss: 3.300, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1551]: seen 155100 examples : 46.4 eps, Loss: 3.142, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1556]: seen 155600 examples : 46.4 eps, Loss: 3.385, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1561]: seen 156100 examples : 46.4 eps, Loss: 3.317, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1566]: seen 156600 examples : 46.4 eps, Loss: 3.277, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1571]: seen 157100 examples : 46.4 eps, Loss: 3.240, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 1576]: seen 157600 examples : 46.4 eps, Loss: 3.126, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1581]: seen 158100 examples : 46.4 eps, Loss: 3.221, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1586]: seen 158600 examples : 46.4 eps, Loss: 3.193, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1591]: seen 159100 examples : 46.4 eps, Loss: 3.355, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1596]: seen 159600 examples : 46.4 eps, Loss: 3.354, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1601]: seen 160100 examples : 46.4 eps, Loss: 3.215, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1606]: seen 160600 examples : 46.4 eps, Loss: 3.252, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1611]: seen 161100 examples : 46.4 eps, Loss: 3.057, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1616]: seen 161600 examples : 46.4 eps, Loss: 3.105, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1621]: seen 162100 examples : 46.4 eps, Loss: 3.068, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1626]: seen 162600 examples : 46.4 eps, Loss: 3.072, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1631]: seen 163100 examples : 46.4 eps, Loss: 3.237, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1636]: seen 163600 examples : 46.4 eps, Loss: 3.383, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1641]: seen 164100 examples : 46.4 eps, Loss: 3.218, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1646]: seen 164600 examples : 46.4 eps, Loss: 3.304, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1651]: seen 165100 examples : 46.4 eps, Loss: 3.234, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1656]: seen 165600 examples : 46.4 eps, Loss: 3.219, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1661]: seen 166100 examples : 46.4 eps, Loss: 3.378, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1666]: seen 166600 examples : 46.4 eps, Loss: 3.426, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 1671]: seen 167100 examples : 46.4 eps, Loss: 3.097, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 1676]: seen 167600 examples : 46.4 eps, Loss: 3.318, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1681]: seen 168100 examples : 46.4 eps, Loss: 3.228, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1685]: seen 168500 examples : 46.4 eps, Loss: 3.289, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1690]: seen 169000 examples : 46.4 eps, Loss: 3.154, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 1695]: seen 169500 examples : 46.4 eps, Loss: 3.192, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1700]: seen 170000 examples : 46.4 eps, Loss: 3.176, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1705]: seen 170500 examples : 46.4 eps, Loss: 3.234, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 1710]: seen 171000 examples : 46.4 eps, Loss: 3.327, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1715]: seen 171500 examples : 46.4 eps, Loss: 3.198, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 1720]: seen 172000 examples : 46.4 eps, Loss: 3.218, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 1725]: seen 172500 examples : 46.4 eps, Loss: 3.247, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1730]: seen 173000 examples : 46.4 eps, Loss: 3.158, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1735]: seen 173500 examples : 46.4 eps, Loss: 3.211, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1740]: seen 174000 examples : 46.4 eps, Loss: 3.401, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1745]: seen 174500 examples : 46.4 eps, Loss: 3.255, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.184\n",
      "    [batch 1750]: seen 175000 examples : 46.4 eps, Loss: 3.273, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 1755]: seen 175500 examples : 46.4 eps, Loss: 3.247, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1760]: seen 176000 examples : 46.4 eps, Loss: 3.338, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1765]: seen 176500 examples : 46.4 eps, Loss: 3.331, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1770]: seen 177000 examples : 46.4 eps, Loss: 3.243, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1774]: seen 177400 examples : 46.4 eps, Loss: 3.207, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 1779]: seen 177900 examples : 46.4 eps, Loss: 3.354, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1784]: seen 178400 examples : 46.4 eps, Loss: 3.113, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1789]: seen 178900 examples : 46.4 eps, Loss: 3.279, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1794]: seen 179400 examples : 46.4 eps, Loss: 3.124, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1799]: seen 179900 examples : 46.4 eps, Loss: 3.288, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1804]: seen 180400 examples : 46.4 eps, Loss: 3.181, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1809]: seen 180900 examples : 46.4 eps, Loss: 3.243, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1814]: seen 181400 examples : 46.4 eps, Loss: 3.129, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1819]: seen 181900 examples : 46.4 eps, Loss: 3.199, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1824]: seen 182400 examples : 46.4 eps, Loss: 2.942, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1829]: seen 182900 examples : 46.4 eps, Loss: 3.242, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1834]: seen 183400 examples : 46.4 eps, Loss: 3.181, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1839]: seen 183900 examples : 46.4 eps, Loss: 3.167, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1844]: seen 184400 examples : 46.4 eps, Loss: 3.521, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 1849]: seen 184900 examples : 46.4 eps, Loss: 3.229, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1854]: seen 185400 examples : 46.4 eps, Loss: 3.301, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1859]: seen 185900 examples : 46.4 eps, Loss: 3.092, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1863]: seen 186300 examples : 46.4 eps, Loss: 3.038, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1868]: seen 186800 examples : 46.4 eps, Loss: 3.197, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1873]: seen 187300 examples : 46.4 eps, Loss: 3.061, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1878]: seen 187800 examples : 46.4 eps, Loss: 3.258, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1883]: seen 188300 examples : 46.4 eps, Loss: 3.384, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1888]: seen 188800 examples : 46.4 eps, Loss: 3.291, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 1893]: seen 189300 examples : 46.4 eps, Loss: 3.267, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1898]: seen 189800 examples : 46.4 eps, Loss: 3.249, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1903]: seen 190300 examples : 46.4 eps, Loss: 3.293, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1908]: seen 190800 examples : 46.4 eps, Loss: 3.132, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1913]: seen 191300 examples : 46.4 eps, Loss: 3.548, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 1918]: seen 191800 examples : 46.4 eps, Loss: 3.157, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1923]: seen 192300 examples : 46.4 eps, Loss: 2.984, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1928]: seen 192800 examples : 46.4 eps, Loss: 3.162, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1933]: seen 193300 examples : 46.4 eps, Loss: 3.115, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1938]: seen 193800 examples : 46.4 eps, Loss: 3.191, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1943]: seen 194300 examples : 46.4 eps, Loss: 3.195, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1948]: seen 194800 examples : 46.4 eps, Loss: 3.151, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 1952]: seen 195200 examples : 46.4 eps, Loss: 3.295, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1957]: seen 195700 examples : 46.4 eps, Loss: 3.120, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 1962]: seen 196200 examples : 46.4 eps, Loss: 3.204, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1967]: seen 196700 examples : 46.4 eps, Loss: 3.070, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 1972]: seen 197200 examples : 46.4 eps, Loss: 3.215, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1977]: seen 197700 examples : 46.4 eps, Loss: 3.412, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1982]: seen 198200 examples : 46.4 eps, Loss: 3.187, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1987]: seen 198700 examples : 46.4 eps, Loss: 3.126, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1992]: seen 199200 examples : 46.4 eps, Loss: 3.227, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1997]: seen 199700 examples : 46.4 eps, Loss: 3.222, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2002]: seen 200200 examples : 46.4 eps, Loss: 3.133, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2007]: seen 200700 examples : 46.4 eps, Loss: 3.308, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 2012]: seen 201200 examples : 46.4 eps, Loss: 3.350, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2017]: seen 201700 examples : 46.4 eps, Loss: 3.144, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2022]: seen 202200 examples : 46.4 eps, Loss: 3.149, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 2027]: seen 202700 examples : 46.4 eps, Loss: 3.224, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 2032]: seen 203200 examples : 46.4 eps, Loss: 3.310, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2037]: seen 203700 examples : 46.4 eps, Loss: 3.233, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2042]: seen 204200 examples : 46.4 eps, Loss: 3.208, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.139\n",
      "    [batch 2046]: seen 204600 examples : 46.4 eps, Loss: 3.235, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2051]: seen 205100 examples : 46.4 eps, Loss: 3.220, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2056]: seen 205600 examples : 46.4 eps, Loss: 3.368, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2061]: seen 206100 examples : 46.4 eps, Loss: 3.207, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2066]: seen 206600 examples : 46.4 eps, Loss: 3.376, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 2071]: seen 207100 examples : 46.4 eps, Loss: 3.095, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2076]: seen 207600 examples : 46.4 eps, Loss: 3.277, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2081]: seen 208100 examples : 46.4 eps, Loss: 3.205, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 2086]: seen 208600 examples : 46.4 eps, Loss: 3.090, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2091]: seen 209100 examples : 46.4 eps, Loss: 3.168, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 2096]: seen 209600 examples : 46.4 eps, Loss: 3.242, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2101]: seen 210100 examples : 46.4 eps, Loss: 3.223, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2106]: seen 210600 examples : 46.4 eps, Loss: 3.103, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2111]: seen 211100 examples : 46.4 eps, Loss: 3.209, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 2116]: seen 211600 examples : 46.4 eps, Loss: 3.298, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2121]: seen 212100 examples : 46.4 eps, Loss: 3.251, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2126]: seen 212600 examples : 46.4 eps, Loss: 3.204, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2130]: seen 213000 examples : 46.4 eps, Loss: 3.191, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 2135]: seen 213500 examples : 46.4 eps, Loss: 3.338, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2140]: seen 214000 examples : 46.4 eps, Loss: 3.286, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2145]: seen 214500 examples : 46.4 eps, Loss: 3.353, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2150]: seen 215000 examples : 46.4 eps, Loss: 3.179, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2155]: seen 215500 examples : 46.4 eps, Loss: 3.192, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.182\n",
      "    [batch 2160]: seen 216000 examples : 46.4 eps, Loss: 3.304, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2165]: seen 216500 examples : 46.4 eps, Loss: 3.275, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2170]: seen 217000 examples : 46.4 eps, Loss: 3.153, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2175]: seen 217500 examples : 46.4 eps, Loss: 3.416, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2180]: seen 218000 examples : 46.4 eps, Loss: 3.227, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 2185]: seen 218500 examples : 46.4 eps, Loss: 3.244, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2190]: seen 219000 examples : 46.4 eps, Loss: 3.290, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2195]: seen 219500 examples : 46.4 eps, Loss: 3.323, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2200]: seen 220000 examples : 46.4 eps, Loss: 3.402, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2205]: seen 220500 examples : 46.4 eps, Loss: 3.269, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2210]: seen 221000 examples : 46.4 eps, Loss: 2.989, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 2215]: seen 221500 examples : 46.4 eps, Loss: 3.385, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 2219]: seen 221900 examples : 46.4 eps, Loss: 3.055, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2224]: seen 222400 examples : 46.4 eps, Loss: 3.208, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2229]: seen 222900 examples : 46.4 eps, Loss: 3.128, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2234]: seen 223400 examples : 46.4 eps, Loss: 3.357, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2239]: seen 223900 examples : 46.4 eps, Loss: 3.393, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2244]: seen 224400 examples : 46.4 eps, Loss: 3.103, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2249]: seen 224900 examples : 46.4 eps, Loss: 3.342, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2254]: seen 225400 examples : 46.4 eps, Loss: 3.273, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2259]: seen 225900 examples : 46.4 eps, Loss: 3.380, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2264]: seen 226400 examples : 46.4 eps, Loss: 3.250, Avg loss: 3.255, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2269]: seen 226900 examples : 46.4 eps, Loss: 3.132, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2274]: seen 227400 examples : 46.4 eps, Loss: 3.153, Avg loss: 3.254, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2279]: seen 227900 examples : 46.4 eps, Loss: 3.278, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2284]: seen 228400 examples : 46.4 eps, Loss: 3.302, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2289]: seen 228900 examples : 46.4 eps, Loss: 3.351, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2294]: seen 229400 examples : 46.4 eps, Loss: 3.196, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2299]: seen 229900 examples : 46.4 eps, Loss: 3.278, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2304]: seen 230400 examples : 46.4 eps, Loss: 3.153, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2308]: seen 230800 examples : 46.4 eps, Loss: 3.179, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 2313]: seen 231300 examples : 46.4 eps, Loss: 3.192, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2318]: seen 231800 examples : 46.4 eps, Loss: 3.037, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.136\n",
      "    [batch 2323]: seen 232300 examples : 46.4 eps, Loss: 3.038, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2328]: seen 232800 examples : 46.4 eps, Loss: 3.316, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2333]: seen 233300 examples : 46.4 eps, Loss: 3.496, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2338]: seen 233800 examples : 46.4 eps, Loss: 3.323, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2343]: seen 234300 examples : 46.4 eps, Loss: 3.291, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.139\n",
      "    [batch 2348]: seen 234800 examples : 46.4 eps, Loss: 3.308, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2353]: seen 235300 examples : 46.4 eps, Loss: 3.371, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2358]: seen 235800 examples : 46.4 eps, Loss: 3.506, Avg loss: 3.261, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2363]: seen 236300 examples : 46.4 eps, Loss: 3.155, Avg loss: 3.259, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2368]: seen 236800 examples : 46.4 eps, Loss: 3.189, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2373]: seen 237300 examples : 46.4 eps, Loss: 3.207, Avg loss: 3.262, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2378]: seen 237800 examples : 46.4 eps, Loss: 3.393, Avg loss: 3.260, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2383]: seen 238300 examples : 46.4 eps, Loss: 3.153, Avg loss: 3.257, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 2388]: seen 238800 examples : 46.4 eps, Loss: 3.355, Avg loss: 3.258, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 2393]: seen 239300 examples : 46.4 eps, Loss: 3.405, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2398]: seen 239800 examples : 46.4 eps, Loss: 3.304, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2402]: seen 240200 examples : 46.4 eps, Loss: 3.201, Avg loss: 3.253, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2407]: seen 240700 examples : 46.4 eps, Loss: 3.078, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2412]: seen 241200 examples : 46.4 eps, Loss: 3.338, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2417]: seen 241700 examples : 46.4 eps, Loss: 3.284, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2422]: seen 242200 examples : 46.4 eps, Loss: 3.086, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2427]: seen 242700 examples : 46.4 eps, Loss: 3.309, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2432]: seen 243200 examples : 46.4 eps, Loss: 3.323, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 2437]: seen 243700 examples : 46.4 eps, Loss: 3.173, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2442]: seen 244200 examples : 46.4 eps, Loss: 3.183, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2447]: seen 244700 examples : 46.4 eps, Loss: 3.146, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2452]: seen 245200 examples : 46.4 eps, Loss: 3.258, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2457]: seen 245700 examples : 46.4 eps, Loss: 3.301, Avg loss: 3.256, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2462]: seen 246200 examples : 46.4 eps, Loss: 3.154, Avg loss: 3.252, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2467]: seen 246700 examples : 46.4 eps, Loss: 3.226, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2472]: seen 247200 examples : 46.4 eps, Loss: 3.206, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 2477]: seen 247700 examples : 46.4 eps, Loss: 3.366, Avg loss: 3.249, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2482]: seen 248200 examples : 46.4 eps, Loss: 3.227, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 2486]: seen 248600 examples : 46.4 eps, Loss: 2.995, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 2491]: seen 249100 examples : 46.4 eps, Loss: 3.082, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2496]: seen 249600 examples : 46.4 eps, Loss: 3.048, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2501]: seen 250100 examples : 46.4 eps, Loss: 3.297, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2506]: seen 250600 examples : 46.4 eps, Loss: 3.064, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2511]: seen 251100 examples : 46.4 eps, Loss: 3.074, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 2516]: seen 251600 examples : 46.4 eps, Loss: 3.279, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2521]: seen 252100 examples : 46.4 eps, Loss: 3.295, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2526]: seen 252600 examples : 46.4 eps, Loss: 3.284, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2531]: seen 253100 examples : 46.4 eps, Loss: 3.113, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2536]: seen 253600 examples : 46.4 eps, Loss: 3.245, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2541]: seen 254100 examples : 46.4 eps, Loss: 3.402, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 2546]: seen 254600 examples : 46.4 eps, Loss: 3.239, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2551]: seen 255100 examples : 46.4 eps, Loss: 3.186, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2556]: seen 255600 examples : 46.4 eps, Loss: 3.391, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 2561]: seen 256100 examples : 46.4 eps, Loss: 3.234, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2566]: seen 256600 examples : 46.4 eps, Loss: 3.391, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2571]: seen 257100 examples : 46.4 eps, Loss: 3.196, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 2575]: seen 257500 examples : 46.4 eps, Loss: 3.195, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2580]: seen 258000 examples : 46.4 eps, Loss: 3.377, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2585]: seen 258500 examples : 46.4 eps, Loss: 3.457, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2590]: seen 259000 examples : 46.4 eps, Loss: 3.224, Avg loss: 3.251, Best loss: 3.187, cov loss: 0.164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2595]: seen 259500 examples : 46.4 eps, Loss: 3.116, Avg loss: 3.250, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 2600]: seen 260000 examples : 46.4 eps, Loss: 3.280, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2605]: seen 260500 examples : 46.4 eps, Loss: 3.177, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2610]: seen 261000 examples : 46.4 eps, Loss: 2.988, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2615]: seen 261500 examples : 46.4 eps, Loss: 3.344, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2620]: seen 262000 examples : 46.4 eps, Loss: 3.239, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2625]: seen 262500 examples : 46.4 eps, Loss: 3.230, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2630]: seen 263000 examples : 46.4 eps, Loss: 3.290, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2635]: seen 263500 examples : 46.4 eps, Loss: 3.061, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2640]: seen 264000 examples : 46.4 eps, Loss: 3.170, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2645]: seen 264500 examples : 46.4 eps, Loss: 3.233, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2650]: seen 265000 examples : 46.4 eps, Loss: 3.311, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2655]: seen 265500 examples : 46.4 eps, Loss: 3.268, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2660]: seen 266000 examples : 46.4 eps, Loss: 3.177, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2665]: seen 266500 examples : 46.4 eps, Loss: 3.362, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2670]: seen 267000 examples : 46.4 eps, Loss: 3.115, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2675]: seen 267500 examples : 46.4 eps, Loss: 3.183, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2680]: seen 268000 examples : 46.4 eps, Loss: 3.473, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2685]: seen 268500 examples : 46.4 eps, Loss: 3.295, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2690]: seen 269000 examples : 46.4 eps, Loss: 3.242, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2695]: seen 269500 examples : 46.4 eps, Loss: 3.180, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2700]: seen 270000 examples : 46.4 eps, Loss: 3.098, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2705]: seen 270500 examples : 46.4 eps, Loss: 3.331, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2710]: seen 271000 examples : 46.4 eps, Loss: 3.262, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2715]: seen 271500 examples : 46.4 eps, Loss: 3.047, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 2720]: seen 272000 examples : 46.4 eps, Loss: 3.238, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 2725]: seen 272500 examples : 46.4 eps, Loss: 3.091, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2730]: seen 273000 examples : 46.4 eps, Loss: 3.146, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2735]: seen 273500 examples : 46.4 eps, Loss: 3.155, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2740]: seen 274000 examples : 46.4 eps, Loss: 3.269, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2745]: seen 274500 examples : 46.4 eps, Loss: 3.583, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.179\n",
      "    [batch 2750]: seen 275000 examples : 46.4 eps, Loss: 3.122, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 2754]: seen 275400 examples : 46.4 eps, Loss: 3.309, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2759]: seen 275900 examples : 46.4 eps, Loss: 3.364, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2764]: seen 276400 examples : 46.4 eps, Loss: 3.174, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2769]: seen 276900 examples : 46.4 eps, Loss: 3.171, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2774]: seen 277400 examples : 46.4 eps, Loss: 3.295, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2779]: seen 277900 examples : 46.4 eps, Loss: 3.103, Avg loss: 3.246, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2784]: seen 278400 examples : 46.4 eps, Loss: 3.227, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2789]: seen 278900 examples : 46.4 eps, Loss: 3.526, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2794]: seen 279400 examples : 46.4 eps, Loss: 3.211, Avg loss: 3.247, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2799]: seen 279900 examples : 46.4 eps, Loss: 3.252, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2804]: seen 280400 examples : 46.4 eps, Loss: 3.104, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.152\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:40:54\n",
      "[EPOCH 26] Complete. Avg Loss: 3.2405756831024064; Best Loss: 3.187039852142334\n",
      "[EPOCH 27] Starting training..\n",
      "    [batch 7]: seen 700 examples : 68.0 eps, Loss: 3.335, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.186\n",
      "    [batch 12]: seen 1200 examples : 57.2 eps, Loss: 3.036, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 17]: seen 1700 examples : 53.6 eps, Loss: 3.314, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 22]: seen 2200 examples : 51.9 eps, Loss: 3.363, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 27]: seen 2700 examples : 50.8 eps, Loss: 3.284, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 32]: seen 3200 examples : 50.1 eps, Loss: 3.236, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 37]: seen 3700 examples : 49.6 eps, Loss: 3.164, Avg loss: 3.245, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 41]: seen 4100 examples : 48.5 eps, Loss: 3.161, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 46]: seen 4600 examples : 48.3 eps, Loss: 3.170, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 51]: seen 5100 examples : 48.1 eps, Loss: 3.132, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.139\n",
      "    [batch 56]: seen 5600 examples : 48.0 eps, Loss: 3.173, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 61]: seen 6100 examples : 47.9 eps, Loss: 3.296, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 66]: seen 6600 examples : 47.8 eps, Loss: 3.266, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 71]: seen 7100 examples : 47.7 eps, Loss: 3.183, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 76]: seen 7600 examples : 47.6 eps, Loss: 3.251, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 81]: seen 8100 examples : 47.6 eps, Loss: 3.110, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 86]: seen 8600 examples : 47.5 eps, Loss: 3.332, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 91]: seen 9100 examples : 47.5 eps, Loss: 3.299, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 96]: seen 9600 examples : 47.4 eps, Loss: 3.155, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 101]: seen 10100 examples : 47.4 eps, Loss: 3.255, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 106]: seen 10600 examples : 47.4 eps, Loss: 3.174, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 111]: seen 11100 examples : 47.3 eps, Loss: 3.180, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 116]: seen 11600 examples : 47.2 eps, Loss: 3.134, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 120]: seen 12000 examples : 46.9 eps, Loss: 3.021, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 125]: seen 12500 examples : 46.9 eps, Loss: 3.316, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 129]: seen 12900 examples : 46.6 eps, Loss: 3.196, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 134]: seen 13400 examples : 46.6 eps, Loss: 3.255, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 139]: seen 13900 examples : 46.6 eps, Loss: 3.267, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 144]: seen 14400 examples : 46.6 eps, Loss: 3.351, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 149]: seen 14900 examples : 46.6 eps, Loss: 3.142, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 154]: seen 15400 examples : 46.6 eps, Loss: 3.165, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 159]: seen 15900 examples : 46.6 eps, Loss: 3.300, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 164]: seen 16400 examples : 46.6 eps, Loss: 3.162, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.137\n",
      "    [batch 169]: seen 16900 examples : 46.7 eps, Loss: 3.080, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 174]: seen 17400 examples : 46.7 eps, Loss: 3.048, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 179]: seen 17900 examples : 46.7 eps, Loss: 3.058, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.152\n",
      "    [EXCEPTION]:  OOM when allocating tensor with shape[100,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/Softmax_98 = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/xw_plus_b_98)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\n",
      "Caused by op 'projection/Softmax_98', defined at:\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 422, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1434, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-430df9b70ad3>\", line 9, in <module>\n",
      "    train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)\n",
      "  File \"<ipython-input-5-17e3ed376236>\", line 3, in train_continue\n",
      "    lm,vocab,batches,train_dir = tutil.training_init(hps)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/training_util.py\", line 37, in training_init\n",
      "    lm.BuildCoreGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 140, in BuildCoreGraph\n",
      "    self.BuildProjectionGraph()\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 60, in wrapper\n",
      "    return function(self, *args, **kwargs)\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 169, in BuildProjectionGraph\n",
      "    vocab_distribution = [tf.nn.softmax(s) for s in self.vocab_scores]\n",
      "  File \"/home/ubuntu/W266/final_0/W266_Final/model_4/model.py\", line 169, in <listcomp>\n",
      "    vocab_distribution = [tf.nn.softmax(s) for s in self.vocab_scores]\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1738, in softmax\n",
      "    return _softmax(logits, gen_nn_ops.softmax, axis, name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1673, in _softmax\n",
      "    return compute_op(logits, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 7097, in softmax\n",
      "    \"Softmax\", logits=logits, name=name)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,50000] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[Node: projection/Softmax_98 = Softmax[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](projection/xw_plus_b_98)]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[Node: loss/Mean/_2373 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_36983_loss/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      " ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-54780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-54780\n",
      "    [batch 183]: seen 18300 examples : 45.2 eps, Loss: 3.226, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 188]: seen 18800 examples : 45.3 eps, Loss: 3.249, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 193]: seen 19300 examples : 45.3 eps, Loss: 3.389, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 198]: seen 19800 examples : 45.3 eps, Loss: 3.164, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 203]: seen 20300 examples : 45.4 eps, Loss: 3.189, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 208]: seen 20800 examples : 45.4 eps, Loss: 3.018, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 213]: seen 21300 examples : 45.4 eps, Loss: 3.216, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 218]: seen 21800 examples : 45.5 eps, Loss: 2.955, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 223]: seen 22300 examples : 45.4 eps, Loss: 3.246, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 228]: seen 22800 examples : 45.4 eps, Loss: 3.054, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 233]: seen 23300 examples : 45.5 eps, Loss: 3.309, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 238]: seen 23800 examples : 45.5 eps, Loss: 3.385, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 243]: seen 24300 examples : 45.5 eps, Loss: 3.237, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 248]: seen 24800 examples : 45.5 eps, Loss: 3.040, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 253]: seen 25300 examples : 45.6 eps, Loss: 3.316, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 258]: seen 25800 examples : 45.6 eps, Loss: 3.286, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.133\n",
      "    [batch 263]: seen 26300 examples : 45.6 eps, Loss: 3.108, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 268]: seen 26800 examples : 45.6 eps, Loss: 3.332, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 273]: seen 27300 examples : 45.6 eps, Loss: 3.010, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 278]: seen 27800 examples : 45.7 eps, Loss: 3.198, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 283]: seen 28300 examples : 45.7 eps, Loss: 3.186, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 288]: seen 28800 examples : 45.7 eps, Loss: 3.222, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 293]: seen 29300 examples : 45.7 eps, Loss: 3.008, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 298]: seen 29800 examples : 45.7 eps, Loss: 3.063, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 303]: seen 30300 examples : 45.7 eps, Loss: 3.220, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 307]: seen 30700 examples : 45.6 eps, Loss: 3.184, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 312]: seen 31200 examples : 45.7 eps, Loss: 3.179, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 317]: seen 31700 examples : 45.7 eps, Loss: 3.270, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 322]: seen 32200 examples : 45.7 eps, Loss: 3.221, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 327]: seen 32700 examples : 45.7 eps, Loss: 3.011, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 332]: seen 33200 examples : 45.7 eps, Loss: 3.163, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 337]: seen 33700 examples : 45.7 eps, Loss: 3.192, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 342]: seen 34200 examples : 45.8 eps, Loss: 3.338, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 347]: seen 34700 examples : 45.8 eps, Loss: 3.107, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 352]: seen 35200 examples : 45.8 eps, Loss: 3.211, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 357]: seen 35700 examples : 45.8 eps, Loss: 3.022, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 362]: seen 36200 examples : 45.8 eps, Loss: 3.350, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 367]: seen 36700 examples : 45.8 eps, Loss: 3.464, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 372]: seen 37200 examples : 45.8 eps, Loss: 3.127, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 377]: seen 37700 examples : 45.9 eps, Loss: 3.123, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 382]: seen 38200 examples : 45.9 eps, Loss: 3.095, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 387]: seen 38700 examples : 45.9 eps, Loss: 3.293, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 392]: seen 39200 examples : 45.9 eps, Loss: 3.101, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 397]: seen 39700 examples : 45.9 eps, Loss: 3.152, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 402]: seen 40200 examples : 45.9 eps, Loss: 3.172, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 407]: seen 40700 examples : 45.9 eps, Loss: 3.248, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 412]: seen 41200 examples : 45.9 eps, Loss: 3.297, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 417]: seen 41700 examples : 45.9 eps, Loss: 3.053, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 422]: seen 42200 examples : 45.9 eps, Loss: 3.274, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 427]: seen 42700 examples : 45.9 eps, Loss: 3.355, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 432]: seen 43200 examples : 45.9 eps, Loss: 3.381, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 437]: seen 43700 examples : 45.9 eps, Loss: 3.166, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 442]: seen 44200 examples : 45.9 eps, Loss: 3.274, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 447]: seen 44700 examples : 46.0 eps, Loss: 3.267, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 452]: seen 45200 examples : 46.0 eps, Loss: 3.028, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 457]: seen 45700 examples : 46.0 eps, Loss: 3.091, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 462]: seen 46200 examples : 46.0 eps, Loss: 3.352, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 467]: seen 46700 examples : 46.0 eps, Loss: 3.255, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 472]: seen 47200 examples : 46.0 eps, Loss: 3.231, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 477]: seen 47700 examples : 46.0 eps, Loss: 3.191, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 482]: seen 48200 examples : 46.0 eps, Loss: 3.267, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 487]: seen 48700 examples : 46.0 eps, Loss: 3.128, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 492]: seen 49200 examples : 46.0 eps, Loss: 3.218, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 497]: seen 49700 examples : 46.0 eps, Loss: 3.122, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 502]: seen 50200 examples : 46.0 eps, Loss: 3.204, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 507]: seen 50700 examples : 46.0 eps, Loss: 3.229, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 512]: seen 51200 examples : 46.0 eps, Loss: 3.070, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 517]: seen 51700 examples : 46.0 eps, Loss: 3.474, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 522]: seen 52200 examples : 46.0 eps, Loss: 3.400, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 527]: seen 52700 examples : 46.0 eps, Loss: 3.228, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 532]: seen 53200 examples : 46.0 eps, Loss: 3.170, Avg loss: 3.210, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 537]: seen 53700 examples : 46.0 eps, Loss: 3.337, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 542]: seen 54200 examples : 46.1 eps, Loss: 3.109, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 547]: seen 54700 examples : 46.1 eps, Loss: 3.097, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 552]: seen 55200 examples : 46.1 eps, Loss: 3.251, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 557]: seen 55700 examples : 46.1 eps, Loss: 3.105, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 562]: seen 56200 examples : 46.1 eps, Loss: 3.120, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 567]: seen 56700 examples : 46.1 eps, Loss: 3.289, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 572]: seen 57200 examples : 46.1 eps, Loss: 3.358, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 576]: seen 57600 examples : 46.0 eps, Loss: 3.407, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 581]: seen 58100 examples : 46.0 eps, Loss: 3.139, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 586]: seen 58600 examples : 46.0 eps, Loss: 3.299, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 591]: seen 59100 examples : 46.0 eps, Loss: 3.161, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 596]: seen 59600 examples : 46.1 eps, Loss: 3.227, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 601]: seen 60100 examples : 46.1 eps, Loss: 3.115, Avg loss: 3.212, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 606]: seen 60600 examples : 46.1 eps, Loss: 3.236, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 611]: seen 61100 examples : 46.1 eps, Loss: 3.385, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 616]: seen 61600 examples : 46.1 eps, Loss: 3.350, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 621]: seen 62100 examples : 46.1 eps, Loss: 3.268, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 626]: seen 62600 examples : 46.1 eps, Loss: 3.356, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 631]: seen 63100 examples : 46.1 eps, Loss: 3.290, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 636]: seen 63600 examples : 46.1 eps, Loss: 3.459, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 641]: seen 64100 examples : 46.1 eps, Loss: 3.374, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 646]: seen 64600 examples : 46.1 eps, Loss: 3.090, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 651]: seen 65100 examples : 46.1 eps, Loss: 3.166, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 656]: seen 65600 examples : 46.1 eps, Loss: 3.118, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 661]: seen 66100 examples : 46.1 eps, Loss: 3.197, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 666]: seen 66600 examples : 46.1 eps, Loss: 3.223, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 671]: seen 67100 examples : 46.1 eps, Loss: 3.306, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 676]: seen 67600 examples : 46.1 eps, Loss: 3.246, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 681]: seen 68100 examples : 46.1 eps, Loss: 3.196, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 686]: seen 68600 examples : 46.1 eps, Loss: 3.148, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 691]: seen 69100 examples : 46.1 eps, Loss: 3.306, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 696]: seen 69600 examples : 46.1 eps, Loss: 3.051, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 701]: seen 70100 examples : 46.1 eps, Loss: 3.297, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 706]: seen 70600 examples : 46.1 eps, Loss: 3.119, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 711]: seen 71100 examples : 46.1 eps, Loss: 3.269, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 716]: seen 71600 examples : 46.1 eps, Loss: 3.172, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 721]: seen 72100 examples : 46.1 eps, Loss: 3.143, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 726]: seen 72600 examples : 46.2 eps, Loss: 3.245, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 731]: seen 73100 examples : 46.2 eps, Loss: 3.397, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 736]: seen 73600 examples : 46.2 eps, Loss: 3.161, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 741]: seen 74100 examples : 46.2 eps, Loss: 3.124, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 746]: seen 74600 examples : 46.2 eps, Loss: 3.309, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 751]: seen 75100 examples : 46.2 eps, Loss: 3.271, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 755]: seen 75500 examples : 46.1 eps, Loss: 3.360, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 760]: seen 76000 examples : 46.1 eps, Loss: 2.961, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 765]: seen 76500 examples : 46.1 eps, Loss: 3.129, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 770]: seen 77000 examples : 46.1 eps, Loss: 3.309, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 775]: seen 77500 examples : 46.1 eps, Loss: 3.317, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 780]: seen 78000 examples : 46.1 eps, Loss: 3.168, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 785]: seen 78500 examples : 46.1 eps, Loss: 3.386, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 790]: seen 79000 examples : 46.1 eps, Loss: 3.124, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 795]: seen 79500 examples : 46.1 eps, Loss: 3.189, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 800]: seen 80000 examples : 46.1 eps, Loss: 3.177, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 804]: seen 80400 examples : 46.1 eps, Loss: 3.232, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 809]: seen 80900 examples : 46.1 eps, Loss: 3.270, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.139\n",
      "    [batch 814]: seen 81400 examples : 46.1 eps, Loss: 3.141, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 819]: seen 81900 examples : 46.1 eps, Loss: 3.196, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 823]: seen 82300 examples : 46.0 eps, Loss: 3.250, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 828]: seen 82800 examples : 46.0 eps, Loss: 3.275, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 833]: seen 83300 examples : 46.0 eps, Loss: 3.403, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 838]: seen 83800 examples : 46.1 eps, Loss: 3.201, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 842]: seen 84200 examples : 46.0 eps, Loss: 3.197, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 847]: seen 84700 examples : 46.0 eps, Loss: 3.344, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 852]: seen 85200 examples : 46.0 eps, Loss: 3.442, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 857]: seen 85700 examples : 46.0 eps, Loss: 3.223, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 862]: seen 86200 examples : 46.0 eps, Loss: 3.194, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 867]: seen 86700 examples : 46.0 eps, Loss: 3.197, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 872]: seen 87200 examples : 46.0 eps, Loss: 3.284, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 877]: seen 87700 examples : 46.0 eps, Loss: 3.263, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 882]: seen 88200 examples : 46.0 eps, Loss: 3.223, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 887]: seen 88700 examples : 46.0 eps, Loss: 3.156, Avg loss: 3.212, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 892]: seen 89200 examples : 46.0 eps, Loss: 3.099, Avg loss: 3.211, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 897]: seen 89700 examples : 46.0 eps, Loss: 3.158, Avg loss: 3.210, Best loss: 3.187, cov loss: 0.165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 902]: seen 90200 examples : 46.0 eps, Loss: 3.399, Avg loss: 3.211, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 907]: seen 90700 examples : 46.0 eps, Loss: 3.049, Avg loss: 3.210, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 912]: seen 91200 examples : 46.0 eps, Loss: 3.180, Avg loss: 3.212, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 917]: seen 91700 examples : 46.0 eps, Loss: 3.104, Avg loss: 3.211, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 922]: seen 92200 examples : 46.1 eps, Loss: 2.941, Avg loss: 3.209, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 927]: seen 92700 examples : 46.1 eps, Loss: 3.036, Avg loss: 3.208, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 932]: seen 93200 examples : 46.1 eps, Loss: 3.024, Avg loss: 3.204, Best loss: 3.187, cov loss: 0.129\n",
      "    [batch 937]: seen 93700 examples : 46.0 eps, Loss: 3.258, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 942]: seen 94200 examples : 46.0 eps, Loss: 3.226, Avg loss: 3.205, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 947]: seen 94700 examples : 46.1 eps, Loss: 3.217, Avg loss: 3.204, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 952]: seen 95200 examples : 46.1 eps, Loss: 2.967, Avg loss: 3.201, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 957]: seen 95700 examples : 46.1 eps, Loss: 3.195, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 962]: seen 96200 examples : 46.1 eps, Loss: 3.155, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 967]: seen 96700 examples : 46.1 eps, Loss: 3.325, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 972]: seen 97200 examples : 46.1 eps, Loss: 3.184, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 977]: seen 97700 examples : 46.1 eps, Loss: 3.249, Avg loss: 3.205, Best loss: 3.187, cov loss: 0.136\n",
      "    [batch 982]: seen 98200 examples : 46.1 eps, Loss: 3.208, Avg loss: 3.202, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 987]: seen 98700 examples : 46.1 eps, Loss: 3.190, Avg loss: 3.202, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 992]: seen 99200 examples : 46.1 eps, Loss: 3.205, Avg loss: 3.205, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 997]: seen 99700 examples : 46.1 eps, Loss: 3.219, Avg loss: 3.208, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1002]: seen 100200 examples : 46.1 eps, Loss: 3.093, Avg loss: 3.208, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1007]: seen 100700 examples : 46.1 eps, Loss: 3.265, Avg loss: 3.209, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1012]: seen 101200 examples : 46.1 eps, Loss: 3.045, Avg loss: 3.210, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1017]: seen 101700 examples : 46.1 eps, Loss: 3.132, Avg loss: 3.211, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 1021]: seen 102100 examples : 46.1 eps, Loss: 3.310, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1026]: seen 102600 examples : 46.1 eps, Loss: 3.231, Avg loss: 3.211, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1031]: seen 103100 examples : 46.1 eps, Loss: 3.299, Avg loss: 3.211, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1036]: seen 103600 examples : 46.1 eps, Loss: 3.197, Avg loss: 3.212, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 1041]: seen 104100 examples : 46.1 eps, Loss: 3.098, Avg loss: 3.211, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1046]: seen 104600 examples : 46.1 eps, Loss: 3.086, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 1051]: seen 105100 examples : 46.1 eps, Loss: 3.377, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1056]: seen 105600 examples : 46.1 eps, Loss: 3.265, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1061]: seen 106100 examples : 46.1 eps, Loss: 3.146, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1066]: seen 106600 examples : 46.1 eps, Loss: 3.330, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1071]: seen 107100 examples : 46.1 eps, Loss: 3.274, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1076]: seen 107600 examples : 46.1 eps, Loss: 3.174, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1081]: seen 108100 examples : 46.1 eps, Loss: 3.232, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1086]: seen 108600 examples : 46.1 eps, Loss: 3.142, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1091]: seen 109100 examples : 46.1 eps, Loss: 3.249, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.179\n",
      "    [batch 1096]: seen 109600 examples : 46.1 eps, Loss: 3.336, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1101]: seen 110100 examples : 46.1 eps, Loss: 3.159, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1106]: seen 110600 examples : 46.1 eps, Loss: 3.350, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1110]: seen 111000 examples : 46.1 eps, Loss: 3.374, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1115]: seen 111500 examples : 46.1 eps, Loss: 3.300, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1120]: seen 112000 examples : 46.1 eps, Loss: 3.284, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1125]: seen 112500 examples : 46.1 eps, Loss: 3.053, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1130]: seen 113000 examples : 46.1 eps, Loss: 3.274, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1135]: seen 113500 examples : 46.1 eps, Loss: 3.224, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 1140]: seen 114000 examples : 46.1 eps, Loss: 3.240, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 1145]: seen 114500 examples : 46.1 eps, Loss: 3.333, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1150]: seen 115000 examples : 46.1 eps, Loss: 3.201, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1155]: seen 115500 examples : 46.1 eps, Loss: 3.264, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1160]: seen 116000 examples : 46.1 eps, Loss: 3.338, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1165]: seen 116500 examples : 46.1 eps, Loss: 3.042, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1170]: seen 117000 examples : 46.1 eps, Loss: 3.030, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 1175]: seen 117500 examples : 46.1 eps, Loss: 3.037, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1180]: seen 118000 examples : 46.1 eps, Loss: 3.174, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1185]: seen 118500 examples : 46.1 eps, Loss: 3.187, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1190]: seen 119000 examples : 46.1 eps, Loss: 3.263, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1195]: seen 119500 examples : 46.1 eps, Loss: 3.169, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1200]: seen 120000 examples : 46.1 eps, Loss: 3.293, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1205]: seen 120500 examples : 46.1 eps, Loss: 3.186, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1210]: seen 121000 examples : 46.1 eps, Loss: 3.389, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.186\n",
      "    [batch 1215]: seen 121500 examples : 46.1 eps, Loss: 3.179, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1220]: seen 122000 examples : 46.1 eps, Loss: 3.333, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1225]: seen 122500 examples : 46.1 eps, Loss: 3.091, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 1230]: seen 123000 examples : 46.1 eps, Loss: 3.272, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1235]: seen 123500 examples : 46.1 eps, Loss: 3.297, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1240]: seen 124000 examples : 46.1 eps, Loss: 3.371, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1245]: seen 124500 examples : 46.1 eps, Loss: 3.247, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1250]: seen 125000 examples : 46.1 eps, Loss: 3.172, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1255]: seen 125500 examples : 46.1 eps, Loss: 2.839, Avg loss: 3.207, Best loss: 3.187, cov loss: 0.147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1260]: seen 126000 examples : 46.1 eps, Loss: 3.255, Avg loss: 3.205, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1265]: seen 126500 examples : 46.1 eps, Loss: 3.261, Avg loss: 3.205, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1270]: seen 127000 examples : 46.1 eps, Loss: 3.096, Avg loss: 3.205, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1275]: seen 127500 examples : 46.1 eps, Loss: 3.200, Avg loss: 3.204, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1280]: seen 128000 examples : 46.1 eps, Loss: 3.154, Avg loss: 3.204, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 1285]: seen 128500 examples : 46.1 eps, Loss: 3.169, Avg loss: 3.206, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 1289]: seen 128900 examples : 46.1 eps, Loss: 3.115, Avg loss: 3.207, Best loss: 3.187, cov loss: 0.137\n",
      "    [batch 1294]: seen 129400 examples : 46.1 eps, Loss: 3.165, Avg loss: 3.208, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1299]: seen 129900 examples : 46.1 eps, Loss: 3.275, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1304]: seen 130400 examples : 46.1 eps, Loss: 3.332, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1309]: seen 130900 examples : 46.1 eps, Loss: 3.285, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1314]: seen 131400 examples : 46.1 eps, Loss: 3.125, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1319]: seen 131900 examples : 46.1 eps, Loss: 3.179, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1324]: seen 132400 examples : 46.1 eps, Loss: 3.323, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1329]: seen 132900 examples : 46.1 eps, Loss: 3.004, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1334]: seen 133400 examples : 46.1 eps, Loss: 3.165, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1339]: seen 133900 examples : 46.1 eps, Loss: 3.184, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1344]: seen 134400 examples : 46.2 eps, Loss: 3.260, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1349]: seen 134900 examples : 46.2 eps, Loss: 3.354, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1354]: seen 135400 examples : 46.2 eps, Loss: 3.074, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1359]: seen 135900 examples : 46.2 eps, Loss: 3.093, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1364]: seen 136400 examples : 46.2 eps, Loss: 3.276, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1369]: seen 136900 examples : 46.2 eps, Loss: 3.165, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1374]: seen 137400 examples : 46.2 eps, Loss: 2.994, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 1379]: seen 137900 examples : 46.2 eps, Loss: 3.253, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.177\n",
      "    [batch 1384]: seen 138400 examples : 46.2 eps, Loss: 3.355, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1389]: seen 138900 examples : 46.2 eps, Loss: 3.101, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1394]: seen 139400 examples : 46.2 eps, Loss: 3.078, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1399]: seen 139900 examples : 46.2 eps, Loss: 3.258, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1404]: seen 140400 examples : 46.2 eps, Loss: 3.150, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 1409]: seen 140900 examples : 46.2 eps, Loss: 3.334, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 1414]: seen 141400 examples : 46.2 eps, Loss: 3.063, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1419]: seen 141900 examples : 46.2 eps, Loss: 3.223, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1424]: seen 142400 examples : 46.2 eps, Loss: 3.203, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1429]: seen 142900 examples : 46.2 eps, Loss: 3.224, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 1434]: seen 143400 examples : 46.2 eps, Loss: 3.405, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1439]: seen 143900 examples : 46.2 eps, Loss: 3.174, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1444]: seen 144400 examples : 46.2 eps, Loss: 3.202, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1449]: seen 144900 examples : 46.2 eps, Loss: 3.239, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1454]: seen 145400 examples : 46.2 eps, Loss: 3.057, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1459]: seen 145900 examples : 46.2 eps, Loss: 3.373, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1464]: seen 146400 examples : 46.2 eps, Loss: 3.208, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1468]: seen 146800 examples : 46.2 eps, Loss: 3.193, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1473]: seen 147300 examples : 46.2 eps, Loss: 3.549, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1478]: seen 147800 examples : 46.2 eps, Loss: 3.267, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1483]: seen 148300 examples : 46.2 eps, Loss: 3.164, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1488]: seen 148800 examples : 46.2 eps, Loss: 3.244, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1493]: seen 149300 examples : 46.2 eps, Loss: 3.024, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.137\n",
      "    [batch 1498]: seen 149800 examples : 46.2 eps, Loss: 3.289, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1503]: seen 150300 examples : 46.2 eps, Loss: 3.343, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1508]: seen 150800 examples : 46.2 eps, Loss: 3.172, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1513]: seen 151300 examples : 46.2 eps, Loss: 3.241, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1518]: seen 151800 examples : 46.2 eps, Loss: 3.167, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1523]: seen 152300 examples : 46.2 eps, Loss: 3.206, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1528]: seen 152800 examples : 46.2 eps, Loss: 3.349, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 1533]: seen 153300 examples : 46.2 eps, Loss: 3.266, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1538]: seen 153800 examples : 46.2 eps, Loss: 3.278, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1543]: seen 154300 examples : 46.2 eps, Loss: 3.109, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1548]: seen 154800 examples : 46.2 eps, Loss: 3.175, Avg loss: 3.212, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1553]: seen 155300 examples : 46.2 eps, Loss: 3.443, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1557]: seen 155700 examples : 46.2 eps, Loss: 3.384, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1562]: seen 156200 examples : 46.2 eps, Loss: 3.203, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 1567]: seen 156700 examples : 46.2 eps, Loss: 3.146, Avg loss: 3.211, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 1572]: seen 157200 examples : 46.2 eps, Loss: 3.243, Avg loss: 3.206, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1577]: seen 157700 examples : 46.2 eps, Loss: 3.100, Avg loss: 3.204, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1582]: seen 158200 examples : 46.2 eps, Loss: 3.121, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1587]: seen 158700 examples : 46.2 eps, Loss: 3.238, Avg loss: 3.206, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1592]: seen 159200 examples : 46.2 eps, Loss: 3.220, Avg loss: 3.206, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 1597]: seen 159700 examples : 46.2 eps, Loss: 3.260, Avg loss: 3.205, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 1602]: seen 160200 examples : 46.2 eps, Loss: 3.223, Avg loss: 3.206, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1607]: seen 160700 examples : 46.2 eps, Loss: 3.086, Avg loss: 3.205, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1612]: seen 161200 examples : 46.2 eps, Loss: 3.071, Avg loss: 3.204, Best loss: 3.187, cov loss: 0.162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1617]: seen 161700 examples : 46.2 eps, Loss: 3.267, Avg loss: 3.206, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1622]: seen 162200 examples : 46.2 eps, Loss: 3.334, Avg loss: 3.212, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1627]: seen 162700 examples : 46.2 eps, Loss: 3.158, Avg loss: 3.210, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 1632]: seen 163200 examples : 46.2 eps, Loss: 3.359, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1637]: seen 163700 examples : 46.2 eps, Loss: 3.140, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1642]: seen 164200 examples : 46.2 eps, Loss: 3.281, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1647]: seen 164700 examples : 46.2 eps, Loss: 3.200, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 1652]: seen 165200 examples : 46.2 eps, Loss: 3.325, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1657]: seen 165700 examples : 46.2 eps, Loss: 3.055, Avg loss: 3.215, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1662]: seen 166200 examples : 46.2 eps, Loss: 3.129, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1667]: seen 166700 examples : 46.2 eps, Loss: 3.026, Avg loss: 3.212, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1672]: seen 167200 examples : 46.2 eps, Loss: 3.355, Avg loss: 3.212, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1677]: seen 167700 examples : 46.2 eps, Loss: 3.261, Avg loss: 3.214, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1682]: seen 168200 examples : 46.2 eps, Loss: 3.032, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 1687]: seen 168700 examples : 46.2 eps, Loss: 3.156, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1692]: seen 169200 examples : 46.2 eps, Loss: 3.206, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 1697]: seen 169700 examples : 46.2 eps, Loss: 3.223, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1702]: seen 170200 examples : 46.2 eps, Loss: 3.142, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1707]: seen 170700 examples : 46.2 eps, Loss: 3.246, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1712]: seen 171200 examples : 46.2 eps, Loss: 3.097, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1717]: seen 171700 examples : 46.2 eps, Loss: 3.188, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1722]: seen 172200 examples : 46.2 eps, Loss: 3.492, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 1727]: seen 172700 examples : 46.2 eps, Loss: 3.190, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1732]: seen 173200 examples : 46.2 eps, Loss: 3.156, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1737]: seen 173700 examples : 46.2 eps, Loss: 3.316, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1741]: seen 174100 examples : 46.2 eps, Loss: 3.402, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1746]: seen 174600 examples : 46.2 eps, Loss: 3.284, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 1751]: seen 175100 examples : 46.2 eps, Loss: 3.450, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 1756]: seen 175600 examples : 46.2 eps, Loss: 3.223, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 1761]: seen 176100 examples : 46.2 eps, Loss: 3.358, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1766]: seen 176600 examples : 46.2 eps, Loss: 3.402, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.179\n",
      "    [batch 1771]: seen 177100 examples : 46.2 eps, Loss: 3.333, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.137\n",
      "    [batch 1776]: seen 177600 examples : 46.2 eps, Loss: 3.244, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1781]: seen 178100 examples : 46.2 eps, Loss: 3.255, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1786]: seen 178600 examples : 46.2 eps, Loss: 3.161, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 1791]: seen 179100 examples : 46.2 eps, Loss: 3.182, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1796]: seen 179600 examples : 46.2 eps, Loss: 3.139, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1801]: seen 180100 examples : 46.2 eps, Loss: 3.106, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 1806]: seen 180600 examples : 46.2 eps, Loss: 3.000, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.138\n",
      "    [batch 1811]: seen 181100 examples : 46.2 eps, Loss: 3.286, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 1816]: seen 181600 examples : 46.2 eps, Loss: 3.232, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 1821]: seen 182100 examples : 46.2 eps, Loss: 3.319, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1825]: seen 182500 examples : 46.2 eps, Loss: 3.184, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 1830]: seen 183000 examples : 46.2 eps, Loss: 3.348, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 1835]: seen 183500 examples : 46.2 eps, Loss: 3.089, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1840]: seen 184000 examples : 46.2 eps, Loss: 3.181, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1845]: seen 184500 examples : 46.2 eps, Loss: 3.192, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 1850]: seen 185000 examples : 46.2 eps, Loss: 3.182, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 1855]: seen 185500 examples : 46.2 eps, Loss: 3.281, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1860]: seen 186000 examples : 46.2 eps, Loss: 3.271, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 1865]: seen 186500 examples : 46.2 eps, Loss: 3.248, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1870]: seen 187000 examples : 46.2 eps, Loss: 3.146, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 1875]: seen 187500 examples : 46.2 eps, Loss: 3.232, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 1880]: seen 188000 examples : 46.2 eps, Loss: 3.254, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1885]: seen 188500 examples : 46.2 eps, Loss: 3.303, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1890]: seen 189000 examples : 46.2 eps, Loss: 3.289, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1895]: seen 189500 examples : 46.2 eps, Loss: 3.430, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 1900]: seen 190000 examples : 46.2 eps, Loss: 3.134, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 1905]: seen 190500 examples : 46.2 eps, Loss: 3.188, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 1910]: seen 191000 examples : 46.2 eps, Loss: 3.218, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1914]: seen 191400 examples : 46.2 eps, Loss: 3.291, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.178\n",
      "    [batch 1919]: seen 191900 examples : 46.2 eps, Loss: 3.243, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 1924]: seen 192400 examples : 46.2 eps, Loss: 3.257, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 1929]: seen 192900 examples : 46.2 eps, Loss: 3.327, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 1934]: seen 193400 examples : 46.2 eps, Loss: 3.451, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 1939]: seen 193900 examples : 46.2 eps, Loss: 3.141, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 1944]: seen 194400 examples : 46.2 eps, Loss: 3.118, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.139\n",
      "    [batch 1949]: seen 194900 examples : 46.2 eps, Loss: 3.099, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 1954]: seen 195400 examples : 46.2 eps, Loss: 2.953, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 1959]: seen 195900 examples : 46.2 eps, Loss: 3.142, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 1964]: seen 196400 examples : 46.2 eps, Loss: 3.210, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 1969]: seen 196900 examples : 46.2 eps, Loss: 3.345, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1974]: seen 197400 examples : 46.2 eps, Loss: 3.448, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1979]: seen 197900 examples : 46.2 eps, Loss: 3.372, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 1984]: seen 198400 examples : 46.2 eps, Loss: 3.060, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 1989]: seen 198900 examples : 46.2 eps, Loss: 3.251, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 1994]: seen 199400 examples : 46.2 eps, Loss: 3.245, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 1999]: seen 199900 examples : 46.2 eps, Loss: 3.190, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2004]: seen 200400 examples : 46.2 eps, Loss: 3.106, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2009]: seen 200900 examples : 46.2 eps, Loss: 3.311, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2014]: seen 201400 examples : 46.2 eps, Loss: 3.242, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2019]: seen 201900 examples : 46.2 eps, Loss: 3.244, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2024]: seen 202400 examples : 46.2 eps, Loss: 3.174, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2029]: seen 202900 examples : 46.2 eps, Loss: 3.371, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2034]: seen 203400 examples : 46.2 eps, Loss: 3.386, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 2039]: seen 203900 examples : 46.2 eps, Loss: 3.330, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2044]: seen 204400 examples : 46.2 eps, Loss: 3.298, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2049]: seen 204900 examples : 46.2 eps, Loss: 3.184, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2054]: seen 205400 examples : 46.2 eps, Loss: 3.065, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2059]: seen 205900 examples : 46.2 eps, Loss: 3.034, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2064]: seen 206400 examples : 46.2 eps, Loss: 2.987, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2069]: seen 206900 examples : 46.2 eps, Loss: 3.195, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2074]: seen 207400 examples : 46.2 eps, Loss: 3.292, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2079]: seen 207900 examples : 46.2 eps, Loss: 3.370, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2084]: seen 208400 examples : 46.2 eps, Loss: 3.422, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2089]: seen 208900 examples : 46.2 eps, Loss: 3.234, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2093]: seen 209300 examples : 46.2 eps, Loss: 3.228, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 2098]: seen 209800 examples : 46.2 eps, Loss: 3.144, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2103]: seen 210300 examples : 46.2 eps, Loss: 3.226, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2108]: seen 210800 examples : 46.2 eps, Loss: 3.215, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2113]: seen 211300 examples : 46.2 eps, Loss: 3.207, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2118]: seen 211800 examples : 46.2 eps, Loss: 3.133, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2123]: seen 212300 examples : 46.2 eps, Loss: 3.159, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2128]: seen 212800 examples : 46.2 eps, Loss: 3.163, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 2133]: seen 213300 examples : 46.2 eps, Loss: 3.303, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2138]: seen 213800 examples : 46.2 eps, Loss: 3.284, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2143]: seen 214300 examples : 46.2 eps, Loss: 3.110, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 2148]: seen 214800 examples : 46.2 eps, Loss: 3.188, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 2153]: seen 215300 examples : 46.2 eps, Loss: 3.254, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.139\n",
      "    [batch 2158]: seen 215800 examples : 46.2 eps, Loss: 3.257, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2163]: seen 216300 examples : 46.2 eps, Loss: 3.138, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2168]: seen 216800 examples : 46.2 eps, Loss: 3.401, Avg loss: 3.237, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2173]: seen 217300 examples : 46.2 eps, Loss: 3.445, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2178]: seen 217800 examples : 46.2 eps, Loss: 3.173, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2182]: seen 218200 examples : 46.2 eps, Loss: 3.291, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2187]: seen 218700 examples : 46.2 eps, Loss: 3.245, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2192]: seen 219200 examples : 46.2 eps, Loss: 3.259, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 2197]: seen 219700 examples : 46.2 eps, Loss: 3.134, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2202]: seen 220200 examples : 46.2 eps, Loss: 3.437, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2207]: seen 220700 examples : 46.2 eps, Loss: 3.198, Avg loss: 3.248, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2212]: seen 221200 examples : 46.2 eps, Loss: 3.084, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2217]: seen 221700 examples : 46.2 eps, Loss: 3.132, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2222]: seen 222200 examples : 46.2 eps, Loss: 3.136, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2227]: seen 222700 examples : 46.2 eps, Loss: 3.252, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2232]: seen 223200 examples : 46.2 eps, Loss: 3.343, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2237]: seen 223700 examples : 46.2 eps, Loss: 3.069, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 2242]: seen 224200 examples : 46.2 eps, Loss: 3.335, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2247]: seen 224700 examples : 46.2 eps, Loss: 3.215, Avg loss: 3.239, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2252]: seen 225200 examples : 46.2 eps, Loss: 3.252, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 2257]: seen 225700 examples : 46.2 eps, Loss: 3.226, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2262]: seen 226200 examples : 46.3 eps, Loss: 3.328, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2267]: seen 226700 examples : 46.2 eps, Loss: 3.262, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 2271]: seen 227100 examples : 46.2 eps, Loss: 3.191, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2276]: seen 227600 examples : 46.2 eps, Loss: 3.106, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2281]: seen 228100 examples : 46.2 eps, Loss: 3.271, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.139\n",
      "    [batch 2286]: seen 228600 examples : 46.2 eps, Loss: 3.288, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2291]: seen 229100 examples : 46.2 eps, Loss: 3.222, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2296]: seen 229600 examples : 46.2 eps, Loss: 3.242, Avg loss: 3.242, Best loss: 3.187, cov loss: 0.173\n",
      "    [batch 2301]: seen 230100 examples : 46.2 eps, Loss: 3.022, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 2306]: seen 230600 examples : 46.2 eps, Loss: 3.250, Avg loss: 3.243, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2311]: seen 231100 examples : 46.2 eps, Loss: 3.149, Avg loss: 3.244, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2316]: seen 231600 examples : 46.2 eps, Loss: 3.115, Avg loss: 3.241, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2321]: seen 232100 examples : 46.2 eps, Loss: 3.282, Avg loss: 3.240, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2326]: seen 232600 examples : 46.2 eps, Loss: 3.204, Avg loss: 3.236, Best loss: 3.187, cov loss: 0.156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2331]: seen 233100 examples : 46.2 eps, Loss: 3.222, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2336]: seen 233600 examples : 46.2 eps, Loss: 3.182, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2341]: seen 234100 examples : 46.2 eps, Loss: 3.335, Avg loss: 3.238, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2346]: seen 234600 examples : 46.2 eps, Loss: 3.186, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2351]: seen 235100 examples : 46.2 eps, Loss: 3.136, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2356]: seen 235600 examples : 46.2 eps, Loss: 3.257, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2361]: seen 236100 examples : 46.3 eps, Loss: 3.346, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2366]: seen 236600 examples : 46.2 eps, Loss: 3.239, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2371]: seen 237100 examples : 46.2 eps, Loss: 3.195, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2376]: seen 237600 examples : 46.2 eps, Loss: 3.146, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 2381]: seen 238100 examples : 46.2 eps, Loss: 3.191, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2386]: seen 238600 examples : 46.2 eps, Loss: 3.226, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2391]: seen 239100 examples : 46.2 eps, Loss: 3.228, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 2396]: seen 239600 examples : 46.2 eps, Loss: 3.230, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2401]: seen 240100 examples : 46.3 eps, Loss: 3.408, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2406]: seen 240600 examples : 46.2 eps, Loss: 3.229, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2411]: seen 241100 examples : 46.2 eps, Loss: 3.066, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 2416]: seen 241600 examples : 46.2 eps, Loss: 3.360, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 2421]: seen 242100 examples : 46.2 eps, Loss: 3.117, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2426]: seen 242600 examples : 46.2 eps, Loss: 3.130, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.139\n",
      "    [batch 2431]: seen 243100 examples : 46.2 eps, Loss: 3.085, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2436]: seen 243600 examples : 46.3 eps, Loss: 3.312, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2441]: seen 244100 examples : 46.3 eps, Loss: 3.426, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2446]: seen 244600 examples : 46.3 eps, Loss: 3.349, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 2451]: seen 245100 examples : 46.2 eps, Loss: 3.151, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2456]: seen 245600 examples : 46.2 eps, Loss: 3.166, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 2461]: seen 246100 examples : 46.2 eps, Loss: 3.277, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.172\n",
      "    [batch 2466]: seen 246600 examples : 46.2 eps, Loss: 3.319, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2471]: seen 247100 examples : 46.2 eps, Loss: 3.117, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2476]: seen 247600 examples : 46.2 eps, Loss: 3.295, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 2481]: seen 248100 examples : 46.2 eps, Loss: 3.373, Avg loss: 3.226, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 2486]: seen 248600 examples : 46.3 eps, Loss: 3.415, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2491]: seen 249100 examples : 46.3 eps, Loss: 3.355, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2496]: seen 249600 examples : 46.3 eps, Loss: 3.308, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 2501]: seen 250100 examples : 46.3 eps, Loss: 3.353, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 2506]: seen 250600 examples : 46.3 eps, Loss: 3.178, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2511]: seen 251100 examples : 46.3 eps, Loss: 3.089, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.139\n",
      "    [batch 2516]: seen 251600 examples : 46.3 eps, Loss: 3.393, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2521]: seen 252100 examples : 46.3 eps, Loss: 3.032, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2526]: seen 252600 examples : 46.3 eps, Loss: 3.313, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2531]: seen 253100 examples : 46.3 eps, Loss: 3.230, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 2536]: seen 253600 examples : 46.3 eps, Loss: 3.124, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 2540]: seen 254000 examples : 46.2 eps, Loss: 3.177, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 2545]: seen 254500 examples : 46.2 eps, Loss: 3.282, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2550]: seen 255000 examples : 46.2 eps, Loss: 3.315, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 2555]: seen 255500 examples : 46.3 eps, Loss: 3.199, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 2560]: seen 256000 examples : 46.3 eps, Loss: 3.112, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2565]: seen 256500 examples : 46.3 eps, Loss: 3.279, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 2570]: seen 257000 examples : 46.2 eps, Loss: 3.131, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 2575]: seen 257500 examples : 46.2 eps, Loss: 3.077, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 2580]: seen 258000 examples : 46.2 eps, Loss: 3.076, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2585]: seen 258500 examples : 46.2 eps, Loss: 3.443, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.176\n",
      "    [batch 2590]: seen 259000 examples : 46.3 eps, Loss: 3.211, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2595]: seen 259500 examples : 46.3 eps, Loss: 3.405, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2600]: seen 260000 examples : 46.3 eps, Loss: 3.300, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 2605]: seen 260500 examples : 46.3 eps, Loss: 3.266, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2610]: seen 261000 examples : 46.3 eps, Loss: 3.268, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 2615]: seen 261500 examples : 46.3 eps, Loss: 3.173, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 2620]: seen 262000 examples : 46.3 eps, Loss: 3.103, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 2625]: seen 262500 examples : 46.3 eps, Loss: 3.131, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 2630]: seen 263000 examples : 46.3 eps, Loss: 3.101, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 2634]: seen 263400 examples : 46.2 eps, Loss: 3.188, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 2639]: seen 263900 examples : 46.2 eps, Loss: 3.242, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2644]: seen 264400 examples : 46.2 eps, Loss: 3.358, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2649]: seen 264900 examples : 46.2 eps, Loss: 2.970, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 2654]: seen 265400 examples : 46.2 eps, Loss: 3.337, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2659]: seen 265900 examples : 46.2 eps, Loss: 3.236, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2664]: seen 266400 examples : 46.2 eps, Loss: 3.378, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2669]: seen 266900 examples : 46.3 eps, Loss: 3.076, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2674]: seen 267400 examples : 46.3 eps, Loss: 3.392, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2679]: seen 267900 examples : 46.3 eps, Loss: 3.162, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.146\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-57277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-57277\n",
      "    [batch 2683]: seen 268300 examples : 46.2 eps, Loss: 3.240, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2688]: seen 268800 examples : 46.2 eps, Loss: 3.287, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 2693]: seen 269300 examples : 46.2 eps, Loss: 3.142, Avg loss: 3.224, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2698]: seen 269800 examples : 46.2 eps, Loss: 3.202, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 2703]: seen 270300 examples : 46.2 eps, Loss: 3.168, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2708]: seen 270800 examples : 46.2 eps, Loss: 3.355, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2713]: seen 271300 examples : 46.2 eps, Loss: 3.164, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 2717]: seen 271700 examples : 46.2 eps, Loss: 3.160, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2722]: seen 272200 examples : 46.2 eps, Loss: 3.115, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 2727]: seen 272700 examples : 46.2 eps, Loss: 3.301, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2732]: seen 273200 examples : 46.2 eps, Loss: 3.169, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 2737]: seen 273700 examples : 46.2 eps, Loss: 3.226, Avg loss: 3.228, Best loss: 3.187, cov loss: 0.167\n",
      "    [batch 2742]: seen 274200 examples : 46.2 eps, Loss: 3.199, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 2747]: seen 274700 examples : 46.2 eps, Loss: 3.248, Avg loss: 3.231, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2752]: seen 275200 examples : 46.2 eps, Loss: 3.315, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 2757]: seen 275700 examples : 46.2 eps, Loss: 3.124, Avg loss: 3.229, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 2762]: seen 276200 examples : 46.2 eps, Loss: 3.299, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 2767]: seen 276700 examples : 46.2 eps, Loss: 3.319, Avg loss: 3.232, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 2772]: seen 277200 examples : 46.2 eps, Loss: 3.269, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 2777]: seen 277700 examples : 46.2 eps, Loss: 3.349, Avg loss: 3.234, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 2782]: seen 278200 examples : 46.2 eps, Loss: 3.119, Avg loss: 3.235, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 2787]: seen 278700 examples : 46.2 eps, Loss: 3.023, Avg loss: 3.233, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 2792]: seen 279200 examples : 46.2 eps, Loss: 3.136, Avg loss: 3.230, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 2797]: seen 279700 examples : 46.2 eps, Loss: 3.104, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 2802]: seen 280200 examples : 46.2 eps, Loss: 3.374, Avg loss: 3.227, Best loss: 3.187, cov loss: 0.180\n",
      "    [batch 2806]: seen 280600 examples : 46.2 eps, Loss: 3.289, Avg loss: 3.225, Best loss: 3.187, cov loss: 0.168\n",
      "    [END] Training complete: Total examples : 280600; Total time: 1:41:12\n",
      "[EPOCH 27] Complete. Avg Loss: 3.225384100902404; Best Loss: 3.187039852142334\n",
      "[EPOCH 28] Starting training..\n",
      "    [batch 7]: seen 700 examples : 68.0 eps, Loss: 3.204, Avg loss: 3.223, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 12]: seen 1200 examples : 57.2 eps, Loss: 3.327, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 17]: seen 1700 examples : 53.7 eps, Loss: 3.216, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 22]: seen 2200 examples : 51.9 eps, Loss: 3.264, Avg loss: 3.221, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 27]: seen 2700 examples : 50.9 eps, Loss: 3.118, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 32]: seen 3200 examples : 50.0 eps, Loss: 3.263, Avg loss: 3.220, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 37]: seen 3700 examples : 49.5 eps, Loss: 3.258, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 42]: seen 4200 examples : 49.2 eps, Loss: 3.434, Avg loss: 3.219, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 47]: seen 4700 examples : 48.9 eps, Loss: 3.232, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 52]: seen 5200 examples : 48.7 eps, Loss: 3.157, Avg loss: 3.222, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 57]: seen 5700 examples : 48.5 eps, Loss: 3.082, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 62]: seen 6200 examples : 48.4 eps, Loss: 3.217, Avg loss: 3.217, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 67]: seen 6700 examples : 48.2 eps, Loss: 3.322, Avg loss: 3.218, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 72]: seen 7200 examples : 48.1 eps, Loss: 3.083, Avg loss: 3.216, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 77]: seen 7700 examples : 48.0 eps, Loss: 3.205, Avg loss: 3.213, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 82]: seen 8200 examples : 47.9 eps, Loss: 3.213, Avg loss: 3.210, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 87]: seen 8700 examples : 47.9 eps, Loss: 3.081, Avg loss: 3.201, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 91]: seen 9100 examples : 47.5 eps, Loss: 3.311, Avg loss: 3.200, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 96]: seen 9600 examples : 47.4 eps, Loss: 3.010, Avg loss: 3.196, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 101]: seen 10100 examples : 47.3 eps, Loss: 3.021, Avg loss: 3.196, Best loss: 3.187, cov loss: 0.134\n",
      "    [batch 106]: seen 10600 examples : 47.3 eps, Loss: 3.383, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 111]: seen 11100 examples : 47.3 eps, Loss: 3.097, Avg loss: 3.193, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 116]: seen 11600 examples : 47.3 eps, Loss: 3.201, Avg loss: 3.192, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 121]: seen 12100 examples : 47.2 eps, Loss: 3.463, Avg loss: 3.192, Best loss: 3.187, cov loss: 0.170\n",
      "    [batch 126]: seen 12600 examples : 47.2 eps, Loss: 3.158, Avg loss: 3.191, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 131]: seen 13100 examples : 47.2 eps, Loss: 3.288, Avg loss: 3.195, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 136]: seen 13600 examples : 47.2 eps, Loss: 3.083, Avg loss: 3.193, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 141]: seen 14100 examples : 47.2 eps, Loss: 3.254, Avg loss: 3.194, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 146]: seen 14600 examples : 47.1 eps, Loss: 3.173, Avg loss: 3.195, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 151]: seen 15100 examples : 47.1 eps, Loss: 3.359, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 156]: seen 15600 examples : 47.1 eps, Loss: 2.982, Avg loss: 3.197, Best loss: 3.187, cov loss: 0.133\n",
      "    [batch 161]: seen 16100 examples : 47.1 eps, Loss: 3.250, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 166]: seen 16600 examples : 47.1 eps, Loss: 3.252, Avg loss: 3.196, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 171]: seen 17100 examples : 47.1 eps, Loss: 3.240, Avg loss: 3.198, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 176]: seen 17600 examples : 47.1 eps, Loss: 3.308, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 180]: seen 18000 examples : 46.9 eps, Loss: 3.119, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 185]: seen 18500 examples : 46.9 eps, Loss: 3.255, Avg loss: 3.202, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 190]: seen 19000 examples : 46.9 eps, Loss: 3.068, Avg loss: 3.200, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 195]: seen 19500 examples : 46.9 eps, Loss: 3.059, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 200]: seen 20000 examples : 46.9 eps, Loss: 3.296, Avg loss: 3.195, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 205]: seen 20500 examples : 46.8 eps, Loss: 3.229, Avg loss: 3.197, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 210]: seen 21000 examples : 46.8 eps, Loss: 3.115, Avg loss: 3.196, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 215]: seen 21500 examples : 46.8 eps, Loss: 3.214, Avg loss: 3.194, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 220]: seen 22000 examples : 46.8 eps, Loss: 3.008, Avg loss: 3.191, Best loss: 3.187, cov loss: 0.141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 225]: seen 22500 examples : 46.8 eps, Loss: 3.287, Avg loss: 3.192, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 230]: seen 23000 examples : 46.8 eps, Loss: 3.181, Avg loss: 3.193, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 235]: seen 23500 examples : 46.8 eps, Loss: 3.062, Avg loss: 3.191, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 240]: seen 24000 examples : 46.8 eps, Loss: 3.256, Avg loss: 3.190, Best loss: 3.187, cov loss: 0.148\n",
      "    [batch 245]: seen 24500 examples : 46.8 eps, Loss: 3.212, Avg loss: 3.192, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 250]: seen 25000 examples : 46.8 eps, Loss: 2.956, Avg loss: 3.189, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 255]: seen 25500 examples : 46.8 eps, Loss: 3.370, Avg loss: 3.192, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 260]: seen 26000 examples : 46.8 eps, Loss: 3.236, Avg loss: 3.195, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 265]: seen 26500 examples : 46.8 eps, Loss: 3.248, Avg loss: 3.197, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 270]: seen 27000 examples : 46.8 eps, Loss: 3.185, Avg loss: 3.197, Best loss: 3.187, cov loss: 0.165\n",
      "    [batch 274]: seen 27400 examples : 46.7 eps, Loss: 3.186, Avg loss: 3.198, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 279]: seen 27900 examples : 46.7 eps, Loss: 3.230, Avg loss: 3.200, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 284]: seen 28400 examples : 46.7 eps, Loss: 3.136, Avg loss: 3.197, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 289]: seen 28900 examples : 46.7 eps, Loss: 3.249, Avg loss: 3.200, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 294]: seen 29400 examples : 46.7 eps, Loss: 3.272, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 299]: seen 29900 examples : 46.7 eps, Loss: 3.197, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 304]: seen 30400 examples : 46.7 eps, Loss: 3.183, Avg loss: 3.200, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 309]: seen 30900 examples : 46.7 eps, Loss: 3.145, Avg loss: 3.200, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 314]: seen 31400 examples : 46.7 eps, Loss: 3.279, Avg loss: 3.201, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 319]: seen 31900 examples : 46.7 eps, Loss: 3.470, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.155\n",
      "    [batch 324]: seen 32400 examples : 46.7 eps, Loss: 3.255, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 329]: seen 32900 examples : 46.7 eps, Loss: 3.095, Avg loss: 3.200, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 334]: seen 33400 examples : 46.7 eps, Loss: 3.062, Avg loss: 3.198, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 339]: seen 33900 examples : 46.7 eps, Loss: 3.222, Avg loss: 3.197, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 344]: seen 34400 examples : 46.7 eps, Loss: 3.167, Avg loss: 3.195, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 349]: seen 34900 examples : 46.7 eps, Loss: 3.238, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 354]: seen 35400 examples : 46.7 eps, Loss: 3.250, Avg loss: 3.204, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 359]: seen 35900 examples : 46.6 eps, Loss: 3.128, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.157\n",
      "    [batch 364]: seen 36400 examples : 46.6 eps, Loss: 3.180, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 369]: seen 36900 examples : 46.6 eps, Loss: 3.077, Avg loss: 3.204, Best loss: 3.187, cov loss: 0.141\n",
      "    [batch 374]: seen 37400 examples : 46.6 eps, Loss: 3.377, Avg loss: 3.206, Best loss: 3.187, cov loss: 0.163\n",
      "    [batch 379]: seen 37900 examples : 46.6 eps, Loss: 3.235, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 384]: seen 38400 examples : 46.6 eps, Loss: 2.999, Avg loss: 3.201, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 389]: seen 38900 examples : 46.6 eps, Loss: 3.334, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 394]: seen 39400 examples : 46.6 eps, Loss: 3.243, Avg loss: 3.200, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 399]: seen 39900 examples : 46.6 eps, Loss: 3.137, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 404]: seen 40400 examples : 46.6 eps, Loss: 3.303, Avg loss: 3.201, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 409]: seen 40900 examples : 46.6 eps, Loss: 3.132, Avg loss: 3.198, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 414]: seen 41400 examples : 46.6 eps, Loss: 3.347, Avg loss: 3.198, Best loss: 3.187, cov loss: 0.168\n",
      "    [batch 419]: seen 41900 examples : 46.6 eps, Loss: 3.134, Avg loss: 3.197, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 424]: seen 42400 examples : 46.6 eps, Loss: 3.421, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.174\n",
      "    [batch 429]: seen 42900 examples : 46.6 eps, Loss: 3.307, Avg loss: 3.200, Best loss: 3.187, cov loss: 0.171\n",
      "    [batch 434]: seen 43400 examples : 46.6 eps, Loss: 3.247, Avg loss: 3.200, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 439]: seen 43900 examples : 46.6 eps, Loss: 3.181, Avg loss: 3.198, Best loss: 3.187, cov loss: 0.154\n",
      "    [batch 444]: seen 44400 examples : 46.6 eps, Loss: 3.247, Avg loss: 3.196, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 449]: seen 44900 examples : 46.6 eps, Loss: 3.107, Avg loss: 3.192, Best loss: 3.187, cov loss: 0.129\n",
      "    [batch 454]: seen 45400 examples : 46.6 eps, Loss: 3.305, Avg loss: 3.190, Best loss: 3.187, cov loss: 0.166\n",
      "    [batch 459]: seen 45900 examples : 46.6 eps, Loss: 3.138, Avg loss: 3.192, Best loss: 3.187, cov loss: 0.151\n",
      "    [batch 464]: seen 46400 examples : 46.6 eps, Loss: 3.468, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.175\n",
      "    [batch 469]: seen 46900 examples : 46.6 eps, Loss: 3.162, Avg loss: 3.198, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 474]: seen 47400 examples : 46.6 eps, Loss: 3.150, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 479]: seen 47900 examples : 46.6 eps, Loss: 3.044, Avg loss: 3.195, Best loss: 3.187, cov loss: 0.132\n",
      "    [batch 484]: seen 48400 examples : 46.6 eps, Loss: 3.381, Avg loss: 3.198, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 489]: seen 48900 examples : 46.6 eps, Loss: 3.313, Avg loss: 3.198, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 494]: seen 49400 examples : 46.6 eps, Loss: 3.164, Avg loss: 3.198, Best loss: 3.187, cov loss: 0.147\n",
      "    [batch 499]: seen 49900 examples : 46.6 eps, Loss: 3.180, Avg loss: 3.196, Best loss: 3.187, cov loss: 0.158\n",
      "    [batch 504]: seen 50400 examples : 46.6 eps, Loss: 3.231, Avg loss: 3.195, Best loss: 3.187, cov loss: 0.160\n",
      "    [batch 509]: seen 50900 examples : 46.6 eps, Loss: 3.069, Avg loss: 3.198, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 514]: seen 51400 examples : 46.6 eps, Loss: 3.346, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.161\n",
      "    [batch 519]: seen 51900 examples : 46.6 eps, Loss: 3.071, Avg loss: 3.201, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 524]: seen 52400 examples : 46.6 eps, Loss: 2.942, Avg loss: 3.203, Best loss: 3.187, cov loss: 0.152\n",
      "    [batch 529]: seen 52900 examples : 46.6 eps, Loss: 3.163, Avg loss: 3.199, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 534]: seen 53400 examples : 46.6 eps, Loss: 3.168, Avg loss: 3.196, Best loss: 3.187, cov loss: 0.169\n",
      "    [batch 538]: seen 53800 examples : 46.6 eps, Loss: 3.106, Avg loss: 3.196, Best loss: 3.187, cov loss: 0.146\n",
      "    [batch 543]: seen 54300 examples : 46.6 eps, Loss: 3.053, Avg loss: 3.193, Best loss: 3.187, cov loss: 0.153\n",
      "    [batch 548]: seen 54800 examples : 46.6 eps, Loss: 3.263, Avg loss: 3.191, Best loss: 3.187, cov loss: 0.143\n",
      "    [batch 553]: seen 55300 examples : 46.6 eps, Loss: 3.053, Avg loss: 3.189, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 558]: seen 55800 examples : 46.6 eps, Loss: 3.292, Avg loss: 3.188, Best loss: 3.187, cov loss: 0.162\n",
      "    [batch 563]: seen 56300 examples : 46.6 eps, Loss: 3.118, Avg loss: 3.187, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 568]: seen 56800 examples : 46.6 eps, Loss: 3.015, Avg loss: 3.188, Best loss: 3.187, cov loss: 0.144\n",
      "    [batch 573]: seen 57300 examples : 46.6 eps, Loss: 3.056, Avg loss: 3.190, Best loss: 3.187, cov loss: 0.142\n",
      "    [batch 578]: seen 57800 examples : 46.6 eps, Loss: 3.130, Avg loss: 3.189, Best loss: 3.187, cov loss: 0.145\n",
      "    [batch 583]: seen 58300 examples : 46.6 eps, Loss: 3.202, Avg loss: 3.191, Best loss: 3.187, cov loss: 0.157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 588]: seen 58800 examples : 46.6 eps, Loss: 3.129, Avg loss: 3.189, Best loss: 3.187, cov loss: 0.159\n",
      "    [batch 593]: seen 59300 examples : 46.6 eps, Loss: 3.175, Avg loss: 3.188, Best loss: 3.187, cov loss: 0.150\n",
      "    [batch 598]: seen 59800 examples : 46.6 eps, Loss: 3.063, Avg loss: 3.189, Best loss: 3.187, cov loss: 0.137\n",
      "    [batch 603]: seen 60300 examples : 46.6 eps, Loss: 3.207, Avg loss: 3.189, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 608]: seen 60800 examples : 46.6 eps, Loss: 3.190, Avg loss: 3.191, Best loss: 3.187, cov loss: 0.149\n",
      "    [batch 613]: seen 61300 examples : 46.6 eps, Loss: 3.052, Avg loss: 3.191, Best loss: 3.187, cov loss: 0.156\n",
      "    [batch 618]: seen 61800 examples : 46.6 eps, Loss: 3.136, Avg loss: 3.192, Best loss: 3.187, cov loss: 0.140\n",
      "    [batch 623]: seen 62300 examples : 46.6 eps, Loss: 3.118, Avg loss: 3.191, Best loss: 3.187, cov loss: 0.164\n",
      "    [batch 627]: seen 62700 examples : 46.5 eps, Loss: 3.424, Avg loss: 3.189, Best loss: 3.186, cov loss: 0.170\n",
      "    [batch 632]: seen 63200 examples : 46.5 eps, Loss: 3.308, Avg loss: 3.187, Best loss: 3.186, cov loss: 0.169\n",
      "    [batch 637]: seen 63700 examples : 46.5 eps, Loss: 3.298, Avg loss: 3.188, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 642]: seen 64200 examples : 46.5 eps, Loss: 3.312, Avg loss: 3.189, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 647]: seen 64700 examples : 46.5 eps, Loss: 3.398, Avg loss: 3.192, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 652]: seen 65200 examples : 46.5 eps, Loss: 3.322, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.167\n",
      "    [batch 657]: seen 65700 examples : 46.5 eps, Loss: 3.226, Avg loss: 3.195, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 662]: seen 66200 examples : 46.5 eps, Loss: 3.123, Avg loss: 3.191, Best loss: 3.186, cov loss: 0.146\n",
      "    [batch 667]: seen 66700 examples : 46.5 eps, Loss: 3.101, Avg loss: 3.187, Best loss: 3.186, cov loss: 0.139\n",
      "    [batch 672]: seen 67200 examples : 46.5 eps, Loss: 3.239, Avg loss: 3.191, Best loss: 3.186, cov loss: 0.171\n",
      "    [batch 677]: seen 67700 examples : 46.5 eps, Loss: 3.200, Avg loss: 3.191, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 682]: seen 68200 examples : 46.5 eps, Loss: 3.338, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 687]: seen 68700 examples : 46.5 eps, Loss: 3.271, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 692]: seen 69200 examples : 46.5 eps, Loss: 3.298, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 697]: seen 69700 examples : 46.5 eps, Loss: 3.081, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 702]: seen 70200 examples : 46.5 eps, Loss: 3.112, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 707]: seen 70700 examples : 46.5 eps, Loss: 3.307, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 712]: seen 71200 examples : 46.5 eps, Loss: 2.995, Avg loss: 3.191, Best loss: 3.186, cov loss: 0.141\n",
      "    [batch 717]: seen 71700 examples : 46.5 eps, Loss: 3.160, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 721]: seen 72100 examples : 46.5 eps, Loss: 3.383, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 726]: seen 72600 examples : 46.5 eps, Loss: 3.197, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 731]: seen 73100 examples : 46.5 eps, Loss: 3.205, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 736]: seen 73600 examples : 46.5 eps, Loss: 3.185, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.168\n",
      "    [batch 741]: seen 74100 examples : 46.5 eps, Loss: 3.180, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 746]: seen 74600 examples : 46.5 eps, Loss: 3.214, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.168\n",
      "    [batch 751]: seen 75100 examples : 46.5 eps, Loss: 2.959, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.141\n",
      "    [batch 756]: seen 75600 examples : 46.5 eps, Loss: 3.325, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 761]: seen 76100 examples : 46.5 eps, Loss: 3.308, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 766]: seen 76600 examples : 46.5 eps, Loss: 3.105, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 771]: seen 77100 examples : 46.5 eps, Loss: 3.387, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 776]: seen 77600 examples : 46.5 eps, Loss: 3.085, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.145\n",
      "    [batch 781]: seen 78100 examples : 46.5 eps, Loss: 3.288, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 786]: seen 78600 examples : 46.5 eps, Loss: 3.392, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 791]: seen 79100 examples : 46.5 eps, Loss: 3.194, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 796]: seen 79600 examples : 46.5 eps, Loss: 3.319, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.171\n",
      "    [batch 801]: seen 80100 examples : 46.5 eps, Loss: 3.135, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 805]: seen 80500 examples : 46.5 eps, Loss: 3.222, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 810]: seen 81000 examples : 46.5 eps, Loss: 3.315, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 815]: seen 81500 examples : 46.5 eps, Loss: 3.385, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 820]: seen 82000 examples : 46.5 eps, Loss: 3.293, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 825]: seen 82500 examples : 46.5 eps, Loss: 3.078, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.130\n",
      "    [batch 830]: seen 83000 examples : 46.5 eps, Loss: 3.247, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.165\n",
      "    [batch 835]: seen 83500 examples : 46.5 eps, Loss: 3.166, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 840]: seen 84000 examples : 46.5 eps, Loss: 3.216, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.145\n",
      "    [batch 845]: seen 84500 examples : 46.5 eps, Loss: 3.211, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 850]: seen 85000 examples : 46.5 eps, Loss: 3.214, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 855]: seen 85500 examples : 46.5 eps, Loss: 3.128, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 860]: seen 86000 examples : 46.5 eps, Loss: 3.212, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 865]: seen 86500 examples : 46.5 eps, Loss: 3.204, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 870]: seen 87000 examples : 46.5 eps, Loss: 3.173, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.166\n",
      "    [batch 875]: seen 87500 examples : 46.5 eps, Loss: 3.287, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 880]: seen 88000 examples : 46.5 eps, Loss: 3.260, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.172\n",
      "    [batch 885]: seen 88500 examples : 46.5 eps, Loss: 3.232, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 890]: seen 89000 examples : 46.5 eps, Loss: 3.228, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.165\n",
      "    [batch 894]: seen 89400 examples : 46.5 eps, Loss: 3.363, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.167\n",
      "    [batch 899]: seen 89900 examples : 46.5 eps, Loss: 3.172, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.137\n",
      "    [batch 904]: seen 90400 examples : 46.5 eps, Loss: 3.308, Avg loss: 3.210, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 909]: seen 90900 examples : 46.5 eps, Loss: 3.182, Avg loss: 3.214, Best loss: 3.186, cov loss: 0.142\n",
      "    [batch 914]: seen 91400 examples : 46.5 eps, Loss: 3.266, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 919]: seen 91900 examples : 46.5 eps, Loss: 3.065, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.142\n",
      "    [batch 924]: seen 92400 examples : 46.5 eps, Loss: 3.276, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.166\n",
      "    [batch 929]: seen 92900 examples : 46.5 eps, Loss: 3.138, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.146\n",
      "    [batch 934]: seen 93400 examples : 46.5 eps, Loss: 3.281, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 939]: seen 93900 examples : 46.5 eps, Loss: 3.079, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 944]: seen 94400 examples : 46.5 eps, Loss: 3.109, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 949]: seen 94900 examples : 46.5 eps, Loss: 3.142, Avg loss: 3.210, Best loss: 3.186, cov loss: 0.145\n",
      "    [batch 954]: seen 95400 examples : 46.5 eps, Loss: 3.122, Avg loss: 3.209, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 959]: seen 95900 examples : 46.5 eps, Loss: 3.118, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 964]: seen 96400 examples : 46.5 eps, Loss: 3.279, Avg loss: 3.209, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 969]: seen 96900 examples : 46.5 eps, Loss: 3.419, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 974]: seen 97400 examples : 46.5 eps, Loss: 3.274, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 979]: seen 97900 examples : 46.5 eps, Loss: 3.146, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 983]: seen 98300 examples : 46.5 eps, Loss: 2.882, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.135\n",
      "    [batch 988]: seen 98800 examples : 46.4 eps, Loss: 3.507, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.173\n",
      "    [batch 993]: seen 99300 examples : 46.4 eps, Loss: 3.037, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.118\n",
      "    [batch 998]: seen 99800 examples : 46.4 eps, Loss: 3.259, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 1003]: seen 100300 examples : 46.5 eps, Loss: 3.153, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 1008]: seen 100800 examples : 46.5 eps, Loss: 2.942, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 1013]: seen 101300 examples : 46.5 eps, Loss: 3.094, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.140\n",
      "    [batch 1018]: seen 101800 examples : 46.5 eps, Loss: 3.287, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 1023]: seen 102300 examples : 46.5 eps, Loss: 3.281, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 1028]: seen 102800 examples : 46.5 eps, Loss: 3.127, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.165\n",
      "    [batch 1033]: seen 103300 examples : 46.5 eps, Loss: 3.128, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 1038]: seen 103800 examples : 46.5 eps, Loss: 3.258, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1043]: seen 104300 examples : 46.5 eps, Loss: 3.310, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.165\n",
      "    [batch 1048]: seen 104800 examples : 46.5 eps, Loss: 3.246, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 1053]: seen 105300 examples : 46.5 eps, Loss: 3.267, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 1058]: seen 105800 examples : 46.5 eps, Loss: 3.143, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.143\n",
      "    [batch 1063]: seen 106300 examples : 46.5 eps, Loss: 3.209, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 1068]: seen 106800 examples : 46.5 eps, Loss: 3.346, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 1073]: seen 107300 examples : 46.5 eps, Loss: 3.178, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.146\n",
      "    [batch 1077]: seen 107700 examples : 46.4 eps, Loss: 3.118, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.144\n",
      "    [batch 1082]: seen 108200 examples : 46.4 eps, Loss: 3.428, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.175\n",
      "    [batch 1087]: seen 108700 examples : 46.4 eps, Loss: 3.125, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 1092]: seen 109200 examples : 46.4 eps, Loss: 3.079, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 1097]: seen 109700 examples : 46.4 eps, Loss: 3.027, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.137\n",
      "    [batch 1102]: seen 110200 examples : 46.4 eps, Loss: 3.218, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.165\n",
      "    [batch 1107]: seen 110700 examples : 46.4 eps, Loss: 3.363, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 1112]: seen 111200 examples : 46.4 eps, Loss: 3.225, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.167\n",
      "    [batch 1117]: seen 111700 examples : 46.4 eps, Loss: 3.150, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 1122]: seen 112200 examples : 46.4 eps, Loss: 3.312, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 1127]: seen 112700 examples : 46.4 eps, Loss: 3.269, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 1132]: seen 113200 examples : 46.4 eps, Loss: 3.260, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.171\n",
      "    [batch 1137]: seen 113700 examples : 46.4 eps, Loss: 3.027, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.141\n",
      "    [batch 1142]: seen 114200 examples : 46.5 eps, Loss: 3.217, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 1147]: seen 114700 examples : 46.5 eps, Loss: 3.132, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 1152]: seen 115200 examples : 46.5 eps, Loss: 3.154, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 1157]: seen 115700 examples : 46.5 eps, Loss: 3.310, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1161]: seen 116100 examples : 46.4 eps, Loss: 3.363, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1166]: seen 116600 examples : 46.4 eps, Loss: 3.033, Avg loss: 3.188, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 1171]: seen 117100 examples : 46.4 eps, Loss: 3.081, Avg loss: 3.188, Best loss: 3.186, cov loss: 0.165\n",
      "    [batch 1176]: seen 117600 examples : 46.4 eps, Loss: 3.370, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 1181]: seen 118100 examples : 46.4 eps, Loss: 3.232, Avg loss: 3.192, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 1186]: seen 118600 examples : 46.4 eps, Loss: 3.222, Avg loss: 3.192, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 1191]: seen 119100 examples : 46.4 eps, Loss: 2.985, Avg loss: 3.191, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 1196]: seen 119600 examples : 46.4 eps, Loss: 3.230, Avg loss: 3.192, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 1201]: seen 120100 examples : 46.4 eps, Loss: 3.385, Avg loss: 3.195, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 1206]: seen 120600 examples : 46.4 eps, Loss: 3.360, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 1211]: seen 121100 examples : 46.4 eps, Loss: 3.187, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.139\n",
      "    [batch 1216]: seen 121600 examples : 46.4 eps, Loss: 3.293, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1221]: seen 122100 examples : 46.4 eps, Loss: 3.149, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.170\n",
      "    [batch 1226]: seen 122600 examples : 46.4 eps, Loss: 3.214, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.145\n",
      "    [batch 1231]: seen 123100 examples : 46.4 eps, Loss: 3.245, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 1236]: seen 123600 examples : 46.4 eps, Loss: 3.191, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 1241]: seen 124100 examples : 46.4 eps, Loss: 3.137, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.176\n",
      "    [batch 1246]: seen 124600 examples : 46.4 eps, Loss: 3.228, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1250]: seen 125000 examples : 46.4 eps, Loss: 3.143, Avg loss: 3.195, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 1255]: seen 125500 examples : 46.4 eps, Loss: 3.251, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1260]: seen 126000 examples : 46.4 eps, Loss: 3.401, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 1265]: seen 126500 examples : 46.4 eps, Loss: 3.338, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 1270]: seen 127000 examples : 46.4 eps, Loss: 3.223, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1275]: seen 127500 examples : 46.4 eps, Loss: 3.169, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.137\n",
      "    [batch 1280]: seen 128000 examples : 46.4 eps, Loss: 3.280, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 1285]: seen 128500 examples : 46.4 eps, Loss: 3.203, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 1290]: seen 129000 examples : 46.4 eps, Loss: 3.208, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.171\n",
      "    [batch 1295]: seen 129500 examples : 46.4 eps, Loss: 3.271, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 1300]: seen 130000 examples : 46.4 eps, Loss: 3.354, Avg loss: 3.195, Best loss: 3.186, cov loss: 0.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1305]: seen 130500 examples : 46.4 eps, Loss: 3.200, Avg loss: 3.195, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 1310]: seen 131000 examples : 46.4 eps, Loss: 3.161, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 1315]: seen 131500 examples : 46.4 eps, Loss: 3.224, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 1320]: seen 132000 examples : 46.4 eps, Loss: 3.247, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 1325]: seen 132500 examples : 46.4 eps, Loss: 3.251, Avg loss: 3.195, Best loss: 3.186, cov loss: 0.139\n",
      "    [batch 1330]: seen 133000 examples : 46.4 eps, Loss: 3.595, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.172\n",
      "    [batch 1335]: seen 133500 examples : 46.4 eps, Loss: 3.316, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1340]: seen 134000 examples : 46.4 eps, Loss: 3.097, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 1344]: seen 134400 examples : 46.4 eps, Loss: 3.230, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 1349]: seen 134900 examples : 46.4 eps, Loss: 3.160, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.168\n",
      "    [batch 1354]: seen 135400 examples : 46.4 eps, Loss: 3.060, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 1359]: seen 135900 examples : 46.4 eps, Loss: 2.991, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 1364]: seen 136400 examples : 46.4 eps, Loss: 3.379, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 1369]: seen 136900 examples : 46.4 eps, Loss: 3.058, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 1374]: seen 137400 examples : 46.4 eps, Loss: 3.267, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.144\n",
      "    [batch 1379]: seen 137900 examples : 46.4 eps, Loss: 3.138, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 1384]: seen 138400 examples : 46.4 eps, Loss: 3.195, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 1389]: seen 138900 examples : 46.4 eps, Loss: 3.108, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1394]: seen 139400 examples : 46.4 eps, Loss: 3.307, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 1399]: seen 139900 examples : 46.4 eps, Loss: 3.253, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 1404]: seen 140400 examples : 46.4 eps, Loss: 3.292, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.165\n",
      "    [batch 1409]: seen 140900 examples : 46.4 eps, Loss: 3.057, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1414]: seen 141400 examples : 46.4 eps, Loss: 3.314, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 1419]: seen 141900 examples : 46.4 eps, Loss: 3.276, Avg loss: 3.191, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 1424]: seen 142400 examples : 46.4 eps, Loss: 3.255, Avg loss: 3.192, Best loss: 3.186, cov loss: 0.137\n",
      "    [batch 1428]: seen 142800 examples : 46.4 eps, Loss: 3.193, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 1433]: seen 143300 examples : 46.4 eps, Loss: 2.984, Avg loss: 3.191, Best loss: 3.186, cov loss: 0.144\n",
      "    [batch 1438]: seen 143800 examples : 46.4 eps, Loss: 3.149, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 1443]: seen 144300 examples : 46.4 eps, Loss: 3.116, Avg loss: 3.192, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 1448]: seen 144800 examples : 46.4 eps, Loss: 3.128, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 1453]: seen 145300 examples : 46.4 eps, Loss: 2.919, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 1458]: seen 145800 examples : 46.4 eps, Loss: 3.214, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1463]: seen 146300 examples : 46.4 eps, Loss: 3.065, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.145\n",
      "    [batch 1468]: seen 146800 examples : 46.4 eps, Loss: 3.135, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 1473]: seen 147300 examples : 46.4 eps, Loss: 3.447, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1478]: seen 147800 examples : 46.4 eps, Loss: 3.242, Avg loss: 3.195, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 1483]: seen 148300 examples : 46.4 eps, Loss: 3.256, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 1488]: seen 148800 examples : 46.4 eps, Loss: 3.081, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 1493]: seen 149300 examples : 46.4 eps, Loss: 3.275, Avg loss: 3.195, Best loss: 3.186, cov loss: 0.174\n",
      "    [batch 1498]: seen 149800 examples : 46.4 eps, Loss: 3.119, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 1503]: seen 150300 examples : 46.4 eps, Loss: 3.131, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.141\n",
      "    [batch 1508]: seen 150800 examples : 46.4 eps, Loss: 3.141, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 1513]: seen 151300 examples : 46.4 eps, Loss: 3.210, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 1517]: seen 151700 examples : 46.4 eps, Loss: 3.300, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 1522]: seen 152200 examples : 46.4 eps, Loss: 3.115, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 1527]: seen 152700 examples : 46.4 eps, Loss: 3.035, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 1532]: seen 153200 examples : 46.4 eps, Loss: 3.164, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1537]: seen 153700 examples : 46.4 eps, Loss: 3.350, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.141\n",
      "    [batch 1542]: seen 154200 examples : 46.4 eps, Loss: 3.195, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 1547]: seen 154700 examples : 46.4 eps, Loss: 2.956, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 1552]: seen 155200 examples : 46.4 eps, Loss: 3.463, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1557]: seen 155700 examples : 46.4 eps, Loss: 3.213, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1562]: seen 156200 examples : 46.4 eps, Loss: 3.443, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 1567]: seen 156700 examples : 46.4 eps, Loss: 3.226, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 1572]: seen 157200 examples : 46.4 eps, Loss: 3.273, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.141\n",
      "    [batch 1577]: seen 157700 examples : 46.4 eps, Loss: 3.330, Avg loss: 3.209, Best loss: 3.186, cov loss: 0.175\n",
      "    [batch 1582]: seen 158200 examples : 46.4 eps, Loss: 3.126, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 1587]: seen 158700 examples : 46.4 eps, Loss: 3.319, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 1592]: seen 159200 examples : 46.4 eps, Loss: 3.245, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 1597]: seen 159700 examples : 46.4 eps, Loss: 3.180, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 1602]: seen 160200 examples : 46.4 eps, Loss: 3.055, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 1606]: seen 160600 examples : 46.4 eps, Loss: 3.144, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.146\n",
      "    [batch 1611]: seen 161100 examples : 46.4 eps, Loss: 3.233, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.144\n",
      "    [batch 1616]: seen 161600 examples : 46.4 eps, Loss: 3.272, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.171\n",
      "    [batch 1621]: seen 162100 examples : 46.4 eps, Loss: 3.172, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 1626]: seen 162600 examples : 46.4 eps, Loss: 3.293, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 1631]: seen 163100 examples : 46.4 eps, Loss: 3.252, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1636]: seen 163600 examples : 46.4 eps, Loss: 2.939, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.138\n",
      "    [batch 1641]: seen 164100 examples : 46.4 eps, Loss: 3.170, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 1646]: seen 164600 examples : 46.4 eps, Loss: 3.276, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.171\n",
      "    [batch 1651]: seen 165100 examples : 46.4 eps, Loss: 3.197, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.168\n",
      "    [batch 1656]: seen 165600 examples : 46.4 eps, Loss: 3.384, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1661]: seen 166100 examples : 46.4 eps, Loss: 3.013, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 1666]: seen 166600 examples : 46.4 eps, Loss: 3.141, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 1671]: seen 167100 examples : 46.4 eps, Loss: 3.363, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 1676]: seen 167600 examples : 46.4 eps, Loss: 3.232, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 1681]: seen 168100 examples : 46.4 eps, Loss: 3.026, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.146\n",
      "    [batch 1686]: seen 168600 examples : 46.4 eps, Loss: 3.113, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 1691]: seen 169100 examples : 46.4 eps, Loss: 3.142, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1696]: seen 169600 examples : 46.4 eps, Loss: 3.277, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 1701]: seen 170100 examples : 46.4 eps, Loss: 3.144, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 1706]: seen 170600 examples : 46.4 eps, Loss: 3.307, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 1711]: seen 171100 examples : 46.4 eps, Loss: 3.275, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.170\n",
      "    [batch 1716]: seen 171600 examples : 46.4 eps, Loss: 3.355, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 1721]: seen 172100 examples : 46.4 eps, Loss: 3.353, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 1726]: seen 172600 examples : 46.4 eps, Loss: 3.204, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 1731]: seen 173100 examples : 46.4 eps, Loss: 3.316, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 1736]: seen 173600 examples : 46.4 eps, Loss: 3.127, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 1741]: seen 174100 examples : 46.4 eps, Loss: 3.064, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 1746]: seen 174600 examples : 46.4 eps, Loss: 3.212, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 1751]: seen 175100 examples : 46.4 eps, Loss: 3.228, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 1756]: seen 175600 examples : 46.4 eps, Loss: 3.302, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 1761]: seen 176100 examples : 46.4 eps, Loss: 3.129, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 1766]: seen 176600 examples : 46.4 eps, Loss: 3.279, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 1771]: seen 177100 examples : 46.4 eps, Loss: 2.927, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 1776]: seen 177600 examples : 46.4 eps, Loss: 3.419, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 1781]: seen 178100 examples : 46.4 eps, Loss: 3.240, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 1785]: seen 178500 examples : 46.4 eps, Loss: 3.297, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.168\n",
      "    [batch 1790]: seen 179000 examples : 46.4 eps, Loss: 3.459, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.181\n",
      "    [batch 1795]: seen 179500 examples : 46.4 eps, Loss: 3.075, Avg loss: 3.209, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 1800]: seen 180000 examples : 46.4 eps, Loss: 3.409, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.171\n",
      "    [batch 1805]: seen 180500 examples : 46.4 eps, Loss: 3.175, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 1810]: seen 181000 examples : 46.4 eps, Loss: 3.313, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.145\n",
      "    [batch 1815]: seen 181500 examples : 46.4 eps, Loss: 3.337, Avg loss: 3.216, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1820]: seen 182000 examples : 46.4 eps, Loss: 3.394, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 1825]: seen 182500 examples : 46.4 eps, Loss: 3.090, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 1830]: seen 183000 examples : 46.4 eps, Loss: 3.114, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 1835]: seen 183500 examples : 46.4 eps, Loss: 3.157, Avg loss: 3.209, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 1840]: seen 184000 examples : 46.4 eps, Loss: 3.203, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 1845]: seen 184500 examples : 46.4 eps, Loss: 3.153, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 1850]: seen 185000 examples : 46.4 eps, Loss: 3.218, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 1855]: seen 185500 examples : 46.4 eps, Loss: 3.293, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 1860]: seen 186000 examples : 46.4 eps, Loss: 3.207, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 1865]: seen 186500 examples : 46.4 eps, Loss: 3.243, Avg loss: 3.217, Best loss: 3.186, cov loss: 0.165\n",
      "    [batch 1870]: seen 187000 examples : 46.4 eps, Loss: 3.308, Avg loss: 3.221, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 1874]: seen 187400 examples : 46.4 eps, Loss: 3.247, Avg loss: 3.221, Best loss: 3.186, cov loss: 0.172\n",
      "    [batch 1879]: seen 187900 examples : 46.4 eps, Loss: 3.396, Avg loss: 3.221, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1884]: seen 188400 examples : 46.4 eps, Loss: 3.230, Avg loss: 3.221, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 1889]: seen 188900 examples : 46.4 eps, Loss: 3.113, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.146\n",
      "    [batch 1894]: seen 189400 examples : 46.4 eps, Loss: 3.013, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.143\n",
      "    [batch 1899]: seen 189900 examples : 46.4 eps, Loss: 3.359, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1904]: seen 190400 examples : 46.4 eps, Loss: 3.145, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 1909]: seen 190900 examples : 46.4 eps, Loss: 3.278, Avg loss: 3.216, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 1914]: seen 191400 examples : 46.4 eps, Loss: 3.188, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1919]: seen 191900 examples : 46.4 eps, Loss: 3.032, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1924]: seen 192400 examples : 46.4 eps, Loss: 3.125, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1929]: seen 192900 examples : 46.4 eps, Loss: 3.183, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 1934]: seen 193400 examples : 46.4 eps, Loss: 3.339, Avg loss: 3.210, Best loss: 3.186, cov loss: 0.169\n",
      "    [batch 1939]: seen 193900 examples : 46.4 eps, Loss: 3.190, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1944]: seen 194400 examples : 46.4 eps, Loss: 3.358, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 1949]: seen 194900 examples : 46.4 eps, Loss: 3.322, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 1954]: seen 195400 examples : 46.4 eps, Loss: 3.126, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 1959]: seen 195900 examples : 46.4 eps, Loss: 3.287, Avg loss: 3.216, Best loss: 3.186, cov loss: 0.146\n",
      "    [batch 1964]: seen 196400 examples : 46.4 eps, Loss: 3.132, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.135\n",
      "    [batch 1969]: seen 196900 examples : 46.4 eps, Loss: 3.202, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 1974]: seen 197400 examples : 46.4 eps, Loss: 3.292, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 1979]: seen 197900 examples : 46.4 eps, Loss: 3.356, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 1984]: seen 198400 examples : 46.4 eps, Loss: 3.208, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 1989]: seen 198900 examples : 46.4 eps, Loss: 3.213, Avg loss: 3.214, Best loss: 3.186, cov loss: 0.141\n",
      "    [batch 1994]: seen 199400 examples : 46.4 eps, Loss: 3.235, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 1999]: seen 199900 examples : 46.4 eps, Loss: 3.186, Avg loss: 3.214, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 2004]: seen 200400 examples : 46.4 eps, Loss: 3.361, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.142\n",
      "    [batch 2009]: seen 200900 examples : 46.4 eps, Loss: 3.199, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 2014]: seen 201400 examples : 46.4 eps, Loss: 3.163, Avg loss: 3.214, Best loss: 3.186, cov loss: 0.156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2019]: seen 201900 examples : 46.4 eps, Loss: 3.219, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 2024]: seen 202400 examples : 46.4 eps, Loss: 3.108, Avg loss: 3.210, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 2029]: seen 202900 examples : 46.4 eps, Loss: 3.207, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 2034]: seen 203400 examples : 46.4 eps, Loss: 3.206, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 2039]: seen 203900 examples : 46.4 eps, Loss: 3.242, Avg loss: 3.210, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 2044]: seen 204400 examples : 46.4 eps, Loss: 3.238, Avg loss: 3.209, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 2049]: seen 204900 examples : 46.4 eps, Loss: 3.202, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 2053]: seen 205300 examples : 46.4 eps, Loss: 3.442, Avg loss: 3.214, Best loss: 3.186, cov loss: 0.165\n",
      "    [batch 2058]: seen 205800 examples : 46.4 eps, Loss: 3.223, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 2063]: seen 206300 examples : 46.4 eps, Loss: 3.052, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 2068]: seen 206800 examples : 46.4 eps, Loss: 3.233, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 2073]: seen 207300 examples : 46.4 eps, Loss: 3.309, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.167\n",
      "    [batch 2078]: seen 207800 examples : 46.4 eps, Loss: 3.151, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 2083]: seen 208300 examples : 46.4 eps, Loss: 3.341, Avg loss: 3.216, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 2088]: seen 208800 examples : 46.4 eps, Loss: 3.145, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.184\n",
      "    [batch 2093]: seen 209300 examples : 46.4 eps, Loss: 3.036, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 2098]: seen 209800 examples : 46.4 eps, Loss: 3.228, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 2103]: seen 210300 examples : 46.4 eps, Loss: 3.059, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.137\n",
      "    [batch 2108]: seen 210800 examples : 46.4 eps, Loss: 3.171, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.143\n",
      "    [batch 2113]: seen 211300 examples : 46.4 eps, Loss: 3.326, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 2118]: seen 211800 examples : 46.4 eps, Loss: 3.116, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.166\n",
      "    [batch 2123]: seen 212300 examples : 46.4 eps, Loss: 3.277, Avg loss: 3.209, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 2128]: seen 212800 examples : 46.4 eps, Loss: 3.312, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.167\n",
      "    [batch 2133]: seen 213300 examples : 46.4 eps, Loss: 3.328, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 2138]: seen 213800 examples : 46.4 eps, Loss: 3.176, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 2143]: seen 214300 examples : 46.4 eps, Loss: 3.255, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 2148]: seen 214800 examples : 46.4 eps, Loss: 2.881, Avg loss: 3.210, Best loss: 3.186, cov loss: 0.139\n",
      "    [batch 2153]: seen 215300 examples : 46.4 eps, Loss: 3.279, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 2158]: seen 215800 examples : 46.4 eps, Loss: 3.253, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 2163]: seen 216300 examples : 46.4 eps, Loss: 3.138, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 2168]: seen 216800 examples : 46.4 eps, Loss: 3.236, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 2173]: seen 217300 examples : 46.4 eps, Loss: 3.121, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 2178]: seen 217800 examples : 46.4 eps, Loss: 3.220, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.172\n",
      "    [batch 2183]: seen 218300 examples : 46.4 eps, Loss: 3.285, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.165\n",
      "    [batch 2188]: seen 218800 examples : 46.4 eps, Loss: 3.133, Avg loss: 3.209, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 2193]: seen 219300 examples : 46.4 eps, Loss: 2.986, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 2198]: seen 219800 examples : 46.4 eps, Loss: 3.204, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 2203]: seen 220300 examples : 46.4 eps, Loss: 3.215, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 2208]: seen 220800 examples : 46.4 eps, Loss: 3.208, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 2213]: seen 221300 examples : 46.4 eps, Loss: 3.148, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 2218]: seen 221800 examples : 46.4 eps, Loss: 3.406, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.166\n",
      "    [batch 2223]: seen 222300 examples : 46.4 eps, Loss: 3.091, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 2228]: seen 222800 examples : 46.4 eps, Loss: 3.241, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.134\n",
      "    [batch 2232]: seen 223200 examples : 46.4 eps, Loss: 3.253, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 2237]: seen 223700 examples : 46.4 eps, Loss: 3.070, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.144\n",
      "    [batch 2242]: seen 224200 examples : 46.4 eps, Loss: 3.337, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 2247]: seen 224700 examples : 46.4 eps, Loss: 3.233, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 2252]: seen 225200 examples : 46.4 eps, Loss: 3.190, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.141\n",
      "    [batch 2257]: seen 225700 examples : 46.4 eps, Loss: 3.002, Avg loss: 3.190, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 2262]: seen 226200 examples : 46.4 eps, Loss: 3.038, Avg loss: 3.187, Best loss: 3.186, cov loss: 0.144\n",
      "    [batch 2267]: seen 226700 examples : 46.4 eps, Loss: 3.134, Avg loss: 3.189, Best loss: 3.186, cov loss: 0.146\n",
      "    [batch 2272]: seen 227200 examples : 46.4 eps, Loss: 3.321, Avg loss: 3.192, Best loss: 3.186, cov loss: 0.167\n",
      "    [batch 2277]: seen 227700 examples : 46.4 eps, Loss: 3.120, Avg loss: 3.195, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 2282]: seen 228200 examples : 46.4 eps, Loss: 3.260, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 2287]: seen 228700 examples : 46.4 eps, Loss: 3.332, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.139\n",
      "    [batch 2292]: seen 229200 examples : 46.4 eps, Loss: 3.129, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 2297]: seen 229700 examples : 46.4 eps, Loss: 3.191, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 2302]: seen 230200 examples : 46.4 eps, Loss: 3.181, Avg loss: 3.195, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 2307]: seen 230700 examples : 46.4 eps, Loss: 3.199, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 2312]: seen 231200 examples : 46.4 eps, Loss: 3.160, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 2317]: seen 231700 examples : 46.4 eps, Loss: 3.297, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 2322]: seen 232200 examples : 46.4 eps, Loss: 3.085, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.144\n",
      "    [batch 2327]: seen 232700 examples : 46.4 eps, Loss: 3.123, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 2332]: seen 233200 examples : 46.4 eps, Loss: 3.207, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.144\n",
      "    [batch 2337]: seen 233700 examples : 46.4 eps, Loss: 3.115, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 2342]: seen 234200 examples : 46.4 eps, Loss: 3.250, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 2347]: seen 234700 examples : 46.4 eps, Loss: 3.138, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 2352]: seen 235200 examples : 46.4 eps, Loss: 3.256, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 2357]: seen 235700 examples : 46.4 eps, Loss: 3.276, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 2362]: seen 236200 examples : 46.4 eps, Loss: 3.274, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 2367]: seen 236700 examples : 46.4 eps, Loss: 3.228, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 2372]: seen 237200 examples : 46.4 eps, Loss: 3.228, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2377]: seen 237700 examples : 46.4 eps, Loss: 3.203, Avg loss: 3.209, Best loss: 3.186, cov loss: 0.128\n",
      "    [batch 2382]: seen 238200 examples : 46.4 eps, Loss: 3.068, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 2387]: seen 238700 examples : 46.4 eps, Loss: 3.265, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 2392]: seen 239200 examples : 46.4 eps, Loss: 3.125, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 2397]: seen 239700 examples : 46.4 eps, Loss: 3.319, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 2402]: seen 240200 examples : 46.4 eps, Loss: 3.080, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.167\n",
      "    [batch 2407]: seen 240700 examples : 46.4 eps, Loss: 3.252, Avg loss: 3.207, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 2412]: seen 241200 examples : 46.4 eps, Loss: 3.153, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.145\n",
      "    [batch 2417]: seen 241700 examples : 46.4 eps, Loss: 3.117, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 2422]: seen 242200 examples : 46.4 eps, Loss: 3.276, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 2427]: seen 242700 examples : 46.4 eps, Loss: 3.188, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 2432]: seen 243200 examples : 46.4 eps, Loss: 3.254, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 2437]: seen 243700 examples : 46.4 eps, Loss: 3.131, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.146\n",
      "    [batch 2442]: seen 244200 examples : 46.4 eps, Loss: 3.155, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.146\n",
      "    [batch 2447]: seen 244700 examples : 46.4 eps, Loss: 3.098, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.142\n",
      "    [batch 2452]: seen 245200 examples : 46.4 eps, Loss: 3.251, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 2457]: seen 245700 examples : 46.4 eps, Loss: 3.336, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 2462]: seen 246200 examples : 46.4 eps, Loss: 3.332, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 2467]: seen 246700 examples : 46.4 eps, Loss: 3.399, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 2472]: seen 247200 examples : 46.4 eps, Loss: 3.132, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.146\n",
      "    [batch 2477]: seen 247700 examples : 46.4 eps, Loss: 3.302, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 2482]: seen 248200 examples : 46.4 eps, Loss: 3.297, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.172\n",
      "    [batch 2487]: seen 248700 examples : 46.4 eps, Loss: 3.101, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.158\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-59888\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-59888\n",
      "    [batch 2491]: seen 249100 examples : 46.4 eps, Loss: 3.026, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 2496]: seen 249600 examples : 46.4 eps, Loss: 3.234, Avg loss: 3.209, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 2500]: seen 250000 examples : 46.4 eps, Loss: 3.408, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 2505]: seen 250500 examples : 46.4 eps, Loss: 3.171, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 2510]: seen 251000 examples : 46.4 eps, Loss: 3.244, Avg loss: 3.216, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 2515]: seen 251500 examples : 46.4 eps, Loss: 3.258, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 2520]: seen 252000 examples : 46.4 eps, Loss: 3.089, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 2525]: seen 252500 examples : 46.4 eps, Loss: 3.271, Avg loss: 3.214, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 2530]: seen 253000 examples : 46.4 eps, Loss: 3.247, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 2535]: seen 253500 examples : 46.4 eps, Loss: 3.135, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 2540]: seen 254000 examples : 46.4 eps, Loss: 3.318, Avg loss: 3.216, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 2545]: seen 254500 examples : 46.4 eps, Loss: 3.260, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.141\n",
      "    [batch 2550]: seen 255000 examples : 46.4 eps, Loss: 3.154, Avg loss: 3.210, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 2555]: seen 255500 examples : 46.4 eps, Loss: 3.125, Avg loss: 3.209, Best loss: 3.186, cov loss: 0.141\n",
      "    [batch 2560]: seen 256000 examples : 46.4 eps, Loss: 3.348, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.166\n",
      "    [batch 2565]: seen 256500 examples : 46.4 eps, Loss: 3.234, Avg loss: 3.214, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 2570]: seen 257000 examples : 46.4 eps, Loss: 3.055, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 2575]: seen 257500 examples : 46.4 eps, Loss: 3.039, Avg loss: 3.210, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 2580]: seen 258000 examples : 46.4 eps, Loss: 3.338, Avg loss: 3.211, Best loss: 3.186, cov loss: 0.143\n",
      "    [batch 2585]: seen 258500 examples : 46.4 eps, Loss: 3.131, Avg loss: 3.213, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 2589]: seen 258900 examples : 46.4 eps, Loss: 3.054, Avg loss: 3.214, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 2594]: seen 259400 examples : 46.4 eps, Loss: 3.214, Avg loss: 3.212, Best loss: 3.186, cov loss: 0.168\n",
      "    [batch 2599]: seen 259900 examples : 46.4 eps, Loss: 3.242, Avg loss: 3.217, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 2604]: seen 260400 examples : 46.4 eps, Loss: 3.373, Avg loss: 3.218, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 2609]: seen 260900 examples : 46.4 eps, Loss: 3.238, Avg loss: 3.220, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 2614]: seen 261400 examples : 46.4 eps, Loss: 3.246, Avg loss: 3.219, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 2619]: seen 261900 examples : 46.4 eps, Loss: 3.179, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 2624]: seen 262400 examples : 46.4 eps, Loss: 3.155, Avg loss: 3.216, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 2629]: seen 262900 examples : 46.4 eps, Loss: 3.306, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.152\n",
      "    [batch 2634]: seen 263400 examples : 46.4 eps, Loss: 3.230, Avg loss: 3.216, Best loss: 3.186, cov loss: 0.184\n",
      "    [batch 2639]: seen 263900 examples : 46.4 eps, Loss: 3.100, Avg loss: 3.215, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 2644]: seen 264400 examples : 46.4 eps, Loss: 3.271, Avg loss: 3.214, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 2649]: seen 264900 examples : 46.4 eps, Loss: 3.106, Avg loss: 3.214, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 2654]: seen 265400 examples : 46.4 eps, Loss: 3.260, Avg loss: 3.208, Best loss: 3.186, cov loss: 0.164\n",
      "    [batch 2659]: seen 265900 examples : 46.4 eps, Loss: 3.109, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 2664]: seen 266400 examples : 46.4 eps, Loss: 3.240, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 2669]: seen 266900 examples : 46.4 eps, Loss: 3.299, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 2674]: seen 267400 examples : 46.4 eps, Loss: 3.093, Avg loss: 3.206, Best loss: 3.186, cov loss: 0.144\n",
      "    [batch 2678]: seen 267800 examples : 46.4 eps, Loss: 3.109, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.161\n",
      "    [batch 2683]: seen 268300 examples : 46.4 eps, Loss: 3.122, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.156\n",
      "    [batch 2688]: seen 268800 examples : 46.4 eps, Loss: 3.132, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.151\n",
      "    [batch 2693]: seen 269300 examples : 46.4 eps, Loss: 3.189, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 2698]: seen 269800 examples : 46.4 eps, Loss: 3.278, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 2703]: seen 270300 examples : 46.4 eps, Loss: 3.089, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 2708]: seen 270800 examples : 46.4 eps, Loss: 3.130, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.143\n",
      "    [batch 2713]: seen 271300 examples : 46.4 eps, Loss: 3.042, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2718]: seen 271800 examples : 46.4 eps, Loss: 3.131, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 2723]: seen 272300 examples : 46.4 eps, Loss: 3.266, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 2728]: seen 272800 examples : 46.4 eps, Loss: 3.177, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 2733]: seen 273300 examples : 46.4 eps, Loss: 3.041, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 2738]: seen 273800 examples : 46.4 eps, Loss: 3.216, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.150\n",
      "    [batch 2743]: seen 274300 examples : 46.4 eps, Loss: 3.231, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 2748]: seen 274800 examples : 46.4 eps, Loss: 3.022, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 2753]: seen 275300 examples : 46.4 eps, Loss: 3.232, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 2758]: seen 275800 examples : 46.4 eps, Loss: 3.170, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 2763]: seen 276300 examples : 46.4 eps, Loss: 3.211, Avg loss: 3.203, Best loss: 3.186, cov loss: 0.131\n",
      "    [batch 2767]: seen 276700 examples : 46.4 eps, Loss: 3.141, Avg loss: 3.202, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 2772]: seen 277200 examples : 46.4 eps, Loss: 3.183, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 2777]: seen 277700 examples : 46.4 eps, Loss: 3.218, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.147\n",
      "    [batch 2782]: seen 278200 examples : 46.4 eps, Loss: 3.135, Avg loss: 3.192, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 2787]: seen 278700 examples : 46.4 eps, Loss: 3.240, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.158\n",
      "    [batch 2792]: seen 279200 examples : 46.4 eps, Loss: 3.187, Avg loss: 3.192, Best loss: 3.186, cov loss: 0.166\n",
      "    [batch 2797]: seen 279700 examples : 46.4 eps, Loss: 3.194, Avg loss: 3.195, Best loss: 3.186, cov loss: 0.146\n",
      "    [batch 2802]: seen 280200 examples : 46.4 eps, Loss: 3.218, Avg loss: 3.198, Best loss: 3.186, cov loss: 0.162\n",
      "    [batch 2807]: seen 280700 examples : 46.4 eps, Loss: 3.226, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.149\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:40:57\n",
      "[EPOCH 28] Complete. Avg Loss: 3.201105022342864; Best Loss: 3.185970396634995\n",
      "[EPOCH 29] Starting training..\n",
      "    [batch 7]: seen 700 examples : 68.2 eps, Loss: 3.290, Avg loss: 3.199, Best loss: 3.186, cov loss: 0.169\n",
      "    [batch 12]: seen 1200 examples : 57.3 eps, Loss: 3.198, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.155\n",
      "    [batch 17]: seen 1700 examples : 53.7 eps, Loss: 3.229, Avg loss: 3.200, Best loss: 3.186, cov loss: 0.160\n",
      "    [batch 22]: seen 2200 examples : 52.0 eps, Loss: 3.282, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.149\n",
      "    [batch 27]: seen 2700 examples : 50.9 eps, Loss: 3.160, Avg loss: 3.201, Best loss: 3.186, cov loss: 0.153\n",
      "    [batch 32]: seen 3200 examples : 50.2 eps, Loss: 3.333, Avg loss: 3.205, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 37]: seen 3700 examples : 49.7 eps, Loss: 2.974, Avg loss: 3.204, Best loss: 3.186, cov loss: 0.135\n",
      "    [batch 42]: seen 4200 examples : 49.3 eps, Loss: 2.892, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.140\n",
      "    [batch 47]: seen 4700 examples : 49.1 eps, Loss: 3.270, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.163\n",
      "    [batch 52]: seen 5200 examples : 48.8 eps, Loss: 3.216, Avg loss: 3.194, Best loss: 3.186, cov loss: 0.145\n",
      "    [batch 57]: seen 5700 examples : 48.3 eps, Loss: 3.295, Avg loss: 3.196, Best loss: 3.186, cov loss: 0.168\n",
      "    [batch 62]: seen 6200 examples : 48.2 eps, Loss: 3.137, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.159\n",
      "    [batch 67]: seen 6700 examples : 48.1 eps, Loss: 3.222, Avg loss: 3.197, Best loss: 3.186, cov loss: 0.173\n",
      "    [batch 72]: seen 7200 examples : 48.0 eps, Loss: 3.079, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.148\n",
      "    [batch 77]: seen 7700 examples : 47.9 eps, Loss: 3.158, Avg loss: 3.193, Best loss: 3.186, cov loss: 0.145\n",
      "    [batch 82]: seen 8200 examples : 47.8 eps, Loss: 3.102, Avg loss: 3.192, Best loss: 3.186, cov loss: 0.154\n",
      "    [batch 87]: seen 8700 examples : 47.8 eps, Loss: 3.089, Avg loss: 3.188, Best loss: 3.186, cov loss: 0.157\n",
      "    [batch 92]: seen 9200 examples : 47.7 eps, Loss: 3.243, Avg loss: 3.186, Best loss: 3.185, cov loss: 0.155\n",
      "    [batch 97]: seen 9700 examples : 47.7 eps, Loss: 3.325, Avg loss: 3.183, Best loss: 3.181, cov loss: 0.160\n",
      "    [batch 102]: seen 10200 examples : 47.6 eps, Loss: 3.202, Avg loss: 3.182, Best loss: 3.181, cov loss: 0.145\n",
      "    [batch 107]: seen 10700 examples : 47.6 eps, Loss: 3.077, Avg loss: 3.177, Best loss: 3.177, cov loss: 0.151\n",
      "    [batch 112]: seen 11200 examples : 47.5 eps, Loss: 3.004, Avg loss: 3.180, Best loss: 3.177, cov loss: 0.141\n",
      "    [batch 117]: seen 11700 examples : 47.5 eps, Loss: 3.235, Avg loss: 3.180, Best loss: 3.177, cov loss: 0.159\n",
      "    [batch 122]: seen 12200 examples : 47.5 eps, Loss: 3.122, Avg loss: 3.179, Best loss: 3.177, cov loss: 0.136\n",
      "    [batch 127]: seen 12700 examples : 47.4 eps, Loss: 3.183, Avg loss: 3.180, Best loss: 3.177, cov loss: 0.141\n",
      "    [batch 132]: seen 13200 examples : 47.4 eps, Loss: 3.081, Avg loss: 3.179, Best loss: 3.177, cov loss: 0.153\n",
      "    [batch 137]: seen 13700 examples : 47.4 eps, Loss: 3.175, Avg loss: 3.176, Best loss: 3.176, cov loss: 0.162\n",
      "    [batch 141]: seen 14100 examples : 47.1 eps, Loss: 3.229, Avg loss: 3.178, Best loss: 3.176, cov loss: 0.170\n",
      "    [batch 146]: seen 14600 examples : 47.1 eps, Loss: 3.140, Avg loss: 3.176, Best loss: 3.176, cov loss: 0.158\n",
      "    [batch 151]: seen 15100 examples : 47.1 eps, Loss: 3.237, Avg loss: 3.177, Best loss: 3.176, cov loss: 0.160\n",
      "    [batch 156]: seen 15600 examples : 47.1 eps, Loss: 3.113, Avg loss: 3.176, Best loss: 3.176, cov loss: 0.152\n",
      "    [batch 161]: seen 16100 examples : 47.1 eps, Loss: 3.317, Avg loss: 3.178, Best loss: 3.175, cov loss: 0.155\n",
      "    [batch 166]: seen 16600 examples : 47.1 eps, Loss: 3.141, Avg loss: 3.178, Best loss: 3.175, cov loss: 0.155\n",
      "    [batch 171]: seen 17100 examples : 47.1 eps, Loss: 3.118, Avg loss: 3.179, Best loss: 3.175, cov loss: 0.158\n",
      "    [batch 176]: seen 17600 examples : 47.1 eps, Loss: 3.082, Avg loss: 3.177, Best loss: 3.175, cov loss: 0.139\n",
      "    [batch 181]: seen 18100 examples : 47.0 eps, Loss: 3.254, Avg loss: 3.179, Best loss: 3.175, cov loss: 0.163\n",
      "    [batch 186]: seen 18600 examples : 47.0 eps, Loss: 3.275, Avg loss: 3.180, Best loss: 3.175, cov loss: 0.166\n",
      "    [batch 191]: seen 19100 examples : 47.0 eps, Loss: 3.244, Avg loss: 3.183, Best loss: 3.175, cov loss: 0.151\n",
      "    [batch 196]: seen 19600 examples : 47.0 eps, Loss: 3.172, Avg loss: 3.185, Best loss: 3.175, cov loss: 0.150\n",
      "    [batch 201]: seen 20100 examples : 47.0 eps, Loss: 3.207, Avg loss: 3.184, Best loss: 3.175, cov loss: 0.152\n",
      "    [batch 206]: seen 20600 examples : 47.0 eps, Loss: 3.184, Avg loss: 3.185, Best loss: 3.175, cov loss: 0.145\n",
      "    [batch 211]: seen 21100 examples : 47.0 eps, Loss: 3.094, Avg loss: 3.186, Best loss: 3.175, cov loss: 0.139\n",
      "    [batch 216]: seen 21600 examples : 47.0 eps, Loss: 3.157, Avg loss: 3.184, Best loss: 3.175, cov loss: 0.153\n",
      "    [batch 221]: seen 22100 examples : 47.0 eps, Loss: 3.416, Avg loss: 3.187, Best loss: 3.175, cov loss: 0.163\n",
      "    [batch 226]: seen 22600 examples : 47.0 eps, Loss: 3.072, Avg loss: 3.181, Best loss: 3.175, cov loss: 0.154\n",
      "    [batch 230]: seen 23000 examples : 46.8 eps, Loss: 3.084, Avg loss: 3.179, Best loss: 3.175, cov loss: 0.152\n",
      "    [batch 235]: seen 23500 examples : 46.8 eps, Loss: 3.099, Avg loss: 3.173, Best loss: 3.173, cov loss: 0.148\n",
      "    [batch 240]: seen 24000 examples : 46.8 eps, Loss: 3.270, Avg loss: 3.174, Best loss: 3.171, cov loss: 0.149\n",
      "    [batch 245]: seen 24500 examples : 46.8 eps, Loss: 3.145, Avg loss: 3.177, Best loss: 3.171, cov loss: 0.143\n",
      "    [batch 250]: seen 25000 examples : 46.8 eps, Loss: 3.168, Avg loss: 3.175, Best loss: 3.171, cov loss: 0.142\n",
      "    [batch 255]: seen 25500 examples : 46.8 eps, Loss: 3.180, Avg loss: 3.174, Best loss: 3.171, cov loss: 0.157\n",
      "    [batch 260]: seen 26000 examples : 46.8 eps, Loss: 3.076, Avg loss: 3.173, Best loss: 3.171, cov loss: 0.148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 265]: seen 26500 examples : 46.8 eps, Loss: 3.142, Avg loss: 3.172, Best loss: 3.171, cov loss: 0.167\n",
      "    [batch 270]: seen 27000 examples : 46.8 eps, Loss: 2.929, Avg loss: 3.169, Best loss: 3.169, cov loss: 0.137\n",
      "    [batch 275]: seen 27500 examples : 46.8 eps, Loss: 3.376, Avg loss: 3.172, Best loss: 3.169, cov loss: 0.163\n",
      "    [batch 280]: seen 28000 examples : 46.8 eps, Loss: 3.214, Avg loss: 3.174, Best loss: 3.169, cov loss: 0.150\n",
      "    [batch 285]: seen 28500 examples : 46.8 eps, Loss: 2.949, Avg loss: 3.170, Best loss: 3.169, cov loss: 0.144\n",
      "    [batch 290]: seen 29000 examples : 46.8 eps, Loss: 3.128, Avg loss: 3.171, Best loss: 3.169, cov loss: 0.141\n",
      "    [batch 295]: seen 29500 examples : 46.8 eps, Loss: 3.510, Avg loss: 3.174, Best loss: 3.169, cov loss: 0.163\n",
      "    [batch 300]: seen 30000 examples : 46.8 eps, Loss: 3.221, Avg loss: 3.172, Best loss: 3.169, cov loss: 0.149\n",
      "    [batch 305]: seen 30500 examples : 46.8 eps, Loss: 3.171, Avg loss: 3.176, Best loss: 3.169, cov loss: 0.128\n",
      "    [batch 310]: seen 31000 examples : 46.8 eps, Loss: 3.159, Avg loss: 3.178, Best loss: 3.169, cov loss: 0.159\n",
      "    [batch 315]: seen 31500 examples : 46.8 eps, Loss: 3.196, Avg loss: 3.178, Best loss: 3.169, cov loss: 0.158\n",
      "    [batch 319]: seen 31900 examples : 46.7 eps, Loss: 3.229, Avg loss: 3.178, Best loss: 3.169, cov loss: 0.160\n",
      "    [batch 324]: seen 32400 examples : 46.7 eps, Loss: 3.323, Avg loss: 3.180, Best loss: 3.169, cov loss: 0.164\n",
      "    [batch 329]: seen 32900 examples : 46.7 eps, Loss: 3.130, Avg loss: 3.178, Best loss: 3.169, cov loss: 0.161\n",
      "    [batch 334]: seen 33400 examples : 46.7 eps, Loss: 3.205, Avg loss: 3.181, Best loss: 3.169, cov loss: 0.146\n",
      "    [batch 339]: seen 33900 examples : 46.7 eps, Loss: 3.173, Avg loss: 3.180, Best loss: 3.169, cov loss: 0.153\n",
      "    [batch 344]: seen 34400 examples : 46.7 eps, Loss: 3.071, Avg loss: 3.180, Best loss: 3.169, cov loss: 0.155\n",
      "    [batch 349]: seen 34900 examples : 46.7 eps, Loss: 3.213, Avg loss: 3.177, Best loss: 3.169, cov loss: 0.163\n",
      "    [batch 354]: seen 35400 examples : 46.7 eps, Loss: 2.986, Avg loss: 3.176, Best loss: 3.169, cov loss: 0.157\n",
      "    [batch 359]: seen 35900 examples : 46.7 eps, Loss: 3.212, Avg loss: 3.177, Best loss: 3.169, cov loss: 0.176\n",
      "    [batch 364]: seen 36400 examples : 46.7 eps, Loss: 3.212, Avg loss: 3.178, Best loss: 3.169, cov loss: 0.158\n",
      "    [batch 369]: seen 36900 examples : 46.7 eps, Loss: 3.216, Avg loss: 3.180, Best loss: 3.169, cov loss: 0.154\n",
      "    [batch 374]: seen 37400 examples : 46.7 eps, Loss: 3.296, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.152\n",
      "    [batch 379]: seen 37900 examples : 46.7 eps, Loss: 3.295, Avg loss: 3.184, Best loss: 3.169, cov loss: 0.154\n",
      "    [batch 384]: seen 38400 examples : 46.7 eps, Loss: 3.190, Avg loss: 3.185, Best loss: 3.169, cov loss: 0.168\n",
      "    [batch 389]: seen 38900 examples : 46.7 eps, Loss: 3.327, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.170\n",
      "    [batch 394]: seen 39400 examples : 46.7 eps, Loss: 3.204, Avg loss: 3.185, Best loss: 3.169, cov loss: 0.146\n",
      "    [batch 399]: seen 39900 examples : 46.7 eps, Loss: 3.124, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.159\n",
      "    [batch 404]: seen 40400 examples : 46.7 eps, Loss: 3.303, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.160\n",
      "    [batch 409]: seen 40900 examples : 46.7 eps, Loss: 3.155, Avg loss: 3.180, Best loss: 3.169, cov loss: 0.167\n",
      "    [batch 414]: seen 41400 examples : 46.6 eps, Loss: 3.392, Avg loss: 3.184, Best loss: 3.169, cov loss: 0.158\n",
      "    [batch 419]: seen 41900 examples : 46.6 eps, Loss: 3.151, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.146\n",
      "    [batch 424]: seen 42400 examples : 46.6 eps, Loss: 3.317, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.158\n",
      "    [batch 429]: seen 42900 examples : 46.6 eps, Loss: 3.333, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.156\n",
      "    [batch 434]: seen 43400 examples : 46.6 eps, Loss: 3.030, Avg loss: 3.184, Best loss: 3.169, cov loss: 0.134\n",
      "    [batch 439]: seen 43900 examples : 46.6 eps, Loss: 3.094, Avg loss: 3.181, Best loss: 3.169, cov loss: 0.141\n",
      "    [batch 444]: seen 44400 examples : 46.6 eps, Loss: 3.051, Avg loss: 3.178, Best loss: 3.169, cov loss: 0.150\n",
      "    [batch 449]: seen 44900 examples : 46.6 eps, Loss: 3.150, Avg loss: 3.177, Best loss: 3.169, cov loss: 0.135\n",
      "    [batch 454]: seen 45400 examples : 46.6 eps, Loss: 3.330, Avg loss: 3.179, Best loss: 3.169, cov loss: 0.174\n",
      "    [batch 459]: seen 45900 examples : 46.6 eps, Loss: 3.320, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.149\n",
      "    [batch 464]: seen 46400 examples : 46.6 eps, Loss: 3.205, Avg loss: 3.188, Best loss: 3.169, cov loss: 0.156\n",
      "    [batch 469]: seen 46900 examples : 46.6 eps, Loss: 2.976, Avg loss: 3.187, Best loss: 3.169, cov loss: 0.143\n",
      "    [batch 474]: seen 47400 examples : 46.6 eps, Loss: 3.373, Avg loss: 3.185, Best loss: 3.169, cov loss: 0.175\n",
      "    [batch 479]: seen 47900 examples : 46.6 eps, Loss: 3.047, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.144\n",
      "    [batch 484]: seen 48400 examples : 46.6 eps, Loss: 3.151, Avg loss: 3.185, Best loss: 3.169, cov loss: 0.159\n",
      "    [batch 489]: seen 48900 examples : 46.6 eps, Loss: 3.061, Avg loss: 3.185, Best loss: 3.169, cov loss: 0.152\n",
      "    [batch 494]: seen 49400 examples : 46.6 eps, Loss: 3.209, Avg loss: 3.185, Best loss: 3.169, cov loss: 0.139\n",
      "    [batch 498]: seen 49800 examples : 46.6 eps, Loss: 3.197, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.140\n",
      "    [batch 503]: seen 50300 examples : 46.6 eps, Loss: 3.203, Avg loss: 3.187, Best loss: 3.169, cov loss: 0.160\n",
      "    [batch 508]: seen 50800 examples : 46.6 eps, Loss: 3.116, Avg loss: 3.188, Best loss: 3.169, cov loss: 0.147\n",
      "    [batch 513]: seen 51300 examples : 46.6 eps, Loss: 3.118, Avg loss: 3.188, Best loss: 3.169, cov loss: 0.150\n",
      "    [batch 518]: seen 51800 examples : 46.6 eps, Loss: 3.115, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.159\n",
      "    [batch 523]: seen 52300 examples : 46.6 eps, Loss: 3.112, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.148\n",
      "    [batch 528]: seen 52800 examples : 46.6 eps, Loss: 2.890, Avg loss: 3.179, Best loss: 3.169, cov loss: 0.145\n",
      "    [batch 533]: seen 53300 examples : 46.6 eps, Loss: 3.337, Avg loss: 3.181, Best loss: 3.169, cov loss: 0.153\n",
      "    [batch 538]: seen 53800 examples : 46.6 eps, Loss: 3.261, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.167\n",
      "    [batch 543]: seen 54300 examples : 46.6 eps, Loss: 3.052, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.137\n",
      "    [batch 548]: seen 54800 examples : 46.6 eps, Loss: 3.305, Avg loss: 3.185, Best loss: 3.169, cov loss: 0.144\n",
      "    [batch 553]: seen 55300 examples : 46.6 eps, Loss: 3.119, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.151\n",
      "    [batch 558]: seen 55800 examples : 46.6 eps, Loss: 3.135, Avg loss: 3.181, Best loss: 3.169, cov loss: 0.139\n",
      "    [batch 563]: seen 56300 examples : 46.6 eps, Loss: 3.096, Avg loss: 3.178, Best loss: 3.169, cov loss: 0.155\n",
      "    [batch 568]: seen 56800 examples : 46.6 eps, Loss: 3.272, Avg loss: 3.180, Best loss: 3.169, cov loss: 0.128\n",
      "    [batch 573]: seen 57300 examples : 46.6 eps, Loss: 3.436, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.157\n",
      "    [batch 578]: seen 57800 examples : 46.6 eps, Loss: 3.331, Avg loss: 3.185, Best loss: 3.169, cov loss: 0.147\n",
      "    [EXCEPTION]:  Loss is not finite. ; Restoring model params\n",
      "INFO:tensorflow:Loading checkpoint /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-60784\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/W266/final_0/W266_Final/model_4/saved/train/model-60784\n",
      "    [batch 582]: seen 58200 examples : 46.5 eps, Loss: 3.172, Avg loss: 3.188, Best loss: 3.169, cov loss: 0.167\n",
      "    [batch 586]: seen 58600 examples : 46.5 eps, Loss: 3.175, Avg loss: 3.187, Best loss: 3.169, cov loss: 0.146\n",
      "    [batch 591]: seen 59100 examples : 46.5 eps, Loss: 3.097, Avg loss: 3.184, Best loss: 3.169, cov loss: 0.146\n",
      "    [batch 596]: seen 59600 examples : 46.5 eps, Loss: 3.094, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.144\n",
      "    [batch 601]: seen 60100 examples : 46.5 eps, Loss: 3.236, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.156\n",
      "    [batch 606]: seen 60600 examples : 46.5 eps, Loss: 3.281, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.150\n",
      "    [batch 611]: seen 61100 examples : 46.5 eps, Loss: 3.192, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 616]: seen 61600 examples : 46.5 eps, Loss: 3.289, Avg loss: 3.181, Best loss: 3.169, cov loss: 0.149\n",
      "    [batch 621]: seen 62100 examples : 46.5 eps, Loss: 3.209, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.165\n",
      "    [batch 626]: seen 62600 examples : 46.5 eps, Loss: 3.048, Avg loss: 3.181, Best loss: 3.169, cov loss: 0.163\n",
      "    [batch 631]: seen 63100 examples : 46.5 eps, Loss: 3.084, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.138\n",
      "    [batch 636]: seen 63600 examples : 46.5 eps, Loss: 3.155, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.160\n",
      "    [batch 641]: seen 64100 examples : 46.5 eps, Loss: 3.415, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.149\n",
      "    [batch 646]: seen 64600 examples : 46.5 eps, Loss: 3.222, Avg loss: 3.184, Best loss: 3.169, cov loss: 0.163\n",
      "    [batch 651]: seen 65100 examples : 46.5 eps, Loss: 3.242, Avg loss: 3.184, Best loss: 3.169, cov loss: 0.154\n",
      "    [batch 656]: seen 65600 examples : 46.5 eps, Loss: 3.095, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.161\n",
      "    [batch 661]: seen 66100 examples : 46.5 eps, Loss: 3.192, Avg loss: 3.185, Best loss: 3.169, cov loss: 0.162\n",
      "    [batch 666]: seen 66600 examples : 46.5 eps, Loss: 3.213, Avg loss: 3.185, Best loss: 3.169, cov loss: 0.150\n",
      "    [batch 671]: seen 67100 examples : 46.5 eps, Loss: 3.220, Avg loss: 3.187, Best loss: 3.169, cov loss: 0.154\n",
      "    [batch 676]: seen 67600 examples : 46.5 eps, Loss: 3.185, Avg loss: 3.185, Best loss: 3.169, cov loss: 0.144\n",
      "    [batch 681]: seen 68100 examples : 46.5 eps, Loss: 3.232, Avg loss: 3.187, Best loss: 3.169, cov loss: 0.161\n",
      "    [batch 686]: seen 68600 examples : 46.5 eps, Loss: 3.163, Avg loss: 3.187, Best loss: 3.169, cov loss: 0.142\n",
      "    [batch 691]: seen 69100 examples : 46.5 eps, Loss: 3.246, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.168\n",
      "    [batch 696]: seen 69600 examples : 46.5 eps, Loss: 3.217, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.148\n",
      "    [batch 701]: seen 70100 examples : 46.5 eps, Loss: 3.220, Avg loss: 3.181, Best loss: 3.169, cov loss: 0.151\n",
      "    [batch 706]: seen 70600 examples : 46.5 eps, Loss: 3.205, Avg loss: 3.181, Best loss: 3.169, cov loss: 0.163\n",
      "    [batch 711]: seen 71100 examples : 46.5 eps, Loss: 3.298, Avg loss: 3.179, Best loss: 3.169, cov loss: 0.135\n",
      "    [batch 716]: seen 71600 examples : 46.5 eps, Loss: 3.275, Avg loss: 3.182, Best loss: 3.169, cov loss: 0.158\n",
      "    [batch 721]: seen 72100 examples : 46.5 eps, Loss: 3.181, Avg loss: 3.184, Best loss: 3.169, cov loss: 0.154\n",
      "    [batch 726]: seen 72600 examples : 46.5 eps, Loss: 3.197, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.137\n",
      "    [batch 731]: seen 73100 examples : 46.5 eps, Loss: 3.270, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.162\n",
      "    [batch 736]: seen 73600 examples : 46.5 eps, Loss: 3.221, Avg loss: 3.184, Best loss: 3.169, cov loss: 0.142\n",
      "    [batch 741]: seen 74100 examples : 46.5 eps, Loss: 3.419, Avg loss: 3.188, Best loss: 3.169, cov loss: 0.170\n",
      "    [batch 746]: seen 74600 examples : 46.5 eps, Loss: 3.304, Avg loss: 3.191, Best loss: 3.169, cov loss: 0.165\n",
      "    [batch 751]: seen 75100 examples : 46.5 eps, Loss: 3.125, Avg loss: 3.191, Best loss: 3.169, cov loss: 0.152\n",
      "    [batch 756]: seen 75600 examples : 46.5 eps, Loss: 3.174, Avg loss: 3.187, Best loss: 3.169, cov loss: 0.148\n",
      "    [batch 761]: seen 76100 examples : 46.5 eps, Loss: 3.111, Avg loss: 3.190, Best loss: 3.169, cov loss: 0.139\n",
      "    [batch 765]: seen 76500 examples : 46.4 eps, Loss: 3.193, Avg loss: 3.195, Best loss: 3.169, cov loss: 0.147\n",
      "    [batch 770]: seen 77000 examples : 46.4 eps, Loss: 3.202, Avg loss: 3.190, Best loss: 3.169, cov loss: 0.149\n",
      "    [batch 775]: seen 77500 examples : 46.4 eps, Loss: 3.017, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.148\n",
      "    [batch 780]: seen 78000 examples : 46.4 eps, Loss: 3.171, Avg loss: 3.184, Best loss: 3.169, cov loss: 0.140\n",
      "    [batch 785]: seen 78500 examples : 46.5 eps, Loss: 3.156, Avg loss: 3.184, Best loss: 3.169, cov loss: 0.144\n",
      "    [batch 790]: seen 79000 examples : 46.5 eps, Loss: 3.311, Avg loss: 3.185, Best loss: 3.169, cov loss: 0.149\n",
      "    [batch 795]: seen 79500 examples : 46.5 eps, Loss: 3.283, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.158\n",
      "    [batch 800]: seen 80000 examples : 46.5 eps, Loss: 3.277, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.157\n",
      "    [batch 805]: seen 80500 examples : 46.5 eps, Loss: 3.218, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.146\n",
      "    [batch 810]: seen 81000 examples : 46.5 eps, Loss: 3.111, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.159\n",
      "    [batch 815]: seen 81500 examples : 46.5 eps, Loss: 3.189, Avg loss: 3.188, Best loss: 3.169, cov loss: 0.154\n",
      "    [batch 820]: seen 82000 examples : 46.5 eps, Loss: 3.197, Avg loss: 3.187, Best loss: 3.169, cov loss: 0.147\n",
      "    [batch 825]: seen 82500 examples : 46.5 eps, Loss: 3.231, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.151\n",
      "    [batch 830]: seen 83000 examples : 46.5 eps, Loss: 3.093, Avg loss: 3.184, Best loss: 3.169, cov loss: 0.141\n",
      "    [batch 835]: seen 83500 examples : 46.5 eps, Loss: 3.213, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.150\n",
      "    [batch 840]: seen 84000 examples : 46.5 eps, Loss: 3.251, Avg loss: 3.184, Best loss: 3.169, cov loss: 0.147\n",
      "    [batch 845]: seen 84500 examples : 46.5 eps, Loss: 3.177, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.141\n",
      "    [batch 850]: seen 85000 examples : 46.5 eps, Loss: 3.325, Avg loss: 3.187, Best loss: 3.169, cov loss: 0.150\n",
      "    [batch 854]: seen 85400 examples : 46.4 eps, Loss: 3.273, Avg loss: 3.187, Best loss: 3.169, cov loss: 0.151\n",
      "    [batch 859]: seen 85900 examples : 46.4 eps, Loss: 3.147, Avg loss: 3.190, Best loss: 3.169, cov loss: 0.135\n",
      "    [batch 864]: seen 86400 examples : 46.4 eps, Loss: 3.207, Avg loss: 3.193, Best loss: 3.169, cov loss: 0.148\n",
      "    [batch 869]: seen 86900 examples : 46.4 eps, Loss: 3.162, Avg loss: 3.193, Best loss: 3.169, cov loss: 0.159\n",
      "    [batch 874]: seen 87400 examples : 46.4 eps, Loss: 3.226, Avg loss: 3.193, Best loss: 3.169, cov loss: 0.156\n",
      "    [batch 879]: seen 87900 examples : 46.4 eps, Loss: 3.044, Avg loss: 3.190, Best loss: 3.169, cov loss: 0.142\n",
      "    [batch 884]: seen 88400 examples : 46.4 eps, Loss: 3.279, Avg loss: 3.190, Best loss: 3.169, cov loss: 0.168\n",
      "    [batch 889]: seen 88900 examples : 46.5 eps, Loss: 3.133, Avg loss: 3.191, Best loss: 3.169, cov loss: 0.142\n",
      "    [batch 894]: seen 89400 examples : 46.4 eps, Loss: 3.351, Avg loss: 3.190, Best loss: 3.169, cov loss: 0.163\n",
      "    [batch 899]: seen 89900 examples : 46.4 eps, Loss: 3.264, Avg loss: 3.193, Best loss: 3.169, cov loss: 0.148\n",
      "    [batch 904]: seen 90400 examples : 46.4 eps, Loss: 3.233, Avg loss: 3.192, Best loss: 3.169, cov loss: 0.160\n",
      "    [batch 909]: seen 90900 examples : 46.5 eps, Loss: 3.099, Avg loss: 3.193, Best loss: 3.169, cov loss: 0.140\n",
      "    [batch 914]: seen 91400 examples : 46.5 eps, Loss: 3.172, Avg loss: 3.194, Best loss: 3.169, cov loss: 0.144\n",
      "    [batch 919]: seen 91900 examples : 46.5 eps, Loss: 3.183, Avg loss: 3.196, Best loss: 3.169, cov loss: 0.162\n",
      "    [batch 924]: seen 92400 examples : 46.5 eps, Loss: 3.109, Avg loss: 3.190, Best loss: 3.169, cov loss: 0.135\n",
      "    [batch 929]: seen 92900 examples : 46.5 eps, Loss: 3.176, Avg loss: 3.193, Best loss: 3.169, cov loss: 0.160\n",
      "    [batch 934]: seen 93400 examples : 46.5 eps, Loss: 3.292, Avg loss: 3.190, Best loss: 3.169, cov loss: 0.164\n",
      "    [batch 939]: seen 93900 examples : 46.5 eps, Loss: 3.126, Avg loss: 3.186, Best loss: 3.169, cov loss: 0.148\n",
      "    [batch 943]: seen 94300 examples : 46.4 eps, Loss: 3.128, Avg loss: 3.187, Best loss: 3.169, cov loss: 0.154\n",
      "    [batch 948]: seen 94800 examples : 46.4 eps, Loss: 3.047, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.145\n",
      "    [batch 953]: seen 95300 examples : 46.4 eps, Loss: 3.140, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.156\n",
      "    [batch 958]: seen 95800 examples : 46.4 eps, Loss: 3.139, Avg loss: 3.183, Best loss: 3.169, cov loss: 0.147\n",
      "    [batch 963]: seen 96300 examples : 46.4 eps, Loss: 3.046, Avg loss: 3.177, Best loss: 3.169, cov loss: 0.153\n",
      "    [batch 968]: seen 96800 examples : 46.4 eps, Loss: 3.140, Avg loss: 3.175, Best loss: 3.169, cov loss: 0.163\n",
      "    [batch 973]: seen 97300 examples : 46.4 eps, Loss: 3.133, Avg loss: 3.172, Best loss: 3.169, cov loss: 0.145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 978]: seen 97800 examples : 46.4 eps, Loss: 3.023, Avg loss: 3.171, Best loss: 3.169, cov loss: 0.140\n",
      "    [batch 983]: seen 98300 examples : 46.4 eps, Loss: 3.154, Avg loss: 3.171, Best loss: 3.169, cov loss: 0.154\n",
      "    [batch 988]: seen 98800 examples : 46.4 eps, Loss: 3.279, Avg loss: 3.173, Best loss: 3.169, cov loss: 0.159\n",
      "    [batch 993]: seen 99300 examples : 46.4 eps, Loss: 3.212, Avg loss: 3.174, Best loss: 3.169, cov loss: 0.149\n",
      "    [batch 998]: seen 99800 examples : 46.4 eps, Loss: 3.246, Avg loss: 3.173, Best loss: 3.169, cov loss: 0.149\n",
      "    [batch 1003]: seen 100300 examples : 46.4 eps, Loss: 3.125, Avg loss: 3.172, Best loss: 3.169, cov loss: 0.150\n",
      "    [batch 1008]: seen 100800 examples : 46.4 eps, Loss: 3.330, Avg loss: 3.173, Best loss: 3.169, cov loss: 0.152\n",
      "    [batch 1013]: seen 101300 examples : 46.4 eps, Loss: 3.062, Avg loss: 3.169, Best loss: 3.169, cov loss: 0.165\n",
      "    [batch 1018]: seen 101800 examples : 46.4 eps, Loss: 3.219, Avg loss: 3.169, Best loss: 3.167, cov loss: 0.137\n",
      "    [batch 1023]: seen 102300 examples : 46.4 eps, Loss: 3.119, Avg loss: 3.169, Best loss: 3.167, cov loss: 0.148\n",
      "    [batch 1028]: seen 102800 examples : 46.4 eps, Loss: 3.068, Avg loss: 3.173, Best loss: 3.167, cov loss: 0.160\n",
      "    [batch 1033]: seen 103300 examples : 46.4 eps, Loss: 3.367, Avg loss: 3.179, Best loss: 3.167, cov loss: 0.156\n",
      "    [batch 1037]: seen 103700 examples : 46.4 eps, Loss: 3.262, Avg loss: 3.181, Best loss: 3.167, cov loss: 0.152\n",
      "    [batch 1042]: seen 104200 examples : 46.4 eps, Loss: 2.993, Avg loss: 3.177, Best loss: 3.167, cov loss: 0.152\n",
      "    [batch 1047]: seen 104700 examples : 46.4 eps, Loss: 3.330, Avg loss: 3.180, Best loss: 3.167, cov loss: 0.171\n",
      "    [batch 1052]: seen 105200 examples : 46.4 eps, Loss: 3.414, Avg loss: 3.181, Best loss: 3.167, cov loss: 0.168\n",
      "    [batch 1057]: seen 105700 examples : 46.4 eps, Loss: 3.195, Avg loss: 3.179, Best loss: 3.167, cov loss: 0.149\n",
      "    [batch 1062]: seen 106200 examples : 46.4 eps, Loss: 3.314, Avg loss: 3.182, Best loss: 3.167, cov loss: 0.138\n",
      "    [batch 1067]: seen 106700 examples : 46.4 eps, Loss: 3.068, Avg loss: 3.181, Best loss: 3.167, cov loss: 0.141\n",
      "    [batch 1072]: seen 107200 examples : 46.4 eps, Loss: 3.134, Avg loss: 3.180, Best loss: 3.167, cov loss: 0.165\n",
      "    [batch 1077]: seen 107700 examples : 46.4 eps, Loss: 3.147, Avg loss: 3.177, Best loss: 3.167, cov loss: 0.155\n",
      "    [batch 1082]: seen 108200 examples : 46.4 eps, Loss: 3.355, Avg loss: 3.180, Best loss: 3.167, cov loss: 0.164\n",
      "    [batch 1087]: seen 108700 examples : 46.4 eps, Loss: 3.113, Avg loss: 3.177, Best loss: 3.167, cov loss: 0.147\n",
      "    [batch 1092]: seen 109200 examples : 46.4 eps, Loss: 3.179, Avg loss: 3.175, Best loss: 3.167, cov loss: 0.146\n",
      "    [batch 1097]: seen 109700 examples : 46.4 eps, Loss: 3.108, Avg loss: 3.176, Best loss: 3.167, cov loss: 0.139\n",
      "    [batch 1102]: seen 110200 examples : 46.4 eps, Loss: 3.228, Avg loss: 3.179, Best loss: 3.167, cov loss: 0.162\n",
      "    [batch 1107]: seen 110700 examples : 46.4 eps, Loss: 3.350, Avg loss: 3.182, Best loss: 3.167, cov loss: 0.159\n",
      "    [batch 1112]: seen 111200 examples : 46.4 eps, Loss: 3.223, Avg loss: 3.183, Best loss: 3.167, cov loss: 0.173\n",
      "    [batch 1117]: seen 111700 examples : 46.4 eps, Loss: 3.182, Avg loss: 3.185, Best loss: 3.167, cov loss: 0.135\n",
      "    [batch 1121]: seen 112100 examples : 46.4 eps, Loss: 3.158, Avg loss: 3.185, Best loss: 3.167, cov loss: 0.159\n",
      "    [batch 1126]: seen 112600 examples : 46.4 eps, Loss: 3.127, Avg loss: 3.182, Best loss: 3.167, cov loss: 0.156\n",
      "    [batch 1131]: seen 113100 examples : 46.4 eps, Loss: 3.213, Avg loss: 3.184, Best loss: 3.167, cov loss: 0.140\n",
      "    [batch 1136]: seen 113600 examples : 46.4 eps, Loss: 3.180, Avg loss: 3.186, Best loss: 3.167, cov loss: 0.153\n",
      "    [batch 1141]: seen 114100 examples : 46.4 eps, Loss: 3.138, Avg loss: 3.185, Best loss: 3.167, cov loss: 0.149\n",
      "    [batch 1146]: seen 114600 examples : 46.4 eps, Loss: 3.287, Avg loss: 3.184, Best loss: 3.167, cov loss: 0.143\n",
      "    [batch 1151]: seen 115100 examples : 46.4 eps, Loss: 3.130, Avg loss: 3.182, Best loss: 3.167, cov loss: 0.161\n",
      "    [batch 1156]: seen 115600 examples : 46.4 eps, Loss: 3.005, Avg loss: 3.175, Best loss: 3.167, cov loss: 0.151\n",
      "    [batch 1161]: seen 116100 examples : 46.4 eps, Loss: 3.138, Avg loss: 3.173, Best loss: 3.167, cov loss: 0.154\n",
      "    [batch 1166]: seen 116600 examples : 46.4 eps, Loss: 3.136, Avg loss: 3.171, Best loss: 3.167, cov loss: 0.155\n",
      "    [batch 1171]: seen 117100 examples : 46.4 eps, Loss: 3.023, Avg loss: 3.169, Best loss: 3.167, cov loss: 0.141\n",
      "    [batch 1176]: seen 117600 examples : 46.4 eps, Loss: 3.037, Avg loss: 3.165, Best loss: 3.165, cov loss: 0.141\n",
      "    [batch 1181]: seen 118100 examples : 46.4 eps, Loss: 2.955, Avg loss: 3.165, Best loss: 3.165, cov loss: 0.145\n",
      "    [batch 1186]: seen 118600 examples : 46.4 eps, Loss: 3.141, Avg loss: 3.163, Best loss: 3.163, cov loss: 0.157\n",
      "    [batch 1191]: seen 119100 examples : 46.4 eps, Loss: 3.003, Avg loss: 3.159, Best loss: 3.159, cov loss: 0.142\n",
      "    [batch 1196]: seen 119600 examples : 46.4 eps, Loss: 3.138, Avg loss: 3.160, Best loss: 3.159, cov loss: 0.146\n",
      "    [batch 1201]: seen 120100 examples : 46.4 eps, Loss: 3.140, Avg loss: 3.159, Best loss: 3.159, cov loss: 0.170\n",
      "    [batch 1206]: seen 120600 examples : 46.4 eps, Loss: 3.258, Avg loss: 3.162, Best loss: 3.159, cov loss: 0.154\n",
      "    [batch 1210]: seen 121000 examples : 46.4 eps, Loss: 2.814, Avg loss: 3.161, Best loss: 3.159, cov loss: 0.139\n",
      "    [batch 1215]: seen 121500 examples : 46.4 eps, Loss: 3.125, Avg loss: 3.160, Best loss: 3.159, cov loss: 0.164\n",
      "    [batch 1220]: seen 122000 examples : 46.4 eps, Loss: 3.190, Avg loss: 3.162, Best loss: 3.159, cov loss: 0.153\n",
      "    [batch 1225]: seen 122500 examples : 46.4 eps, Loss: 3.201, Avg loss: 3.165, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 1230]: seen 123000 examples : 46.4 eps, Loss: 3.370, Avg loss: 3.166, Best loss: 3.159, cov loss: 0.154\n",
      "    [batch 1235]: seen 123500 examples : 46.4 eps, Loss: 2.903, Avg loss: 3.163, Best loss: 3.159, cov loss: 0.125\n",
      "    [batch 1240]: seen 124000 examples : 46.4 eps, Loss: 3.268, Avg loss: 3.165, Best loss: 3.159, cov loss: 0.167\n",
      "    [batch 1245]: seen 124500 examples : 46.4 eps, Loss: 3.061, Avg loss: 3.164, Best loss: 3.159, cov loss: 0.153\n",
      "    [batch 1250]: seen 125000 examples : 46.4 eps, Loss: 3.322, Avg loss: 3.166, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1255]: seen 125500 examples : 46.4 eps, Loss: 3.085, Avg loss: 3.165, Best loss: 3.159, cov loss: 0.145\n",
      "    [batch 1260]: seen 126000 examples : 46.4 eps, Loss: 3.236, Avg loss: 3.167, Best loss: 3.159, cov loss: 0.142\n",
      "    [batch 1265]: seen 126500 examples : 46.4 eps, Loss: 3.173, Avg loss: 3.167, Best loss: 3.159, cov loss: 0.153\n",
      "    [batch 1270]: seen 127000 examples : 46.4 eps, Loss: 3.308, Avg loss: 3.169, Best loss: 3.159, cov loss: 0.154\n",
      "    [batch 1275]: seen 127500 examples : 46.4 eps, Loss: 3.104, Avg loss: 3.173, Best loss: 3.159, cov loss: 0.162\n",
      "    [batch 1280]: seen 128000 examples : 46.4 eps, Loss: 3.200, Avg loss: 3.175, Best loss: 3.159, cov loss: 0.144\n",
      "    [batch 1285]: seen 128500 examples : 46.4 eps, Loss: 3.471, Avg loss: 3.179, Best loss: 3.159, cov loss: 0.165\n",
      "    [batch 1290]: seen 129000 examples : 46.4 eps, Loss: 3.139, Avg loss: 3.179, Best loss: 3.159, cov loss: 0.158\n",
      "    [batch 1295]: seen 129500 examples : 46.4 eps, Loss: 3.154, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1300]: seen 130000 examples : 46.4 eps, Loss: 3.047, Avg loss: 3.178, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 1305]: seen 130500 examples : 46.4 eps, Loss: 3.217, Avg loss: 3.179, Best loss: 3.159, cov loss: 0.169\n",
      "    [batch 1310]: seen 131000 examples : 46.4 eps, Loss: 3.318, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.139\n",
      "    [batch 1315]: seen 131500 examples : 46.4 eps, Loss: 3.063, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 1320]: seen 132000 examples : 46.4 eps, Loss: 3.073, Avg loss: 3.177, Best loss: 3.159, cov loss: 0.152\n",
      "    [batch 1325]: seen 132500 examples : 46.4 eps, Loss: 3.207, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1330]: seen 133000 examples : 46.4 eps, Loss: 3.448, Avg loss: 3.179, Best loss: 3.159, cov loss: 0.160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1335]: seen 133500 examples : 46.4 eps, Loss: 3.116, Avg loss: 3.180, Best loss: 3.159, cov loss: 0.155\n",
      "    [batch 1340]: seen 134000 examples : 46.4 eps, Loss: 3.368, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.164\n",
      "    [batch 1345]: seen 134500 examples : 46.4 eps, Loss: 3.258, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.169\n",
      "    [batch 1350]: seen 135000 examples : 46.4 eps, Loss: 3.162, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.176\n",
      "    [batch 1355]: seen 135500 examples : 46.4 eps, Loss: 3.150, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.144\n",
      "    [batch 1360]: seen 136000 examples : 46.4 eps, Loss: 3.167, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 1365]: seen 136500 examples : 46.4 eps, Loss: 3.180, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.143\n",
      "    [batch 1370]: seen 137000 examples : 46.4 eps, Loss: 3.270, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.166\n",
      "    [batch 1375]: seen 137500 examples : 46.4 eps, Loss: 3.018, Avg loss: 3.179, Best loss: 3.159, cov loss: 0.157\n",
      "    [batch 1380]: seen 138000 examples : 46.4 eps, Loss: 3.130, Avg loss: 3.178, Best loss: 3.159, cov loss: 0.147\n",
      "    [batch 1385]: seen 138500 examples : 46.4 eps, Loss: 3.264, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.172\n",
      "    [batch 1390]: seen 139000 examples : 46.4 eps, Loss: 3.348, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 1395]: seen 139500 examples : 46.4 eps, Loss: 3.223, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.155\n",
      "    [batch 1400]: seen 140000 examples : 46.4 eps, Loss: 3.205, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.144\n",
      "    [batch 1405]: seen 140500 examples : 46.4 eps, Loss: 3.013, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1410]: seen 141000 examples : 46.4 eps, Loss: 3.143, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 1415]: seen 141500 examples : 46.4 eps, Loss: 3.122, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.133\n",
      "    [batch 1420]: seen 142000 examples : 46.4 eps, Loss: 3.221, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.160\n",
      "    [batch 1425]: seen 142500 examples : 46.4 eps, Loss: 3.261, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 1430]: seen 143000 examples : 46.4 eps, Loss: 3.191, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 1435]: seen 143500 examples : 46.4 eps, Loss: 3.019, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.141\n",
      "    [batch 1440]: seen 144000 examples : 46.4 eps, Loss: 3.231, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.143\n",
      "    [batch 1445]: seen 144500 examples : 46.4 eps, Loss: 3.059, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.140\n",
      "    [batch 1450]: seen 145000 examples : 46.4 eps, Loss: 3.096, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.146\n",
      "    [batch 1455]: seen 145500 examples : 46.4 eps, Loss: 3.330, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 1460]: seen 146000 examples : 46.4 eps, Loss: 3.308, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.167\n",
      "    [batch 1465]: seen 146500 examples : 46.4 eps, Loss: 3.208, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.138\n",
      "    [batch 1470]: seen 147000 examples : 46.4 eps, Loss: 3.078, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.147\n",
      "    [batch 1475]: seen 147500 examples : 46.4 eps, Loss: 3.298, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.158\n",
      "    [batch 1479]: seen 147900 examples : 46.4 eps, Loss: 3.287, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.153\n",
      "    [batch 1484]: seen 148400 examples : 46.4 eps, Loss: 3.188, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.164\n",
      "    [batch 1489]: seen 148900 examples : 46.4 eps, Loss: 3.375, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.140\n",
      "    [batch 1494]: seen 149400 examples : 46.4 eps, Loss: 3.277, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 1499]: seen 149900 examples : 46.4 eps, Loss: 3.187, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 1504]: seen 150400 examples : 46.4 eps, Loss: 3.123, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1509]: seen 150900 examples : 46.4 eps, Loss: 3.132, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.169\n",
      "    [batch 1514]: seen 151400 examples : 46.4 eps, Loss: 3.302, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.164\n",
      "    [batch 1519]: seen 151900 examples : 46.4 eps, Loss: 2.993, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.145\n",
      "    [batch 1524]: seen 152400 examples : 46.4 eps, Loss: 3.145, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.154\n",
      "    [batch 1529]: seen 152900 examples : 46.4 eps, Loss: 3.411, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.158\n",
      "    [batch 1534]: seen 153400 examples : 46.4 eps, Loss: 3.130, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1539]: seen 153900 examples : 46.4 eps, Loss: 3.193, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 1544]: seen 154400 examples : 46.4 eps, Loss: 3.168, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.166\n",
      "    [batch 1549]: seen 154900 examples : 46.4 eps, Loss: 3.156, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1554]: seen 155400 examples : 46.4 eps, Loss: 3.240, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.154\n",
      "    [batch 1559]: seen 155900 examples : 46.4 eps, Loss: 3.185, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.145\n",
      "    [batch 1564]: seen 156400 examples : 46.4 eps, Loss: 3.228, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 1569]: seen 156900 examples : 46.4 eps, Loss: 3.236, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 1574]: seen 157400 examples : 46.4 eps, Loss: 3.011, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.156\n",
      "    [batch 1579]: seen 157900 examples : 46.4 eps, Loss: 3.198, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.158\n",
      "    [batch 1584]: seen 158400 examples : 46.4 eps, Loss: 3.000, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.141\n",
      "    [batch 1589]: seen 158900 examples : 46.4 eps, Loss: 3.074, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.148\n",
      "    [batch 1594]: seen 159400 examples : 46.4 eps, Loss: 3.093, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.137\n",
      "    [batch 1599]: seen 159900 examples : 46.4 eps, Loss: 3.254, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.169\n",
      "    [batch 1604]: seen 160400 examples : 46.4 eps, Loss: 3.218, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1609]: seen 160900 examples : 46.4 eps, Loss: 3.282, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.156\n",
      "    [batch 1614]: seen 161400 examples : 46.4 eps, Loss: 3.166, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.171\n",
      "    [batch 1619]: seen 161900 examples : 46.4 eps, Loss: 3.119, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.139\n",
      "    [batch 1624]: seen 162400 examples : 46.4 eps, Loss: 3.368, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.163\n",
      "    [batch 1629]: seen 162900 examples : 46.4 eps, Loss: 3.202, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.162\n",
      "    [batch 1634]: seen 163400 examples : 46.4 eps, Loss: 3.024, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.132\n",
      "    [batch 1639]: seen 163900 examples : 46.4 eps, Loss: 3.278, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.167\n",
      "    [batch 1644]: seen 164400 examples : 46.4 eps, Loss: 3.130, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 1649]: seen 164900 examples : 46.4 eps, Loss: 3.207, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.148\n",
      "    [batch 1654]: seen 165400 examples : 46.4 eps, Loss: 3.113, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.147\n",
      "    [batch 1658]: seen 165800 examples : 46.4 eps, Loss: 3.046, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1663]: seen 166300 examples : 46.4 eps, Loss: 2.838, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.147\n",
      "    [batch 1668]: seen 166800 examples : 46.4 eps, Loss: 3.214, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 1673]: seen 167300 examples : 46.4 eps, Loss: 3.407, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.169\n",
      "    [batch 1678]: seen 167800 examples : 46.4 eps, Loss: 3.129, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.148\n",
      "    [batch 1683]: seen 168300 examples : 46.4 eps, Loss: 3.311, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.165\n",
      "    [batch 1688]: seen 168800 examples : 46.4 eps, Loss: 3.011, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 1693]: seen 169300 examples : 46.4 eps, Loss: 3.319, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.155\n",
      "    [batch 1698]: seen 169800 examples : 46.4 eps, Loss: 3.265, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.140\n",
      "    [batch 1703]: seen 170300 examples : 46.4 eps, Loss: 3.428, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.180\n",
      "    [batch 1708]: seen 170800 examples : 46.4 eps, Loss: 3.138, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 1713]: seen 171300 examples : 46.4 eps, Loss: 3.068, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.166\n",
      "    [batch 1718]: seen 171800 examples : 46.4 eps, Loss: 3.124, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 1723]: seen 172300 examples : 46.4 eps, Loss: 3.028, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.163\n",
      "    [batch 1728]: seen 172800 examples : 46.4 eps, Loss: 3.212, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 1733]: seen 173300 examples : 46.4 eps, Loss: 3.254, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 1738]: seen 173800 examples : 46.4 eps, Loss: 3.283, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.170\n",
      "    [batch 1743]: seen 174300 examples : 46.4 eps, Loss: 2.975, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.133\n",
      "    [batch 1748]: seen 174800 examples : 46.4 eps, Loss: 3.421, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.166\n",
      "    [batch 1752]: seen 175200 examples : 46.4 eps, Loss: 3.092, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.162\n",
      "    [batch 1757]: seen 175700 examples : 46.4 eps, Loss: 3.247, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.139\n",
      "    [batch 1762]: seen 176200 examples : 46.4 eps, Loss: 3.311, Avg loss: 3.197, Best loss: 3.159, cov loss: 0.168\n",
      "    [batch 1767]: seen 176700 examples : 46.4 eps, Loss: 3.134, Avg loss: 3.198, Best loss: 3.159, cov loss: 0.147\n",
      "    [batch 1772]: seen 177200 examples : 46.4 eps, Loss: 3.144, Avg loss: 3.194, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 1777]: seen 177700 examples : 46.4 eps, Loss: 3.351, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 1782]: seen 178200 examples : 46.4 eps, Loss: 3.167, Avg loss: 3.194, Best loss: 3.159, cov loss: 0.160\n",
      "    [batch 1787]: seen 178700 examples : 46.4 eps, Loss: 3.149, Avg loss: 3.194, Best loss: 3.159, cov loss: 0.143\n",
      "    [batch 1792]: seen 179200 examples : 46.4 eps, Loss: 3.198, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.145\n",
      "    [batch 1797]: seen 179700 examples : 46.4 eps, Loss: 3.053, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 1802]: seen 180200 examples : 46.4 eps, Loss: 3.147, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.155\n",
      "    [batch 1807]: seen 180700 examples : 46.4 eps, Loss: 3.283, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.160\n",
      "    [batch 1812]: seen 181200 examples : 46.4 eps, Loss: 3.115, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.156\n",
      "    [batch 1817]: seen 181700 examples : 46.4 eps, Loss: 3.171, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1822]: seen 182200 examples : 46.4 eps, Loss: 3.040, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.146\n",
      "    [batch 1827]: seen 182700 examples : 46.4 eps, Loss: 3.115, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 1832]: seen 183200 examples : 46.4 eps, Loss: 3.100, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.142\n",
      "    [batch 1836]: seen 183600 examples : 46.4 eps, Loss: 3.177, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.156\n",
      "    [batch 1841]: seen 184100 examples : 46.4 eps, Loss: 3.038, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1846]: seen 184600 examples : 46.4 eps, Loss: 3.147, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.141\n",
      "    [batch 1851]: seen 185100 examples : 46.4 eps, Loss: 3.095, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.140\n",
      "    [batch 1856]: seen 185600 examples : 46.4 eps, Loss: 3.088, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.144\n",
      "    [batch 1861]: seen 186100 examples : 46.4 eps, Loss: 3.325, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.143\n",
      "    [batch 1866]: seen 186600 examples : 46.4 eps, Loss: 3.102, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.130\n",
      "    [batch 1871]: seen 187100 examples : 46.4 eps, Loss: 3.122, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.145\n",
      "    [batch 1876]: seen 187600 examples : 46.4 eps, Loss: 3.185, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.165\n",
      "    [batch 1881]: seen 188100 examples : 46.4 eps, Loss: 3.347, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.166\n",
      "    [batch 1886]: seen 188600 examples : 46.4 eps, Loss: 3.174, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.163\n",
      "    [batch 1891]: seen 189100 examples : 46.4 eps, Loss: 3.226, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.145\n",
      "    [batch 1896]: seen 189600 examples : 46.4 eps, Loss: 2.937, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.140\n",
      "    [batch 1901]: seen 190100 examples : 46.4 eps, Loss: 2.928, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.144\n",
      "    [batch 1906]: seen 190600 examples : 46.4 eps, Loss: 3.271, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.166\n",
      "    [batch 1911]: seen 191100 examples : 46.4 eps, Loss: 3.335, Avg loss: 3.178, Best loss: 3.159, cov loss: 0.170\n",
      "    [batch 1916]: seen 191600 examples : 46.4 eps, Loss: 3.064, Avg loss: 3.173, Best loss: 3.159, cov loss: 0.153\n",
      "    [batch 1921]: seen 192100 examples : 46.4 eps, Loss: 3.098, Avg loss: 3.174, Best loss: 3.159, cov loss: 0.145\n",
      "    [batch 1925]: seen 192500 examples : 46.4 eps, Loss: 3.218, Avg loss: 3.175, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1930]: seen 193000 examples : 46.4 eps, Loss: 3.141, Avg loss: 3.175, Best loss: 3.159, cov loss: 0.156\n",
      "    [batch 1935]: seen 193500 examples : 46.4 eps, Loss: 3.245, Avg loss: 3.176, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 1940]: seen 194000 examples : 46.4 eps, Loss: 3.251, Avg loss: 3.176, Best loss: 3.159, cov loss: 0.153\n",
      "    [batch 1945]: seen 194500 examples : 46.4 eps, Loss: 3.304, Avg loss: 3.177, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 1950]: seen 195000 examples : 46.4 eps, Loss: 3.354, Avg loss: 3.179, Best loss: 3.159, cov loss: 0.169\n",
      "    [batch 1955]: seen 195500 examples : 46.4 eps, Loss: 3.246, Avg loss: 3.180, Best loss: 3.159, cov loss: 0.164\n",
      "    [batch 1960]: seen 196000 examples : 46.4 eps, Loss: 3.326, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.136\n",
      "    [batch 1965]: seen 196500 examples : 46.4 eps, Loss: 3.248, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.165\n",
      "    [batch 1970]: seen 197000 examples : 46.4 eps, Loss: 3.331, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.158\n",
      "    [batch 1975]: seen 197500 examples : 46.4 eps, Loss: 3.090, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 1980]: seen 198000 examples : 46.4 eps, Loss: 3.292, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.152\n",
      "    [batch 1985]: seen 198500 examples : 46.4 eps, Loss: 3.253, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 1990]: seen 199000 examples : 46.4 eps, Loss: 3.255, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.160\n",
      "    [batch 1995]: seen 199500 examples : 46.4 eps, Loss: 3.086, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 2000]: seen 200000 examples : 46.4 eps, Loss: 3.293, Avg loss: 3.179, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 2005]: seen 200500 examples : 46.4 eps, Loss: 3.139, Avg loss: 3.178, Best loss: 3.159, cov loss: 0.140\n",
      "    [batch 2010]: seen 201000 examples : 46.4 eps, Loss: 3.065, Avg loss: 3.180, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 2015]: seen 201500 examples : 46.4 eps, Loss: 3.253, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.162\n",
      "    [batch 2020]: seen 202000 examples : 46.4 eps, Loss: 3.174, Avg loss: 3.179, Best loss: 3.159, cov loss: 0.165\n",
      "    [batch 2025]: seen 202500 examples : 46.4 eps, Loss: 3.347, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 2030]: seen 203000 examples : 46.4 eps, Loss: 3.404, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.157\n",
      "    [batch 2035]: seen 203500 examples : 46.4 eps, Loss: 3.146, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.146\n",
      "    [batch 2040]: seen 204000 examples : 46.4 eps, Loss: 3.253, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.153\n",
      "    [batch 2045]: seen 204500 examples : 46.4 eps, Loss: 3.092, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2050]: seen 205000 examples : 46.4 eps, Loss: 3.001, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.145\n",
      "    [batch 2055]: seen 205500 examples : 46.4 eps, Loss: 3.283, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.163\n",
      "    [batch 2060]: seen 206000 examples : 46.4 eps, Loss: 3.133, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.158\n",
      "    [batch 2065]: seen 206500 examples : 46.4 eps, Loss: 3.219, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 2070]: seen 207000 examples : 46.4 eps, Loss: 3.306, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.165\n",
      "    [batch 2075]: seen 207500 examples : 46.4 eps, Loss: 3.163, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.148\n",
      "    [batch 2080]: seen 208000 examples : 46.4 eps, Loss: 3.054, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.157\n",
      "    [batch 2085]: seen 208500 examples : 46.4 eps, Loss: 3.296, Avg loss: 3.194, Best loss: 3.159, cov loss: 0.183\n",
      "    [batch 2090]: seen 209000 examples : 46.4 eps, Loss: 3.198, Avg loss: 3.194, Best loss: 3.159, cov loss: 0.137\n",
      "    [batch 2095]: seen 209500 examples : 46.4 eps, Loss: 3.220, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.153\n",
      "    [batch 2100]: seen 210000 examples : 46.4 eps, Loss: 3.334, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.163\n",
      "    [batch 2104]: seen 210400 examples : 46.4 eps, Loss: 3.317, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.177\n",
      "    [batch 2109]: seen 210900 examples : 46.4 eps, Loss: 3.061, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.144\n",
      "    [batch 2114]: seen 211400 examples : 46.4 eps, Loss: 3.245, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 2119]: seen 211900 examples : 46.4 eps, Loss: 3.340, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.163\n",
      "    [batch 2124]: seen 212400 examples : 46.4 eps, Loss: 3.132, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.153\n",
      "    [batch 2129]: seen 212900 examples : 46.4 eps, Loss: 3.167, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.147\n",
      "    [batch 2134]: seen 213400 examples : 46.4 eps, Loss: 3.207, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.148\n",
      "    [batch 2139]: seen 213900 examples : 46.4 eps, Loss: 3.121, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 2144]: seen 214400 examples : 46.4 eps, Loss: 3.102, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.158\n",
      "    [batch 2149]: seen 214900 examples : 46.4 eps, Loss: 3.405, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.154\n",
      "    [batch 2154]: seen 215400 examples : 46.4 eps, Loss: 3.225, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.144\n",
      "    [batch 2159]: seen 215900 examples : 46.4 eps, Loss: 3.315, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 2164]: seen 216400 examples : 46.4 eps, Loss: 3.051, Avg loss: 3.194, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 2169]: seen 216900 examples : 46.4 eps, Loss: 2.947, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.148\n",
      "    [batch 2174]: seen 217400 examples : 46.4 eps, Loss: 3.266, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 2179]: seen 217900 examples : 46.4 eps, Loss: 3.266, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 2184]: seen 218400 examples : 46.4 eps, Loss: 3.000, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.145\n",
      "    [batch 2189]: seen 218900 examples : 46.4 eps, Loss: 3.197, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.144\n",
      "    [batch 2194]: seen 219400 examples : 46.4 eps, Loss: 3.200, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 2198]: seen 219800 examples : 46.4 eps, Loss: 3.398, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.147\n",
      "    [batch 2203]: seen 220300 examples : 46.4 eps, Loss: 3.469, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.172\n",
      "    [batch 2208]: seen 220800 examples : 46.4 eps, Loss: 3.060, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.145\n",
      "    [batch 2213]: seen 221300 examples : 46.4 eps, Loss: 3.340, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.147\n",
      "    [batch 2218]: seen 221800 examples : 46.4 eps, Loss: 3.132, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.152\n",
      "    [batch 2223]: seen 222300 examples : 46.4 eps, Loss: 3.102, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 2228]: seen 222800 examples : 46.4 eps, Loss: 3.191, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.163\n",
      "    [batch 2233]: seen 223300 examples : 46.4 eps, Loss: 3.152, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 2238]: seen 223800 examples : 46.4 eps, Loss: 3.172, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.152\n",
      "    [batch 2243]: seen 224300 examples : 46.4 eps, Loss: 3.189, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.157\n",
      "    [batch 2248]: seen 224800 examples : 46.4 eps, Loss: 2.983, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.153\n",
      "    [batch 2253]: seen 225300 examples : 46.4 eps, Loss: 3.120, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.163\n",
      "    [batch 2258]: seen 225800 examples : 46.4 eps, Loss: 3.248, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.167\n",
      "    [batch 2263]: seen 226300 examples : 46.4 eps, Loss: 3.092, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 2268]: seen 226800 examples : 46.4 eps, Loss: 3.178, Avg loss: 3.180, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 2273]: seen 227300 examples : 46.4 eps, Loss: 3.117, Avg loss: 3.177, Best loss: 3.159, cov loss: 0.139\n",
      "    [batch 2278]: seen 227800 examples : 46.4 eps, Loss: 3.132, Avg loss: 3.175, Best loss: 3.159, cov loss: 0.142\n",
      "    [batch 2282]: seen 228200 examples : 46.4 eps, Loss: 3.030, Avg loss: 3.175, Best loss: 3.159, cov loss: 0.138\n",
      "    [batch 2287]: seen 228700 examples : 46.4 eps, Loss: 3.277, Avg loss: 3.172, Best loss: 3.159, cov loss: 0.157\n",
      "    [batch 2292]: seen 229200 examples : 46.4 eps, Loss: 3.208, Avg loss: 3.170, Best loss: 3.159, cov loss: 0.155\n",
      "    [batch 2297]: seen 229700 examples : 46.4 eps, Loss: 3.229, Avg loss: 3.172, Best loss: 3.159, cov loss: 0.146\n",
      "    [batch 2302]: seen 230200 examples : 46.4 eps, Loss: 3.069, Avg loss: 3.173, Best loss: 3.159, cov loss: 0.138\n",
      "    [batch 2307]: seen 230700 examples : 46.4 eps, Loss: 3.237, Avg loss: 3.177, Best loss: 3.159, cov loss: 0.162\n",
      "    [batch 2312]: seen 231200 examples : 46.4 eps, Loss: 3.217, Avg loss: 3.176, Best loss: 3.159, cov loss: 0.146\n",
      "    [batch 2317]: seen 231700 examples : 46.4 eps, Loss: 3.259, Avg loss: 3.178, Best loss: 3.159, cov loss: 0.163\n",
      "    [batch 2322]: seen 232200 examples : 46.4 eps, Loss: 3.300, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.158\n",
      "    [batch 2327]: seen 232700 examples : 46.4 eps, Loss: 3.157, Avg loss: 3.180, Best loss: 3.159, cov loss: 0.161\n",
      "    [batch 2332]: seen 233200 examples : 46.4 eps, Loss: 3.049, Avg loss: 3.176, Best loss: 3.159, cov loss: 0.148\n",
      "    [batch 2337]: seen 233700 examples : 46.4 eps, Loss: 3.259, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.159\n",
      "    [batch 2342]: seen 234200 examples : 46.4 eps, Loss: 3.019, Avg loss: 3.177, Best loss: 3.159, cov loss: 0.142\n",
      "    [batch 2347]: seen 234700 examples : 46.4 eps, Loss: 3.139, Avg loss: 3.175, Best loss: 3.159, cov loss: 0.147\n",
      "    [batch 2352]: seen 235200 examples : 46.4 eps, Loss: 3.103, Avg loss: 3.173, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 2357]: seen 235700 examples : 46.4 eps, Loss: 3.119, Avg loss: 3.175, Best loss: 3.159, cov loss: 0.152\n",
      "    [batch 2362]: seen 236200 examples : 46.4 eps, Loss: 3.098, Avg loss: 3.172, Best loss: 3.159, cov loss: 0.146\n",
      "    [batch 2367]: seen 236700 examples : 46.4 eps, Loss: 3.209, Avg loss: 3.168, Best loss: 3.159, cov loss: 0.158\n",
      "    [batch 2371]: seen 237100 examples : 46.4 eps, Loss: 3.243, Avg loss: 3.169, Best loss: 3.159, cov loss: 0.163\n",
      "    [batch 2376]: seen 237600 examples : 46.4 eps, Loss: 3.274, Avg loss: 3.175, Best loss: 3.159, cov loss: 0.155\n",
      "    [batch 2381]: seen 238100 examples : 46.4 eps, Loss: 3.138, Avg loss: 3.178, Best loss: 3.159, cov loss: 0.155\n",
      "    [batch 2386]: seen 238600 examples : 46.4 eps, Loss: 3.026, Avg loss: 3.176, Best loss: 3.159, cov loss: 0.168\n",
      "    [batch 2391]: seen 239100 examples : 46.4 eps, Loss: 3.141, Avg loss: 3.174, Best loss: 3.159, cov loss: 0.135\n",
      "    [batch 2396]: seen 239600 examples : 46.4 eps, Loss: 3.122, Avg loss: 3.172, Best loss: 3.159, cov loss: 0.142\n",
      "    [batch 2401]: seen 240100 examples : 46.4 eps, Loss: 3.144, Avg loss: 3.173, Best loss: 3.159, cov loss: 0.137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2406]: seen 240600 examples : 46.4 eps, Loss: 3.260, Avg loss: 3.174, Best loss: 3.159, cov loss: 0.163\n",
      "    [batch 2411]: seen 241100 examples : 46.4 eps, Loss: 3.069, Avg loss: 3.176, Best loss: 3.159, cov loss: 0.145\n",
      "    [batch 2416]: seen 241600 examples : 46.4 eps, Loss: 3.185, Avg loss: 3.179, Best loss: 3.159, cov loss: 0.168\n",
      "    [batch 2421]: seen 242100 examples : 46.4 eps, Loss: 3.283, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.146\n",
      "    [batch 2426]: seen 242600 examples : 46.4 eps, Loss: 3.104, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.167\n",
      "    [batch 2431]: seen 243100 examples : 46.4 eps, Loss: 3.123, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.166\n",
      "    [batch 2436]: seen 243600 examples : 46.4 eps, Loss: 3.073, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.132\n",
      "    [batch 2441]: seen 244100 examples : 46.4 eps, Loss: 3.218, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.156\n",
      "    [batch 2446]: seen 244600 examples : 46.4 eps, Loss: 3.184, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.162\n",
      "    [batch 2451]: seen 245100 examples : 46.4 eps, Loss: 3.161, Avg loss: 3.180, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 2456]: seen 245600 examples : 46.4 eps, Loss: 3.254, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 2460]: seen 246000 examples : 46.4 eps, Loss: 3.207, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.156\n",
      "    [batch 2465]: seen 246500 examples : 46.4 eps, Loss: 3.204, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 2470]: seen 247000 examples : 46.4 eps, Loss: 3.321, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.147\n",
      "    [batch 2475]: seen 247500 examples : 46.4 eps, Loss: 3.250, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.156\n",
      "    [batch 2480]: seen 248000 examples : 46.4 eps, Loss: 3.096, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.137\n",
      "    [batch 2485]: seen 248500 examples : 46.4 eps, Loss: 3.258, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.170\n",
      "    [batch 2490]: seen 249000 examples : 46.4 eps, Loss: 3.212, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.143\n",
      "    [batch 2495]: seen 249500 examples : 46.4 eps, Loss: 3.367, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.166\n",
      "    [batch 2500]: seen 250000 examples : 46.4 eps, Loss: 3.176, Avg loss: 3.194, Best loss: 3.159, cov loss: 0.153\n",
      "    [batch 2505]: seen 250500 examples : 46.4 eps, Loss: 3.264, Avg loss: 3.196, Best loss: 3.159, cov loss: 0.165\n",
      "    [batch 2510]: seen 251000 examples : 46.4 eps, Loss: 3.072, Avg loss: 3.194, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 2515]: seen 251500 examples : 46.4 eps, Loss: 3.252, Avg loss: 3.194, Best loss: 3.159, cov loss: 0.162\n",
      "    [batch 2520]: seen 252000 examples : 46.4 eps, Loss: 3.190, Avg loss: 3.196, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 2525]: seen 252500 examples : 46.4 eps, Loss: 2.994, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 2530]: seen 253000 examples : 46.4 eps, Loss: 3.000, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.134\n",
      "    [batch 2535]: seen 253500 examples : 46.4 eps, Loss: 3.112, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.166\n",
      "    [batch 2540]: seen 254000 examples : 46.4 eps, Loss: 3.268, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.148\n",
      "    [batch 2545]: seen 254500 examples : 46.4 eps, Loss: 3.228, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 2550]: seen 255000 examples : 46.4 eps, Loss: 3.232, Avg loss: 3.196, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 2555]: seen 255500 examples : 46.4 eps, Loss: 3.129, Avg loss: 3.196, Best loss: 3.159, cov loss: 0.152\n",
      "    [batch 2560]: seen 256000 examples : 46.4 eps, Loss: 3.154, Avg loss: 3.196, Best loss: 3.159, cov loss: 0.131\n",
      "    [batch 2565]: seen 256500 examples : 46.4 eps, Loss: 3.335, Avg loss: 3.195, Best loss: 3.159, cov loss: 0.174\n",
      "    [batch 2570]: seen 257000 examples : 46.4 eps, Loss: 3.232, Avg loss: 3.201, Best loss: 3.159, cov loss: 0.146\n",
      "    [batch 2575]: seen 257500 examples : 46.4 eps, Loss: 3.130, Avg loss: 3.201, Best loss: 3.159, cov loss: 0.139\n",
      "    [batch 2580]: seen 258000 examples : 46.4 eps, Loss: 3.111, Avg loss: 3.196, Best loss: 3.159, cov loss: 0.143\n",
      "    [batch 2585]: seen 258500 examples : 46.4 eps, Loss: 2.909, Avg loss: 3.195, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 2590]: seen 259000 examples : 46.4 eps, Loss: 3.215, Avg loss: 3.194, Best loss: 3.159, cov loss: 0.154\n",
      "    [batch 2595]: seen 259500 examples : 46.4 eps, Loss: 3.284, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.148\n",
      "    [batch 2600]: seen 260000 examples : 46.4 eps, Loss: 3.392, Avg loss: 3.193, Best loss: 3.159, cov loss: 0.144\n",
      "    [batch 2605]: seen 260500 examples : 46.4 eps, Loss: 3.128, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.164\n",
      "    [batch 2610]: seen 261000 examples : 46.4 eps, Loss: 3.176, Avg loss: 3.192, Best loss: 3.159, cov loss: 0.141\n",
      "    [batch 2615]: seen 261500 examples : 46.4 eps, Loss: 3.247, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 2620]: seen 262000 examples : 46.4 eps, Loss: 3.103, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.146\n",
      "    [batch 2625]: seen 262500 examples : 46.4 eps, Loss: 3.135, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 2630]: seen 263000 examples : 46.4 eps, Loss: 3.219, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.170\n",
      "    [batch 2635]: seen 263500 examples : 46.4 eps, Loss: 3.270, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.157\n",
      "    [batch 2640]: seen 264000 examples : 46.4 eps, Loss: 3.178, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.144\n",
      "    [batch 2645]: seen 264500 examples : 46.4 eps, Loss: 3.307, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.167\n",
      "    [batch 2650]: seen 265000 examples : 46.4 eps, Loss: 3.095, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.144\n",
      "    [batch 2655]: seen 265500 examples : 46.4 eps, Loss: 2.984, Avg loss: 3.179, Best loss: 3.159, cov loss: 0.127\n",
      "    [batch 2660]: seen 266000 examples : 46.4 eps, Loss: 3.186, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.164\n",
      "    [batch 2665]: seen 266500 examples : 46.4 eps, Loss: 3.171, Avg loss: 3.180, Best loss: 3.159, cov loss: 0.153\n",
      "    [batch 2670]: seen 267000 examples : 46.4 eps, Loss: 3.142, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.168\n",
      "    [batch 2675]: seen 267500 examples : 46.4 eps, Loss: 3.101, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.140\n",
      "    [batch 2680]: seen 268000 examples : 46.4 eps, Loss: 3.101, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.172\n",
      "    [batch 2685]: seen 268500 examples : 46.4 eps, Loss: 3.268, Avg loss: 3.182, Best loss: 3.159, cov loss: 0.176\n",
      "    [batch 2690]: seen 269000 examples : 46.4 eps, Loss: 3.243, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.141\n",
      "    [batch 2695]: seen 269500 examples : 46.4 eps, Loss: 3.095, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.146\n",
      "    [batch 2700]: seen 270000 examples : 46.4 eps, Loss: 3.364, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.157\n",
      "    [batch 2705]: seen 270500 examples : 46.4 eps, Loss: 3.097, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.141\n",
      "    [batch 2710]: seen 271000 examples : 46.4 eps, Loss: 3.048, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.149\n",
      "    [batch 2715]: seen 271500 examples : 46.4 eps, Loss: 3.199, Avg loss: 3.181, Best loss: 3.159, cov loss: 0.154\n",
      "    [batch 2720]: seen 272000 examples : 46.4 eps, Loss: 3.299, Avg loss: 3.178, Best loss: 3.159, cov loss: 0.169\n",
      "    [batch 2725]: seen 272500 examples : 46.4 eps, Loss: 3.113, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.148\n",
      "    [batch 2730]: seen 273000 examples : 46.4 eps, Loss: 3.149, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 2735]: seen 273500 examples : 46.4 eps, Loss: 3.027, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.144\n",
      "    [batch 2740]: seen 274000 examples : 46.4 eps, Loss: 2.986, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.155\n",
      "    [batch 2745]: seen 274500 examples : 46.4 eps, Loss: 3.356, Avg loss: 3.189, Best loss: 3.159, cov loss: 0.150\n",
      "    [batch 2750]: seen 275000 examples : 46.4 eps, Loss: 3.293, Avg loss: 3.191, Best loss: 3.159, cov loss: 0.157\n",
      "    [batch 2755]: seen 275500 examples : 46.4 eps, Loss: 3.295, Avg loss: 3.194, Best loss: 3.159, cov loss: 0.156\n",
      "    [batch 2760]: seen 276000 examples : 46.4 eps, Loss: 3.347, Avg loss: 3.190, Best loss: 3.159, cov loss: 0.158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [batch 2765]: seen 276500 examples : 46.4 eps, Loss: 3.265, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.152\n",
      "    [batch 2770]: seen 277000 examples : 46.4 eps, Loss: 3.237, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.155\n",
      "    [batch 2775]: seen 277500 examples : 46.4 eps, Loss: 3.214, Avg loss: 3.185, Best loss: 3.159, cov loss: 0.160\n",
      "    [batch 2780]: seen 278000 examples : 46.4 eps, Loss: 3.227, Avg loss: 3.184, Best loss: 3.159, cov loss: 0.138\n",
      "    [batch 2785]: seen 278500 examples : 46.4 eps, Loss: 3.299, Avg loss: 3.186, Best loss: 3.159, cov loss: 0.157\n",
      "    [batch 2790]: seen 279000 examples : 46.4 eps, Loss: 3.247, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.156\n",
      "    [batch 2795]: seen 279500 examples : 46.4 eps, Loss: 3.070, Avg loss: 3.188, Best loss: 3.159, cov loss: 0.151\n",
      "    [batch 2800]: seen 280000 examples : 46.4 eps, Loss: 3.142, Avg loss: 3.187, Best loss: 3.159, cov loss: 0.146\n",
      "    [batch 2805]: seen 280500 examples : 46.4 eps, Loss: 2.997, Avg loss: 3.183, Best loss: 3.159, cov loss: 0.139\n",
      "    [END] Training complete: Total examples : 280700; Total time: 1:40:49\n",
      "[EPOCH 29] Complete. Avg Loss: 3.1801447799711737; Best Loss: 3.158703739970086\n",
      "[EPOCH 30] Starting training..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-430df9b70ad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurr_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-17e3ed376236>\u001b[0m in \u001b[0;36mtrain_continue\u001b[0;34m(hps, epochs, train_step, curr_best, best_loss, avg_loss, restore, epoch_start)\u001b[0m\n\u001b[1;32m     36\u001b[0m                                                             \u001b[0mhps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                                             \u001b[0mbest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                                                             avg_loss)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcurr_best\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266/final_0/W266_Final/model_4/training_util.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(lm, session, batches, summary_writer, train_dir, train_step, saver, hps, best_loss, avg_loss, save_all)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunTrainStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/W266/final_0/W266_Final/model_4/training_util.py\u001b[0m in \u001b[0;36mrunTrainStep\u001b[0;34m(lm, session, batch)\u001b[0m\n\u001b[1;32m     87\u001b[0m     }\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_loss = 0\n",
    "best_loss = None\n",
    "curr_best = best_loss\n",
    "train_step = 46184\n",
    "epochs = 8\n",
    "restore = True\n",
    "epoch_start = 23\n",
    "\n",
    "train_continue(hps,epochs,train_step,curr_best,best_loss,avg_loss,restore,epoch_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
